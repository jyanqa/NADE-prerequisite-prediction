# -*- coding: utf-8 -*-
"""rag-OpenSourceLLMs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WZWqOUk88FU50cZeyQxAfJ8hVFHtj2sx

# Build RAG pipeline using Open Source Large Languages

In the notebook we will build a Chat with Website use cases using Zephyr 7B model

## Installation
"""

!pip install langchain faiss-cpu sentence-transformers chromadb
!pip install -q faiss-cpuw

"""## Import RAG components required to build pipeline"""

from langchain.llms import HuggingFaceHub
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain.chains import RetrievalQA, LLMChain

"""## Setup HuggingFace Access Token

- Log in to [HuggingFace.co](https://huggingface.co/)
- Click on your profile icon at the top-right corner, then choose [“Settings.”](https://huggingface.co/settings/)
- In the left sidebar, navigate to [“Access Token”](https://huggingface.co/settings/tokens)
- Generate a new access token, assigning it the “write” role.

"""

import os
from getpass import getpass

HF_TOKEN = getpass("HF Token:")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HF_TOKEN

"""## External data/document - ETL"""

# import nest_asyncio

# nest_asyncio.apply()

# WEBSITE_URL = "https://tarunjain.netlify.app/"

# loader = WebBaseLoader(WEBSITE_URL)
# loader.requests_per_second = 1
# docs = loader.aload()

from langchain.document_loaders import HuggingFaceDatasetLoader
!pip install -q datasets

# Specify the dataset name and the column containing the content
dataset_name = "databricks/databricks-dolly-15k"
page_content_column = "context"  # or any other column you're interested in

# Create a loader instance
loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)

# Load the data
data = loader.load()

# Display the first 15 entries
data[:2]

"""## Text Splitting - Chunking"""

# Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.
# It splits text into chunks of 1000 characters each with a 150-character overlap
#(used in case the document is long, it should splitted into different smaller chunk)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=15)

# 'data' holds the text you want to split, split the text into documents using the text splitter.
docs = text_splitter.split_documents(data)
docs = docs[0:100] #taking 100 docs from the dataset
docs[0]

# chunks[1]

"""## Embeddings"""

# embeddings = HuggingFaceInferenceAPIEmbeddings(
#     api_key=HF_TOKEN, model_name="BAAI/bge-base-en-v1.5"
# )

from langchain.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(
    # model_name=modelPath,     # Provide the pre-trained model's path
    # model_kwargs=model_kwargs, # Pass the model configuration options
    # encode_kwargs=encode_kwargs # Pass the encoding options
)

"""## Vector Store - FAISS or ChromaDB"""

!pip install -q faiss-cpu
from langchain.vectorstores import FAISS

# vectorstore = Chroma.from_documents(chunks, embeddings)
vectorstore = FAISS.from_documents(docs, embeddings)

vectorstore

query = "Where does Tarun work?"
search = vectorstore.similarity_search(query)

search[0].page_content

"""## Retriever"""

retriever = vectorstore.as_retriever(
    search_type="mmr", #similarity
    search_kwargs={'k': 4}
)

retriever.get_relevant_documents(query)

"""## Large Language Model - Open Source"""

llm = HuggingFaceHub(
    # repo_id="gpt2",
    repo_id="google-t5/t5-base",
    model_kwargs={"temperature": 0.5, "max_length": 64,"max_new_tokens":512}
)

"""## Prompt Template and User Input (Augment - Step 2)"""

query = "Name the projects Tarun has worked on?"

prompt = f"""
 <|system|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
 <|user|>
 {query}
 </s>
 <|assistant|>
"""

"""## RAG RetrievalQA chain"""

qa = RetrievalQA.from_chain_type(llm=llm, chain_type="refine", retriever=retriever)

response = qa.run(prompt)

question = "Who is Thomas Jefferson?"
result = qa.run({"query": question})

result

"""## Chain"""

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
 <|system|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
 <|user|>
 {query}
 </s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever,  "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

response = rag_chain.invoke("Name the projects Tarun has worked on?")

print(response)