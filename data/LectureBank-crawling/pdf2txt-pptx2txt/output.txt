Logistic  Regression,    Nonlinear  Features,    Regularization
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  9February  15,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Logistic  Regression  Readings:Murphy  8.1-­‐8.3,  8.6Bishop  4.3.2,  4.3.4HTF  4.1,  4.4Mitchell  –“Generative  …Logistic  Regression”  (Mitchell,  2016)“Maximum  …Gradient  Training”(Elkan,  2014)
Reminders•Homework3:  Linear  /  LogisticRegression–Release:  Mon,  Feb.  13–Due:  Wed,  Feb.  22  at  11:59pm
2Note  the  change  in  time.
Outline•Motivation:–Choosing  the  right  classifier–Example:  Image  Classification•Logistic  Regression–Background:  Hyperplanes–Data,  Model,  Learning,  Prediction–Log-­‐odds–Bernoulli  interpretation–Maximum  Conditional  Likelihood  Estimation•Gradient  descent  for  Logistic  Regression–Stochastic  Gradient  Descent  (SGD)–Computing  the  gradient–Details  (learning  rate,  finite  differences)•Nonlinear  Features3
MOTIVATION:  LOGISTIC  REGRESSION4
ClassifiersWhich  classification  method  should  we  use?
51.The  one  that  gives  the  best  predictions…–on  the  training  data–on  the  (unseen)  test  data–on  the  (held-­‐out)  validation  data2.The  one  that  is  computationally  efficient…–during  training–during  classification3.The  most  interpretable  one…–in  terms  of  its  parameters–as  a  model4.The  one  that  is  easiest  to  implement…–for  learning–for  classification
ClassifiersWhich  classification  method  should  we  use?
6Naïve  Bayes  defined  a  generative  model  p(x, y)of  the  features  xand  the  class  y.  Why  should  we  define  a  model  of  p(x, y) at  all?  Why  not  directly  model  p(y | x)?
Example:  Image  Classification•ImageNetLSVRC-­‐2010  contest:  –Dataset:  1.2  million  labeled  images,  1000  classes–Task:  Given  a  new  image,  label  it  with  the  correct  class–Multiclassclassification  problem•Examples  from  http://image-­‐net.org/
7
8

9

10

Example:  Image  Classification
11Figure 2:An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilitiesbetween the two GPUs. One GPU runs the layer-parts at the top of the ﬁgure while the other runs the layer-partsat the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, andthe number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264–4096–4096–1000.neurons in a kernel map). The second convolutional layer takes as input the (response-normalizedand pooled) output of the ﬁrst convolutional layer and ﬁlters it with 256 kernels of size5⇥5⇥48.The third, fourth, and ﬁfth convolutional layers are connected to one another without any interveningpooling or normalization layers. The third convolutional layer has 384 kernels of size3⇥3⇥256connected to the (normalized, pooled) outputs of the second convolutional layer. The fourthconvolutional layer has 384 kernels of size3⇥3⇥192, and the ﬁfth convolutional layer has 256kernels of size3⇥3⇥192. The fully-connected layers have 4096 neurons each.4 Reducing OverﬁttingOur neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRCmake each training example impose 10 bits of constraint on the mapping from image to label, thisturns out to be insufﬁcient to learn so many parameters without considerable overﬁtting. Below, wedescribe the two primary ways in which we combat overﬁtting.4.1 Data AugmentationThe easiest and most common method to reduce overﬁtting on image data is to artiﬁcially enlargethe dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct formsof data augmentation, both of which allow transformed images to be produced from the originalimages with very little computation, so the transformed images do not need to be stored on disk.In our implementation, the transformed images are generated in Python code on the CPU while theGPU is training on the previous batch of images. So these data augmentation schemes are, in effect,computationally free.The ﬁrst form of data augmentation consists of generating image translations and horizontal reﬂec-tions. We do this by extracting random224⇥224patches (and their horizontal reﬂections) from the256⇥256images and training our network on these extracted patches4. This increases the size of ourtraining set by a factor of 2048, though the resulting training examples are, of course, highly inter-dependent. Without this scheme, our network suffers from substantial overﬁtting, which would haveforced us to use much smaller networks. At test time, the network makes a prediction by extractingﬁve224⇥224patches (the four corner patches and the center patch) as well as their horizontalreﬂections (hence ten patches in all), and averaging the predictions made by the network’s softmaxlayer on the ten patches.The second form of data augmentation consists of altering the intensities of the RGB channels intraining images. Speciﬁcally, we perform PCA on the set of RGB pixel values throughout theImageNet training set. To each training image, we add multiples of the found principal components,4This is the reason why the input images in Figure 2 are224⇥224⇥3-dimensional.5CNN  for  Image  Classification(Krizhevsky,  Sutskever&  Hinton,  2011)17.5%  error  on  ImageNetLSVRC-­‐2010  contestInput  image  (pixels)•Five  convolutional  layers  (w/max-­‐pooling)•Three  fully  connected  layers1000-­‐way  softmax
Example:  Image  Classification
12Figure 2:An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilitiesbetween the two GPUs. One GPU runs the layer-parts at the top of the ﬁgure while the other runs the layer-partsat the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, andthe number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264–4096–4096–1000.neurons in a kernel map). The second convolutional layer takes as input the (response-normalizedand pooled) output of the ﬁrst convolutional layer and ﬁlters it with 256 kernels of size5⇥5⇥48.The third, fourth, and ﬁfth convolutional layers are connected to one another without any interveningpooling or normalization layers. The third convolutional layer has 384 kernels of size3⇥3⇥256connected to the (normalized, pooled) outputs of the second convolutional layer. The fourthconvolutional layer has 384 kernels of size3⇥3⇥192, and the ﬁfth convolutional layer has 256kernels of size3⇥3⇥192. The fully-connected layers have 4096 neurons each.4 Reducing OverﬁttingOur neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRCmake each training example impose 10 bits of constraint on the mapping from image to label, thisturns out to be insufﬁcient to learn so many parameters without considerable overﬁtting. Below, wedescribe the two primary ways in which we combat overﬁtting.4.1 Data AugmentationThe easiest and most common method to reduce overﬁtting on image data is to artiﬁcially enlargethe dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct formsof data augmentation, both of which allow transformed images to be produced from the originalimages with very little computation, so the transformed images do not need to be stored on disk.In our implementation, the transformed images are generated in Python code on the CPU while theGPU is training on the previous batch of images. So these data augmentation schemes are, in effect,computationally free.The ﬁrst form of data augmentation consists of generating image translations and horizontal reﬂec-tions. We do this by extracting random224⇥224patches (and their horizontal reﬂections) from the256⇥256images and training our network on these extracted patches4. This increases the size of ourtraining set by a factor of 2048, though the resulting training examples are, of course, highly inter-dependent. Without this scheme, our network suffers from substantial overﬁtting, which would haveforced us to use much smaller networks. At test time, the network makes a prediction by extractingﬁve224⇥224patches (the four corner patches and the center patch) as well as their horizontalreﬂections (hence ten patches in all), and averaging the predictions made by the network’s softmaxlayer on the ten patches.The second form of data augmentation consists of altering the intensities of the RGB channels intraining images. Speciﬁcally, we perform PCA on the set of RGB pixel values throughout theImageNet training set. To each training image, we add multiples of the found principal components,4This is the reason why the input images in Figure 2 are224⇥224⇥3-dimensional.5CNN  for  Image  Classification(Krizhevsky,  Sutskever&  Hinton,  2011)17.5%  error  on  ImageNetLSVRC-­‐2010  contestInput  image  (pixels)•Five  convolutional  layers  (w/max-­‐pooling)•Three  fully  connected  layers1000-­‐way  softmax
This  “softmax”  layer  is  Logistic  Regression!
The  rest  is  justsome  fancy  feature  extraction  (discussed  later  in  the  course)

LOGISTIC  REGRESSION
13
Logistic  Regression
14
We  are  back  to  classification.Despite  the  name  logistic  regression.Data:  Inputs  are  continuous  vectors  of  length  K.  Outputs  are  discrete.

Why  don’t  we  drop  the  generative  model  and  try  to  learn  this  hyperplanedirectly?
Background:  Hyperplanes
Background:  HyperplanesH={x:wTx=b}Hyperplane(Definition  1):  
w
Half-­‐spaces:  H+={x:wTx>0 andx1=1}H ={x:wTx<0 andx1=1}Hyperplane(Definition  2):  H={x:wTx=0andx1=1}x0x0x0
Why  don’t  we  drop  the  generative  model  and  try  to  learn  this  hyperplanedirectly?
Background:  Hyperplanes
Directly  modeling  the  hyperplanewould  use  a  decision  function:for:h(t)=sign( Tt)y { 1,+1}
Using  gradient  ascent  for  linear  classifiersKey  idea  behind  today’s  lecture:1.Define  a  linear  classifier  (logistic  regression)2.Define  an  objective  function  (likelihood)3.Optimize  it  with  gradient  descent  to  learn  parameters4.Predict  the  class  with  highest  probability  under  the  model
18
Using  gradient  ascent  for  linear  classifiers
19Use  a  differentiable  function  instead:
logistic(u)≡11+e−up (y=1|t)=11+2tT(  Tt)This  decision  function  isn’t  differentiable:
sign(x)h(t)=sign( Tt)
Using  gradient  ascent  for  linear  classifiers
20Use  a  differentiable  function  instead:
logistic(u)≡11+e−up (y=1|t)=11+2tT(  Tt)This  decision  function  isn’t  differentiable:
sign(x)h(t)=sign( Tt)
Logistic  Regression
21Learning:  finds  the  parameters  that  minimize  some  objective  function.  = argmin J( )Prediction:  Output  is  the  most  probable  class.ˆy=`;Kty {0,1}p (y|t)Model:  Logistic  function  applied  to  dot  product  of  parameters  with  input  vector.p (y=1|t)=11+2tT(  Tt)Data:  Inputs  are  continuous  vectors  of  length  K.  Outputs  are  discrete.

Whiteboard•Decision  boundary•Bernoulli  interpretation
22
Logistic  Regression
23
Logistic  Regression
24
Logistic  Regression
25
LEARNING  LOGISTIC  REGRESSION
26
Maximum  ConditionalLikelihood  Estimation
27Learning:  finds  the  parameters  that  minimize  some  objective  function.We  minimize  the  negativelog  conditional  likelihood:Why?1.We  can’t  maximize  likelihood  (as  in  Naïve  Bayes)  because  we  don’t  have  a  joint  model  p(x,y)2.It  worked  well  for  Linear  Regression  (least  squares  is  MCLE)  = argmin J( )J( )= HQ;N i=1p (y(i)|t(i))
Maximum  ConditionalLikelihood  Estimation
28Learning:  Four  approaches  to  solving  Approach  1:  Gradient  Descent(take  larger  –more  certain  –steps  opposite  the  gradient)Approach  2:  Stochastic  Gradient  Descent  (SGD)(take  many  small  steps  opposite  the  gradient)Approach  3:  Newton’s  Method(use  second  derivatives  to  better  follow  curvature)Approach  4:  Closed  Form???(set  derivatives  equal  to  zero  and  solve  for  parameters)  = argmin J( )
Maximum  ConditionalLikelihood  Estimation
29Learning:  Four  approaches  to  solving  Approach  1:  Gradient  Descent(take  larger  –more  certain  –steps  opposite  the  gradient)Approach  2:  Stochastic  Gradient  Descent  (SGD)(take  many  small  steps  opposite  the  gradient)Approach  3:  Newton’s  Method(use  second  derivatives  to  better  follow  curvature)Approach  4:  Closed  Form???(set  derivatives  equal  to  zero  and  solve  for  parameters)  = argmin J( )
Logistic  Regression  does  not  have  a  closed  form  solution  for  MLE  parameters.
Algorithm 1Gradient Descent1:procedureGD(D, (0))2:   (0)3:whilenot convergeddo4:   +   J( )5:return —Gradient  Descent
30
In  order  to  apply  GD  to  Logistic  Regression  all  we  need  is  the  gradientof  the  objective  function  (i.e.  vector  of  partial  derivatives).    J( )=     dd 1J( )dd 2J( )...dd NJ( )     

Stochastic  Gradient  Descent  (SGD)
31
We  need  a  per-­‐example  objective:We  can  also  apply  SGD  to  solve  the  MCLE  problem  for  Logistic  Regression.LetJ( )= Ni=1J(i)( )whereJ(i)( )= HQ;p (yi|ti).—
GRADIENT  FOR  LOGISTIC  REGRESSION33
Whiteboard•Partial  derivative  for  Logistic  Regression•Gradient  for  Logistic  Regression
34
Details:  Picking  learning  rate•Use  grid-­‐search  in  log-­‐space  over  small  values  on  a  tuning  set:–e.g.,  0.01,  0.001,  …•Sometimes,  decrease  after  each  pass:–e.gfactor  of  1/(1  +  dt),  t=epoch–sometimes  1/t2•Fancier  techniques  I  won’t  talk  about:–Adaptive  gradient:  scale  gradient  differently  for  each  dimension  (Adagrad,  ADAM,  ….)39Slide  courtesy  of  William  Cohen
SGD  for  Logistic  Regression
41We  need  a  per-­‐example  objective:We  can  also  apply  SGD  to  solve  the  MCLE  problem  for  Logistic  Regression.LetJ( )= Ni=1J(i)( )whereJ(i)( )= HQ;p (yi|ti).

Takeaways1.Discriminative  classifiers  directly  model  the  conditional,  p(y|x)2.Logistic  regression  is  a  simple  linear  classifier,  that  retains  a  probabilistic  semantics3.Parameters  in  LR  are  learned  by  iterative  optimization  (e.g.  SGD)
42
NON-­‐LINEAR  FEATURES
43
Nonlinear  FeaturesWhiteboard–Example  functions–Nonlinear  Features  for  Linear  Regression–Nonlinear  Features  for  Logistic  Regression–Nonlinear  Features  for  KNN–Nonlinear  Features  for  Naïve  Bayes
44
Example:  Linear  Regression  Nonlinear  Features
–From  Bishop  Ch  1Slide  courtesy  of  William  CohenPolynomial  basis  vectors  on  a  small  dataset
0thOrder  Polynomial
n=10Slide  courtesy  of  William  Cohen
1stOrder  Polynomial
Slide  courtesy  of  William  Cohen
3rdOrder  Polynomial
Slide  courtesy  of  William  Cohen
9thOrder  Polynomial
Slide  courtesy  of  William  Cohen
Over-­fitting
Root-­Mean-­Square  (RMS)  Error:
Slide  courtesy  of  William  Cohen
Polynomial  Coefficients      
Slide  courtesy  of  William  Cohen
OverfittingDefinition:  The  problem  of  overfittingis  when  the  model  captures  the  noise  in  the  training  data  instead  of  the  underlying  structure  Overfitting  can  occur  in  all  the  models  we’ve  seen  so  far:  –KNN  (e.g.  when  k  is  small)–Naïve  Bayes  (e.g.  without  a  prior)–Linear  Regression  (e.g.  with  basis  function)–Logistic  Regression  (e.g.  with  many  rare  features)55
9thOrder  Polynomial
Slide  courtesy  of  William  Cohen
(Small  #  of  examples)
9thOrder  Polynomial
Slide  courtesy  of  William  Cohen
(Large  #  of  examples)
Gaussian  Naïve  Bayes
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  6February  6,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Naïve  Bayes  Readings:“Generative  and  Discriminative  Classifiers:  Naive  Bayes  and  Logistic  Regression”  (Mitchell,  2016)Murphy  3Bishop  -­‐-­‐HTF  -­‐-­‐Mitchell  6.1-­‐6.10Optimization  Readings:  (next  lecture)Lecture  notes  from  10-­‐600  (see  Piazza  note)“Convex  Optimization”  Boyd  and  Vandenberghe(2009)    [See  Chapter  9.  This  advanced  reading  is  entirely  optional.]
Reminders•Homework2:  NaiveBayes–Release:  Wed,  Feb.  1–Due:  Mon,  Feb.  13  at  5:30pm•Homework3:  Linear  /  LogisticRegression–Release:  Mon,  Feb.  13–Due:  Wed,  Feb.  22  at  5:30pm
2
Naïve  Bayes  Outline•Probabilistic  (Generative)  View  of  Classification–Decision  rule  for  probability  model•Real-­‐world  Dataset–Economist  vs.  Onion  articles–Document  àbag-­‐of-­‐words  àbinary  feature  vector•Naive  Bayes:  Model–Generating  synthetic  "labeled  documents"–Definition  of  model–Naive  Bayes  assumption–Counting  #  of  parameters  with  /  without  NB  assumption•Naïve  Bayes:  Learning  from  Data–Data  likelihood–MLE  for  Naive  Bayes–MAP  for  Naive  Bayes•Visualizing  Gaussian  Naive  Bayes3
This  Lecture
Last  Lecture
Naive  Bayes:  ModelWhiteboard–Generating  synthetic  "labeled  documents"–Definition  of  model–Naive  Bayes  assumption–Counting  #  of  parameters  with  /  without  NB  assumption
4
What’s  wrong  with  the  Naïve  Bayes  Assumption?The  features  might  not  be  independent!!
5
•Example  1:–If  a  document  contains  the  word  “Donald”,  it’s  extremely  likely  to  contain  the  word  “Trump”–These  are  not  independent!•Example  2:–If  the  petal  width  is  very  high,  the  petal  length  is  also  likely  to  be  very  high
Naïve  Bayes:  Learning  from  DataWhiteboard–Data  likelihood–MLE  for  Naive  Bayes–MAP  for  Naive  Bayes
6
VISUALIZING  NAÏVE  BAYES
7Slides  in  this  section  from  William  Cohen  (10-­‐601B,  Spring  2016)

Fisher  Iris  DatasetFisher  (1936)  used  150  measurements  of  flowers  from  3  different  species:  Iris  setosa(0),  Iris  virginica(1),  Iris  versicolor(2)  collected  by  Anderson  (1936)
9Full  dataset:  https://en.wikipedia.org/wiki/Iris_flower_data_setSpeciesSepal  LengthSepal  WidthPetal  LengthPetal  Width04.33.01.10.104.93.61.40.105.33.71.50.214.92.43.31.015.72.84.11.316.33.34.71.616.73.05.01.7
Slide  from  William  Cohen
Slide  from  William  Cohen
Plot  the  difference  of  the  probabilities
Slide  from  William  Cohenz-­‐axis  is  the  difference  of  the  posterior  probabilities:  p(y=1  |  x)  –p(y=0  |  x)
Question:  what  does  the  boundary  between  positive  and  negative  look  like  for  Naïve  Bayes?
Slide  from  William  Cohen  (10-­‐601B,  Spring  2016)
Iris  Data  (2  classes)
14
Iris  Data  (sigmanot  shared)
15
Iris  Data  (sigma=1)
16
Iris  Data  (3  classes)
17
Iris  Data  (sigmanot  shared)
18
Iris  Data  (sigma=1)
19
Naïve  Bayes  has  a  lineardecision  boundary  (if  sigma  is  shared  across  classes)
Slide  from  William  Cohen  (10-­‐601B,  Spring  2016)
Figure  from  William  Cohen  (10-­‐601B,  Spring  2016)
Why  don’t  we  drop  the  generative  model  and  try  to  learn  this  hyperplanedirectly?
Figure  from  William  Cohen  (10-­‐601B,  Spring  2016)
Beyond  the  Scope  of  this  Lecture•MultinomialNaïve  Bayes  can  be  used  for  integerfeatures•Multi-­‐class  Naïve  Bayes  can  be  used  if  your  classification  problem  has  >  2  classes
23
Summary1.Naïve  Bayes  provides  a  framework  for  generative  modeling2.Choose  p(xm|  y)  appropriate  to  the  data(e.g.  Bernoulli  for  binary  features,  Gaussian  for  continuous  features)3.Train  by  MLEor  MAP4.Classify  by  maximizing  the  posterior
24
CSC321 Lecture 21: Policy Gradient
Roger Grosse
Roger Grosse CSC321 Lecture 21: Policy Gradient 1 / 21
Overview
Most of this course was about supervised learning, plus a little
unsupervised learning.
Final 3 lectures: reinforcement learning
Middle ground between supervised and unsupervised learning
An agent acts in an environment and receives a reward signal.
Today: policy gradient (directly do SGD over a stochastic policy
using trial-and-error)
Next lecture: Q-learning (learn a value function predicting returns
from a state)
Final lecture: policies and value functions are way more powerful in
combination
Roger Grosse CSC321 Lecture 21: Policy Gradient 2 / 21
Reinforcement learning
Anagent interacts with an environment (e.g. game of Breakout)
In each time step t,
the agent receives observations (e.g. pixels) which give it information
about the state s t(e.g. positions of the ball and paddle)
the agent picks an action a t(e.g. keystrokes) which aects the state
The agent periodically receives a reward r(st;at), which depends on
the state and action (e.g. points)
The agent wants to learn a policy(atjst)
Distribution over actions depending on the current state and
parameters 
Roger Grosse CSC321 Lecture 21: Policy Gradient 3 / 21
Markov Decision Processes
The environment is represented as a Markov decision process M.
Markov assumption: all relevant information is encapsulated in the
current state; i.e. the policy, reward, and transitions are all
independent of past states given the current state
Components of an MDP:
initial state distribution p(s0)
policy(atjst)
transition distribution p(st+1jst;at)
reward function r(st;at)
Assume a fully observable environment, i.e. stcan be observed directly
Rollout, or trajectory = (s0;a0;s1;a1;:::; sT;aT)
Probability of a rollout
p() =p(s0)(a0js0)p(s1js0;a0)p(sTjsT 1;aT 1)(aTjsT)
Roger Grosse CSC321 Lecture 21: Policy Gradient 4 / 21
Markov Decision Processes
Continuous control in simulation, e.g. teaching an ant to walk
State: positions, angles, and velocities of the joints
Actions: apply forces to the joints
Reward: distance from starting point
Policy: output of an ordinary MLP, using the state as input
More environments: https://gym.openai.com/envs/#mujoco
Roger Grosse CSC321 Lecture 21: Policy Gradient 5 / 21
Markov Decision Processes
Return for a rollout: r() =PT
t=0r(st;at)
Note: we're considering a nite horizon T, or number of time steps;
we'll consider the innite horizon case later.
Goal: maximize the expected return, R=Ep()[r()]
The expectation is over both the environment's dynamics and the
policy, but we only have control over the policy.
The stochastic policy is important, since it makes Ra continuous
function of the policy parameters.
Reward functions are often discontinuous, as are the dynamics
(e.g. collisions)
Roger Grosse CSC321 Lecture 21: Policy Gradient 6 / 21
REINFORCE
REINFORCE is an elegant algorithm for maximizing the expected
return R=Ep()[r()].
Intuition: trial and error
Sample a rollout . If you get a high reward, try to make it more likely.
If you get a low reward, try to make it less likely.
Interestingly, this can be seen as stochastic gradient ascent on R.
Roger Grosse CSC321 Lecture 21: Policy Gradient 7 / 21
REINFORCE
Recall the derivative formula for log:
@
@logp() =@
@p()
p()=)@
@p() =p()@
@logp()
Gradient of the expected return:
@
@Ep()[r()] =@
@X
r()p()
=X
r()@
@p()
=X
r()p()@
@logp()
=Ep()
r()@
@logp()
Compute stochastic estimates of this expectation by sampling rollouts.
Roger Grosse CSC321 Lecture 21: Policy Gradient 8 / 21
REINFORCE
For reference:
@
@Ep()[r()] =Ep()
r()@
@logp()
If you get a large reward, make the rollout more likely. If you get a
small reward, make it less likely.
Unpacking the REINFORCE gradient:
@
@logp() =@
@log"
p(s0)TY
t=0(atjst)TY
t=1p(stjst 1;at 1)#
=@
@logTY
t=0(atjst)
=TX
t=0@
@log(atjst)
Hence, it tries to make allthe actions more likely or less likely,
depending on the reward. I.e., it doesn't do credit assignment.
This is a topic for next lecture.
Roger Grosse CSC321 Lecture 21: Policy Gradient 9 / 21
REINFORCE
Repeat forever:
Sample a rollout = (s0;a0;s1;a1;:::; sT;aT)
r() PT
k=0r(sk;ak)
Fort= 0;:::; T:
 +r()@
@log(akjsk)
Observation: actions should only be reinforced based on future
rewards, since they can't possibly in
uence past rewards.
You can show that this still gives unbiased gradient estimates.
Repeat forever:
Sample a rollout = (s0;a0;s1;a1;:::; sT;aT)
Fort= 0;:::; T:
rt() PT
k=tr(sk;ak)
 +rt()@
@log(akjsk)
Roger Grosse CSC321 Lecture 21: Policy Gradient 10 / 21
Optimizing Discontinuous Objectives
Edge case of RL: handwritten digit classication, but maximizing
accuracy (or minimizing 0{1 loss)
Gradient descent completely fails if the cost function is discontinuous:
Original solution: use a surrogate loss function, e.g.
logistic-cross-entropy
RL formulation: in each episode, the agent is shown an image, guesses
a digit class, and receives a reward of 1 if it's right or 0 if it's wrong
We'd never actually do it this way, but it will give us an interesting
comparison with backprop
Roger Grosse CSC321 Lecture 21: Policy Gradient 11 / 21
Optimizing Discontinuous Objectives
RL formulation
one time step
state x: an image
action a: a digit class
reward r(x;a): 1 if correct, 0 if wrong
policy(ajx): a distribution over categories
Compute using an MLP with softmax outputs { this is a policy network
Roger Grosse CSC321 Lecture 21: Policy Gradient 12 / 21
Optimizing Discontinuous Objectives
Letzkdenote the logits, ykdenote the softmax output, tthe integer
target, and tkthe target one-hot representation.
To apply REINFORCE, we sample a(jx) and apply:
 +r(a;t)@
@log(ajx)
=+r(a;t)@
@logya
=+r(a;t)X
k(ak yk)@
@zk
Compare with the logistic regression SGD update:
 +@
@logyt
 +X
k(tk yk)@
@zk
Roger Grosse CSC321 Lecture 21: Policy Gradient 13 / 21
Reward Baselines
For reference:
 +r(a;t)@
@log(ajx)
Clearly, we can add a constant oset to the reward, and we get an
equivalent optimization problem.
Behavior if r= 0 for wrong answers and r= 1 for correct answers
wrong: do nothing
correct: make the action more likely
Ifr= 10 for wrong answers and r= 11 for correct answers
wrong: make the action more likely
correct: make the action more likely (slightly stronger)
Ifr= 10 for wrong answers and r= 9 for correct answers
wrong: make the action less likely
correct: make the action less likely (slightly weaker)
Roger Grosse CSC321 Lecture 21: Policy Gradient 14 / 21
Reward Baselines
Problem: the REINFORCE update depends on arbitrary constant
factors added to the reward.
Observation: we can subtract a baseline bfrom the reward without
biasing the gradient.
Ep()
(r() b)@
@logp()
=Ep()
r()@
@logp()
 bEp()@
@logp()
=Ep()
r()@
@logp()
 bX
p()@
@logp()
=Ep()
r()@
@logp()
 bX
@
@p()
=Ep()
r()@
@logp()
 0
We'd like to pick a baseline such that good rewards are positive and
bad ones are negative.
E[r()] is a good choice of baseline, but we can't always compute it
easily. There's lots of research on trying to approximate it.
Roger Grosse CSC321 Lecture 21: Policy Gradient 15 / 21
More Tricks
We left out some more tricks that can make policy gradients work a
lot better.
Evaluate each action using only future rewards, since it has no in
uence
on past rewards. It can be shown this gives unbiased gradients.
Natural policy gradient corrects for the geometry of the space of
policies, preventing the policy from changing too quickly.
Rather than use the actual return, evaluate actions based on estimates
of future returns. This is a class of methods known as actor-critic,
which we'll touch upon next lecture.
Trust region policy optimization (TRPO) and proximal policy
optimization (PPO) are modern policy gradient algorithms which are
very eective for continuous control problems.
Roger Grosse CSC321 Lecture 21: Policy Gradient 16 / 21
Discussion
What's so great about backprop and gradient descent?
Backprop does credit assignment { it tells you exactly which
activations and parameters should be adjusted upwards or downwards
to decrease the loss on some training example.
REINFORCE doesn't do credit assignment. If a rollout happens to be
good, all the actions get reinforced, even if some of them were bad.
Reinforcing all the actions as a group leads to random walk behavior.
Roger Grosse CSC321 Lecture 21: Policy Gradient 17 / 21
Discussion
Why policy gradient?
Can handle discontinuous cost functions
Don't need an explicit model of the environment, i.e. rewards and
dynamics are treated as black boxes
Policy gradient is an example of model-free reinforcement learning,
since the agent doesn't try to t a model of the environment
Almost everyone thinks model-based approaches are needed for AI, but
nobody has a clue how to get it to work
Roger Grosse CSC321 Lecture 21: Policy Gradient 18 / 21
Evolution Strategies (optional)
REINFORCE can handle discontinuous dynamics and reward
functions, but it requires a dierentiable network since it computes
@
@log(atjst)
Evolution strategies (ES) take the policy gradient idea a step further,
and avoid backprop entirely.
ES can use deterministic policies. It randomizes over the choice of
policy rather than over the choice of actions.
I.e., sample a random policy from a distribution p() parameterized
byand apply the policy gradient trick
@
@Ep[r(())] =Ep
r(())@
@logp()
The neural net architecture itself can be discontinuous.
Roger Grosse CSC321 Lecture 21: Policy Gradient 19 / 21
Evolution Strategies (optional)
https://arxiv.org/pdf/1703.03864.pdf
Roger Grosse CSC321 Lecture 21: Policy Gradient 20 / 21
Evolution Strategies (optional)
The IEEE 
oating point standard is nonlinear, since small enough
numbers get truncated to zero.
This acts as a discontinuous activation
function, which ES is able to handle.
ES was able to train a good MNIST
classier using a \linear" activation
function.
https://blog.openai.com/
nonlinear-computation-in-linear-networks/
Roger Grosse CSC321 Lecture 21: Policy Gradient 21 / 21
Parsing, and Context-Free Grammars
Michael Collins, Columbia University
Overview
IAn introduction to the parsing problem
IContext free grammars
IA brief(!) sketch of the syntax of English
IExamples of ambiguous structures
Parsing (Syntactic Structure)
INPUT:
Boeing is located in Seattle.
OUTPUT:
S
NP
N
BoeingVP
V
isVP
V
locatedPP
P
inNP
N
Seattle
Syntactic Formalisms
IWork in formal syntax goes back to Chomsky's PhD thesis in
the 1950s
IExamples of current formalisms: minimalism, lexical
functional grammar (LFG), head-driven phrase-structure
grammar (HPSG), tree adjoining grammars (TAG), categorial
grammars
Data for Parsing Experiments
IPenn WSJ Treebank = 50,000 sentences with associated trees
IUsual set-up: 40,000 training sentences, 2400 test sentences
An example tree:
CanadianNNP
UtilitiesNNPSNP
hadVBD
1988CD
revenueNNNP
ofIN
C$$
1.16CD
billionCD
,PUNC,QPNPPPNP
mainlyRBADVP
fromIN
itsPRP$
naturalJJ
gasNN
andCC
electricJJ
utilityNN
businessesNNSNP
inIN
AlbertaNNP
,PUNC,NP
whereWRBWHADVP
theDT
companyNNNP
servesVBZ
aboutRB
800,000CDQP
customersNNS
.PUNC.NPVPSSBARNPPPNPPPVPSTOP
Canadian Utilities had 1988 revenue of C$ 1.16 billion ,
mainly from its natural gas and electric utility businesses in
Alberta , where the company serves about 800,000
customers .
The Information Conveyed by Parse Trees
(1) Part of speech for each word
(N = noun, V = verb, DT = determiner)
S
NP
DT
theN
burglarVP
V
robbedNP
DT
theN
apartment
The Information Conveyed by Parse Trees (continued)
(2) Phrases
S
NP
DT
theN
burglarVP
V
robbedNP
DT
theN
apartment
Noun Phrases (NP): \the burglar", \the apartment"
Verb Phrases (VP): \robbed the apartment"
Sentences (S): \the burglar robbed the apartment"
The Information Conveyed by Parse Trees (continued)
(3) Useful Relationships
S
NP
subjectVP
V
verbS
NP
DT
theN
burglarVP
V
robbedNP
DT
theN
apartment
)\the burglar" is the subject of \robbed"
An Example Application: Machine Translation
IEnglish word order is subject { verb { object
IJapanese word order is subject { object { verb
English: IBM bought Lotus
Japanese: IBM Lotus bought
English: Sources said that IBM bought Lotus yesterday
Japanese: Sources yesterday IBM Lotus bought that said
S
NP-A
SourcesVP
,
SBAR-A
,
S
NP
yesterdayNP-A
IBMVP
,
NP-A
LotusVB
boughtCOMP
thatVB
said
Overview
IAn introduction to the parsing problem
IContext free grammars
IA brief(!) sketch of the syntax of English
IExamples of ambiguous structures
Context-Free Grammars
Hopcroft and Ullman, 1979
A context free grammar G= (N;;R;S )where:
INis a set of non-terminal symbols
Iis a set of terminal symbols
IRis a set of rules of the form X!Y1Y2:::Y n
forn0,X2N,Yi2(N[)
IS2Nis a distinguished start symbol
A Context-Free Grammar for English
N=fS, NP, VP, PP, DT, Vi, Vt, NN, IN g
S= S
=fsleeps, saw, man, woman, telescope, the, with, in g
R=S! NP VP
VP! Vi
VP! Vt NP
VP! VP PP
NP! DT NN
NP! NP PP
PP! IN NPVi! sleeps
Vt! saw
NN! man
NN! woman
NN! telescope
DT! the
IN! with
IN! in
Note: S=sentence, VP=verb phrase, NP=noun phrase,
PP=prepositional phrase, DT=determiner, Vi=intransitive verb,
Vt=transitive verb, NN=noun, IN=preposition
Left-Most Derivations
A left-most derivation is a sequence of strings s1:::s n, where
Is1=S, the start symbol
Isn2, i.e.snis made up of terminal symbols only
IEachsifori= 2:::n is derived from si 1by picking the
left-most non-terminal Xinsi 1and replacing it by some 
whereX!is a rule inR
For example: [S], [NP VP], [D N VP], [the N VP], [the man VP],
[the man Vi], [the man sleeps]
Representation of a derivation as a tree: S
NP
D
theN
manVP
Vi
sleeps
An Example
DERIVATION RULES USED
SS!NP VP
NP VPNP!DT N
DT N VPDT!the
the N VPN!dog
the dog VPVP!VB
the dog VBVB!laughs
the dog laughsS
NP
DT
theN
dogVP
VB
laughs
An Example
DERIVATION RULES USED
SS!NP VP
NP VPNP!DT N
DT N VPDT!the
the N VPN!dog
the dog VPVP!VB
the dog VBVB!laughs
the dog laughsS
NP
DT
theN
dogVP
VB
laughs
An Example
DERIVATION RULES USED
SS!NP VP
NP VPNP!DT N
DT N VPDT!the
the N VPN!dog
the dog VPVP!VB
the dog VBVB!laughs
the dog laughsS
NP
DT
theN
dogVP
VB
laughs
An Example
DERIVATION RULES USED
SS!NP VP
NP VPNP!DT N
DT N VPDT!the
the N VPN!dog
the dog VPVP!VB
the dog VBVB!laughs
the dog laughsS
NP
DT
theN
dogVP
VB
laughs
An Example
DERIVATION RULES USED
SS!NP VP
NP VPNP!DT N
DT N VPDT!the
the N VPN!dog
the dog VPVP!VB
the dog VBVB!laughs
the dog laughsS
NP
DT
theN
dogVP
VB
laughs
An Example
DERIVATION RULES USED
SS!NP VP
NP VPNP!DT N
DT N VPDT!the
the N VPN!dog
the dog VPVP!VB
the dog VBVB!laughs
the dog laughsS
NP
DT
theN
dogVP
VB
laughs
An Example
DERIVATION RULES USED
SS!NP VP
NP VPNP!DT N
DT N VPDT!the
the N VPN!dog
the dog VPVP!VB
the dog VBVB!laughs
the dog laughsS
NP
DT
theN
dogVP
VB
laughs
An Example
DERIVATION RULES USED
SS!NP VP
NP VPNP!DT N
DT N VPDT!the
the N VPN!dog
the dog VPVP!VB
the dog VBVB!laughs
the dog laughsS
NP
DT
theN
dogVP
VB
laughs
Properties of CFGs
IA CFG denes a set of possible derivations
IA strings2is in the language dened by the
CFG if there is at least one derivation that yields s
IEach string in the language generated by the CFG
may have more than one derivation (\ambiguity")
An Example of Ambiguity
S
NP
heVP
VP
VB
drovePP
IN
downNP
DT
theNN
streetPP
IN
inNP
DT
theNN
car
An Example of Ambiguity (continued)
S
NP
heVP
VB
drovePP
IN
downNP
NP
DT
theNN
streetPP
IN
inNP
DT
theNN
car
The Problem with Parsing: Ambiguity
INPUT:
She announced a program to promote safety in trucks and vans
+
POSSIBLE OUTPUTS:
SNP
SheVPannouncedNPNPap r o g r a mVP
to promoteNPsafetyPPinNP
trucks and vansSNP
SheVP
announcedNP
NPNPap r o g r a mVP
to promoteNPsafetyPPinNP
trucksandNP
vansSNP
SheVPannouncedNPNPap r o g r a mVP
to promoteNP
NPsafetyPPinNP
trucksandNP
vansSNP
SheVPannouncedNPNPap r o g r a mVPto promoteNP
safetyPPinNP
trucks and vansSNP
SheVP
announcedNP
NPNPap r o g r a mVPto promoteNP
safetyPPinNP
trucksandNP
vansSNP
SheVPannouncedNPNPNPap r o g r a mVP
to promoteNP
safetyPPinNP
trucks and vans
And there are more...
Overview
IAn introduction to the parsing problem
IContext free grammars
IA brief(!) sketch of the syntax of English
IExamples of ambiguous structures
2/8/13 9:36 AMA Comprehensive Grammar of the English Language: Randolph Quirk, Si…um, Geoffrey Leech, Jan Svartvik: 9780582517349: Amazon.com: Books
Page 1 of 1http://www.amazon.com/gp/product/images/0582517346/ref=dp_image_0?ie=UTF8&n=283155&s=books
A Comprehensive Grammar of the English Language
Close WindowProduct Details (from Amazon)
Hardcover: 1779 pages
Publisher: Longman; 2nd Revised edition
Language: English
ISBN-10: 0582517346
ISBN-13: 978-0582517349
Product Dimensions: 8.4 x 2.4 x 10 inches
Shipping Weight: 4.6 pounds
A Brief Overview of English Syntax
Parts of Speech (tags from the Brown corpus):
INouns
NN = singular noun e.g., man, dog, park
NNS = plural noun e.g., telescopes, houses, buildings
NNP = proper noun e.g., Smith, Gates, IBM
IDeterminers
DT = determiner e.g., the, a, some, every
IAdjectives
JJ = adjective e.g., red, green, large, idealistic
A Fragment of a Noun Phrase Grammar
N) NN
N) NN N
N) JJ N
N) N N
NP) DT NNN) box
NN) car
NN) mechanic
NN) pigeon
DT) the
DT) aJJ) fast
JJ) metal
JJ) idealistic
JJ) clay
Prepositions, and Prepositional Phrases
IPrepositions
IN = preposition e.g., of, in, out, beside, as
An Extended Grammar
N) NN
N) NN N
N) JJ N
N) N N
NP) DT N
PP) IN NP
N) N PPNN) box
NN) car
NN) mechanic
NN) pigeon
DT) the
DT) aJJ) fast
JJ) metal
JJ) idealistic
JJ) clay
IN) in
IN) under
IN) of
IN) on
IN) with
IN) as
Generates:
in a box, under the box, the fast car mechanic under the pigeon in
the box,:::
An Extended Grammar
N) NN
N) NN N
N) JJ N
N) N N
NP) DT N
PP) IN NP
N) N PP
Verbs, Verb Phrases, and Sentences
IBasic Verb Types
Vi = Intransitive verb e.g., sleeps, walks, laughs
Vt = Transitive verb e.g., sees, saw, likes
Vd = Ditransitive verb e.g., gave
IBasic VP Rules
VP! Vi
VP! Vt NP
VP! Vd NP NP
IBasic S Rule
S! NP VP
Examples of VP:
sleeps, walks, likes the mechanic, gave the mechanic the fast car
Examples of S:
the man sleeps, the dog walks, the dog gave the mechanic the fast car
PPs Modifying Verb Phrases
A new rule: VP! VP PP
New examples of VP:
sleeps in the car, walks like the mechanic, gave the mechanic the
fast car on Tuesday, :::
Complementizers, and SBARs
IComplementizers
COMP = complementizer e.g., that
ISBAR
SBAR! COMP S
Examples:
that the man sleeps, that the mechanic saw the dog :::
More Verbs
INew Verb Types
V[5] e.g., said, reported
V[6] e.g., told, informed
V[7] e.g., bet
INew VP Rules
VP! V[5] SBAR
VP! V[6] NP SBAR
VP! V[7] NP NP SBAR
Examples of New VPs:
said that the man sleeps
told the dog that the mechanic likes the pigeon
bet the pigeon $50 that the mechanic owns a fast car
Coordination
IA New Part-of-Speech:
CC = Coordinator e.g., and, or, but
INew Rules
NP! NP CC NP
N! N CC N
VP! VP CC VP
S! S CC S
SBAR! SBAR CC SBAR
We've Only Scratched the Surface...
IAgreement
The dogs laugh vs.The dog laughs
IWh-movement
The dog that the cat liked
IActive vs. passive
The dog saw the cat vs.
The cat was seen by the dog
IIf you're interested in reading more:
Syntactic Theory: A Formal Introduction, 2nd
Edition. Ivan A. Sag, Thomas Wasow, and Emily
M. Bender.
Overview
IAn introduction to the parsing problem
IContext free grammars
IA brief(!) sketch of the syntax of English
IExamples of ambiguous structures
Sources of Ambiguity
IPart-of-Speech ambiguity
NN! duck
Vi! duck
VP
VP
Vt
sawNP
PRP
herNN
duckPP
IN
withNP
the telescopeVP
VP
V
sawS
NP
herVP
Vi
duckPP
IN
withNP
the telescope
S
NP
IVP
VP
Vi
drovePP
IN
downNP
DT
theNN
roadPP
IN
inNP
DT
theNN
car
S
NP
IVP
Vi
drovePP
IN
downNP
NP
DT
theNN
roadPP
IN
inNP
DT
theNN
car
Two analyses for: John was believed to have been shot by Bill
Sources of Ambiguity: Noun Premodiers
INoun premodiers:
NP
D
theN
JJ
fastN
NN
carN
NN
mechanicNP
D
theN
N
JJ
fastN
NN
carN
NN
mechanic
Text	Classification&	Linear	ModelsCMSC	723	/	LING	723	/	INST	725Marine	CarpuatSlides	credit:	Dan	Jurafsky&	James	Martin,	Jacob	Eisenstein	
Logistics/Reminders•Homework	1	–due	Thursday	Sep	7	by	12pm.•Project	1	coming	up	•Thursday	lecture	time:	project	set-up	office	hour	in	CSIC	1121
Recap:	Word	Meaning2	core	issues	from	an	NLP	perspective•Semantic	similarity:	given	two	words,	how	similar	are	they	in	meaning?•Key	concepts:	vector	semantics,	PPMI	and	its	variants,	cosine	similarity•Word	sense	disambiguation:	given	a	word	that	has	more	than	one	meaning,		which	one	is	used	in	a	specific	context?•Key	concepts:	word	sense,	WordNet	and	sense	inventories,	unsupervised	disambiguation	(Lesk),	supervised	disambiguation
Today•Text	classification	problems•and	their	evaluation•Linear	classifiers•Features	&	Weights•Bag	of	words•Naïve	Bayes
Text	classification
Is	this	spam?From:"Fabian Starr“ <Patrick_Freeman@pamietaniepeerelu.pl>Subject: Hey! Sofwarefor the funny prices!Get the great discounts on popular software today for PC and Macintoshhttp://iiled.org/Cj4Lmx70-90% Discounts from retail price!!!All sofwareis instantly available to download -No Need Wait!
What	is	the	subject	of	this	article?•Antogonistsand	Inhibitors•Blood	Supply•Chemistry•Drug	Therapy•Embryology•Epidemiology•…MeSHSubject	Category	Hierarchy?MEDLINE Article

Text	Classification•Assigning	subject	categories,	topics,	or	genres•Spam	detection•Authorship	identification•Age/gender	identification•Language	Identification•Sentiment	analysis•…
Text	Classification:	definition•Input:•a	document	d•a	fixed	set	of	classes		Y	={y1,	y2,…,	yJ}•Output:	a	predicted	class	yÎY
Classification	Methods:	Hand-coded	rules•Rules	based	on	combinations	of	words	or	other	features•spam:	black-list-address	OR	(“dollars”	AND	“have	been	selected”)•Accuracy	can	be	high•If	rules	carefully	refined	by	expert•But	building	and	maintaining	these	rules	is	expensive
Classification	Methods:Supervised	Machine	Learning•Input•a	document	d•a	fixed	set	of	classes		Y	={y1,	y2,…,	yJ}•atraining	set	of	mhand-labeled	documents	(d1,y1),....,(dm,ym)•Output•a	learned	classifier	dày
Aside:	getting	examples	for	supervised	learning•Human	annotation•By	experts	or	non-experts	(crowdsourcing)•Found	data•How	do	we	know	how	good	a	classifier	is?•Compare	classifier	predictions	with	human	annotation•On	held	outtest	examples•Evaluation	metrics:	accuracy,	precision,	recall
The	2-by-2	contingency	tablecorrectnot	correctselectedtpfpnot	selectedfntn
Precision	and	recall•Precision:	%	of	selected	items	that	are	correctRecall:	%	of	correct	items	that	are	selectedcorrectnot	correctselectedtpfpnot	selectedfntn
A	combined	measure:	F•A	combined	measure	that	assesses	the	P/R	tradeoff	is	F	measure	(weighted	harmonic	mean):•People	usually	use	balanced	F1	measure•i.e.,	with	b=	1	(that	is,	a=	½):			F=	2PR/(P+R)RPPRRPF++=−+=22)1(1)1(11ββαα
Linear	Classifiers
Bag	of	words

Defining	features

Defining	features

Linear	classification

Linear	Modelsfor	Classification
Feature	function	representationWeights
How	can	we	learn	weights?•By	hand•Probability•e.g.,NaïveBayes•Discriminative	training•e.g.,	perceptron,	support	vector	machines	
Generative	Story	for	Multinomial	Naïve	Bayes•A	hypothetical	stochastic	process	describing	how	training	examples	are	generated

Prediction	with	Naïve	Bayes
Score(x,y)
Prediction	with	Naïve	Bayes
Score(x,y)
Parameter	Estimation	•“count	and	normalize”•Parameters	of	a	multinomial	distribution•Relative	frequency	estimator•Formally:	this	is	the	maximum	likelihood	estimate•See	CIML	for	derivation

Smoothing	(add	alpha	/	Laplace)

Naïve	Bayes	recap

Today•Text	classification	problems•and	their	evaluation•Linear	classifiers•Features	&	Weights•Bag	of	words•Naïve	Bayes
CS224d:	Deep	NLPLecture	9:Wrap	up:	LSTMsandRecursive	Neural	NetworksRichard	Socherrichard@metamind.io
Overview
4/29/16Richard	Socher2Video	issues	and	fire	alarmFinish	LSTMsRecursive	Neural	Networks•Motivation:	Compositionality•Structure	prediction:	Parsing•Backpropagationthrough	Structure•Vision	ExampleNext	Lecture:•RvNNimprovements
Long-short-term-memories	(LSTMs)
4/29/16Richard	Socher3•We	can	make	the	units	even	more	complex•Allow	each	time	step	to	modify	•Input	gate	(current	cell	matters)•Forget	(gate	0,	forget	past)•Output	(how	much	cell	is	exposed)•New	memory	cell•Final	memory	cell:•Final	hidden	state:	

Illustrations	all	a	bit	overwhelming	;)
4/29/16Richard	Socher4
http://people.idsia.ch/~juergen/lstm/sld017.htm
http://deeplearning.net/tutorial/lstm.htmlIntuition:	memory	cells	can	keep	information	 intact,	unless	inputs	makes	themforget	it	or	overwrite	it	with	new	input.Cell	can	decide	to	output	this	information	or	just	store	itLong	Short-Term	Memory	by	Hochreiterand	Schmidhuber(1997)
injinjoutjoutjwicjwicjycjgh1.0netwiwiyinjyoutjnetcjgyinj=g+scjscjyinjhyoutjnet

LSTMs	are	currently	very	hip!
4/29/16Richard	Socher5•En	vogue	default	model	for	most	sequence	labeling	tasks•Very	powerful,	especially	when	stacked	and	made	even	deeper	(each	hidden	layer	is	already	computed	by	a	deep	internal	network)•Most	useful	if	you	have	lots	and	lots	of	data
Deep	LSTMs	compared	to	traditional	systems	2015
6Methodtest BLEU score (ntst14)Bahdanau et al. [2]28.45Baseline System [29]33.30Single forward LSTM, beam size 1226.17Single reversed LSTM, beam size 1230.59Ensemble of 5 reversed LSTMs, beam size 133.00Ensemble of 2 reversed LSTMs, beam size 1233.27Ensemble of 5 reversed LSTMs, beam size 234.50Ensemble of 5 reversed LSTMs, beam size 1234.81Table 1: The performance of the LSTM on WMT’14 English to Frencht e s ts e t( n t s t 1 4 ) .N o t et h a tan ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam ofsize 12.Methodtest BLEU score (ntst14)Baseline System [29]33.30Cho et al. [5]34.54Best WMT’14 result [9]37.0Rescoring the baseline 1000-best with a single forward LSTM35.61Rescoring the baseline 1000-best with a single reversed LSTM35.85Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs36.5Oracle Rescoring of the Baseline 1000-best lists∼45Table 2: Methods that use neural networks together with an SMT system on the WMT’14 Englishto French test set (ntst14).task by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM iswithin 0.5 BLEU points of the best WMT’14 result if it is used torescore the 1000-best list of thebaseline system.3.7 Performance on long sentencesWe were surprised to discover that the LSTM did well on long sentences, which is shown quantita-tively in ﬁgure 3. Table 3 presents several examples of long sentences and their translations.3.8 Model Analysis
−8−6−4−20246810−6−5−4−3−2−101234
John respects MaryMary respects JohnJohn admires MaryMary admires JohnMary is in love with JohnJohn is in love with Mary
−15−10−505101520−20−15−10−5051015
I gave her a card in the gardenIn the garden , I gave her a cardShe was given a card by me in the gardenShe gave me a card in the gardenIn the garden , she gave me a cardI was given a card by her in the garden
Figure 2:The ﬁgure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtainedafter processing the phrases in the ﬁgures. The phrases are clustered by meaning, which in these examples isprimarily a function of word order, which would be difﬁcult to capture with abag-of-words model. Notice thatboth clusters have similar internal structure.One of the attractive features of our model is its ability to turn a sequence of words into a vectorof ﬁxed dimensionality. Figure 2 visualizes some of the learned representations. The ﬁgure clearlyshows that the representations are sensitive to the order ofwords, while being fairly insensitive to the6Sequence	 to	Sequence	Learning	by	Sutskeveret	al.	2014	
Deep	LSTMs	(with	a	lot	more	tweaks)	today
4/29/16Richard	Socher7WMT	2016	competition	results	from	yesterday

Deep	LSTM	for	Machine	Translation
4/29/16Richard	Socher8Methodtest BLEU score (ntst14)Bahdanau et al. [2]28.45Baseline System [29]33.30Single forward LSTM, beam size 1226.17Single reversed LSTM, beam size 1230.59Ensemble of 5 reversed LSTMs, beam size 133.00Ensemble of 2 reversed LSTMs, beam size 1233.27Ensemble of 5 reversed LSTMs, beam size 234.50Ensemble of 5 reversed LSTMs, beam size 1234.81Table 1: The performance of the LSTM on WMT’14 English to Frencht e s ts e t( n t s t 1 4 ) .N o t et h a tan ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam ofsize 12.Methodtest BLEU score (ntst14)Baseline System [29]33.30Cho et al. [5]34.54Best WMT’14 result [9]37.0Rescoring the baseline 1000-best with a single forward LSTM35.61Rescoring the baseline 1000-best with a single reversed LSTM35.85Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs36.5Oracle Rescoring of the Baseline 1000-best lists∼45Table 2: Methods that use neural networks together with an SMT system on the WMT’14 Englishto French test set (ntst14).task by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM iswithin 0.5 BLEU points of the best WMT’14 result if it is used torescore the 1000-best list of thebaseline system.3.7 Performance on long sentencesWe were surprised to discover that the LSTM did well on long sentences, which is shown quantita-tively in ﬁgure 3. Table 3 presents several examples of long sentences and their translations.3.8 Model Analysis
−8−6−4−20246810−6−5−4−3−2−101234
John respects MaryMary respects JohnJohn admires MaryMary admires JohnMary is in love with JohnJohn is in love with Mary
−15−10−505101520−20−15−10−5051015
I gave her a card in the gardenIn the garden , I gave her a cardShe was given a card by me in the gardenShe gave me a card in the gardenIn the garden , she gave me a cardI was given a card by her in the garden
Figure 2:The ﬁgure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtainedafter processing the phrases in the ﬁgures. The phrases are clustered by meaning, which in these examples isprimarily a function of word order, which would be difﬁcult to capture with abag-of-words model. Notice thatboth clusters have similar internal structure.One of the attractive features of our model is its ability to turn a sequence of words into a vectorof ﬁxed dimensionality. Figure 2 visualizes some of the learned representations. The ﬁgure clearlyshows that the representations are sensitive to the order ofwords, while being fairly insensitive to the6Sequence	 to	Sequence	Learning	by	Sutskeveret	al.	2014	PCA	of	 vectors	from	last	time	step	hidden	 layer
Further	Improvements:	More	Gates!
4/29/16Richard	Socher9Gated	Feedback	Recurrent	Neural	Networks,	Chung	et	al.	2015Gated Feedback Recurrent Neural Networks
(a) Conventional stacked RNN (b) Gated Feedback RNNFigure 1.Illustrations of (a) conventional stacking approach and (b) gated-feedback approach to form a deep RNN architecture. Bulletsin (b) correspond to global reset gates. Skip connections are omitted to simplify the visualization of networks.The global reset gate is computed as:gi!j= ⇣wi!jghj 1t+ui!jgh⇤t 1⌘,(12)whereLis the number of hidden layers,wi!jgandui!jgare the weight vectors for the input and the hidden states ofall the layers at time-stept 1, respectively. Forj=1,hj 1tisxt.The global reset gategi!jis applied collectively to the sig-nal from thei-th layerhit 1to thej-th layerhjt. In otherwords, the signal from the layerito the layerjis controlledbased on the input and the previous hidden states.Fig.1illustrates the difference between the conventionalstacked RNN and our proposed GF-RNN. In both mod-els, information ﬂows from lower layers to upper layers,respectively, corresponding to ﬁner timescale and coarsertimescale. The GF-RNN, however, further allows infor-mation from the upper recurrent layer, corresponding tocoarser timescale, ﬂows back into the lower layers, corre-sponding to ﬁner timescales.We call this RNN with a fully-connected recurrent tran-sition and global reset gates, agated-feedback RNN(GF-RNN). In the remainder of this section, we describe how touse the previously described LSTM unit, GRU, and moretraditionaltanhunit in the GF-RNN.3.1. Practical Implementation of GF-RNN3.1.1.tanhUNITFor a stackedtanh-RNN, the signal from the previoustime-step is gated. The hidden state of thej-th layer iscomputed byhjt= tanh Wj 1!jhj 1t+LXi=1gi!jUi!jhit 1!,whereWj 1!jandUi!jare the weight matrices of theincoming connections from the input and thei-th module,respectively. Compared to Eq. (2), the only difference isthat the previous hidden states are controlled by the globalreset gates.3.1.2. LONGSHORT-TERMMEMORY ANDGATEDRECURRENTUNITIn the cases of LSTM and GRU, we do not use the globalreset gates when computing the unit-wise gates. In otherwords, Eqs. (5)–(7) for LSTM, and Eqs. (9) and (11) forGRU are not modiﬁed. We only use the global reset gateswhen computing the new state (see Eq. (4) for LSTM, andEq. (10) for GRU).The new memory content of an LSTM at thej-th layer iscomputed by˜cjt= tanh Wj 1!jchj 1t+LXi=1gi!jUi!jchit 1!.In the case of a GRU, similarly,˜hjt= tanh Wj 1!jhj 1t+rjt LXi=1gi!jUi!jhit 1!.4. Experiment Settings4.1. TasksWe evaluated the proposed gated-feedback RNN (GF-RNN) on character-level language modeling and Python
Summary
4/29/16Richard	Socher10•Recurrent	Neural	Networks	are	powerful•A	lot	of	ongoing	work	right	now•Gated	Recurrent	Units	even	better•LSTMs	maybe	even	better	(jury	still	out)•This	was	an	advanced	lecture	àgain	intuition,	encourage	exploration•Next	up:	Recursive	Neural	Networkssimpler	and	also	powerful	:)
Building	on	Word	Vector	Space	Models
11x2
x10								1						2					3					4						5					6					7					8						9					1054321Monday92Tuesday9.51.5By	mapping	them	into	the	same	vector	space!151.14
the	country	of	my	birththe	place	where	I	was	bornBut	how	can	we	represent	the	meaning	of	longer	phrases?France22.5Germany13
Semantic	Vector	Spaces
•Distributional	Techniques•Brown	Clusters•Useful	as	features	inside	models,	e.g.	CRFs	for	NER,	etc.•Cannot	capture	longer	phrasesSingle	Word	VectorsDocuments	Vectors•Bag	of	words	models•PCA	(LSA,LDA)•Great	for	IR,	document	exploration,	etc.•Ignore	word	order,	no	detailed	understandingVectors	representingPhrases	and	Sentencesthat	do	not	ignore	word	orderand	capture	semantics	for	NLP	tasks
How	should	we	map	phrases	into	a	vector	space?
the		country							of							my		birth0.40.32.33.644.5772.13.32.53.85.56.113.515Use	principle	of	compositionalityThe	meaning	(vector)	of	a	sentence	is		determined	by	(1)the	meanings	of	its	words	and(2)the	rules	that	combine	them.Models	in	this	section	can	jointly	learn	parse	trees	and	compositional	vector	representationsx2
x10								1						2							3						4						5						6						7							8						9					1054321the	country	of	my	birththe	place	where	I	was	bornMondayTuesdayFranceGermany
13
Sentence	Parsing:	What	we	want
9153859143NPNPPPS
71VP
The																cat														sat														on														the															mat.14
Learn	Structure	and	Representation
NPNPPPSVP
5233835473
The															cat														sat															on														the															mat.91538591437115
Learn	Structure	and	Representation?•Do	we	really	need	to	learn	this	structure?
4/29/16Richard	SocherLecture	1,	Slide	16
Sidenote:	Recursive	vsrecurrent	neural	networks
4/29/16the		country							of							my		birth0.40.32.33.644.5772.13.32.53.85.56.113.515
the		country							of							my		birth0.40.32.33.644.5772.13.34.53.85.56.113.5152.53.8
Sidenote:	Are	languages	recursive?•Cognitively	debatable•But:	recursion	helpful	in	describing	natural	language•Example:	“the	church	which	has	nice	windows”,	a	noun	phrase	containing	a	relative	clause	that	contains	a	noun	phrases•Arguments	for	now:	1)	Helpful	in	disambiguation:
4/29/16Richard	SocherLecture	1,	Slide	18

Is	recursion	useful?2)	Helpful	for	some	tasks	to	refer	to	specific	phrases:•John	and	Jane	went	to	a	big	festival.	They	enjoyed	the	trip	and	the	music	there.•“they”:	John	and	Jane•“the	trip”:	went	to	a	big	festival•“there”:	big	festival3)	Labeling	less	clear	if	specific	to	only	subphrases•I	liked	the	bright	screen	but	not	the	buggy	slow	keyboardof	the	phone.	It	was	a	pain	to	type	with.	It	was	nice	to	look	at.4)	Works	better	for	some	tasks	to	use	grammatical	tree	structure•This	is	still	up	for	debate.4/29/16Richard	SocherLecture	1,	Slide	19
Recursive	Neural	Networks	for	Structure	Prediction
on											the													mat.91433383
8533Neural Network831.3Inputs:	two	candidate	children’s	representationsOutputs:1.The	semantic	representation	if	the	two	nodes	are	merged.2.Score	of	how	plausible	the	new	node	would	be.
8520
Recursive	Neural	Network	Definitionscore		=		UTpp=		tanh(W+	b),SameWparameters	at	all	nodes	of	the	tree8533Neural Network831.3score		==	parent
c1c2c1c2
21
Parsing	a	sentence	with	an	RNN
Neural Network0.120Neural Network0.410Neural Network2.333
915385914371Neural Network3.152Neural Network0.301
The																cat													sat															on														the																mat.22
Parsing	a	sentence
915352Neural Network1.121
Neural Network0.120Neural Network0.410Neural Network2.333
538591437123The																cat													sat															on														the																mat.
Parsing	a	sentence
52Neural Network1.121
Neural Network0.12033Neural Network3.683
9153538591437124The																cat													sat															on														the																mat.
Parsing	a	sentence
5233835473
9153538591437125The																cat													sat															on														the																mat.
Max-Margin	Framework	-Details•The	score	of	a	tree	is	computed	by	the	sum	of	the	parsing	decisionscores	at	each	node:
8533RNN831.3
26

Max-Margin	Framework	-Details•Similar	to	max-margin	parsing	(Taskaret	al.	2004),	a	supervised	max-margin	objective•The	loss																penalizes	all	incorrect	decisions•Structure	search	for	A(x)	was	maximally	greedy•Instead:	Beam	Search	with	Chart
27
Backpropagation	Through	StructureIntroduced	by	Goller&	Küchler(1996)	Principally	the	same	as	general	backpropagationThree	differences	resulting	from	the	recursion	and	tree	structure:1.Sum	derivatives	of	Wfrom	all	nodes2.Split	derivatives	at	each	node3.Add	error	messages	from	parent	+	node	itself28The second derivative in eq. 28 for output units is simply@a(nl)i@W(nl 1)ij=@@W(nl 1)ijz(nl)i=@@W(nl 1)ij⇣W(nl 1)i·a(nl 1)⌘=a(nl 1)j.(46)We adopt standard notation and introduce the error related to an output unit:@En@W(nl 1)ij=(yi ti)a(nl 1)j= (nl)ia(nl 1)j.(47)So far, we only computed errors for output units, now we will derive ’s for normal hidden units andshow how these errors are backpropagated to compute weight derivatives of lower levels. We will start withsecond to top layer weights from which a generalization to arbitrarily deep layers will become obvious.Similar to eq. 28, we start with the error derivative:@E@W(nl 2)ij=Xn@En@a(nl)|{z} (nl)@a(nl)@W(nl 2)ij+ W(nl 2)ji.(48)Now,( (nl))T@a(nl)@W(nl 2)ij=( (nl))T@z(nl)@W(nl 2)ij(49)=( (nl))T@@W(nl 2)ijW(nl 1)a(nl 1)(50)=( (nl))T@@W(nl 2)ijW(nl 1)·ia(nl 1)i(51)=( (nl))TW(nl 1)·i@@W(nl 2)ija(nl 1)i(52)=( (nl))TW(nl 1)·i@@W(nl 2)ijf(z(nl 1)i) (53)=( (nl))TW(nl 1)·i@@W(nl 2)ijf(W(nl 2)i·a(nl 2)) (54)=( (nl))TW(nl 1)·if0(z(nl 1)i)a(nl 2)j(55)=⇣( (nl))TW(nl 1)·i⌘f0(z(nl 1)i)a(nl 2)j(56)=0@sl+1Xj=1W(nl 1)ji (nl)j)1Af0(z(nl 1)i)|{z}a(nl 2)j(57)= (nl 1)ia(nl 2)j(58)where we used in the ﬁrst line that the top layer is linear. This is a very detailed account of essentiallyjust the chain rule.So, we can write the errors of all layersl(except the top layer) (in vector format, using the Hadamardproduct ): (l)=⇣(W(l))T (l+1)⌘ f0(z(l)),(59)7where the sigmoid derivative from eq. 14 givesf0(z(l))=( 1 a(l))a(l). Using that deﬁnition, we get thehidden layer backprop derivatives:@@W(l)ijER=a(l)j (l+1)i+ W(l)ij(60)(61)Which in one simpliﬁed vector notation becomes:@@W(l)ER= (l+1)(a(l))T+ W(l).(62)In summary, the backprop procedure consists of four steps:1. Apply an inputxnand forward propagate it through the network to get the hidden and outputactivations using eq. 18.2. Evaluate (nl)for output units using eq. 42.3. Backpropagate the ’s to obtain a (l)for each hidden layer in the network using eq. 59.4. Evaluate the required derivatives with eq. 62 and update all the weights using an optimizationprocedure such as conjugate gradient or L-BFGS. CG seems to be faster and work better whenusing mini-batches of training data to estimate the derivatives.If you have any further questions or found errors, please send an email torichard@socher.org5 Recursive Neural NetworksSame as backprop in previous section but splitting error derivatives and noting that the derivatives of thesameWat each node can all be added up. Lastly, the delta’s from the parent node and possible delta’sfrom a softmax classiﬁer at each node are just added.References[Ben07] Yoshua Bengio. Learning deep architectures for ai. Technical report, Dept. IRO, Universite deMontreal, 2007.
8
BTS:	1)	Sum	derivatives	of	all	nodesYou	can	actually	assume	it’s	a	different	Wat	each	nodeIntuition	via	example:If	we	take	separate	derivatives	of	each	occurrence,	we	get	same:
29
BTS:	2)	Split	derivatives	at	each	nodeDuring	forward	prop,	the	parent	is	computed	using	2	childrenHence,	the	errors	need	to	be	computed	wrteach	of	them:where	each	child’s	error	is	n-dimensional853383c1p		=		tanh(W							+	b)c1c2c2
853383c1c2
30
BTS:	3)	Add	error	messages•At	each	node:	•What	came	up	(fprop)	must	come	down	(bprop)•Total	error	messages	±=	error	messages	from	parent	+	error	message	from	own	score
4/29/16Richard	SocherLecture	1,	Slide	31853383c1c2parentscore
BTS	Python	Code:	forwardProp
4/29/16Richard	SocherLecture	1,	Slide	32

BTS	Python	Code:	backProp
4/29/16Richard	SocherLecture	1,	Slide	33
The second derivative in eq. 28 for output units is simply@a(nl)i@W(nl 1)ij=@@W(nl 1)ijz(nl)i=@@W(nl 1)ij⇣W(nl 1)i·a(nl 1)⌘=a(nl 1)j.(46)We adopt standard notation and introduce the error related to an output unit:@En@W(nl 1)ij=(yi ti)a(nl 1)j= (nl)ia(nl 1)j.(47)So far, we only computed errors for output units, now we will derive ’s for normal hidden units andshow how these errors are backpropagated to compute weight derivatives of lower levels. We will start withsecond to top layer weights from which a generalization to arbitrarily deep layers will become obvious.Similar to eq. 28, we start with the error derivative:@E@W(nl 2)ij=Xn@En@a(nl)|{z} (nl)@a(nl)@W(nl 2)ij+ W(nl 2)ji.(48)Now,( (nl))T@a(nl)@W(nl 2)ij=( (nl))T@z(nl)@W(nl 2)ij(49)=( (nl))T@@W(nl 2)ijW(nl 1)a(nl 1)(50)=( (nl))T@@W(nl 2)ijW(nl 1)·ia(nl 1)i(51)=( (nl))TW(nl 1)·i@@W(nl 2)ija(nl 1)i(52)=( (nl))TW(nl 1)·i@@W(nl 2)ijf(z(nl 1)i) (53)=( (nl))TW(nl 1)·i@@W(nl 2)ijf(W(nl 2)i·a(nl 2)) (54)=( (nl))TW(nl 1)·if0(z(nl 1)i)a(nl 2)j(55)=⇣( (nl))TW(nl 1)·i⌘f0(z(nl 1)i)a(nl 2)j(56)=0@sl+1Xj=1W(nl 1)ji (nl)j)1Af0(z(nl 1)i)|{z}a(nl 2)j(57)= (nl 1)ia(nl 2)j(58)where we used in the ﬁrst line that the top layer is linear. This is a very detailed account of essentiallyjust the chain rule.So, we can write the errors of all layersl(except the top layer) (in vector format, using the Hadamardproduct ): (l)=⇣(W(l))T (l+1)⌘ f0(z(l)),(59)7where the sigmoid derivative from eq. 14 givesf0(z(l))=( 1 a(l))a(l). Using that deﬁnition, we get thehidden layer backprop derivatives:@@W(l)ijER=a(l)j (l+1)i+ W(l)ij(60)(61)Which in one simpliﬁed vector notation becomes:@@W(l)ER= (l+1)(a(l))T+ W(l).(62)In summary, the backprop procedure consists of four steps:1. Apply an inputxnand forward propagate it through the network to get the hidden and outputactivations using eq. 18.2. Evaluate (nl)for output units using eq. 42.3. Backpropagate the ’s to obtain a (l)for each hidden layer in the network using eq. 59.4. Evaluate the required derivatives with eq. 62 and update all the weights using an optimizationprocedure such as conjugate gradient or L-BFGS. CG seems to be faster and work better whenusing mini-batches of training data to estimate the derivatives.If you have any further questions or found errors, please send an email torichard@socher.org5 Recursive Neural NetworksSame as backprop in previous section but splitting error derivatives and noting that the derivatives of thesameWat each node can all be added up. Lastly, the delta’s from the parent node and possible delta’sfrom a softmax classiﬁer at each node are just added.References[Ben07] Yoshua Bengio. Learning deep architectures for ai. Technical report, Dept. IRO, Universite deMontreal, 2007.
8
BTS:	Optimization•As	before,	we	can	plug	the	gradients	into	a	standard	off-the-shelf	L-BFGS	optimizer	or	SGD•Best	results	with	AdaGrad(Duchiet	al,	2011):	•For	non-continuous	objective	use	subgradientmethod(Ratliff	et	al.	2007)34

Discussion:	Simple	RNN•Good	results	with	single	matrix	RNN	(more	later)•Single	weight	matrix	RNN	could	capture	some	phenomena	but	not	adequate	for	more	complex,	higher	order	composition	and	parsing	long	sentences•The	composition	function	is	the	same	for	all	syntactic	categories,	punctuation,	etcWc1c2pWscores
Solution:	Syntactically-Untied	RNN•Idea:	Condition	the	composition	function	on	the	syntactic	categories,	“untie	the	weights”•Allows	for	different	composition	functions	for	pairs	of	syntactic	categories,	e.g.	Adv	+	AdjP,	VP	+	NP•Combines	discrete	syntactic	categories	with	continuous	semantic	information

Solution:	Compositional	Vector	Grammars•Problem:	Speed.	Every	candidate	score	in	beam	search	needs	a	matrix-vector	product.•Solution:	Compute	score	only	for	a	subset	of	trees	coming	from	a	simpler,	faster	model	(PCFG)•Prunes	very	unlikely	candidates	for	speed•Provides	coarse	syntactic	categories	of	the	children	for	each	beam	candidate•Compositional	Vector	Grammars:	CVG	=	PCFG	+	RNN
Details:	Compositional	Vector	Grammar•Scores	at	each	node	computed	by	combination	of	PCFG	and	SU-RNN:•Interpretation:	Factoring	discrete	and	continuous	parsing	in	one	model:•Socher	et	al.	(2013)

Related	work	for	recursive	neural	networks	Pollack	(1990):	Recursive	auto-associative	memoriesPrevious	Recursive	Neural	Networks	work	by	Goller&	Küchler(1996),	Costa	et	al.	(2003)assumed	fixed	tree	structure	and	used	one	hot	vectors.Hinton	(1990)	and	Bottou(2011):	Related	ideas	about	recursive	models	and	recursive	operators	as	smooth	versions	of	logic	operations39
Related	Work	for	parsing•Resulting	CVG	Parser	is	related	to	previous	work	that	extends	PCFG	parsers•Klein	and	Manning	(2003a)	:	manual	feature	engineering•Petrovet	al.	(2006)	:	learning	algorithm	that	splits	and	merges	syntactic	categories	•Lexicalized	parsers	(Collins,	2003;	Charniak,	2000):	describe	each	category	with	a	lexical	item•Hall	and	Klein	(2012)	combine	several	such	annotation	schemes	in	a	factored	parser.	•CVGs	extend	these	ideas	from	discrete	representations	to	richer	continuous	ones
Experiments•Standard	WSJ	split,	labeled	F1•Based	on	simple	PCFG	with	fewer	states•Fast	pruning	of	search	space,	few	matrix-vector	products•3.8%	higher	F1,	20%	faster	than	Stanford	factored	parserParserTest,All	SentencesStanford	PCFG,(Klein	 and	Manning,	 2003a)85.5StanfordFactored	(Klein	 and	Manning,	 2003b)86.6Factored	PCFGs	(Halland	Klein,	2012)89.4Collins	(Collins,1997)87.7SSN	(Henderson,2004)89.4BerkeleyParser	(Petrovand	Klein,	2007)90.1CVG	(RNN)	(Socher	et	al.,ACL	2013)85.0CVG	(SU-RNN)	(Socher	et	al.,ACL	2013)90.4Charniak-Self	Trained(McCloskyet	al.	2006)91.0Charniak-Self	Trained-ReRanked(McCloskyet	al.	2006)92.1
SU-RNN	Analysis•Learns	notion	of	soft	head	words
DT-NP	VP-NP

Analysis	of	resulting	vector	representations
All	the	figures	are	adjusted	for	seasonal	variations1.	All	the	numbers	are	adjusted	for	seasonal	fluctuations2.	All	the	figures	are	adjusted	to	remove	usual	seasonal	patternsKnight-Ridder	wouldn’t	comment	on	the	offer1.	Harsco	declined	to	say	what	country	placed	the	order2.	Coastal	wouldn’t	disclose	the	termsSales	grew	almost	7%	to	$UNK	m.	from	$UNK	m.1.	Sales	rose	more	than	7%	to	$94.9	m.	from	$88.3	m.2.	Sales	surged	40%	to	UNK	b.	yen	from	UNK	b.
SU-RNN	Analysis•Can	transfer	semantic	information	from	single	related	example•Train	sentences:•He	eats	spaghetti	with	a	fork.	•She	eats	spaghetti	with	pork.	•Test	sentences	•He	eats	spaghetti	with	a	spoon.	•He	eats	spaghetti	with	meat.
SU-RNN	Analysis

Labeling	in	Recursive	Neural	Networks
Neural Network83•We	can	use	eachnode’s	representation	as	features	for	a	softmaxclassifier:•Training	similar	to	model	in	part	1	with	standard	cross-entropy	error	+	scoresSoftmaxLayerNP
46
Scene	Parsing•The	meaning	of	a	scene	image	is	also	a	function	of	smaller	regions,	
•how	they	combine	as	parts	to	form	larger	objects,•and	how	the	objects	interact.Similar	principle	of	compositionality.
47
Algorithm	for	Parsing	ImagesSame	Recursive	Neural	Network	as	for	natural	language	parsing!	(Socher	et	al.	ICML	2011)
FeaturesGrassTree
SegmentsSemantic		RepresentationsPeopleBuildingParsing	Natural	Scene	ImagesParsing	Natural	Scene	Images
48
Multi-class	segmentation
MethodAccuracyPixel	CRF(Gould	 et	al.,	ICCV	2009)74.3Classifieron	superpixelfeatures75.9Region-based	energy(Gould	 et	al.,	ICCV	2009)76.4Local	labelling(Tighe&	Lazebnik,	ECCV	2010)76.9SuperpixelMRF	(Tighe&	Lazebnik,ECCV	2010)77.5Simultaneous	 MRF	(Tighe&	Lazebnik,	ECCV	2010)77.5Recursive	Neural	Network78.1Stanford	Background	Dataset	(Gould	et	al.	2009)49
4/29/16Richard	SocherLecture	1,	Slide	50
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  February 4, 2015 
Today: • Generative – discriminative classifiers • Linear regression • Decomposition of error into bias, variance, unavoidable Readings:  • Mitchell: “Naïve Bayes and Logistic Regression”      (required) • Ng and Jordan paper (optional) • Bishop, Ch 9.1, 9.2 (optional) 
•  Consider learning f: X à Y, where •  X is a vector of real-valued features, < X1 … Xn > •  Y is boolean •  assume all Xi are conditionally independent given Y •  model P(Xi | Y = yk) as Gaussian N(µik,σi) •  model P(Y) as Bernoulli (π) •  Then P(Y|X) is of this form, and we can directly estimate W •  Furthermore, same holds if the Xi are boolean •  trying proving that to yourself •  Train by gradient ascent estimation of w’s (no assumptions!) 
Logistic Regression 
MLE vs MAP  • Maximum conditional likelihood estimate • Maximum a posteriori estimate with prior W~N(0,σI) 

MAP estimates and Regularization • Maximum a posteriori estimate with prior W~N(0,σI) 
called a “regularization” term •  helps reduce overfitting, especially when training data is sparse •  keep weights nearer to zero (if P(W) is zero mean Gaussian prior), or whatever the prior suggests •  used very frequently in Logistic Regression 
Generative vs. Discriminative Classifiers Training classifiers involves estimating f: X à Y, or P(Y|X)  Generative classifiers (e.g., Naïve Bayes) • Assume some functional form for P(Y), P(X|Y)  • Estimate parameters of P(X|Y), P(Y) directly from training data • Use Bayes rule to calculate P(Y=y |X= x) Discriminative classifiers (e.g., Logistic regression) • Assume some functional form for P(Y|X) • Estimate parameters of P(Y|X) directly from training data • NOTE: even though our derivation of the form of P(Y|X) made GNB-style assumptions, the training procedure for Logistic Regression does not! 
Use Naïve Bayes or Logisitic Regression? Consider • Restrictiveness of modeling assumptions (how well can we learn with infinite data?)   • Rate of convergence (in amount of training data)  toward asymptotic (infinite data) hypothesis  – i.e., the learning curve 
Naïve Bayes vs Logistic Regression Consider Y boolean, Xi continuous, X=<X1 ... Xn>  Number of parameters: • NB: 4n +1 • LR: n+1 Estimation method: • NB parameter estimates are uncoupled • LR parameter estimates are coupled  

Gaussian Naïve Bayes – Big Picture assume P(Y=1) = 0.5 
Gaussian Naïve Bayes – Big Picture assume P(Y=1) = 0.5 
G.Naïve Bayes vs. Logistic Regression Recall two assumptions deriving form of LR from GNBayes: 1. Xi conditionally independent of Xk given Y 2. P(Xi | Y = yk)  =  N(µik,σi),   ß not N(µik,σik) Consider three learning methods: • GNB (assumption 1 only) • GNB2 (assumption 1 and 2) • LR   Which method works better if we have infinite training data, and...  • Both (1) and (2) are satisfied • Neither (1) nor (2) is satisfied • (1) is satisfied, but not (2)  [Ng & Jordan, 2002] 
G.Naïve Bayes vs. Logistic Regression Recall two assumptions deriving form of LR from GNBayes: 1. Xi conditionally independent of Xk given Y 2. P(Xi | Y = yk)  =  N(µik,σi),   ß not N(µik,σik) Consider three learning methods: • GNB (assumption 1 only)     -- decision surface can be non-linear • GNB2 (assumption 1 and 2) – decision surface linear • LR                                         -- decision surface linear, trained differently  Which method works better if we have infinite training data, and...  • Both (1) and (2) are satisfied:    LR = GNB2 = GNB • Neither (1) nor (2) is satisfied:   LR > GNB2,   GNB>GNB2 • (1) is satisfied, but not (2) :        GNB > LR,   LR > GNB2  [Ng & Jordan, 2002] 
G.Naïve Bayes vs. Logistic Regression What if we have only finite training data?  They converge at different rates to their asymptotic (∞ data) error  Let          refer to expected error of learning algorithm A after n training examples  Let d be the number of features: <X1 … Xd>          So, GNB requires n = O(log d) to converge, but LR requires n = O(d) [Ng & Jordan, 2002] 

Some experiments from UCI data sets 
[Ng & Jordan, 2002]  
Naïve Bayes vs. Logistic Regression The bottom line:  GNB2 and LR both use linear decision surfaces, GNB need not  Given infinite data, LR is better than GNB2 because training procedure does not make assumptions 1 or 2 (though our derivation of the form of P(Y|X) did).  But GNB2 converges more quickly to its perhaps-less-accurate asymptotic error  And GNB is both more biased (assumption1) and less (no assumption 2) than LR, so either might beat the other 
What you should know: • Logistic regression – Functional form follows from Naïve Bayes assumptions • For Gaussian Naïve Bayes assuming variance σi,k = σi • For discrete-valued Naïve Bayes too – But training procedure picks parameters without the conditional independence assumption – MCLE training: pick W to maximize P(Y | X, W) – MAP training: pick W to maximize P(W | X,Y) • regularization:   e.g., P(W)  ~ N(0,σ) • helps reduce overfitting  • Gradient ascent/descent – General approach when closed-form solutions for MLE, MAP are unavailable • Generative vs. Discriminative classifiers – Bias vs. variance tradeoff 
Regression So far, we’ve been interested in learning P(Y|X) where Y has discrete values (called ‘classification’)  What if Y is continuous? (called ‘regression’) • predict weight from gender, height, age, … • predict Google stock price today from Google, Yahoo, MSFT prices yesterday • predict each pixel intensity in robot’s current camera image, from previous image and previous action 
Regression Wish to learn f:XàY, where Y is real, given {<x1,y1>…<xn,yn>}  Approach:  1. choose some parameterized form for P(Y|X; θ) ( θ is the vector of parameters)  2. derive learning algorithm as MCLE or MAP estimate for θ 
1. Choose parameterized form for P(Y|X; θ)  Assume Y is some deterministic f(X), plus random noise   Therefore Y is a random variable that follows the distribution   and the expected value of y for any given x is f(x)  Y X 
where 

Consider Linear Regression    E.g., assume f(x) is linear function of x      Notation: to make our parameters explicit, let’s write     

Training Linear Regression    How can we learn W from the training data?   

Training Linear Regression    How can we learn W from the training data?  Learn Maximum Conditional Likelihood Estimate!     where   

Training Linear Regression  Learn Maximum Conditional Likelihood Estimate   where        

Training Linear Regression  Learn Maximum Conditional Likelihood Estimate   where      so:   

Training Linear Regression  Learn Maximum Conditional Likelihood Estimate   Can we derive gradient descent rule for training?          

How about MAP instead of MLE estimate? 

Regression – What you should know Under general assumption  1. MLE corresponds to minimizing sum of squared prediction errors 2. MAP estimate minimizes SSE plus sum of squared weights 3. Again, learning is an optimization problem once we choose our objective function • maximize data likelihood • maximize posterior prob of W 4. Again, we can use gradient descent as a general learning algorithm • as long as our objective fn is differentiable wrt W • though we might learn local optima ins  5. Almost nothing we said here required that f(x) be linear in x             

Bias/Variance Decomposition of Error 
Bias and Variance given some estimator Y for some parameter θ, we define  the bias of estimator Y =  the variance of estimator Y =  e.g., define Y as the MLE estimator for probability of heads, based on n independent coin flips  biased or unbiased?  variance decreases as sqrt(1/n) 

• Consider simple regression problem f:XàY  y = f(x) + εWhat are sources of prediction error?   noise N(0,σ) deterministic Bias – Variance decomposition of error  Reading: Bishop chapter 9.1, 9.2 
learned estimate of f(x)  
Sources of error • What if we have perfect learner, infinite data? – Our learned h(x) satisfies h(x)=f(x) – Still have remaining, unavoidable error                                                                   σ2 
Sources of error • What if we have only n training examples? • What is our expected error – Taken over random training sets of size n, drawn from distribution D=p(x,y) 

Sources of error 

Impact	
  of	
  Deep	
  Learning	
  • 	
  Speech	
  Recogni4on	
  • 	
  Computer	
  Vision	
  • 	
  Language	
  Understanding	
  	
  • 	
  Recommender	
  Systems	
  	
  • 	
  Drug	
  Discovery	
  and	
  Medical	
  Image	
  Analysis	
  	
  
[Courtesy	
  of	
  R.	
  Salakhutdinov]	
  
Deep Belief Networks: Training [Hinton & Salakhutdinov, 2006] 

Very Large Scale Use of DBN’s Data: 10 million 200x200 unlabeled images, sampled from YouTube Training: use 1000 machines (16000 cores) for 1 week Learned network: 3 multi-stage layers, 1.15 billion parameters Achieves 15.8% (was 9.5%) accuracy classifying 1 of 20k ImageNet items [Quoc Le, et al., ICML, 2012] 
Real images that most excite the feature: Image synthesized to most excite the feature: 
Restricted	
  Boltzmann	
  Machines	
  
RBM	
  is	
  a	
  Markov	
  Random	
  Field	
  with:	
  • 	
  Stochas4c	
  binary	
  hidden	
  variables	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  • 	
  Bipar4te	
  connec4ons.	
  
Pair-­‐wise	
  Unary	
  
• 	
  Stochas4c	
  binary	
  visible	
  variables	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  
Markov	
  random	
  ﬁelds,	
  Boltzmann	
  machines,	
  log-­‐linear	
  models.	
  	
  
Image	
  	
  	
  	
  	
  	
  visible	
  variables	
  	
  	
  hidden	
  variables	
  
Graphical	
  Models:	
  Powerful	
  framework	
  for	
  represen4ng	
  dependency	
  structure	
  between	
  random	
  variables.	
  Feature	
  Detectors	
  
[Courtesy,	
  R.	
  Salakhutdinov]	
  
Model	
  Learning	
  
Image	
  	
  	
  	
  	
  	
  visible	
  units	
  	
  	
  Hidden	
  units	
  
Given	
  a	
  set	
  of	
  i.i.d.	
  training	
  examples	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  ,	
  we	
  want	
  to	
  learn	
  	
  model	
  parameters	
  	
  	
  	
  	
  	
  	
  	
  .	
  	
  	
  	
  
Maximize	
  log-­‐likelihood	
  objec4ve:	
  
Deriva4ve	
  of	
  the	
  log-­‐likelihood:	
  
[Courtesy,	
  R.	
  Salakhutdinov]	
  
Image	
  Low-­‐level	
  features:	
  Edges	
  Input:	
  Pixels	
  Built	
  from	
  unlabeled	
  inputs.	
  	
  Deep	
  Boltzmann	
  Machines	
  
(Salakhutdinov & Hinton, Neural Computation 2012)[Courtesy,	
  R.	
  Salakhutdinov]	
  
Image	
  Higher-­‐level	
  features:	
  Combina4on	
  of	
  edges	
  Low-­‐level	
  features:	
  Edges	
  Input:	
  Pixels	
  Built	
  from	
  unlabeled	
  inputs.	
  	
  Deep	
  Boltzmann	
  Machines	
  Learn	
  simpler	
  representa4ons,	
  then	
  compose	
  more	
  complex	
  ones	
  
(Salakhutdinov 2008, Salakhutdinov & Hinton 2012)[Courtesy,	
  R.	
  Salakhutdinov]	
  
Model	
  Formula4on	
  h3h2h1vW3W2W1Input	
  
Same	
  as	
  RBMs	
  
requires	
  approximate	
  inference	
  to	
  train,	
  but	
  it	
  can	
  be	
  done…	
  and	
  scales	
  to	
  millions	
  of	
  examples	
  
[Courtesy,	
  R.	
  Salakhutdinov]	
  
Samples	
  Generated	
  by	
  the	
  Model	
  
Model-­‐Generated	
  Samples	
  Data	
  
[Courtesy,	
  R.	
  Salakhutdinov]	
  Training	
  Data	
  
Handwri4ng	
  Recogni4on	
  
Learning	
  Algorithm	
  Error	
  Logis4c	
  regression	
  12.0%	
  K-­‐NN	
  	
  3.09%	
  Neural	
  Net	
  (Pla_	
  2005)	
  1.53%	
  SVM	
  (Decoste	
  et.al.	
  2002)	
  1.40%	
  Deep	
  Autoencoder	
  (Bengio	
  et.	
  al.	
  2007)	
  	
  1.40%	
  Deep	
  Belief	
  Net	
  (Hinton	
  et.	
  al.	
  2006)	
  	
  1.20%	
  DBM	
  	
  0.95%	
  
Learning	
  Algorithm	
  Error	
  Logis4c	
  regression	
  22.14%	
  K-­‐NN	
  	
  18.92%	
  Neural	
  Net	
  14.62%	
  SVM	
  (Larochelle	
  et.al.	
  2009)	
  9.70%	
  Deep	
  Autoencoder	
  (Bengio	
  et.	
  al.	
  2007)	
  	
  10.05%	
  Deep	
  Belief	
  Net	
  (Larochelle	
  et.	
  al.	
  2009)	
  	
  9.68%	
  DBM	
  8.40%	
  MNIST	
  Dataset	
  Op4cal	
  Character	
  Recogni4on	
  60,000	
  examples	
  of	
  10	
  digits	
  42,152	
  examples	
  of	
  26	
  English	
  le_ers	
  	
  
Permuta4on-­‐invariant	
  version.	
  [Courtesy,	
  R.	
  Salakhutdinov]	
  
3-­‐D	
  object	
  Recogni4on	
  
Learning	
  Algorithm	
  Error	
  Logis4c	
  regression	
  22.5%	
  K-­‐NN	
  (LeCun	
  2004)	
  18.92%	
  SVM	
  (Bengio	
  &	
  LeCun	
  	
  2007)	
  11.6%	
  Deep	
  Belief	
  Net	
  (Nair	
  &	
  Hinton	
  	
  2009)	
  	
  9.0%	
  DBM	
  7.2%	
  Pa_ern	
  Comple4on	
  
NORB	
  Dataset:	
  24,000	
  examples	
  
[Courtesy,	
  R.	
  Salakhutdinov]	
  
Learning	
  Shared	
  Representa4ons	
  Across	
  Sensory	
  Modali4es	
  “Concept”	
  
sunset,	
  paciﬁc	
  ocean,	
  baker	
  beach,	
  seashore,	
  ocean	
  [Courtesy,	
  R.	
  Salakhutdinov]	
  
0	
  0	
  1	
  0	
  0	
  
Dense,	
  real-­‐valued	
  image	
  features	
  
Gaussian	
  model	
  Replicated	
  Sojmax	
  
Mul4modal	
  DBM	
  
Word	
  counts	
  (Srivastava & Salakhutdinov, NIPS 2012, JMLR 2014)	
  [Courtesy,	
  R.	
  Salakhutdinov]	
  
Mul4modal	
  DBM	
  
0	
  0	
  1	
  0	
  0	
  
Dense,	
  real-­‐valued	
  image	
  features	
  
Gaussian	
  model	
  Replicated	
  Sojmax	
  
Word	
  counts	
  
(Srivastava & Salakhutdinov, NIPS 2012, JMLR 2014)	
  [Courtesy,	
  R.	
  Salakhutdinov]	
  
Gaussian	
  model	
  Replicated	
  Sojmax	
  
0	
  0	
  1	
  0	
  0	
  
Mul4modal	
  DBM	
  
Word	
  counts	
  
Dense,	
  real-­‐valued	
  image	
  features	
  (Srivastava & Salakhutdinov, NIPS 2012, JMLR 2014)	
  [Courtesy,	
  R.	
  Salakhutdinov]	
  
0	
  0	
  1	
  0	
  0	
  
Dense,	
  real-­‐valued	
  image	
  features	
  Word	
  counts	
  
Gaussian	
  model	
  Replicated	
  Sojmax	
  
Mul4modal	
  DBM	
  
Bo_om-­‐up	
  +	
  Top-­‐down	
  
(Srivastava & Salakhutdinov, NIPS 2012, JMLR 2014)	
  [Courtesy,	
  R.	
  Salakhutdinov]	
  
0	
  0	
  1	
  0	
  0	
  
Dense,	
  real-­‐valued	
  image	
  features	
  Word	
  counts	
  
Gaussian	
  model	
  Replicated	
  Sojmax	
  
Mul4modal	
  DBM	
  
Bo_om-­‐up	
  +	
  Top-­‐down	
  
(Srivastava & Salakhutdinov, NIPS 2012, JMLR 2014)	
  
[Courtesy,	
  R.	
  Salakhutdinov]	
  
Text	
  Generated	
  from	
  Images	
  
canada,	
  nature,	
  sunrise,	
  ontario,	
  fog,	
  mist,	
  bc,	
  morning	
  insect,	
  bu_erﬂy,	
  insects,	
  bug,	
  bu_erﬂies,	
  lepidoptera	
  graﬃ4,	
  streetart,	
  stencil,	
  s4cker,	
  urbanart,	
  graﬀ,	
  sanfrancisco	
  portrait,	
  child,	
  kid,	
  ritra_o,	
  kids,	
  children,	
  boy,	
  cute,	
  boys,	
  italy	
  dog,	
  cat,	
  pet,	
  ki_en,	
  puppy,	
  ginger,	
  tongue,	
  ki_y,	
  dogs,	
  furry	
  sea,	
  france,	
  boat,	
  mer,	
  beach,	
  river,	
  bretagne,	
  plage,	
  bri_any	
  
Given	
  	
  	
  Generated	
  	
  	
  Given	
  	
  	
  Generated	
  	
  	
  
[Courtesy,	
  R.	
  Salakhutdinov]	
  
Text	
  Generated	
  from	
  Images	
  Given	
  	
  	
  Generated	
  	
  	
  
water,	
  glass,	
  beer,	
  bo_le,	
  drink,	
  wine,	
  bubbles,	
  splash,	
  drops,	
  drop	
  portrait,	
  women,	
  army,	
  soldier,	
  mother,	
  postcard,	
  soldiers	
  
obama,	
  barackobama,	
  elec4on,	
  poli4cs,	
  president,	
  hope,	
  change,	
  sanfrancisco,	
  conven4on,	
  rally	
  
Images	
  Selected	
  from	
  Text	
  water,	
  red,	
  sunset	
  
nature,	
  ﬂower,	
  red,	
  green	
  
blue,	
  green,	
  yellow,	
  colors	
  chocolate,	
  cake	
  
Given	
  	
  	
  Retrieved	
  
[Courtesy,	
  R.	
  Salakhutdinov]	
  
Summary	
  • Eﬃcient	
  learning	
  algorithms	
  for	
  Deep	
  Learning	
  Models.	
  Learning	
  more	
  adap4ve,	
  robust,	
  and	
  structured	
  representa4ons.	
  	
  	
  
• Deep	
  models	
  improve	
  the	
  current	
  state-­‐of-­‐the	
  art	
  in	
  many	
  applica4on	
  domains:	
  Ø Object	
  recogni4on	
  and	
  detec4on,	
  text	
  and	
  image	
  retrieval,	
  handwri_en	
  character	
  and	
  speech	
  recogni4on,	
  and	
  others.	
  
HMM	
  decoder	
  Speech	
  RecogniGon	
  
sunset,	
  paciﬁc	
  ocean,	
  beach,	
  seashore	
  	
  	
  	
  	
  	
  MulGmodal	
  Data	
  
	
  	
  	
  	
  	
  CapGon	
  GeneraGon	
  
Text	
  &	
  image	
  retrieval	
  /	
  	
  Object	
  recogniGon	
  
Learning	
  a	
  Category	
  Hierarchy	
  
mosque,	
  tower,	
  building,	
  cathedral,	
  dome,	
  castle	
  Image	
  Tagging	
  
[Courtesy,	
  R.	
  Salakhutdinov]	
  
CMSC 422 Introduction to Machine LearningLecture 7 The PerceptronFurong Huang / furongh@cs.umd.eduSlides adapted from Prof. Carpuat
This weekqThe perception: a new model/algorithmqits variants: voted, averagedqconvergence proofqFundamental Machine Learning ConceptsqOnline vs. batch learningqError-driven learningqLinear separabilityand margin of a datasetqProject 1 published today
Recap: Perceptron for binary classification•Classifier = hyperplane that separates positive from negative examples!"=$%&'()*++-)•Perceptron training•Finds such a hyperplane•Online & error-driven

Learning•Find algorithm that gives us !and "for a given data set D	(&,()•Many algorithms possible•Once we know !and "we can predict the class of a new data point &*by evaluating(+=-./0(!1&*+")•We learned a particular way of finding these parameters–via the perceptron update rule•Iterative online algorithm–visits all the data over epochs
Perceptron update: geometric interpretation
!"#$A training example %,'is misclassified, i.e.,()*+,"#$-%+/≠'Let’s say '=+1
Perceptron update: geometric interpretation
!"#$Update: %&'(←%"#$++,, i.e., %&'(←%"#$+,
Perceptron update: geometric interpretation
!"#$!%&'Update: (%&'←("#$++,, i.e., (%&'←("#$+,
Recap: Perceptron updates
!= 1
Recap: Perceptron updates
!= -1
Today•Example of perceptron + averaged perceptron training•Perceptron convergence proof•Fundamental Machine Learning Concepts•Linear separabilityand margin of a dataset
Standard Perceptron: predict based on final parameters

Predict based on final + intermediate parameters•The voted perceptron•The averaged perceptron•Require keeping track of “survival time” ofweight vectors

Averaged perceptron decision rule
can be rewritten as
Averaged Perceptron: predict based on average of intermediate parameters

Convergence of Perceptron•The perceptron has converged if it can classify every training example correctly•i.e. if it has found a hyperplane that correctly separates positive and negative examples•Under which conditions does the perceptron converge and how long does it take?
Convergence of Perceptron

Margin of a data set D
Distance between the hyperplane (w,b) and the nearest point in DLargest attainable margin on D

What does this mean?•Perceptron converges quickly when margin is large, slowly when it is small•Bound does not depend on number of training examples N, nor on number of features d•Proof guarantees that perceptron converges, but not necessarily to the max margin separator
What you should know•Perceptron concepts•training/prediction algorithms (standard, voting, averaged)•convergence theorem and what practical guarantees it gives us•how to draw/describe the decision boundary of a perceptron classifier•Fundamental ML concepts•Determine whether a data set is linearly separable and define its margin•Error driven algorithms, online vs. batch algorithms
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
COMS 4721: Machine Learning for Data Science
Lecture 11, 2/23/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
MAXIMUM MARGIN CLASSIFIERS
MAXIMUM MARGIN IDEA
Setting
Linear classiﬁcation, two linearly separable classes.
Recall Perceptron
ISelects some hyperplane separating the classes.
ISelected hyperplane depends on several factors.
Maximum margin
To achieve good generalization (low prediction error), place the hyperplane
“in the middle” between the two classes.
More precisely, choose a plane such that its distance to the closest point in
each class is maximized. This distance is called the margin .
GENERALIZATION ERROR
Possible Perceptron solution
 (dotted) poor generalization, (solid) better Maximum margin solution
Example: Gaussian data
IIntuitively, the classiﬁer on the left isn’t good because sampling more data
could lead to misclassiﬁcations.
IIf we imagine the data from each class as Gaussian, we could frame the goal as
to ﬁnd a decision boundary that cuts into as little probability mass as possible.
IWith no distribution assumptions, we can argue that max-margin is best.
SUBSTITUTING CONVEX SETS
Observation
Where a separating hyperplane may be placed depends on the “outer” points
on the sets. Points in the center do not matter.
In geometric terms, we can represent each class by the smallest convex set
which contains all point in the class. This is called a convex hull .
SUBSTITUTING CONVEX SETS
Convex hulls
The thing to remember for this lecture is that a convex hull is deﬁned by all
possible weighted averages of points in a set.
That is, let x1;:::; xnbe the above data coordinates. Every point x0in the
shaded region – i.e., the convex hull – can be reached by setting
x0=nX
i=1ixi;  i0;nX
i=1i=1;
for some (1;:::; n). No point outside this region can be reached this way.
MARGIN
Deﬁnition
Themargin of a classifying hyperplane His the shortest distance between
the plane and any point in either set (equivalently, the convex hull)
When we maximize this margin, His “exactly in the middle” of the two
convex hulls. Of course, the difﬁcult part is how do we ﬁnd this H?
SUPPORT VECTOR MACHINES
SUPPORT VECTOR MACHINE
Finding the hyperplane
Fornlinearly separable points (x1;y1);:::; (xn;yn)with yi2f 1g, solve:
min
w;w01
2kwk2
subject to yi(xT
iw+w0)1 for i=1;:::; n
Comments
IRecall that yi(xT
iw+w0)>0 ifyi=sign(xT
iw+w0).
IIf there exists a hyperplane Hthat separates the classes, we can scale w
so that yi(xT
iw+w0)>1 for all i(this is useful later).
IThe resulting classiﬁer is called a support vector machine . This
formulation only has a solution when the classes are linearly separable.
IIt is not at all obvious why this maximizes the margin. This will
become more clear when we look at the solution.
SUPPORT VECTOR MACHINE
Skip to the end
Q: First, can we intuitively say what the
solution should look like?
A: Yes, but we won’t give the proof.
1:Find the closest two points from the
convex hulls of class +1 and 1.
2:Connect them with a line and put a perpendicular hyperplane in the middle.
3:IfS1andS0are the sets of xin class +1 and 1 respectively, we’re looking
for two probability vectors1and0such that we minimize



(P
xi2S11ixi)
|{z}
in conv. hull of S1 (P
xi2S00ixi)
|{z}
in conv. hull of S0


2
4:Then we deﬁne the hyperplane using the two points found with 1and0.
PRIMAL AND DUAL PROBLEMS
Primal problem
Theprimal optimization problem is the one we deﬁned:
min
w;w01
2kwk2
subject to yi(xT
iw+w0)1 for i=1;:::; n
This is tricky, so we use Lagrange multipliers to set up the “dual” problem.
Lagrange multipliers
Deﬁne Lagrange multipliers i>0 for i=1;:::; n. The Lagrangian is
L=1
2kwk2 nX
i=1i(yi(xT
iw+w0) 1)
=1
2kwk2 nX
i=1iyi(xT
iw+w0) +nX
i=1i
We want to minimize Lover wandw0and maximize over (1;:::; n).
SETTING UP THE DUAL PROBLEM
First minimize over wandw0:
L=1
2kwk2 nX
i=1iyi(xT
iw+w0) +nX
i=1i
+
rwL=w nX
i=1iyixi=0) w=nX
i=1iyixi
@L
@w0= nX
i=1iyi=0)nX
i=1iyi=0
Therefore,
1. We can plug the solution for wback into the problem.
2. We know that (1;:::; n)must satisfyPn
i=1iyi=0.
SVM DUAL PROBLEM
Lagrangian :L=1
2kwk2 Pn
i=1iyi(xT
iw+w0) +Pn
i=1i
Dual problem
Plugging these values in from the previous slide, we get the dual problem
max
1;:::; nL=nX
i=1i 1
2nX
i=1nX
j=1ijyiyj(xT
ixj)
subject tonX
i=1iyi=0;  i0 for i=1;:::; n
Comments
IWhere did w0go? The conditionPn
i=1iyi=0 gives 0w0in the dual.
IWe now maximize over the i. This requires an algorithm that we won’t
discuss in class. Many good software implementations are available.
AFTER SOLVING THE DUAL
Solving the primal problem
Before discussing the solution of the dual, we ask:
After ﬁnding each ihow do we predict a new y 0=sign(xT
0w+w0) ?
We have:L=1
2kwk2 Pn
i=1i(yi(xT
iw+w0) 1)
With conditions: i0; yi(xT
iw+w0) 10
Solve for w.
rwL=0) w=nX
i=1iyixi(just plug in the learned i’s)
What about w0?
IWe can show that at the solution, i(yi(xT
iw+w0) 1) =0 for all i.
ITherefore, pick ifor whichi>0 and solve yi(xT
iw+w0) 1=0 for
w0using the solution for w(all possible iwill give the same solution).
UNDERSTANDING THE DUAL
Dual problem
We can manipulate the dual problem to ﬁnd out what it’s trying to do.
max
1;:::; nL=nX
i=1i 1
2nX
i=1nX
j=1ijyiyj(xT
ixj)
subject tonX
i=1iyi=0;  i0 for i=1;:::; n
Since yi2f  1;+1g
InX
i=1iyi=0)C=X
i2S1i=X
j2S0j
InX
i=1nX
j=1ijyiyj(xT
ixj) =


nX
i=1iyixi


2
=C2


X
i2S1i
Cxi X
j2S0j
Cxj


2
UNDERSTANDING THE DUAL
Dual problem
We can change notation to write the dual as
max
1;:::; nL=2C 1
2C2


X
i2S1i
Cxi X
j2S0j
Cxj


2
subject to C:=X
i2S1i=X
j2S0j;  i0
We observe that the maximum of this function satisﬁes
min
1;:::; n


P
i2S1i
Cxi
|{z}
in conv. hull of S1 P
j2S0j
Cxj
|{z}
in conv. hull of S0


2
Therefore, the dual problem is trying to ﬁnd the closest points in the convex
hulls constructed from data in class +1 and 1.
RETURNING TO THE PICTURE
Recall
We wanted to ﬁnd:
min
u2H(S1)
v2H(S0)ku vk2
The direction of wisu v.
We previously claimed we can ﬁnd the max-margin hyperplane as follows:
1. Find shortest line connecting the convex hulls.
2. Place hyperplane orthogonal to line and exactly at the midpoint.
With the SVM we want to minimize kwk2and we can write this solution as
w=nX
i=1iyixi=C0
@X
i2S1i
Cxi X
j2S0j
Cxj1
A
SOFT-MARGIN SVM
Question : What if the data isn’t linearly separable?
Answer : Permit training data be on wrong side of hyperplane, but at a cost.
Slack variables
Replace the training rule yi(xT
iw+w0)1 with
yi(xT
iw+w0)1 i;
withi0.
Theiare called slack variables .

>1
< 1
= 0= 0
SOFT-MARGIN SVM
Soft-margin objective function
Adding the slack variables gives a new objective to optimize
min
w;w0;1;:::; n1
2kwk2+nX
i=1i
subject to yi(xT
iw+w0)1 ifor i=1;:::; n
i0 for i=1;:::; n
We also have to choose the parameter > 0. We solve the dual as before.
Role of
ISpeciﬁes the “cost” of allowing a point on the wrong side.
IIfis very small, we’re happy to misclassify.
IFor!1 , we recover the original SVM because we want i=0.
IWe can use cross-validation to choose it.
INFLUENCE OF MARGIN PARAMETER
=100000
 =0:01
Hyperplane is sensitive to . Either way, a linear classiﬁer isn’t ideal :::
KERNELIZING THE SVM
Primal problem with slack variables
Let’s map the data into higher dimensions using the function (xi),
min
w;w0;1;:::; n1
2kwk2+nX
i=1i
subject to yi((xi)Tw+w0)1 ifor i=1;:::; n
i0 for i=1;:::; n
Dual problem
Maximize over each (i;i)and minimize over w;w0;1;:::; n
L=1
2kwk2+nX
i=1i nX
i=1i(yi((xi)Tw+w0) 1+i) nX
i=1ii
subject toi0;  i0;yi((xi)Tw+w0) 1+i0
KERNELIZING THE SVM
Dual problem
Minimizing for w,w0and eachi, we ﬁnd
w=nX
i=1iyixi;nX
i=1iyi=0;  i i=0
If we plug wandi= iback into theL, we have the dual problem
max
1;:::; nL=nX
i=1i 1
2nX
i=1nX
j=1ijyiyj(xi)T(xj)|{z}
K(xi;xj)
subject toPn
i=1iyi=0; 0i
Classiﬁcation : Using the solution w=Pn
i=1iyi(xi), declare
y0=signnX
i=1iyi(x0)T(xi) +w0
=signnX
i=1iyiK(x0;xi) +w0
KERNELIZING THE SVM
Elements of Statistical Learning (2nd Ed.)c/circlecopyrtHastie, Tibshirani & Friedman 2009 Chap 12SVM - Degree-4 Polynomial in Feature Space
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................ ..................... ........................ ......... .................. .......... .................. ........... .................. ............. .................... .............. .................... ............... .................... ................ ..................... ................ ..................... ................. ..................... .................. ....................... ................... ....................... .................... ....................... .................... ........................ ..................... .......................... ...................... ........................... ........................ ...................................................... ...................................................... ...................................................... ...................................................... ..................................................... ...................................................... ...................................................... ...................................................... ...................................................... ...................................................... ...................................................... ...................................................... ...................................................... ....................................................... ....................................................... ...................................................... ....................................................... ....................................................... ........................................................ ........................................................ ......................................................... .......................................................... .......................................................... ........................................................... ............................................................. .............................................................. ..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... .............................................. ............................................ ......................................... ........................................ ....................................... ................................... .................................. ................................. ................................ ............................... .............................. ........................... .......................... ......................... ........................ ..................... .................. .................................................................................................................................................................................................................................................................................................................................................................ooooooooooooooooooooooo
oooo
oooo
oooooooooooooo
ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo
ooooooooooooooooooooo
oooooooooooooooooooooooooooooooooooooo
ooooooooooooooooooooooooooooooooooooo•••••••••••••••
Training Error: 0.180Test Error:       0.245Bayes Error:    0.210SVM - Radial Kernel in Feature Space
.................................................................................................................................................................................................................................. ................ ................... ...................... ....................... ......................... ........................... .......... .................... .......... ....................... ........... ....................... ........... ......................... ........... ........................... ........... ............................. ............ ............................. ............ .............................. ............ ................................. ............ .................................. ............ .................................. ............ ................................... ............. ..................................... ............. ....................................... .............. ........................................................ ......................................................... ......................................................... ......................................................... ......................................................... ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ....................................................... ....................................................... ....................................................... ....................................................... ....................................................... ........................................................ ........................................................ ........................................................ ....................................................... ........................................................ ........................................................ ........................................................ ....................................................... ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ........................................................ ....................................................... ........................................................ ........................................................ ...
........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... .................................................... ................................................. .............................................. .............................................. ........................................... .......................................... .............. ......................... ............. ...................... ............. ...................... ............. .................... ............ ................... ............ ................ ............ ................ ............ ............... ........... ............. ........... ........... ............ ........... ............ ......... ................... ................ ..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
ooooooooooooooooooooooo
oooo
oooo
oooooooooooooo
ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo
ooooooooooooooooooooo
oooooooooooooooooooooooooooooooooooooo
ooooooooooooooooooooooooooooooooooooo•••••••••••••••••••••••••••••••
Training Error: 0.160Test Error:       0.218Bayes Error:    0.210FIGURE 12.3.Two nonlinear SVMs for the mix-ture data. The upper plot uses a4th degree polynomial
Black solid line
SVM decision boundary
Classiﬁcation rule
signnX
i=1iyiK(x0;xi)+w0
Dots
Support vectors ( i>0)
Purple line
A Bayes classiﬁer.
SUMMARY : SUPPORT VECTOR MACHINE
Basic SVM
ILinear classiﬁer for linearly separable data.
IPosition of afﬁne hyperplane is determined to maximize the margin.
IThe dual is a convex, so we can ﬁnd exact solution with optimization.
Full-ﬂedged SVM
Ingredient Purpose
Maximum margin Good generalization properties
Slack variables Overlapping classes, robust against outliers
Kernel Nonlinear decision boundary
Use in practice
ISoftware packages (many options)
IChoose a kernel function (e.g., RBF)
ICross-validate parameter and RBF kernel width
Discriminative
Language Models
Prof. Sameer Singh
CS 295: STATISTICAL NLP
WINTER 2017
January 26, 2017
Based on slides from Noah Smith, Richard Socher, and everyone e lse they copied from.
Language Models
CS 295: STATISTICAL NLP (WINTER 2017) 2Probability of a Sentence
•Is a given sentence something you would expect to see?
•Syntactically (grammar) and Semantically (meaning)
Probability of the Next Word
•Predict what comes next for a given sequence of words.
•Think of it as V‐way classification
Outline
CS 295: STATISTICAL NLP (WINTER 2017) 3Discriminative Language Models
Feed‐forward Neural Networks
Recurrent Neural Networks
Upcoming..
Outline
CS 295: STATISTICAL NLP (WINTER 2017) 4Discriminative Language Models
Feed‐forward Neural Networks
Recurrent Neural Networks
Upcoming..
Logistic Regression Model
CS 295: STATISTICAL NLP (WINTER 2017) 5
N‐Grams as Logistic Reg.
CS 295: STATISTICAL NLP (WINTER 2017) 6
Other features…
CS 295: STATISTICAL NLP (WINTER 2017) 7
Outline
CS 295: STATISTICAL NLP (WINTER 2017) 8
Discriminative Language Models
Feed‐forward Neural Networks
Recurrent Neural Networks
Upcoming..
Logistic Reg. w/ Embeddings
CS 295: STATISTICAL NLP (WINTER 2017) 9
Neural Networks
CS 295: STATISTICAL NLP (WINTER 2017) 10
Activation Functions
CS 295: STATISTICAL NLP (WINTER 2017) 11And many others…ReLUs, PReLUs, ELU, step, max, and so on..sigmoid
tanhsoftmax
Why do they work?
CS 295: STATISTICAL NLP (WINTER 2017) 12
https://colah.github.io
Why do they work?
CS 295: STATISTICAL NLP (WINTER 2017) 13
x1x2
yz
Simulated Example
CS 295: STATISTICAL NLP (WINTER 2017) 14
https://github.com/clab/cnn/b lob/master/examples/xor.cc
Simple Feedforward NN LM
CS 295: STATISTICAL NLP (WINTER 2017) 15Bigram Model
Simple Feedforward NN LM
CS 295: STATISTICAL NLP (WINTER 2017) 16N‐gram Model
Deep Feedforward NN LM
CS 295: STATISTICAL NLP (WINTER 2017) 17
Bengioet al. 2003
Outline
CS 295: STATISTICAL NLP (WINTER 2017) 18
Discriminative Language Models
Feed‐forward Neural Networks
Recurrent Neural Networks
Upcoming..
Sequence View of Simple NNs
CS 295: STATISTICAL NLP (WINTER 2017) 19
Recurrent Neural Networks
CS 295: STATISTICAL NLP (WINTER 2017) 20
Example: “I love food”
CS 295: STATISTICAL NLP (WINTER 2017) 21I love foodlove food <eos>
Power of RNNs: Characters!
CS 295: STATISTICAL NLP (WINTER 2017) 22
http://karpathy.github.io/2015/05/21/rnn‐effectiveness/
Char‐RNNs: Shakespeare!
CS 295: STATISTICAL NLP (WINTER 2017) 23

Char‐RNNs: Wikipedia!
CS 295: STATISTICAL NLP (WINTER 2017) 24

Char‐RNNs: Linux Code!
CS 295: STATISTICAL NLP (WINTER 2017) 25

Extension: Stacking
CS 295: STATISTICAL NLP (WINTER 2017) 26
Extension: Bidirectional RNNs
CS 295: STATISTICAL NLP (WINTER 2017) 27
Deep Bidirectional RNNs
CS 295: STATISTICAL NLP (WINTER 2017) 28
Extension: GRUs
CS 295: STATISTICAL NLP (WINTER 2017) 29Gated Recurrent Units
Extension: GRUs
CS 295: STATISTICAL NLP (WINTER 2017) 30Gated Recurrent Units
Estimating Parameters
CS 295: STATISTICAL NLP (WINTER 2017) 31Beyond the scope of the course
•Lots of tricks, heuristics, “domain knowledge”
•Lot of engineering for efficiency, e.g. GPUs
•New training algorithms being proposed every year
•sometimes, architecture‐specific
•Lots of available tools you can use!
•Tensorflow, Torch, Keras, MxNET, etc.
Outline
CS 295: STATISTICAL NLP (WINTER 2017) 32
Discriminative Language Models
Feed‐forward Neural Networks
Recurrent Neural Networks
Upcoming..
Homework 1 so far…
CS 295: STATISTICAL NLP (WINTER 2017) 33
Public
Private
RuslanSalakhutdinov
CS 295: STATISTICAL NLP (WINTER 2017) 34
Professor at Carnegie Mellon University
Director of Artificial Intelligence, Apple Inc.
Learning Deep Unsupervised and Multimodal Models
Location: DBH 6011
Time: 11am ‐12pm
Date:January 27, 2017
Meeting with PhD students, will post on Piazza
Upcoming…
CS 295: STATISTICAL NLP (WINTER 2017) 35•Homework 1 is due tonight:  January 26, 2017
•Write‐up, data, and code for Homework 2 is up
•Homework 2 is due:  February 9, 2017Homework
•Proposal is due:  February 7, 2017  (~2 weeks)
•Only 2 pagesProject
Discourse	and	SummarizationProf.	Sameer	SinghCS	295:	STATISTICAL	NLPWINTER	2017March	16,	2017Based	on	slides	from	Dan	Jurafsky,	Jacob	Eisenstein,	and	everyone	else	they	copied	from.
Upcoming…
•Final	report	due	in	a	week:	March	20,	2017•Instructions	up:	ACL	style,	5	pages	(+references)Project
2CS	295:	STATISTICAL	NLP	(WINTER	2017)
OutlineDiscourse
3SummarizationWrapup
CS	295:	STATISTICAL	NLP	(WINTER	2017)
OutlineDiscourse
4SummarizationWrapup
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse
5CoreferenceCoherenceRelationsResolving	entitiesand	events.What	makes	the	text	coherent?Rhetoricaland	narrativelinks	between	unitsCS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse
6CoreferenceCoherenceRelationsResolving	entitiesand	events.What	makes	the	text	coherent?Rhetoricaland	narrativelinks	between	unitsCS	295:	STATISTICAL	NLP	(WINTER	2017)
Coherence
7
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Coherence
8
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Coherence	vs	Semantics
9A	meaningless	sentence	can	be	grammatical..Colorless	green	ideas	sleep	furiouslyThe	discourse	equivalent	of	grammaticality	is	coherenceCan	a	coherent	text	be	without	meaning?CS	295:	STATISTICAL	NLP	(WINTER	2017)
Example	Essay
10
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Example	Essay
11The	second	reason	for	the	five-paragraph	theme	is	that	it	makes	you	focus	on	a	single	topic.	Some	people	start	writing	on	the	usual	topic,	like	TV	commercials,	and	they	wind	up	all	over	the	place,	talking	about	where	TV	came	from	or	capitalism	or	health	foods	or	whatever.	But	with	only	five	paragraphs	and	one	topic	you’re	not	tempted	to	get	beyond	your	original	idea,	like	commercials	are	a	good	source	of	information	about	products.	You	give	your	three	examples,	and	zap!	you’re	done.	This	is	another	way	the	five-paragraph	theme	keeps	you	from	thinking	too	much.CS	295:	STATISTICAL	NLP	(WINTER	2017)
Detecting	“Coherency”
12
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse	Connectors
13CS	295:	STATISTICAL	NLP	(WINTER	2017)
Lexical	Chains
14CS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse	Relations
151.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistants
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse	Relations
161.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistants
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse	Relations
171.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistants
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse	Relations
181.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistants
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse	Relations
191.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistants
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse	Relations
201.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistants
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Coherence	Structure
21SegmentationZoning/OrderingCentering/Salience1.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistantsCS	295:	STATISTICAL	NLP	(WINTER	2017)
Coherence	Structure
22Segmentation`Zoning/OrderingCentering/Salience1.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistantsCS	295:	STATISTICAL	NLP	(WINTER	2017)
Coherence	Structure
23SegmentationZoning/OrderingCentering/Salience1.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistants1.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistantsCS	295:	STATISTICAL	NLP	(WINTER	2017)
Coherence	Structure
24SegmentationZoning/OrderingCentering/Salience1.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistants1.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistants1.	In	today’s	society,	college	is	ambiguous.2.	We	need	it	to	live,3.	but	we	also	need	it	to	love.4.	Moreover,	without	college	most	of	the	world’s	learning	would	be	egregious.5.	College,	however,	has	myriad	costs.6.	One	of	the	most	important	issues	facing	the	world	is	how	to	reduce	college	costs.7.	Some	have	argued	that	college	costs	are	due	to	the	luxuries	students	now	expect.8.	Others	have	argued	that	the	costs	are	a	result	of	athletics.9.	In	reality,	high	college	costs	are	the	result	of	excessive	pay	for	teaching	assistantsCS	295:	STATISTICAL	NLP	(WINTER	2017)
Applications	of	Coherence
25Sentence	OrderingWhen	generating	summaries,reorder	till	sentences	are	coherent.ReadabilityAssessmentIs	a	piece	of	text	easily	readable?
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse
26CoreferenceCoherenceRelationsResolving	entitiesand	events.What	makes	the	text	coherent?Rhetoricaland	narrativelinks	between	unitsCS	295:	STATISTICAL	NLP	(WINTER	2017)
Discourse	Relations
27
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Use	in	Sentiment	Analysis
28
CS	295:	STATISTICAL	NLP	(WINTER	2017)
OutlineDiscourse
29SummarizationWrapup
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Text	SummarizationGoal:	produce	an	abridged	version	of	a	text	that	contains	information	that	is	important	or	relevant	to	a	user.Summarization	Applications◦outlines	or	abstracts	of	any	document,	article,	etc◦summaries	of	email	threads◦action	items	from	a	meeting◦simplifying	text	by	compressing	sentences
30CS	295:	STATISTICAL	NLP	(WINTER	2017)
What	to	summarize?	Single-document	summarization◦Given	a	single	document,	produce◦abstract◦outline◦headlineMultiple-document	summarization◦Given	a	group	of	documents,	produce	a	“gist”	:◦a	series	of	news	stories	on	the	same	event◦a	set	of	web	pages	about	some	topic	or	question
31CS	295:	STATISTICAL	NLP	(WINTER	2017)
Query-focused	vs	GenericGeneric	summarization:◦Summarize	the	content	of	a	documentQuery-focused	summarization:◦summarize	a	document	with	respect	to	an	information	need	expressed	in	a	user	query.◦a	kind	of	complex	question	answering:◦Answer	a	question	by	summarizing	a	document	that	has	the	information	to	construct	the	answer	
32CS	295:	STATISTICAL	NLP	(WINTER	2017)
Extractive	summarization	&	Abstractive	summarizationExtractive	summarization:◦create	the	summary	from	phrases	or	sentences	in	the	source	document(s)Abstractive	summarization:◦express	the	ideas	in	the	source	documents	using	(at	least	in	part)	different	words
33CS	295:	STATISTICAL	NLP	(WINTER	2017)
Summarization:	Three	Stages1.content	selection:	choose	sentences	to	extract	from	the	document2.information	ordering:	choose	an	order	to	place	them	in	the	summary3.sentence	realization:	clean	up	the	sentences
34
DocumentSentence SegmentationSentenceExtractionAll sentencesfrom documentsExtracted sentencesInformation OrderingSentence RealizationSummaryContent Selection
Sentence Simpliﬁcation
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Simplifying	sentencesappositivesRajam,	28,	an	artist	who	was	living	at	the	time	in	Philadelphia,	found	the	inspiration	in	the	back	of	city	magazines.attributionclausesRebels	agreed	to	talks	with	government	officials,	international	observers	said	Tuesday.PPs	without	namedentitiesThe	commercial	fishing	restrictions	in	Washington	will	not	be	lifted	unless	the	salmon	population	increases	[PP	to	a	sustainable	number]]initial	adverbials“For	example”,	“On	the	other	hand”,	“As	a	matter	of	fact”,	“At	this	point”CS	295:	STATISTICAL	NLP	(WINTER	2017)35Zajicet	al.	(2007),	Conroy	et	al.	(2006),	Vanderwendeet	al.	(2007)Simplest	method:	parse	sentences,	use	rules	to	decide	which	modifiers	to	prune(more	recently	a	wide	variety	of	machine-learning	methods)
ROUGE	(Recall	Oriented	Understudy	for	Gisting	Evaluation)	Intrinsic	metric	for	automatically	evaluating	summaries◦Based	on	BLEU	(a	metric	used	for	machine	translation)◦Not	as	good	as	human	evaluation	(“Did	this	answer	the	user’s	question?”)◦But	much	more	convenientGiven	a	document	D,	and	an	automatic	summary	X:◦Have	N	humans	produce	a	set	of	reference	summaries		of	D◦Run	system,	giving	automatic	summary	X◦What	percentage	of	the	bigrams	from	the	reference	summaries	appear	in	X?
36ROUGE−2=min(count(i,X),count(i,S))bigrams i∈S∑s∈{RefSummaries}∑count(i,S)bigrams i∈S∑s∈{RefSummaries}∑CS	295:	STATISTICAL	NLP	(WINTER	2017)
OutlineDiscourse
37SummarizationWrapup
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Word	out	of	Context
38CS	295:	STATISTICAL	NLP	(WINTER	2017)

Words	in	Context
39CS	295:	STATISTICAL	NLP	(WINTER	2017)

Sentences
40CS	295:	STATISTICAL	NLP	(WINTER	2017)

Information	Extraction
CS	295:	STATISTICAL	NLP	(WINTER	2017)41

Machine	Translation
42CS	295:	STATISTICAL	NLP	(WINTER	2017)

Other	“Applications”
43CS	295:	STATISTICAL	NLP	(WINTER	2017)
DocumentDocumentDocument
Document
Document
Document
Document
Document
Question ProcessingPassageRetrievalQuery FormulationAnswer Type DetectionQuestionPassage RetrievalDocument RetrievalAnswer ProcessingAnswerpassagesIndexingRelevantDocsDocument
DocumentDocument
DocumentSentence SegmentationSentenceExtractionAll sentencesfrom documentsExtracted sentencesInformation OrderingSentence RealizationSummaryContent Selection
Sentence Simpliﬁcation
Wrapupof	the	Course
CS	295:	STATISTICAL	NLP	(WINTER	2017)44

And	Now!
CS	295:	STATISTICAL	NLP	(WINTER	2017)45

Do	research	in	NLP!
46
CS	295:	STATISTICAL	NLP	(WINTER	2017)
CMSC 422 Introduction to Machine LearningLecture 4 Geometry and Nearest NeighborsFurong Huang / furongh@cs.umd.edu
What we know so farDecision Trees•What is a decision tree, and how to induce it from dataFundamental Machine Learning Concepts•Difference between memorization and generalization•What inductive bias is, and what is its role in learning•What underfittingand overfitting means•How to take a task and cast it as a learning problem•Why you should never ever touch your test data!!
Today’s Topics•Nearest Neighbors (NN) algorithms for classificationØK-NN, Epsilon ball NN•Fundamental Machine Learning ConceptsØDecision boundary

Linear Algebra•Provides compact representation of dataØFor a given example, all its features can be represented as a single vectorØAn entire dataset can be represented as a single matrix•Provide ways of manipulating these objectsØDot products, vector/matrix operations, etc•Provides formal ways of describing and discovering patterns in dataØExamples are points in a Vector SpaceØWe can use Norms and Distances to compare them•Some are valid for feature data types•Some can be made valid, with generalization ...
Mathematical view of vectors•Ordered set of numbers: (1,2,3,4)•Example: (x,y,z) coordinates of a point in space.•The 16384 pixels in a 128×128 image of a face•List of choices in the tennis example•Vectors usually indicated with bold lower case letters. Scalars with lower case•Usual mathematical operations with vectors:ØAddition operation !	+$, with:vZero%$+%=$vInverse−$+−$=%ØScalar multiplication:vDistributive rule: (!	+$=(!+($((+*)!=(!+*!
Dot Product•The dot product or, more generally, inner product of two vectors is a scalar:!!	• !"=$!$"+&!&"+'!'"(in 3D)•Useful for many purposesØComputing the Euclidean length of a vector:length(v) = sqrt(v• v)ØNormalizing a vector, making it unit-lengthØComputing the angle between two vectors: u• v= |u| |v| cos(θ)ØChecking two vectors for orthogonalityØProjecting one vector onto another•Other ways of measuring length and distance are possible

Vector Norms!=($%,$',…,$))•Two norm (Euclidean norm)!'=+$,'),-%If !'=1, !is a unit vector•Infinity norm!/=max$%,$',…,$)•One norm (Manhattan distance)!%=+$,),-%
Law of Large Numbers•Suppose that !!,!",…!#are independentand identically distributedrandom variables•The empirical sample average approaches the population average as the number of sample goes to infinityPrlim#→%1*+!&&=-!=1
Nearest Neighbor•Intuition–points close in a feature space are likely to belong to the same classØChoosing right features is very important•Nearest Neighbors (NN) algorithms for ClassificationØK-NN, Epsilon ball NN•Fundamental Machine Learning ConceptsØDecision boundary
Intuition for NearestNeighbor ClassificationThis “rule of nearest neighbor” has considerable elementary intuitive appeal and probably corresponds to practice in many situations. For example, it is possible that much medical diagnosis is influenced by the doctor’s recollection of the subsequent history of an earlier patient whose symptoms resemblein some way those of the current patient.(Fix and Hodges, 1952)
Intuition for NearestNeighbor Classification•Simple ideaØStore all training examplesØClassify new examples based on label for K closest training examplesØTraining may just involve making structures to make computing closest examples cheaper
K Nearest Neighbor Classification
Training DataK: number of neighbors that classification is based onTest instance with unknown class in {	−1;	+1	}
2 approaches to learningEager learning(egdecision trees)•Learn/TrainØInduce an abstract model from data•Test/Predict/ClassifyØApply learned model to new dataLazy learning(egnearest neighbors)•LearnØJust store data in memory•Test/Predict/ClassifyØCompare new data to stored data•PropertiesØRetains all information seen in training ØComplex hypothesis spaceØClassification can be very slow
Components of a k-NN Classifier•Distance metricØHow do we measure distance between instances?ØDetermines the layout of the example space•The k hyperparameterØHow large a neighborhood should we consider?ØDetermines the complexity of the hypothesis space
Distance metrics•We can use any distance function to select nearest neighbors.•Different distances yield different neighborhoods
L2 distance ( = Euclidean distance)L1 distanceMax norm
K=1 and VoronoiDiagrams•Imagine we are given a bunch of training examples•Find regions in the feature space which are closest to every training example•Algorithm–if our test point is in the region corresponding to a given input point–return its label

Decision Boundary of a Classifier•It is simply the line that separates positive and negative regions in the feature space•Why is it useful?Øit helps us visualize how examples will be classified for the entire feature spaceØit helps us visualize the complexity of the learned model
Decision Boundaries for 1-NN

Decision  Boundaries change with the distance function

Decision Boundaries change with K

The k hyperparameter•Tunes the complexity of the hypothesis spaceØIf k = 1, every training example has its own neighborhoodØIf k = N, the entire feature space is one neighborhood!•Higher k yields smoother decision boundaries•How would you set k in practice?
What is the inductive bias ofk-NN?•Nearby instances should have the same label•All features are equally important•Complexity is tuned by the k parameter
Variations on k-NN:Weighted voting•Weighted votingØDefault: all neighbors have equal weightØExtension: weight neighbors by (inverse) distance
Variations on k-NN:Epsilon Ball Nearest Neighbors•Same general principle as K-NN, but change the method for selecting which training examples vote•Instead of using K nearest neighbors, use all examples x such that!"#$%&'()*,)≤	.
Exercise:  How would you modify KNN-Predict  to perform Epsilon Ball NN?

Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
CMSC 422 Introduction to Machine LearningLecture 11 Review and Practice ProblemsFurong Huang / furongh@cs.umd.edu
What you should know •Decision TreesØWhat is a decision tree, and how to induce it from data•Fundamental Machine Learning ConceptsØDifference between memorization and generalizationØWhat inductive bias is, and what is its role in learning. ØWhat underfittingand overfitting meansØHow to take a task and cast it as a learning problemWhy you should never ever touch your test data!!
What you should know•New AlgorithmsØK-NN classificationØK-means clustering•Fundamental ML conceptsØHow to draw decision boundariesØWhat decision boundaries tells us about the underlying classifiersØThe difference between supervised and unsupervised learning
What you should knowØPerceptron conceptsØtraining/prediction algorithms (standard, voting, averaged)Øconvergence theorem and what practical guarantees it gives usØhow to draw/describe the decision boundary of a perceptron classifierØFundamental ML conceptsØDetermine whether a data set is linearly separable and define its marginØError driven algorithms, online vs. batch algorithms
What you should knowØWhat are reductions and why they are usefulØImplement, analyze and prove error bounds of algorithms forØWeighted binary classificationØMulticlass classification (OVA, AVA)
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
Machine	Translation	ContdProf.	Sameer	SinghCS	295:	STATISTICAL	NLPWINTER	2017March	2,	2017Based	on	slides	from	Dan	Klein,	Philipp	Koehn,	Jacob	Eisenstein,	and	everyone	else	they	copied	from.
Omer	Levy
CS	295:	STATISTICAL	NLP	(WINTER	2017)2
Understanding	Word	EmbeddingsAI/ML	SeminarMonday,	March	6th1pm-2pmDBH	4011Meeting	with	Graduate	Students4:00-4:45pmRoom	TBA	(email	me)
Upcoming…
•Homework	4	is	due	on	March	13•Write-up	and	data	releasing	soon.Homework•Status	report	due	in	1	weeks:	March	7,	2017•Instructions	coming	today!•Almost	final	report,	only	5	pagesProject•Paper	summaries:	March	14•Summary	1	gradedSummaries
CS	295:	STATISTICAL	NLP	(WINTER	2017)3
OutlineEM-Algorithm	for	AlignmentsPhrase-Based	MTDecoding	AlgorithmsSyntax-Based	MTCS	295:	STATISTICAL	NLP	(WINTER	2017)4
OutlineEM-Algorithm	for	AlignmentsPhrase-Based	MTDecoding	AlgorithmsSyntax-Based	MTCS	295:	STATISTICAL	NLP	(WINTER	2017)5
Parameters	of	the	IBM	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)6

Parameters	of	the	IBM	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)7
Translation	from	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)8
Alignments	from	Translation
CS	295:	STATISTICAL	NLP	(WINTER	2017)9
Expectation	Maximization
CS	295:	STATISTICAL	NLP	(WINTER	2017)10ExpectationMaximization
Example
CS	295:	STATISTICAL	NLP	(WINTER	2017)11

Example
CS	295:	STATISTICAL	NLP	(WINTER	2017)12

Word-based	MT:	Problems
CS	295:	STATISTICAL	NLP	(WINTER	2017)13Multi-word	AlignmentsNon-compositionalityPhrasal	Translations
OutlineEM-Algorithm	for	AlignmentsPhrase-Based	MTDecoding	AlgorithmsSyntax-Based	MTCS	295:	STATISTICAL	NLP	(WINTER	2017)14
The	VauquiosTriangle
CS	295:	STATISTICAL	NLP	(WINTER	2017)15

Phrase-based	MT
CS	295:	STATISTICAL	NLP	(WINTER	2017)16
Mary	did	not	slap	the	green	witch
Phrase	Lexicon
CS	295:	STATISTICAL	NLP	(WINTER	2017)17
Learning	Phrasal	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)18

Learning	Phrasal	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)19

Learning	Phrasal	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)20

Phrasal	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)21
Should	contain	all	the	alignment	points	for	covered	words
Learning	Phrasal	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)22
(Maria,	Mary),	(no,	did	not),	(slap,	dabaunabofetada),	(a	la,	the),	(bruja,	witch),	(verde,	green)
Learning	Phrasal	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)23
(Maria,	Mary),	(no,	did	not),(slap,	dabaunabofetada),	(a	la,	the),	(bruja,	witch),	(verde,	green)(Maria	no,	Mary	did	not),(no	dabaunabofetada,	did	not	slap),(dabaunabofetadaa	la,	slap	the),(brujaverde,	green	witch)
Learning	Phrasal	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)24
(Maria,	Mary),	(no,	did	not),(slap,	dabaunabofetada),	(a	la,	the),	(bruja,	witch),	(verde,	green)(Maria	no,	Mary	did	not),(no	dabaunabofetada,	did	not	slap),(dabaunabofetadaa	la,	slap	the),(brujaverde,	green	witch)(Maria	no	dabaunabofetada,	Mary	did	not	slap),	(no	dabaunabofetadaa	la,	did	not	slap	the),(a	la	brujaverde,	the	green	witch)

Learning	Phrasal	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)25(Maria,	Mary),	(no,	did	not),(slap,	dabaunabofetada),	(a	la,	the),	(bruja,	witch),	(verde,	green)(Maria	no,	Mary	did	not),(no	dabaunabofetada,	did	not	slap),(dabaunabofetadaa	la,	slap	the),(brujaverde,	green	witch)(Maria	no	dabaunabofetada,	Mary	did	not	slap),	(no	dabaunabofetadaa	la,	did	not	slap	the),(a	la	brujaverde,	the	green	witch)(Maria	no	dabaunabofetadaa	la,	Mary	did	not	slap	the),	(dabaunabofetadaa	la	brujaverde,	slap	the	green	witch)

Phrase	Translation	Scores
CS	295:	STATISTICAL	NLP	(WINTER	2017)26
Phrases	for	a	Sentence
CS	295:	STATISTICAL	NLP	(WINTER	2017)27wirmüssenauchdiesekritikernstnehmen(wirmüssen,	we	must)(wirmüssenauch,	we	must	also)(ernst,	seriously)….
Derivations	for	a	Sentence
CS	295:	STATISTICAL	NLP	(WINTER	2017)28
Distortion	Limits
CS	295:	STATISTICAL	NLP	(WINTER	2017)29
Distortion	Scores
CS	295:	STATISTICAL	NLP	(WINTER	2017)30
Scoring	Derivations
CS	295:	STATISTICAL	NLP	(WINTER	2017)31
The	Translation	Problem
CS	295:	STATISTICAL	NLP	(WINTER	2017)32
A	Secret	of	Statistical	MT
CS	295:	STATISTICAL	NLP	(WINTER	2017)33
OutlineEM-Algorithm	for	AlignmentsPhrase-Based	MTDecoding	AlgorithmsSyntax-Based	MTCS	295:	STATISTICAL	NLP	(WINTER	2017)34
The	Decoding	Task
CS	295:	STATISTICAL	NLP	(WINTER	2017)35
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)36

Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)37

Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)38
Mary
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)39
Marydid	not
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)40
Marydid	notgive
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)41
Marydid	notgivea
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)42
Marydid	notgiveaslap
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)43
Marydid	notgiveaslapto
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)44
Marydid	notgiveaslaptothe
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)45
Marydid	notgiveaslaptothewitch
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)46
Marydid	notgiveaslaptothewitchgreen
Monotonic	Word	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)47
Phrase	Decoding:	Stacks
CS	295:	STATISTICAL	NLP	(WINTER	2017)48
Phrase	Decoding:	Stacks
CS	295:	STATISTICAL	NLP	(WINTER	2017)49

Monotonic	Phrase	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)50

Monotonic	Phrase	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)51

Monotonic	Phrase	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)52
(Mary)
Monotonic	Phrase	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)53
(Mary)	(did	not)
Monotonic	Phrase	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)54
(Mary)	(did	not)	(slap)
Monotonic	Phrase	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)55
(Mary)	(did	not)	(slap)	(the)
Monotonic	Phrase	Decoding
CS	295:	STATISTICAL	NLP	(WINTER	2017)56
(Mary)	(did	not)	(slap)	(the)	(green	witch)
Machine	TranslationProf.	Sameer	SinghCS	295:	STATISTICAL	NLPWINTER	2017February	28,	2017Based	on	slides	from	Jason	Eisenstein,	Chris	Dyer,	Alan	Ritter,	Yejin	Choi,	and	everyone	else	they	copied	from.
Upcoming…
•Homework	4	is	due	on	March	13•Write-up	and	data	releasing	soon.Homework•Status	report	due	in	1	weeks:	March	7,	2017•Instructions	coming	today!•Almost	final	report,	only	5	pagesProject•Paper	summaries:	February	28,	March	14•Summary	1	gradedSummaries
CS	295:	STATISTICAL	NLP	(WINTER	2017)2
OutlineMachine	TranslationIntroduction	to	Statistical	MTIBM	Translation	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)3
OutlineMachine	TranslationIntroduction	to	Statistical	MTIBM	Translation	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)4
Machine	Translation
CS	295:	STATISTICAL	NLP	(WINTER	2017)5I	have	always	imagined	Paradise	as	a	kind	of	library.Yo,	que	me	figuraba	el	Paraíso	/	Bajo	la	especie	de	una	biblioteca.

Challenges:	Word	Order
CS	295:	STATISTICAL	NLP	(WINTER	2017)6Even	for	SVOEnglish:	I	will	buy	itFrench:	Je	vaisl’acheter(I	will	it	buy)English:	I	bought	itFrench:	Je	l’aiachet´e	(I	it	have	bought)SVO	vs	SOVEnglish:	IBM	bought	LotusJapanese:	IBM	Lotus	bought
Challenges:	Lexical	Ambiguity
CS	295:	STATISTICAL	NLP	(WINTER	2017)7
billpicocuenta
Challenges:	Pronouns
CS	295:	STATISTICAL	NLP	(WINTER	2017)8
In	Spanish,	you	can	recover	the	pronoun	from	verb	inflection:VivimosenAtlanta	→	Welive	in	AtlantaI	Again,	discourse	context	is	often	crucial:ViveenAtlanta	→	She/he/itlives	in	AtlantaEnglish	possessive	pronouns	take	the	gender	of	the	owner:Marie	rides	herbikeFrench	possessive	pronouns	take	the	gender	of	the	object:Marie	montesur	sonvéloDifferentPronounsDroppingPronouns
Challenges:	Tenses
CS	295:	STATISTICAL	NLP	(WINTER	2017)9The	preteritetense	is	for	events	with	a	definite	time,	e.g.I	biked	to	work	this	morningThe	imperfect	is	for	events	with	indefinite	times,	e.g.I	biked	to	work	all	last	summerTo	translate	English	to	Spanish,	we	must	pick	the	right	tense.
Challenges:	Idioms
CS	295:	STATISTICAL	NLP	(WINTER	2017)10Why	in	the	worldKick	the	bucket	Lend	me	your	earsDead	As	A	DoornailAs	Cool	As	a	CucumberHold	Your	HorsesStorm	in	a	TeacupBob's	Your	UncleBlue	in	the	FaceHead	In	The	Clouds
Rulesfor	Machine	Translation
CS	295:	STATISTICAL	NLP	(WINTER	2017)11Rules	for	translating	muchor	manyinto	Russian:ifpreceding	word	is	howreturn	skol’koelse	if	preceding	word	is	asreturnstol’kozheelse	ifword	is	muchif	preceding	word	is	veryreturn	nilelse	if	following	word	is	a	noun	return	mnogoelse(word	is	many)if	preceding	word	is	a	preposition	and	following	word	is	noun	return	mnogiielse	return	mnogo
Panov(1960)
The	VauquiosTriangle
CS	295:	STATISTICAL	NLP	(WINTER	2017)12

OutlineMachine	TranslationIntroduction	to	Statistical	MTIBM	Translation	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)13
Statistical	Machine	Translation
CS	295:	STATISTICAL	NLP	(WINTER	2017)14

Parallel	Corpus:	Examples
CS	295:	STATISTICAL	NLP	(WINTER	2017)15

Parallel	Corpus:	Examples
CS	295:	STATISTICAL	NLP	(WINTER	2017)16

Parallel	Corpus:	Examples
CS	295:	STATISTICAL	NLP	(WINTER	2017)17

Parallel	Corpus:	Examples
CS	295:	STATISTICAL	NLP	(WINTER	2017)18

The	Rosetta	Stone
CS	295:	STATISTICAL	NLP	(WINTER	2017)19

Warren	Weaver	(1949)
CS	295:	STATISTICAL	NLP	(WINTER	2017)20

Parallel	Corpus:	Examples
CS	295:	STATISTICAL	NLP	(WINTER	2017)21

Parallel	Corpus:	Examples
CS	295:	STATISTICAL	NLP	(WINTER	2017)22

Noisy	Channel	Model
CS	295:	STATISTICAL	NLP	(WINTER	2017)23“Noisy	Channel”Decoder
Noisy	Channel	Model
CS	295:	STATISTICAL	NLP	(WINTER	2017)24“Noisy	Channel”Decoder
Example:	Noisy	Channel
CS	295:	STATISTICAL	NLP	(WINTER	2017)25

Example:	Noisy	Channel
CS	295:	STATISTICAL	NLP	(WINTER	2017)26

Components	of	an	MT	system
CS	295:	STATISTICAL	NLP	(WINTER	2017)27Language	ModelTranslation	ModelDecoding	Algo
Components	of	an	MT	system
CS	295:	STATISTICAL	NLP	(WINTER	2017)28

Evaluating	MT
CS	295:	STATISTICAL	NLP	(WINTER	2017)29

Human	Evaluation
CS	295:	STATISTICAL	NLP	(WINTER	2017)30FluencyAdequacyA:	furious	nAgAon	wednesday,	the	tribal	minimum	purof	ten	schools	also	was	burntB:	furious	nAgAon	wednesdaythe	tribal	purmini	ten	schools	of	them	was	also	burnt
Automated	Evaluation
CS	295:	STATISTICAL	NLP	(WINTER	2017)31FluencyAdequacy
BLEU	Score
CS	295:	STATISTICAL	NLP	(WINTER	2017)32
BLEU	Score:	Example
CS	295:	STATISTICAL	NLP	(WINTER	2017)33‘	extension	of	isiin	uttarpradesh’‘	isi’s	expansion	in	uttarpradesh’‘	the	spread	of	isiin	uttarpradesh’‘	isispreading	in	uttarpradesh’the	spread	of	isiin	uttarpradesh
BLEU	Score:	Example
CS	295:	STATISTICAL	NLP	(WINTER	2017)34‘	extension	of	isiin	uttarpradesh’‘	isi’s	expansion	in	uttarpradesh’‘	the	spread	of	isiin	uttarpradesh’‘	isispreading	in	uttarpradesh’the	spread	of	isiin	uttarpradesh
BLEU’s	not	bad…
CS	295:	STATISTICAL	NLP	(WINTER	2017)35
G.	Doddington,	NIST
OutlineMachine	TranslationIntroduction	to	Statistical	MTIBM	Translation	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)36
StatisticalTranslation	Model
CS	295:	STATISTICAL	NLP	(WINTER	2017)37And	the	program	was	implementedLa	programmationa	étémiseenapplication
Word	Alignment:	Direct
CS	295:	STATISTICAL	NLP	(WINTER	2017)38

Word	Alignment:	1-to-Many
CS	295:	STATISTICAL	NLP	(WINTER	2017)39

Word	Alignment:	Reordering
CS	295:	STATISTICAL	NLP	(WINTER	2017)40

Word	Alignment:	Inserting
CS	295:	STATISTICAL	NLP	(WINTER	2017)41

Word	Alignment:	Dropping
CS	295:	STATISTICAL	NLP	(WINTER	2017)42

Translating	with	Alignments
CS	295:	STATISTICAL	NLP	(WINTER	2017)43
Example:	Translation	Prob
CS	295:	STATISTICAL	NLP	(WINTER	2017)44

IBM	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)45Model	1Model	2Model	3/4/5
Word	Alignment	Algorithm
CS	295:	STATISTICAL	NLP	(WINTER	2017)46
COMS 4721: Machine Learning for Data Science
Lecture 4, 1/26/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
REGRESSION WITH /WITHOUT REGULARIZATION
Given:
A data set (x1;y1);:::; (xn;yn), where x2Rdandy2R. We standardize
such that each dimension of xis zero mean unit variance, and yis zero mean.
Model:
We deﬁne a model of the form
yf(x;w):
We particularly focus on the case where f(x;w) =xTw.
Learning:
We can learn the model by minimizing the objective (aka, “loss”) function
L=Pn
i=1(yi xT
iw)2+wTw, L =ky Xwk2+kwk2
We’ve focused on =0 (least squares) and > 0 (ridge regression).
BIAS-VARIANCE TRADE -OFF
BIAS-VARIANCE FOR LINEAR REGRESSION
We can go further and hypothesize a generative model yN(Xw;2I)and
some true (but unknown) underlying value for the parameter vector w.
IWe saw how the least squares solution, wLS= (XTX) 1XTy, is unbiased
but potentially has high variance:
E[wLS] =w;Var[wLS] =2(XTX) 1:
IBy contrast, the ridge regression solution is wRR= (I+XTX) 1XTy.
Using the same procedure as for least squares, we can show that
E[wRR] = (I+XTX) 1XTXw;Var[wRR] =2Z(XTX) 1ZT;
where Z= (I+(XTX) 1) 1.
BIAS-VARIANCE FOR LINEAR REGRESSION
The expectation and covariance of wLSandwRRgives insight into how well
we can hope to learn win the case where our model assumption is correct.
ILeast squares solution: unbiased, but potentially high variance
IRidge regression solution: biased, but lower variance than LS
So which is preferable?
Ultimately, we really care about how well our solution for wgeneralizes to
new data. Let (x0;y0)be future data for which we have x0, but not y0.
ILeast squares predicts y0=xT
0wLS
IRidge regression predicts y0=xT
0wRR
BIAS-VARIANCE FOR LINEAR REGRESSION
In keeping with the square error measure of performance, we could calculate
the expected squared error of our prediction:
E
(y0 xT
0^w)2jX;x0
=Z
RZ
Rn(y0 xT
0^w)2p(yjX;w)p(y0jx0;w)dy dy 0:
IThe estimate ^wis either wLSorwRR.
IThe distributions on y;y0are Gaussian with the true (but unknown) w.
IWe condition on knowing x0;x1;:::; xn.
In words this is saying:
IImagine I know X;x0and assume some true underlying w.
II generate yN(Xw;2I)and approximate wwith ^w=wLSorwRR.
II then predict y0N(xT
0w;2)using y0xT
0^w.
What is the expected squared error of my prediction?
BIAS-VARIANCE FOR LINEAR REGRESSION
We can calculate this as follows (assume conditioning on x0andX),
E[(y0 xT
0^w)2] =E[y2
0] 2E[y0]xT
0E[^w] +xT
0E[^w^wT]x0
ISince y0and^ware independent, E[y0^w] =E[y0]E[^w].
IRemember: E[^w^wT] = Var[^w] +E[^w]E[^w]T
E[y2
0] =2+ (xT
0w)2Plugging these values in:
E[(y0 xT
0^w)2] =2+ (xT
0w)2 2(xT
0w)(xT
0E[^w]) + ( xT
0E[^w])2+xT
0Var[^w]x0
=2+xT
0(w E[^w])(w E[^w])Tx0+xT
0Var[^w]x0
BIAS-VARIANCE FOR LINEAR REGRESSION
We can calculate this as follows (assume conditioning on x0andX),
E[(y0 xT
0^w)2] =E[y2
0] 2E[y0]xT
0E[^w] +xT
0E[^w^wT]x0
ISince y0and^ware independent, E[y0^w] =E[y0]E[^w].
IRemember: E[^w^wT] = Var[^w] +E[^w]E[^w]T
E[y2
0] =2+ (xT
0w)2
Plugging these values in:
E[(y0 xT
0^w)2] =2+ (xT
0w)2 2(xT
0w)(xT
0E[^w]) + ( xT
0E[^w])2+xT
0Var[^w]x0
=2+xT
0(w E[^w])(w E[^w])Tx0+xT
0Var[^w]x0
BIAS-VARIANCE FOR LINEAR REGRESSION
We have shown that if
1.yN(Xw;2)andy0N(xT
0w;2), and
2. we approximate wwith ^waccording to some algorithm,
then
E[(y0 xT
0^w)2jX;x0] =2
|{z}
noise+xT
0(w E[^w])(w E[^w])Tx0|{z}
squared bias+xT
0Var[^w]x0|{z}
variance
We see that the generalization error is a combination of three factors:
1. Measurement noise – we can’t control this given the model.
2. Model bias – how close to the solution we expect to be on average.
3. Model variance – how sensitive our solution is to the data.
We saw how we can ﬁnd E[^w]and Var [^w]for the LS and RR solutions.
BIAS-VARIANCE TRADE -OFF
This idea is more general:
IImagine we have a model: y=f(x;w) +;E() =0;Var() =2
IWe approximate fby minimizing a loss function: ^f=arg min fLf.
IWe apply ^fto new data, y0^f(x0)^f0.
Then integrating everything out ( y;X;y0;x0):
E[(y0 ^f0)2] = E[y2
0] 2E[y0^f0] +E[^f2
0]
=2+f2
0 2f0E[^f0] +E[^f0]2+Var[^f0]
=2
|{z}
noise+ (f0 E[^f0])2
|{z}
squared bias+Var[^f0]|{z}
variance
This is interesting in principle, but is deliberately vague (What is f?) and
usually can’t be calculated (What is the distribution on the data?)
CROSS -VALIDATION
An easier way to evaluate the model is to use cross-validation.
The procedure for K-fold cross-validation is very simple:
1. Randomly split the data into Kroughly equal groups.
2. Learn the model on K 1 groups and predict the held-out Kth group.
3. Do this Ktimes, holding out each group once.
4. Evaluate performance using the cumulative set of predictions.
For the case of the regularization parameter , the above sequence can be
run for several values with the best-performing value of chosen.
The data you test the model on should never be used to train the model!

BAYES RULE
PRIOR INFORMATION /BELIEF
Motivation
We’ve discussed the ridge regression objective function
L=nX
i=1(yi xT
iw)2+wTw:
The regularization term wTwwas imposed to penalize values in wthat are
large. This reduced potential high-variance predictions from least squares.
In a sense, we are imposing a “prior belief” about what values of wwe
consider to be good.
Question : Is there a mathematical way to formalize this?
Answer : Using probability we can frame this via Bayes rule.
REVIEW : PROBABILITY STATEMENTS
Imagine we have two events, AandB, that may or may not be related, e.g.,
IA= “It is raining”
IB= “The ground is wet”
We can talk about probabilities of these events,
IP(A)= Probability it is raining
IP(B)= Probability the ground is wet
We can also talk about their conditional probabilities,
IP(AjB)= Probability it is raining given that the ground is wet
IP(BjA)= Probability the ground is wet given that it is raining
We can also talk about their joint probabilities,
IP(A;B)= Probability it is raining andthe ground is wet
CALCULUS OF PROBABILITY
There are simple rules for moving from one probability to another
1.P(A;B) =P(AjB)P(B) =P(BjA)P(A)
2.P(A) =P
bP(A;B=b)
3.P(B) =P
aP(A=a;B)
Using these three equalities, we automatically can say
P(AjB) =P(BjA)P(A)
P(B)=P(BjA)P(A)P
aP(BjA=a)P(A=a)
P(BjA) =P(AjB)P(B)
P(A)=P(AjB)P(B)P
bP(AjB=b)P(B=b)
This is known as “Bayes rule.”
BAYES RULE
Bayes rule lets us quantify what we don’t know. Imagine we want to say
something about the probability of Bgiven that Ahappened.
Bayes rule says that the probability of Bafter knowing Ais:
P(BjA)|{z}
posterior=P(AjB)|{z}
likelihoodP(B)|{z}
prior=P(A)|{z}
marginal
Notice that with this perspective, these probabilities take on new meanings.
That is, P(BjA)andP(AjB)are both “conditional probabilities,” but they
have different signiﬁcance.
BAYES RULE WITH CONTINUOUS VARIABLES
Bayes rule generalizes to continuous-valued random variables as follows.
However, instead of probabilities we work with densities .
ILetbe a continuous-valued model parameter.
ILetXbe data we possess. Then by Bayes rule,
p(jX) =p(Xj)p()R
p(Xj)p()d=p(Xj)p()
p(X)
In this equation,
Ip(Xj)is the likelihood, known from the model deﬁnition.
Ip()is a prior distribution that we deﬁne.
IGiven these two, we can (in principle) calculate p(jX).
EXAMPLE : COIN BIAS
We have a coin with bias towards “heads”. (Encode: heads = 1, tails = 0)
We ﬂip the coin many times and get a sequence of nnumbers (x1;:::; xn).
Assume the ﬂips are independent, meaning
p(x1;:::; xnj) =nY
i=1p(xij) =nY
i=1xi(1 )1 xi:
We choose a prior for which we deﬁne to be a beta distribution,
p() =Beta(ja;b) = (a+b)
 (a) (b)a 1(1 )b 1:
What is the posterior distribution of given x1;:::; xn?
EXAMPLE : COIN BIAS
From Bayes rule,
p(jx1;:::; xn) =p(x1;:::; xnj)p()R1
0p(x1;:::; xnj)p()d:
There is a trick that is often useful:
IThe denominator only normalizes the numerator, doesn’t depend on .
IWe can write p(jx)/p(xj)p(). (“/”!“proportional to”)
IMultiply the two and see if we recognize anything:
p(jx1;:::; xn)/Qn
i=1xi(1 )1 xih
 (a+b)
 (a) (b)a 1(1 )b 1i
/Pn
i=1xi+a 1(1 )Pn
i=1(1 xi)+b 1
We recognize this as p(jx1;:::; xn) =Beta(Pn
i=1xi+a;Pn
i=1(1 xi) +b).
MAXIMUM A POSTERIORI
LIKELIHOOD MODEL
Least squares and maximum likelihood
When we modeled data pairs (xi;yi)with a linear model, yixT
iw, we saw
that the least squares solution,
wLS=arg min
w(y Xw)T(y Xw);
was equivalent to the maximum likelihood solution when yN(Xw;2I).
The question now is whether a similar probabilistic connection can be made
for the ridge regression problem.
PRIOR MODEL
Ridge regression and Bayesian modeling
The likelihood model is yN(Xw;2I). What about a prior for w?
Let us assume that the prior for wis Gaussian, wN(0; 1I). Then
p(w) =
2d
2e 
2wTw:
We can now try to ﬁnd a wthat satisﬁes both the data likelihood, and our
prior conditions about w.
MAXIMUM A POSERIORI ESTIMATION
Maximum a poseriori (MAP) estimation seeks the most probable value w
under the posterior:
wMAP =arg max
wlnp(wjy;X)
=arg max
wlnp(yjw;X)p(w)
p(yjX)
=arg max
wlnp(yjw;X) +lnp(w) lnp(yjX)
IContrast this with ML, which only focuses on the likelihood.
IThe normalizing constant term ln p(yjX)doesn’t involve w. Therefore,
we can maximize the ﬁrst two terms alone.
IIn many models we don’t know ln p(yjX), so this fact is useful.
MAP FOR LINEAR REGRESSION
MAP using our deﬁned prior gives:
wMAP =arg max
wlnp(yjw;X) +lnp(w)
=arg max
w 1
22(y Xw)T(y Xw) 
2wTw+const.
Calling this objective L, then as before we ﬁnd wsuch that
rwL=1
2XTy 1
2XTXw w=0
IThe solution is wMAP= (2I+XTX) 1XTy.
INotice that wMAP=wRR(modulo a switch from to2)
IRR maximizes the posterior, while LS maximizes the likelihood.
CMSC 422 Introduction to Machine Learning
Lecture 21 Deep Learning II
Furong Huang / furongh@cs.umd.edu
Logistic Regression
•Goal: model the probability of a random 
variable Y being 0 or 1 given experimental 
data. 
•Consider a generalized linear model 
function parameterized by 𝜃,
ℎ𝜃𝑥=1
1+𝑒−𝜃⊤𝑥
•Attempt to model the probability that y is 0 
or 1 with function
Pr𝑦𝑥;𝜃=ℎ𝜃𝑥𝑦1−ℎ𝜃𝑥1−𝑦
Logistic Regression
•The likelihood assuming all the samples are 
independent
𝐿𝜃𝑥
=Pr𝑌𝑋;𝜃=ෑ
𝑖Pr𝑦𝑖𝑥𝑖;𝜃=ෑ
𝑖ℎ𝜃𝑥𝑖𝑦𝑖1−ℎ𝜃𝑥𝑖1−𝑦𝑖
•Maximum likelihood
max
𝜃𝐿(𝜃|𝑥)≡max
𝜃ෑ
𝑖ℎ𝜃𝑥𝑖𝑦𝑖1−ℎ𝜃𝑥𝑖1−𝑦𝑖
Neural Network with Softmax Classifier
•Softmax Classifier: multinomial Logistic 
Regression, the number of classes 
more than 2.
•Score: Instead of linear function as the 
exponent, we use a nonlinear function 
(e.g., a neural network 𝑠=𝑓(𝑥𝑖;𝑊)) 
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Softmax Classifier (Multinomial 
Logistic Regression)
Slide Credit: Fei -Fei Li & Justin Johnson & Serena Yeung
Multi -Layer: Backpropagation
22
Neuron𝑗
 Neuron𝑖𝑥𝑘
 𝚺
Sigmoid𝑤𝑘𝑖 𝑧𝑖
𝓛𝑦
𝐸 ො𝑦𝑖
 𝚺
Sigmoid𝑧𝑗ො𝑦𝑗𝑤𝑖𝑗
𝜕𝐸
𝜕ො𝑦𝑖=෍
𝑗𝜕𝐸
𝜕𝑧𝑗𝑑𝑧𝑗
𝑑ො𝑦𝑖=෍
𝑗𝑤𝑖𝑗𝜕𝐸
𝜕𝑧𝑗
𝜕𝐸
𝜕𝑤𝑘𝑖=෍
𝑛𝜕𝐸
𝜕ො𝑦𝑖𝑛𝑑ො𝑦𝑖𝑛
𝑑𝑧𝑖𝑛𝜕𝑧𝑖𝑛
𝜕𝑤𝑘𝑖𝜕𝐸
𝜕𝑧𝑗=𝜕𝐸
𝜕ො𝑦𝑗𝑑ො𝑦𝑗
𝑑𝑧𝑗
=෍
𝑛𝑑ො𝑦𝑖𝑛
𝑑𝑧𝑖𝑛𝜕𝑧𝑖𝑛
𝜕𝑤𝑘𝑖෍
𝑗𝑤𝑖𝑗𝜕𝐸
𝜕ො𝑦𝑗𝑛𝑑ො𝑦𝑗𝑛
𝑑𝑧𝑗𝑛=෍
𝑗𝑤𝑖𝑗𝜕𝐸
𝜕ො𝑦𝑗𝑑ො𝑦𝑗
𝑑𝑧𝑗
Slide credit: Bohyung Han
Back Propagation Revisited
•On board (gradient w.r.t elements ): 
Please take notes!
•Vector format of the backpropagation 
on slides
Neural Network: Definition

Neural Network: Forward Pass

Neural Network: Back Prop I

Neural Network: Back Prop II

Revival in the 1980’s
•Backpropagation discovered in 1970’s but popularized 
in 1986
➢David E. Rumelhart , Geoffrey E. Hinton, Ronald J. 
Williams. “Learning representations by back -
propagating errors.” In Nature, 1986.
•MLP is a universal approximator
➢Can approximate any non -linear function in theory, 
given enough neurons, data
➢Kurt Hornik , Maxwell Stinchcombe , Halbert White. 
“Multilayer feedforward networks are universal 
approximators .” Neural Networks, 1989
•Generated lots of excitement and applications
28http://www.andreykurenkov.com/writing/a -brief-history -of-neural -nets-and-deep -learning/
Neural Networks Applied to Vision
LeNet –vision application
LeCun , Y; Boser , B; Denker , J; Henderson, D; Howard, R; 
Hubbard, W; Jackel , L, “Backpropagation Applied to Handwritten 
Zip Code Recognition,” in Neural Computation, 1989
USPS digit recognition, later check reading
Convolution, pooling (“weight sharing”), fully connected layers
29Image credit: LeCun , Y., Bottou , L., Bengio , Y., Haffner , P. “Gradient -based learning applied to 
document recognition.” Proceedings of the IEEE, 1998.

Unsupervised Neural Networks
Autoencoders
•Encode then decode the same 
input
•No supervision needed
(Restricted) Boltzman
Machines (RBMs)
•Stochastic networks that can 
learn representations 
•Restricted version: neurons 
must form bipartite graph
30input xhidden layeroutput x’
input xhidden layerH. Bourlard and Y. Kamp. 1988. Auto -association by multilayer perceptrons and singular value decomposition.
Biol. Cybern . 59, 4 -5 (September 1988), 291 -294.
Ackley, David H; Hinton Geoffrey E; Sejnowski , Terrence J, "A learning algorithm for Boltzmann machines", Cognitive 
science, Elsevier, 1985.
Smolensky , Paul. "Chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony Theory.” In 
Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, 1986.
Recurrent Neural Networks
Networks with loops
•The output of a layer is used as input 
for the same (or lower) layer
•Can model dynamics (e.g. in space or 
time)
Loops are unrolled
•Now a standard feed -forward network 
with many layers
•Suffers from vanishing gradient 
problem
•In theory, can learn long term memory, 
in practice not ( Bengio et al, 1994)
31
Image credit: Chritopher Olah’s blog http://colah.github.io/posts/2015 -08-Understanding -LSTMs/
Sepp Hochreiter (1991), Untersuchungen zu dynamischen neuronalen Netzen, Diploma thesis. Institut 
f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber.
Y. Bengio, P. Simard, P. Frasconi. Learning Long -Term Dependencies with Gradient Descent is 
Difficult. In TNN 1994.
Long Short Term Memory (LSTM)
•A type of RNN explicitly designed not to have the vanishing or exploding 
gradient problem
•Models long -term dependencies
•Memory is propagated and accessed by gates
•Used for speech recognition, language modeling …
32Hochreiter , Sepp; and Schmidhuber , Jürgen. “Long Short -Term Memory.” Neural Computation, 1997.
Image credit: Christopher Colah’s blog,  http://colah.github.io/posts/2015 -08-Understanding -
LSTMs/
Issues in Deep Neural Networks
Large amount of training time
There are sometimes a lot of training data
Many iterations (epochs) are typically required for optimization
Computing gradients in each iteration takes too much time
Overfitting
Learned function fits training data well, but performs poorly on new data 
(high capacity model, not enough training data)
Vanishing gradient problem
Gradients in the lower layers are typically extremely small
Optimizing multi -layer neural networks takes huge amount of time
33𝜕𝐸
𝜕𝑤𝑘𝑖=෍
𝑛𝜕𝐸
𝜕ො𝑦𝑖𝑛𝑑ො𝑦𝑖𝑛
𝑑𝑧𝑖𝑛𝜕𝑧𝑖𝑛
𝜕𝑤𝑘𝑖=෍
𝑛𝜕𝑧𝑖𝑛
𝜕𝑤𝑘𝑖𝑑ො𝑦𝑖𝑛
𝑑𝑧𝑖𝑛෍
𝑗𝑤𝑖𝑗𝑑ො𝑦𝑗𝑛
𝑑𝑧𝑗𝑛𝜕𝐸
𝜕ො𝑦𝑗𝑛
Sigmoid𝑧 ො𝑦
Slide credit: adapted from Bohyung Han
New “winter” and revival in early 2000’s
New “winter” in the early 2000’s due to
•problems with training NNs
•Support Vector Machines (SVMs), Random Forests 
(RF) –easy to train, nice theory
Revival again by 2011 -2012
•Name change (“neural networks” -> “deep learning”)
•+ Algorithmic developments
➢unsupervised layer -wise pre -training
•ReLU , dropout, layer normalizatoin
•+ Big data + GPU computing = 
•Large outperformance on many datasets (Vision: 
ILSVRC’12)
34http://www.andreykurenkov.com/writing/a -brief-history -of-neural -nets-and-deep -learning -part-4/
Big Data
ImageNet Large Scale Visual Recognition Challenge
➢1000 categories w/ 1000 images per category
➢1.2 million training images, 50,000 validation, 150,000 testing
35O. Russakovsky , J. Deng, H. Su, J. Krause, S. Satheesh , S. Ma, Z. Huang, A. Karpathy , A. Khosla,
M. Bernstein, A. C. Berg and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge . IJCV, 2015.

AlexNet Architecture
60 million parameters!
Various tricks
• ReLU nonlinearity
• Overlapping pooling
• Local response normalization
• Dropout –set hidden neuron output to 0 with probability .5
• Data augmentation
• Training on GPUs
36Alex Krizhevsky , Ilya Sutskeyer , Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. NIPS, 2012.
Figure credit: Krizhevsky et al, NIPS 2012.
GPU Computing
•Big data and big models require lots of 
computational power
•GPUs
➢thousands of cores for parallel operations
➢multiple GPUs
➢still took about 5 -6 days to train AlexNet on two 
NVIDIA GTX 580 3GB GPUs (much faster 
today)
37
Image Classification Performance
38
Image Classification Top -5 Errors (%)
Slide credit: Bohyung HanFigure from: K. He, X. Zhang, S. Ren, J. Sun.  “Deep Residual Learning for Image  Recognition”. arXiv 2015. (slides)
Questions?
References (& great tutorials):
http://www.andreykurenkov.com/writing/a -brief-history -of-neural -nets-and-deep -learning -part-1/
http://cs231n.github.io/neural -networks -1/
http://colah.github.io/posts/2015 -08-Understanding -LSTMs/
39
Furong Huang
3251 A.V. Williams, College Park, MD 20740
301.405.8010 / furongh@cs.umd.edu
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  January 28, 2015 
Today: • Naïve Bayes • discrete-valued Xi’s • Document classification • Gaussian Naïve Bayes • real-valued Xi’s • Brain image classification   Readings:  Required: • Mitchell: “Naïve Bayes and Logistic Regression”      (available on class website)  Optional • Bishop 1.2.4 • Bishop 4.2  
Recently: • Bayes classifiers to learn P(Y|X) • MLE and MAP estimates for parameters of P • Conditional independence • Naïve Bayes à make Bayesian learning practical Next: • Text classification • Naïve Bayes and continuous variables Xi: • Gaussian Naïve Bayes classifier • Learn P(Y|X) directly • Logistic regression, Regularization, Gradient ascent • Naïve Bayes or Logistic Regression? • Generative vs. Discriminative classifiers 
Naïve Bayes in a Nutshell Bayes rule: Assuming conditional independence among Xi’s:   So, classification rule for Xnew = < X1, …, Xn > is:  

Example: Live in Sq Hill?  P(S|G,D,B) • S=1 iff live in Squirrel Hill • G=1 iff shop at SH Giant Eagle • D=1 iff Drive or Carpool to CMU • B=1 iff Birthday is before July 1   P(S=1) : P(D=1 | S=1) : P(D=1 | S=0) : P(G=1 | S=1) : P(G=1 | S=0) : P(B=1 | S=1) : P(B=1 | S=0) : P(S=0) : P(D=0 | S=1) : P(D=0 | S=0) : P(G=0 | S=1) : P(G=0 | S=0) : P(B=0 | S=1) : P(B=0 | S=0) :              Tom: D=1, G=0, B=0 P(S=1|D=1,G=0,B=0) =                                          P(S=1) P(D=1|S=1) P(G=0|S=1) P(B=0|S=1)      _____________________________________________________________________________   [P(S=1) P(D=1|S=1) P(G=0|S=1) P(B=0|S=1) + P(S=0) P(D=1|S=0) P(G=0|S=0) P(B=0|S=0)]      
Another way to view Naïve Bayes (Boolean Y): Decision rule: is this quantity greater or less than 1?  

Another way to view Naïve Bayes (Boolean Y): Decision rule: is this quantity greater or less than 1?  

Naïve Bayes: classifying text documents • Classify which emails are spam? • Classify which emails promise an attachment?  How shall we represent text documents for Naïve Bayes? 

Learning to classify documents: P(Y|X)  • Y discrete valued.   – e.g., Spam or not • X = <X1, X2, … Xn> = document   • Xi is a random variable describing… 

Learning to classify documents: P(Y|X)  • Y discrete valued.   – e.g., Spam or not • X = <X1, X2, … Xn> = document   • Xi is a random variable describing… Answer 1: Xi is boolean, 1 if word i is in document, else 0                  e.g., Xpleased = 1    Issues? 

Learning to classify documents: P(Y|X)  • Y discrete valued.   – e.g., Spam or not • X = <X1, X2, … Xn> = document   • Xi is a random variable describing… Answer 2:  • Xi represents the ith word position in document • X1 = “I”,  X2 = “am”, X3 = “pleased” • and, let’s assume the Xi are iid (indep, identically distributed) 

Learning to classify document: P(Y|X) the “Bag of Words” model • Y discrete valued.  e.g., Spam or not • X = <X1, X2, … Xn> = document  • Xi are iid random variables.  Each represents the word at its position i in the document • Generating a document according to this distribution = rolling a 50,000 sided die, once for each word position in the document • The observed counts for each word follow a ??? distribution  
Multinomial Distribution 

Multinomial Bag of Words 
aardvark 0 about 2 all 2 Africa 1 apple 0 anxious 0 ... gas 1 ... oil 1 … Zaire 0 
MAP estimates for bag of words  Map estimate for multinomial     What β’s should we choose?  

Naïve Bayes Algorithm – discrete Xi  • Train Naïve Bayes (examples)    for each value yk  estimate   for each value xij of each attribute Xi   estimate   • Classify (Xnew)    
prob that word xij appears in position i, given Y=yk  * Additional assumption:  word probabilities are position independent 


For code and data, see www.cs.cmu.edu/~tom/mlbook.html  click on “Software and Data” 
What if we have continuous Xi ? Eg., image classification: Xi is real-valued ith pixel  

What if we have continuous Xi ? Eg., image classification: Xi is real-valued ith pixel  Naïve Bayes requires P(Xi | Y=yk), but Xi is real (continuous)     Common approach: assume P(Xi | Y=yk) follows a Normal (Gaussian) distribution    

What if we have continuous Xi ? Eg., image classification: Xi is real-valued ith pixel  Naïve Bayes requires P(Xi | Y=yk), but Xi is real (continuous)     Common approach: assume P(Xi | Y=yk) follows a Normal (Gaussian) distribution    

Gaussian Distribution (also called “Normal”)  p(x) is a probability density function, whose  integral (not sum) is 1 
What if we have continuous Xi ? Gaussian Naïve Bayes (GNB): assume     Sometimes assume variance • is independent of Y (i.e., σi),  • or independent of Xi (i.e., σk) • or both (i.e., σ) 
• Train Naïve Bayes (examples)    for each value yk  estimate*   for each attribute Xi estimate  • class conditional mean        , variance         • Classify (Xnew)    
Gaussian Naïve Bayes Algorithm – continuous Xi   (but still discrete Y) 
 * probabilities must sum to 1, so need estimate only n-1 parameters... 

Estimating Parameters: Y discrete, Xi continuous  Maximum likelihood estimates: 
jth training example δ()=1 if (Yj=yk) else 0 ith feature kth class 
How many parameters must we estimate for Gaussian Naïve Bayes if Y has k possible values, X=<X1, … Xn>?  


GNB Example: Classify a person’s cognitive state, based on brain image •  reading a sentence or viewing a picture? •  reading the word describing a “Tool” or “Building”?   •  answering the question, or getting confused? 

Y is the mental state (reading “house” or “bottle”) Xi are the voxel activities,   this is a plot of the µ’s defining P(Xi | Y=“bottle”) 
fMRI activation  high 
below average average Mean activations over all training examples for Y=“bottle” 
Classification task: is person viewing a “tool” or “building”? 
p4p8p6p11p5p7p10p9p2p12p3p100.10.20.30.40.50.60.70.80.91
ParticipantsClassification accuracystatistically significant p<0.05 Classification accuracy 
Where is information encoded in the brain? 
Accuracies of  cubical 27-voxel  classifiers centered at each significant voxel [0.7-0.8] 

Naïve Bayes: What you should know • Designing classifiers based on Bayes rule • Conditional independence – What it is – Why it’s important • Naïve Bayes assumption and its consequences – Which (and how many) parameters must be estimated under different generative models (different forms for P(X|Y) ) • and why this matters • How to train Naïve Bayes classifiers – MLE and MAP estimates  – with discrete and/or continuous inputs Xi 
Questions to think about: • Can you use Naïve Bayes for a combination of discrete and real-valued Xi?  • How can we easily model just 2 of n attributes as dependent? • What does the decision surface of a Naïve Bayes classifier look like? • How would you select a subset of Xi’s? 
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  March 4, 2015 
Today: • Graphical models • Bayes Nets:  • EM • Mixture of Gaussian clustering • Learning Bayes Net structure (Chow-Liu) Readings: • Bishop chapter 8 • Mitchell chapter 6  
Learning of Bayes Nets • Four categories of learning problems – Graph structure may be known/unknown – Variable values may be fully observed / partly unobserved • Easy case: learn parameters for graph structure is known, and data is fully observed  • Interesting case: graph known, data partly known • Gruesome case: graph structure unknown, data partly unobserved 
EM Algorithm - Informally EM is a general procedure for learning from partly observed data Given  observed variables X, unobserved Z  (X={F,A,H,N}, Z={S})  Begin with arbitrary choice for parameters θ Iterate until convergence: •  E Step: estimate the values of unobserved Z, using θ   •  M Step: use observed values plus E-step estimates to                  derive a better θGuaranteed to find local maximum. Each iteration increases   

EM Algorithm - Precisely EM is a general procedure for learning from partly observed data Given  observed variables X, unobserved Z  (X={F,A,H,N}, Z={S}) Define Iterate until convergence: •  E Step: Use X and current θ to calculate P(Z|X,θ) •  M Step: Replace current θ by  
Guaranteed to find local maximum. Each iteration increases   

E Step: Use X, θ, to Calculate P(Z|X,θ) • How?  Bayes net inference problem. Flu Allergy Sinus Headache Nose 
observed X={F,A,H,N}, unobserved Z={S} 
let’s use p(a,b) as shorthand for p(A=a, B=b) 
EM and estimating   Flu Allergy Sinus Headache Nose 
observed X = {F,A,H,N}, unobserved Z={S} E step:  Calculate P(Zk|Xk; θ) for each training example, k  
M step: update all relevant parameters.  For example: 
Recall MLE was: 

EM and estimating   Flu Allergy Sinus Headache Nose More generally,  Given observed set X, unobserved set Z of boolean values E step:  Calculate for each training example, k   the expected value of each unobserved variable in   each training example    M step: 
Calculate     similar to MLE estimates, but replacing each count by its expected count 

Using Unlabeled Data to Help Train  Naïve Bayes Classifier YX1 X4 X3 X2 Y X1 X2 X3 X4 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 ? 0 1 1 0 ? 0 1 0 1 Learn P(Y|X) 
EM and estimating   Given observed set X, unobserved set Y of boolean values E step:  Calculate for each training example, k   the expected value of each unobserved variable Y M step: 
Calculate estimates similar to MLE, but replacing each count by its expected count 
MLE would be: 
Experimental Evaluation  • Newsgroup postings  – 20 newsgroups, 1000/group • Web page classification  – student, faculty, course, project – 4199 web pages • Reuters newswire articles  – 12,902 articles – 90 topics categories From [Nigam et al., 2000] 
20 Newsgroups  

Using one labeled example per class word w ranked by P(w|Y=course) /P(w|Y ≠ course) 
20 Newsgroups  

Usupervised clustering   Just extreme case for EM with zero labeled examples… 
Clustering • Given set of data points, group them • Unsupervised learning • Which patients are similar? (or which earthquakes, customers, faces, web pages, …) 
Mixture Distributions Model joint                     as mixture of multiple distributions. Use discrete-valued random var Z to indicate which distribution is being use for each random draw So   Mixture of Gaussians: • Assume each data point X=<X1, … Xn> is generated by one of several Gaussians, as follows: 1. randomly choose Gaussian i, according to P(Z=i) 2. randomly generate a data point <x1,x2 .. xn> according to N(µi, Σi) 

Mixture of Gaussians  

EM for Mixture of Gaussian Clustering Let’s simplify to make this easier:    1. assume X=<X1 ... Xn>, and the Xi are conditionally independent given Z.     2. assume only 2 clusters (values of Z), and 3. Assume σ known, π1 … πK, µ1i …µKi unknown Observed: X=<X1 ... Xn> Unobserved: Z  
ZX1 X4 X3 X2 

EM Given  observed variables X, unobserved Z   Define where  
Iterate until convergence: •  E Step: Calculate P(Z(n)|X(n),θ) for each example X(n). Use this to construct  •  M Step: Replace current θ by  
ZX1 X4 X3 X2 

EM – E Step Calculate P(Z(n)|X(n),θ) for each observed example X(n) X(n)=<x1(n), x2(n), … xT(n)>.   
ZX1 X4 X3 X2 

EM – M Step        ZX1 X4 X3 X2 
First consider update for π
π’ has no influence 
z=1 for nth example 
EM – M Step        ZX1 X4 X3 X2 
Now consider update for µji 
µji’ has no influence 
… … … 
Compare above to MLE if Z were observable: 
EM – putting it together Given  observed variables X, unobserved Z   Define where  
Iterate until convergence: •  E Step: For each observed example X(n), calculate P(Z(n)|X(n),θ)    •   M Step: Update ZX1 X4 X3 X2 

Mixture of Gaussians applet  Go to: http://www.socr.ucla.edu/htmls/SOCR_Charts.html then go to Go to “Line Charts”  à SOCR EM Mixture Chart • try it with 2 Gaussian mixture components (“kernels”) • try it with 4    
• For learning from partly unobserved data • MLE of θ =  • EM estimate: θ = Where X is observed part of data, Z is unobserved • Nice case is Bayes net of boolean vars: – M step is like MLE, with with unobserved values replaced by their expected values, given the other observed values • EM for training Bayes networks • Can also develop MAP version of EM • Can also derive your own EM algorithm for your own problem – write out expression for – E step: for each training example Xk, calculate P(Zk | Xk, θ) – M step: chose new θ to maximize                             
What you should know about EM  

Learning Bayes Net Structure     
How can we learn Bayes Net graph structure?  In general case, open problem • can require lots of data (else high risk of overfitting) • can use Bayesian methods to constrain search One key result: • Chow-Liu algorithm: finds “best” tree-structured network   • What’s best? – suppose P(X) is true distribution, T(X) is our tree-structured network, where X = <X1, … Xn>  – Chow-Liu minimizes Kullback-Leibler divergence: 

Chow-Liu Algorithm  Key result:  To minimize KL(P || T), it suffices to find the tree network T that maximizes the sum of mutual informations over its edges  Mutual information for an edge between variable A and B:     This works because for tree networks with nodes   

Chow-Liu Algorithm  1. for each pair of vars A,B, use data to estimate P(A,B),  P(A), P(B)  2. for each pair of vars A,B calculate mutual information 3. calculate the maximum spanning tree over the set of variables, using edge weights I(A,B)  (given N vars, this costs only O(N2) time) 4. add arrows to edges to form a directed-acyclic graph 5. learn the CPD’s for this graph  

Chow-Liu algorithm example  Greedy Algorithm to find Max-Spanning Tree  
1/ 1/ 1/ 1/ 1/ 1/ 1/ 1/ 1/ 1/ 1/ 
[courtesy A. Singh, C. Guestrin] 

Bayes Nets – What You Should Know  • Representation – Bayes nets represent joint distribution as a DAG + Conditional Distributions – D-separation lets us decode conditional independence assumptions • Inference – NP-hard in general – For some graphs, closed form inference is feasible – Approximate methods too, e.g., Monte Carlo methods, … • Learning – Easy for known graph, fully observed data (MLE’s, MAP est.) – EM for partly observed data, known graph – Learning graph structure: Chow-Liu for tree-structured networks – Hardest when graph unknown, data incompletely observed  
CS224dDeep	NLPLecture	8:Recurrent	Neural	NetworksRichard	Socherrichard@metamind.io
Overview
4/21/16Richard	Socher2•Feedback•Traditional	language	models•RNNs•RNN	language	models•Important	training	problems	and	tricks•Vanishing	and	exploding	gradient	problems•RNNs	for	other	sequence	tasks•Bidirectional	and	deep	RNNs
Feedback
4/21/16Richard	Socher3

Feedback	àSuper	useful	àThanks!
4/21/16Richard	Socher4Explain	the	intuition	behind	the	math	and	models	more	àsome	today	:)Give	more	examples,	more	toy	examples	and	recap	slides	can	help	us	understand	fasteràSome	toy	examples	today.	Recap	of	main	concepts	next	weekConsistency	issues	in	dimensionality,	row	vscolumn,	etc.àAll	vectors	should	be	column	vectors	…	unless	I	messed	up,	please	send	errataI	like	the	quality	of	the	problem	sets	and	especially	the	starter	code.	It	would	be	nice	to	include	ballpark	values	we	should	expectàWill	add	in	future	Psetsand	on	Piazza.	We’ll	also	add	dimensionality.
Feedback	on	Project
4/21/16Richard	Socher5Please	give	list	of	proposed	projectsà•Great	feedback,	I	asked	research	groups	at	Stanford	and	will	compile	a	list	for	next	Tuesday.•We’ll	move	project	proposal	deadline	to	next	week	Thursday.•Extra	credit	deadline	for	dataset	+	first	baseline	is	for	project	milestone.
Language	Models
4/21/16Richard	Socher6A	language	model	computes	a	probability	for	a	sequence	of	words:•Useful	for	machine	translation•Word	ordering:p(the	cat	is	small)	>	p(small	the	is	cat)•Word	choice:p(walking	home	after	school)	>	p(walking	house	after	school)

Traditional	Language	Models
4/21/16Richard	Socher7•Probability	is	usually	conditioned	on	window	of	n	previous	words•An	incorrect	but	necessary	Markov	assumption!•To	estimate	probabilities,	compute	for	unigrams	and	bigrams	(conditioning	on	one/two	previous	word(s):

Traditional	Language	Models
4/21/16Richard	Socher8•Performance	improves	with	keeping	around	higher	n-grams	counts	and	doing	smoothing	and	so-called	backoff(e.g.	if	4-gram	not	found,	try	3-gram,	etc)•There	are	A	LOT	of	n-grams!àGigantic	RAM	requirements!	•Recent	state	of	the	art:	Scalable	Modified	Kneser-Ney	Language	Model	Estimationby	Heafieldet	al.:	“Using	one	machine	with	140	GB	RAM	for	2.8	days,	we	built	an	unprunedmodel	on	126	billion	tokens”
Recurrent	Neural	Networks!
4/21/16Richard	Socher9•RNNs	tie	the	weights	at	each	time	step•Condition	the	neural	network	on	all	previous	words•RAM	requirement	only	scales	with	number	of	words
xt−1xtxt+1ht−1htht+1WW
yt−1
yt
yt+1
Recurrent	Neural	Network	language	model
4/21/16Richard	Socher10Given	list	of	word	vectors:At	a	single	time	step:
xtht
ßà
Recurrent	Neural	Network	language	modelMain	idea:	we	use	the	same	set	of	W	weights	at	all	time	steps!Everything	else	is	the	same:is	some	initialization	vector	for	the	hidden	layer	at	time	step	0is	the	column	vector	of	L	at	index	[t]	at	time	step	t

Recurrent	Neural	Network	language	model
4/21/16Richard	Socher12is	a	probability	distribution	over	the	vocabularySame	cross	entropy	loss	function	but	predicting	words	instead	of	classes

Recurrent	Neural	Network	language	model
4/21/16Richard	Socher13Evaluation	could	just	be	negative	of	average	log	probability	over	dataset	of	size	(number	of	words)	T:But	more	common:	Perplexity:				2JLower	is	better!

Training	RNNs	is	hard•Multiply	 the	same	matrix	at	each	time	step	during	forward	prop
•Ideally	inputs	from	many	time	steps	ago	can	modify	output	y•Take										for	an	example	RNN	with	2	time	steps!	Insightful!4/21/16Richard	SocherLecture	1,	Slide	14
xt−1xtxt+1ht−1htht+1WW
yt−1
yt
yt+1
The	vanishing/exploding	gradient	problem•Multiply	 the	same	matrix	at	each	time	step	during	backprop
4/21/16Richard	SocherLecture	1,	Slide	15
xt−1xtxt+1ht−1htht+1WW
yt−1
yt
yt+1
The	vanishing	gradient	problem	-Details•Similar	but	simpler	RNN	formulation:•Total	error	is	the	sum	of	each	error	at	time	steps	t•Hardcore	chain	rule	application:4/21/16Richard	SocherLecture	1,	Slide	16

The	vanishing	gradient	problem	-Details•Similar	to	backpropbut	less	efficient	formulation•Useful	for	analysis	we’ll	look	at:•Remember:•More	chain	rule,	remember:•Each	partial	is	a	Jacobian:4/21/16Richard	SocherLecture	1,	Slide	17

The	vanishing	gradient	problem	-Details•From	previous	slide:	•Remember:•To	compute	Jacobian,	derive	each	elementof	matrix:	•Where:4/21/16Richard	SocherLecture	1,	Slide	18
ht−1ht
Check	at	home	that	you	understandthe	diagmatrix	formulation
The	vanishing	gradient	problem	-Details•Analyzing	the	norms	of	the	Jacobians,	yields:•Where	we	defined	¯‘s	as	upper	bounds	of	the	norms•The	gradient	is	a	product	of	Jacobianmatrices,	each	associated	with	a	step	in	the	forward	computation.	•This	can	become	very	small	or	very	large	quickly	[Bengioet	al	1994],	and	the	locality	assumption	of	gradient	descent	breaks	down.	àVanishing	or	exploding	gradient4/21/16Richard	SocherLecture	1,	Slide	19

Why	is	the	vanishing	gradient	a	problem?•The	error	at	a	time	step	ideally	can	tell	a	previous	time	step	from	many	steps	away	to	change	during	backprop
4/21/16Richard	SocherLecture	1,	Slide	20
xt−1xtxt+1ht−1htht+1WW
yt−1
yt
yt+1
The	vanishing	gradient	problem	for	language	models•In	the	case	of	language	modeling	or	question	answering	words	from	time	steps	far	away	are	not	taken	into	consideration	when	training	to	predict	the	next	word•Example:	Jane	walked	into	the	room.	John	walked	in	too.	It	was	late	in	the	day.	Jane	said	hi	to	____
4/21/16Richard	SocherLecture	1,	Slide	21
IPythonNotebook	with	vanishing	gradient	example•Example	of	simple	and	clean	NNetimplementation	•Comparison	of	sigmoid	and	ReLuunits•A	little	bit	of	vanishing	gradient
4/21/16Richard	SocherLecture	1,	Slide	22
4/21/16Richard	SocherLecture	1,	Slide	23

Trick	for	exploding	gradient:	clipping	trick•The	solution	first	introduced	by	Mikolovis	to	clip	gradientsto	a	maximum	value.	
•Makes	a	big	difference	in	RNNs.24On the di culty of training Recurrent Neural Networksregion of space. It has been shown that in practiceit can reduce the chance that gradients explode, andeven allow training generator models or models thatwork with unbounded amounts of memory(Pascanuand Jaeger, 2011; Doya and Yoshizawa, 1991). Oneimportant downside is that it requires a target to bedeﬁned at every time step.In Hochreiter and Schmidhuber (1997); Graveset al.(2009) a solution is proposed for the vanishing gra-dients problem, where the structure of the model ischanged. Speciﬁcally it introduces a special set ofunits called LSTM units which are linear and have arecurrent connection to itself which is ﬁxed to 1. Theﬂow of information into the unit and from the unit isguarded by an input and output gates (their behaviouris learned). There are several variations of this basicstructure. This solution does not address explicitly theexploding gradients problem.Sutskeveret al.(2011) use the Hessian-Free opti-mizer in conjunction withstructural damping,as p e -ciﬁc damping strategy of the Hessian. This approachseems to deal very well with the vanishing gradient,though more detailed analysis is still missing. Pre-sumably this method works because in high dimen-sional spaces there is a high probability for long termcomponents to be orthogonal to short term ones. Thiswould allow the Hessian to rescale these componentsindependently. In practice, one can not guarantee thatthis property holds. As discussed in section 2.3, thismethod is able to deal with the exploding gradientas well. Structural damping is an enhancement thatforces the change in the state to be small, when the pa-rameter changes by some small value ✓. This asks forthe Jacobian matrices@xt@✓to have small norm, hencefurther helping with the exploding gradients problem.The fact that it helps when training recurrent neuralmodels on long sequences suggests that while the cur-vature might explode at the same time with the gradi-ent, it might not grow at the same rate and hence notbe su cient to deal with the exploding gradient.Echo State Networks (Lukoˇ seviˇ cius and Jaeger, 2009)avoid the exploding and vanishing gradients problemby not learning the recurrent and input weights. Theyare sampled from hand crafted distributions. Becauseusually the largest eigenvalue of the recurrent weightis, by construction, smaller than 1, information fed into the model has to die out exponentially fast. Thismeans that these models can not easily deal with longterm dependencies, even though the reason is slightlydi↵erent from the vanishing gradients problem. Anextension to the classical model is represented by leakyintegration units (Jaegeret al., 2007), wherexk=↵xk 1+( 1 ↵) (Wrecxk 1+Winuk+b).While these units can be used to solve the standardbenchmark proposed by Hochreiter and Schmidhu-ber (1997) for learning long term dependencies (see(Jaeger, 2012)), they are more suitable to deal withlow frequency information as they act as a low passﬁlter. Because most of the weights are randomly sam-pled, is not clear what size of models one would needto solve complex real world tasks.We would make a ﬁnal note about the approach pro-posed by Tomas Mikolov in his PhD thesis (Mikolov,2012)(and implicitly used in the state of the art re-sults on language modelling (Mikolovet al., 2011)).It involves clipping the gradient’s temporal compo-nents element-wise (clipping an entry when it exceedsin absolute value a ﬁxed threshold). Clipping has beenshown to do well in practice and it forms the backboneof our approach.3.2. Scaling down the gradientsAs suggested in section 2.3, one simple mechanism todeal with a sudden increase in the norm of the gradi-ents is to rescale them whenever they go over a thresh-old (see algorithm 1).Algorithm 1Pseudo-code for norm clipping the gra-dients whenever they explodeˆg @E@✓ifkˆgk thresholdthenˆg thresholdkˆgkˆgend ifThis algorithm is very similar to the one proposed byTomas Mikolov and we only diverged from the originalproposal in an attempt to provide a better theoreticalfoundation (ensuring that we always move in a de-scent direction with respect to the current mini-batch),though in practice both variants behave similarly.The proposed clipping is simple to implement andcomputationally e cient, but it does however in-troduce an additional hyper-parameter, namely thethreshold. One good heuristic for setting this thresh-old is to look at statistics on the average norm overas u ciently large number of updates. In our ex-periments we have noticed that for a given task andmodel size, training is not very sensitive to this hyper-parameter and the algorithm behaves well even forrather small thresholds.The algorithm can also be thought of as adaptingthe learning rate based on the norm of the gradient.Compared to other learning rate adaptation strate-gies, which focus on improving convergence by col-lecting statistics on the gradient (as for example in
Gradient	clipping	intuition
4/21/16Richard	Socher25•Error	surface	of	a	single	hidden	unit	RNN,	•High	curvature	walls•Solid	lines:	standard	gradient	descent	trajectories	•Dashed	lines	gradients	rescaled	to	fixed	sizeOn the di culty of training Recurrent Neural Networks
Figure 6.We plot the error surface of a single hidden unitrecurrent network, highlighting the existence of high cur-vature walls. The solid lines depicts standard trajectoriesthat gradient descent might follow. Using dashed arrowthe diagram shows what would happen if the gradients isrescaled to a ﬁxed size when its norm is above a threshold.explode so does the curvature alongv,l e a d i n gt oawall in the error surface, like the one seen in Fig. 6.If this holds, then it gives us a simple solution to theexploding gradients problem depicted in Fig. 6.If both the gradient and the leading eigenvector of thecurvature are aligned with the exploding directionv,i tfollows that the error surface has a steep wall perpen-dicular tov(and consequently to the gradient). Thismeans that when stochastic gradient descent (SGD)reaches the wall and does a gradient descent step, itwill be forced to jump across the valley moving perpen-dicular to the steep walls, possibly leaving the valleyand disrupting the learning process.The dashed arrows in Fig. 6 correspond toignoringthe norm of this large step, ensuring that the modelstays close to the wall. The key insight is that all thesteps taken when the gradient explodes are alignedwithvand ignore other descent direction (i.e. themodel moves perpendicular to the wall). At the wall, asmall-norm step in the direction of the gradient there-fore merely pushes us back inside the smoother low-curvature region besides the wall, whereas a regulargradient step would bring us very far, thus slowing orpreventing further training. Instead, with a boundedstep, we get back in that smooth region near the wallwhere SGD is free to explore other descent directions.The important addition in this scenario to the classicalhigh curvature valley, is that we assume that the val-ley is wide, as we have a large region around the wallwhere if we land we can rely on ﬁrst order methodsto move towards the local minima. This is why justclipping the gradient might be su cient, not requiringthe use a second order method. Note that this algo-rithm should work even when the rate of growth of thegradient is not the same as the one of the curvature(a case for which a second order method would failas the ratio between the gradient and curvature couldstill explode).Our hypothesis could also help to understand the re-cent success of the Hessian-Free approach comparedto other second order methods. There are two key dif-ferences between Hessian-Free and most other second-order algorithms. First, it uses the full Hessian matrixand hence can deal with exploding directions that arenot necessarily axis-aligned. Second, it computes anew estimate of the Hessian matrix before each up-date step and can take into account abrupt changes incurvature (such as the ones suggested by our hypothe-sis) while most other approaches use a smoothness as-sumption, i.e., averaging 2nd order signals over manysteps.3. Dealing with the exploding andvanishing gradient3.1. Previous solutionsUsing an L1 or L2 penalty on the recurrent weights canhelp with exploding gradients. Given that the parame-ters initialized with small values, the spectral radius ofWrecis probably smaller than 1, from which it followsthat the gradient can not explode (see necessary condi-tion found in section 2.1). The regularization term canensure that during training the spectral radius neverexceeds 1. This approach limits the model to a sim-ple regime (with a single point attractor at the origin),where any information inserted in the model has to dieout exponentially fast in time. In such a regime we cannot train a generator network, nor can we exhibit longterm memory traces.Doya (1993) proposes to pre-program the model (toinitialize the model in the right regime) or to useteacher forcing. The ﬁrst proposal assumes that ifthe model exhibits from the beginning the same kindof asymptotic behaviour as the one required by thetarget, then there is no need to cross a bifurcationboundary. The downside is that one can not alwaysknow the required asymptotic behaviour, and, even ifsuch information is known, it is not trivial to initial-ize a model in this speciﬁc regime. We should alsonote that such initialization does not prevent cross-ing the boundary between basins of attraction, which,as shown, could happen even though no bifurcationboundary is crossed.Teacher forcing is a more interesting, yet a not verywell understood solution. It can be seen as a way ofinitializing the model in the right regime and the rightFigure	 from	paper:	On	the	difficulty	of	training	Recurrent	Neural	Networks,	Pascanuet	al.	2013
For	vanishing	gradients:	Initialization	+	ReLus!
4/21/16Richard	Socher26•Initialize	W(*)‘s	toidentity	matrix	Iandf(z)		=•àHuge	difference!•Initialization	idea	first	introduced	in	Parsing	with	Compositional	Vector	Grammars,	Socher	et	al.	2013•New	experiments	with	recurrent	neural	nets	2	weeks	ago	(!)	in	A	Simple	Way	to	Initialize	Recurrent	Networks	of	Rectified	Linear	Units,	Le	et	al.	2015
T
LSTM
RNN + Tanh
IRNN
150
lr=0.01,g c=1 0,fb=1.0
lr=0.01,g c=1 0 0
lr=0.01,g c=1 0 0
200
lr=0.001,g c=1 0 0,fb=4.0
N/A
lr=0.01,g c=1
300
lr=0.01,g c=1,fb=4.0
N/A
lr=0.01,g c=1 0
400
lr=0.01,g c=1 0 0,fb=1 0.0
N/A
lr=0.01,g c=1
Table 1: Best hyperparameters found for adding problems after grid search.lris the learning rate,gcis gradient clipping, andfbis forget gate bias. N/A is when there is no hyperparameter combinationthat gives good result.4.2 MNIST Classiﬁcation from a Sequence of PixelsAnother challenging toy problem is to learn to classify the MNIST digits [21] when the 784 pixelsare presented sequentially to the recurrent net. In our experiments, the networks read one pixel at atime in scanline order (i.e.starting at the top left corner of the image, and ending at thebottom rightcorner). The networks are asked to predict the category of theM N I S Ti m a g eo n l ya f t e rs e e i n ga l l784 pixels. This is therefore a huge long range dependency problem because each recurrent networkhas 784 time steps.To make the task even harder, we also used a ﬁxed random permutation of the pixels of the MNISTdigits and repeated the experiments.All networks have 100 recurrent hidden units. We stop the optimization after it converges or whenit reaches 1,000,000 iterations and report the results in ﬁgure 3 (best hyperparameters are listed intable 2).
012345678910x 1050102030405060708090100
StepsTest AccuracyPixel−by−pixel MNIST
  LSTMRNN + TanhRNN + ReLUsIRNN
012345678910x 1050102030405060708090100
StepsTest AccuracyPixel−by−pixel permuted MNIST
  LSTMRNN + TanhRNN + ReLUsIRNN
Figure 3: The results of recurrent methods on the “pixel-by-pixel MNIST” problem. We report thetest set accuracy for all methods. Left: normal MNIST. Right:p e r m u t e dM N I S T .
Problem
LSTM
RNN + Tanh
RNN + ReLUs
IRNN
MNIST
lr=0.01,g c=1
lr=1 0−8,g c=1 0
lr=1 0−8,g c=1 0
lr=1 0−8,g c=1
fb=1.0
permuted
lr=0.01,g c=1
lr=1 0−8,g c=1
lr=1 0−6,g c=1 0
lr=1 0−9,g c=1
MNIST
fb=1.0
Table 2: Best hyperparameters found for pixel-by-pixel MNIST problems after grid search.lris thelearning rate,gcis gradient clipping, andfbis the forget gate bias.The results using the standard scanline ordering of the pixels show that this problem is so difﬁcultthat standard RNNs fail to work, even with ReLUs, whereas theIRNN achieves 3% test error ratewhich is better than most off-the-shelf linear classiﬁers [21]. We were surprised that the LSTM didnot work as well as IRNN given the various initialization schemes that we tried. While it still possi-ble that a better tuned LSTM would do better, the fact that theIRNN perform well is encouraging.5rect(z)=max(z,0)
Perplexity	Results
4/21/16Richard	Socher27KN5	=	Count-based	language	model	with	Kneser-Ney	smoothing	&	5-gramsTable	from	paper	Extensions	of	recurrent	neural	network	language	modelby	Mikolovet	al	2011

Problem:	Softmaxis	huge	and	slow
4/21/16Richard	Socher28Trick:	Class-based	word	predictionp(wt|history)	=	p(ct|history)p(wt|ct)=	p(ct|ht)p(wt|ct)The	more	classes,the	better	perplexitybut	also	worse	speed:

One	last	implementation	trick
4/21/16Richard	Socher29•You	only	need	to	pass	backwards	through	your	sequence	once	and	accumulate	all	the	deltas	from	each	Et
Sequence	modeling	for	other	tasks
4/21/16Richard	Socher30•Classify	each	word	into:	•NER•Entity	level	sentiment	in	context	•opinionated	expressions•Example	application	and	slides	from	paper	Opinion	Mining	with	Deep	Recurrent	Nets	by	Irsoyand	Cardie2014
Opinion	Mining	with	Deep	Recurrent	Nets	
4/21/16Richard	Socher31Goal:	Classify	each	word	asdirect	subjective	expressions	(DSEs)	and	expressive	subjective	expressions	(ESEs).	DSE:	Explicit	mentions	of	private	states	or	speech	events	expressing	private	states	ESE:	Expressions	that	indicate	sentiment,	emotion,	etc.	without	explicitly	conveying	them.	
Example	Annotation
4/21/16Richard	Socher32In	BIO	notation	(tags	either	begin-of-entity	(B_X)	or	continuation-of-entity	(I_X)):The	committee,	[as	usual]ESE,	[has	refused	to	make	any	statements]DSE.	

Approach:	Recurrent	Neural	Network
4/21/16Richard	Socher33•Notation	from	paper	(so	you	get	used	to	different	ones)
•x	represents	a	token	(word)	as	a	vector.	•y	represents	the	output	label	(B,	I	or	O)–g	=	softmax!•h	is	the	memory,	computed	from	the	past	memory	and	current	word.	It	summarizes	the	sentence	up	to	that	time.Recurrent Neural Network ht=f(Wxt+Vht−1+b)yt=g(Uht+c)yhx    represents a token (word) as a vector.     represents the output label (B, I or O).     is the memory, computed from the past memory and current word. It summarizes the sentence up to that time. xyh
Bidirectional	RNNs
4/21/16Richard	Socher34Problem:	For	classification	you	want	to	incorporate	information	from	words	both	preceding	and	followingIdeas?Bidirectionality h!t=f(W!"!xt+V!"h!t−1+b!)h!t=f(W!""xt+V!"h!t+1+b!)yt=g(U[h!t;h!t]+c)yhx                 now represents (summarizes) the past and future around a single token. h=[h!;h!]
Deep	Bidirectional	RNNs
4/21/16Richard	Socher35Going Deep h!(i)t=f(W!"!(i)ht(i−1)+V!"(i)h!(i)t−1+b!(i))h!(i)t=f(W!""(i)ht(i−1)+V!"(i)h!(i)t+1+b!(i))yt=g(U[h!t(L);h!t(L)]+c)yh(3)xEach memory layer passes an intermediate sequential representation to the next. h(2)h(1)
Data
4/21/16Richard	Socher36•MPQA	1.2	corpus	(Wiebeet	al.,	2005)	•consists	of	535	news	articles	(11,111	sentences)	•manually	labeled	with	DSE	and	ESEs	at	the	phrase	level	•Evaluation:	F1

Evaluation
4/21/16Richard	Socher37Results: Deep vs Shallow RNNs 
57 59 61 63 65 67 Prop F1 DSE 
64 66 68 70 72 74 1 2 3 4 5 Bin F1 # Layers 47 49 51 53 55 57 ESE 24k 200k 
58 60 62 64 66 68 1 2 3 4 5 # Layers Results: Deep vs Shallow RNNs 
57 59 61 63 65 67 Prop F1 DSE 
64 66 68 70 72 74 1 2 3 4 5 Bin F1 # Layers 47 49 51 53 55 57 ESE 24k 200k 
58 60 62 64 66 68 1 2 3 4 5 # Layers 
Recap
4/21/16Richard	Socher38•Recurrent	Neural	Network	is	one	of	the	best	deepNLPmodel	families•Training	them	is	hard	because	of	vanishing	and	exploding	gradient	problems•They	can	be	extended	in	many	ways	and	their	training	improved	with	many	tricks	(more	to	come)•Next	week:	Most	important	and	powerful	RNN	extensions	with	LSTMs	and	GRUs
Introduction to Machine Translation
CMSC 723 / LING 723 / INST 725
Marine Carpuat
Slides & figure credits : Philipp Koehn 
mt-class.org
T oday’s topics
Machine Translation
•Historical Background
•Machine Translation is an old idea
•Machine Translation Today
•Use cases and method
•Machine Translation Evaluation
1947
When I look at an article in 
Russian, I say to myself: 
This is really written in 
English, but it has been 
coded in some strange 
symbols.  I will now 
proceed to decode.
Warren Weaver
1950s -1960s
•1954 Georgetown -IBM experiment
•250 words, 6 grammar rules
•1966 ALPAC report
•Skeptical in research progress
•Led to decreased US government funding for MT

Rule based systems
•Approach
•Build dictionaries
•Write transformation rules
•Refine, refine, refine
•Meteo system for weather 
forecasts (1976)
•Systran (1968), …

1988
More about the IBM story: 20 years of bitext workshop

Statistical Machine Translation
•1990s: increased research
•Mid 2000s: phrase -based MT
•(Moses, Google Translate)
•Around 2010: commercial viability
•Since mid 2010s: neural network models
MT History: Hype vs. Reality

How Good is Machine Translation?
Chinese > English

How Good is Machine Translation?
French > English

The Vauquois Triangle

Learning from Data
•What is the best translation?
•Counts in parallel corpus (aka bitext )
•Here European Parliament corpus

Learning from Data
•What is most fuent ?
•A language modeling problem!

Word Alignment

Phrase -based Models
•Input segmented in phrases
•Each phrase is translated in 
output language
•Phrases are reordered

Neural MT

What is MT good (enough) for?
•Assimilation: reader initiates translation, wants to know content
•User is tolerant of inferior quality
•Focus of majority of research
•Communication: participants in conversation don’t speak same language
•Users can ask questions when something is unclear
•Chat room translations, hand -held devices
•Often combined with speech recognition
•Dissemination: publisher wants to make content available in other 
languages
•High quality required
•Almost exclusively done by human translators
Applications

State of the Art
(rough estimates)

T oday’s topics
Machine Translation
•Historical Background
•Machine Translation is an old idea
•Machine Translation Today
•Use cases and method
•Machine Translation Evaluation
How good is a translation?
Problem: no single right answer

Evaluation
•How good is a given machine translation system?
•Many different translations acceptable
•Evaluation metrics
•Subjective judgments by human evaluators
•Automatic evaluation metrics
•Task -based evaluation
Adequacy and Fluency
•Human judgment
•Given: machine translation output
•Given: input and/or reference translation
•Task: assess quality of MT output
•Metrics
•Adequacy: does the output convey the meaning of the input sentence? Is 
part of the message lost, added, or distorted?
•Fluency: is the output fluent? Involves both grammatical correctness and 
idiomatic word choices.
Fluency and Adequacy: Scales


Let’s try:
rate fluency & adequacy on 1 -5 scale

Challenges in MT evaluation
•No single correct answer
•Human evaluators disagree
Automatic Evaluation Metrics
•Goal: computer program that computes quality of translations
•Advantages: low cost, optimizable, consistent
•Basic strategy
•Given: MT output
•Given: human reference translation
•Task: compute similarity between them
Precision and Recall of Words

Precision and Recall of Words

Word Error Rate

WER example

BLEU 
Bilingual Evaluation Understudy

Multiple Reference Translations

BLEU examples

Semantics -aware metrics: e.g., 
METEOR

Drawbacks of Automatic Metrics
•All words are treated as equally relevant
•Operate on local level
•Scores are meaningless (absolute value not informative)
•Human translators score low on BLEU
Yet automatic metrics such as BLEU 
correlate with human judgement

Caveats: bias toward statistical systems

Automatic metrics
•Essential tool for system development
•Use with caution: not suited to rank systems of different types
•Still an open area of research
•Connects with semantic analysis
T ask-Based Evaluation
Post-Editing Machine Translation

T ask-Based Evaluation
Content Understanding T ests

T oday’s topics
Machine Translation
•Historical Background
•Machine Translation is an old idea
•Machine Translation Today
•Use cases and method
•Machine Translation Evaluation
Feedforward Neural Networks
Michael Collins, Columbia University
Recap: Log-linear Models
A log-linear model takes the following form:
p(yjx;v) =exp (vf(x;y))P
y02Yexp (vf(x;y0))
If(x;y)is the representation of (x;y)
IAdvantage: f(x;y)is highly 
exible in terms of the features
that can be included
IDisadvantage: can be hard to design features by hand
INeural networks allow the representation itself to be
learned . Recent empirical results across a broad set of
domains have shown that learned representations in neural
networks can give very signicant improvements in accuracy
over hand-engineered features.
Example 1: The Language Modeling Problem
Iwiis thei'th word in a document
IEstimate a distribution p(wijw1;w2;:::wi 1)given previous
\history"w1;:::;wi 1.
IE.g.,w1;:::;wi 1=
Third, the notion \grammatical in English" cannot be
identied in any way with the notion \high order of
statistical approximation to English". It is fair to assume
that neither sentence (1) nor (2) (nor indeed any part of
these sentences) has ever occurred in an English
discourse. Hence, in any statistical
Example 2: Part-of-Speech Tagging
Hispaniola/NNP quickly/RB became/VB an/DT important/JJ
base/?? from which Spain expanded its empire into the rest of the
Western Hemisphere .
There are many possible tags in the position ??
fNN, NNS, Vt, Vi, IN, DT, . . . g
The task: model the distribution
p(tijt1;:::;ti 1;w1:::wn)
wheretiis thei'th tag in the sequence, wiis thei'th word
Overview
IBasic denitions
IStochastic gradient descent
IDening the input to a neural network
IA single neuron
IA single-layer feedforward network
IMotivation: the XOR problem
An Alternative Form for Log-Linear Models
Old form:
p(yjx;v) =exp (vf(x;y))P
y02Yexp (vf(x;y0))(1)
New form:
p(yjx;v) =exp (v(y)f(x) +
y)P
y02Yexp (v(y0)f(x) +
y0)(2)
IFeature vector f(x)maps input xtof(x)2RD.
IParameters: v(y)2RD,
y2Rfor eachy2Y.
IThe scorevf(x;y)in Eq. 1 has essentially been replaced by
v(y)f(x) +
yin Eq. 2.
IWe will use vto refer to the set of all parameter vectors and
bias values: that is, v=f(v(y);
y) :y2Yg
Introducing Learned Representations
p(yjx;;v) =exp (v(y)(x;) +
y)P
y02Yexp (v(y0)(x;) +
y0)(3)
IReplacedf(x)by(x;)whereare some additional
parameters of the model
IThe parameter values will be estimated from training
examples: the representation of xis then \learned"
IIn this lecture we'll show how feedforward neural networks
can be used to dene (x;).
Denition (Multi-Class Feedforward Models)
A multi-class feedforward model consists of:
IA setXof possible inputs. A nite set Yof possible labels. A
positive integer Dspecifying the number of features in the
feedforward representation.
IA parameter vector dening the feedforward parameters of the
network. We use 
to refer to the set of possible values for .
IA function:X
!RDthat maps any (x;)pair to a
\feedforward representation" (x;).
IFor each label y2Y, a parameter vector v(y)2RD, and a bias
value
y2R.
For anyx2X ,y2Y,p(yjx;;v) =exp (v(y)(x;) +
y)P
y02Yexp 
v(y0)(x;) +
y0
Two Questions
IHow can we dene the feedforward representation (x;)?
IGiven training examples (xi;yi)fori= 1:::n , how can we
train the parameters andv?
Overview
IBasic denitions
IStochastic gradient descent
IDening the input to a neural network
IA single neuron
IA single-layer feedforward network
IMotivation: the XOR problem
A Simple Version of Stochastic Gradient Descent
Inputs: Training examples (xi;yi)fori= 1:::n . A feedforward
representation (x;). An integer Tspecifying the number of
updates. A sequence of learning rate values 1:::Twhere each
t>0.
Initialization: Setvandto random parameter values.
A Simple Version of Stochastic Gradient Descent
(Continued)
Algorithm:
IFort= 1:::T
ISelect an integer iuniformly at random from f1:::ng
IDeneL(;v) = logp(yijxi;;v)
IFor each parameter j,j=j tdL(;v)
dj
IFor each label y, for each parameter vk(y),
vk(y) =vk(y) tdL(;v)
dvk(y)
IFor each label y,
y=
y tdL(;v)
d
y
Output: parameters andv
Overview
IBasic denitions
IStochastic gradient descent
IDening the input to a neural network
IA single neuron
IA single-layer feedforward network
IMotivation: the XOR problem
Dening the Input to a Feedforward Network
IGiven an input x, we need to dene a function f(x)2Rd
that species the input to the network
IIn general it is assumed that the representation f(x)is
\simple", not requiring careful hand-engineering.
IThe neural network will take f(x)as input, and will produce
a representation (x;)that depends on the input xand the
parameters .
Linear Models
We could build a log-linear model using f(x)as the
representation:
p(yjx;v) =expfv(y)f(x) +
ygP
y0expfv(y0)f(x) +
y0g(4)
This is a \linear" model, because the score v(y)f(x)is linear in
the input features f(x). The general assumption is that a model
of this form will perform poorly or at least non-optimally. Neural
networks enable \non-linear" models that often perform at much
higher levels of accuracy.
An Example: Digit Classication
ITask is to map an image xto a labely
IEach image contains a hand-written digit in the set
f0;1;2;:::9g
IThe representation f(x)simply represents pixel values in the
image.
IFor example if the image is 1616grey-scale pixels, where
each pixel takes some value indicating how bright it is, we
would have d= 256 , withf(x)just being the list of values
for the 256 dierent pixels in the image.
ILinear models under this representation perform poorly,
neural networks give much better performance
Simplifying Notation
IFrom now on assume that x=f(x): that is, the input xis
already dened as a vector
IThis will simplify notation
IBut remember that when using a neural network you will
have to dene a representation of the inputs
Overview
IBasic denitions
IStochastic gradient descent
IDening the input to a neural network
IA single neuron
IA single-layer feedforward network
IMotivation: the XOR problem
A Single Neuron
IA neuron is dened by a weight vector w2Rd, a biasb2R,
and a transfer function g:R!R.
IThe neuron maps an input vector x2Rdto an output has
follows:
h=g(wx+b)
IThe vectorw2Rdand scalarb2Rare parameters of the
model, which are learned from training examples.
Transfer Functions
IIt is important that the transfer function g(z)isnon-linear
IA linear transfer function would be
g(z) =z+
for some constants and
The Rectied Linear Unit (ReLU) Transfer Function
The ReLU transfer function is dened as
g(z) =fzifz0, or0ifz <0g
Or equivalently, g(z) = maxf0;zg
It follows that the derivative is
dg(z)
dz=f1ifz >0, or0ifz <0, or undened if z= 0g
The tanh Transfer Function
The tanh transfer function is dened as
g(z) =e2z 1
e2z+ 1
It can be shown that the derivative is
dg(z)
dz= (1 g(z))2
Calculating Derivatives
Given
h=g(wx+b)
it will be useful to calculate derivatives
dh
dwj
for the parameters w1;w2;:::wd, and also
dh
db
for the bias parameter b
Calculating Derivatives (Continued)
We can use the chain rule of dierentiation . First introduce an
intermediate variable z2R:
z=wx+b; h =g(z)
Then by the chain rule we have
dh
dwj=dh
dzdz
dwj=dg(z)
dzxj
Here we have useddh
dz=dg(z)
dz,dz
dwj=xj.
Calculating Derivatives (Continued)
We can use the chain rule of dierentiation . First introduce an
intermediate variable z2R:
z=wx+b; h =g(z)
Then by the chain rule we have
dh
db=dh
dzdz
db=dg(z)
dz1
Here we have useddh
dz=dg(z)
dz, anddz
db= 1.
Denition (Single-Layer Feedforward Representation)
A single-layer feedforward representation consists of the following:
IAn integerdspecifying the input dimension. Each input to
the network is a vector x2Rd.
IAn integermspecifying the number of hidden units.
IA parameter matrix W2Rmd. We use the vector Wk2Rd
for eachk2f1;2;:::mgto refer to the k'th row ofW.
IA vectorb2Rmof bias parameters.
IA transfer function g:R!R. Common choices are
g(x) =ReLU (x)org(x) =tanh(x).
Denition (Single-Layer Feedforward Representation
(Continued))
We then dene the following:
IFork= 1:::m , the input to the k'th neuron is
zk=Wkx+bk.
IFork= 1:::m , the output from the k'th neuron is
hk=g(zk).
IFinally, dene the vector (x;)2Rmask(x;) =hkfor
k= 1:::m . Heredenotes the parameters W2Rmdand
b2Rm. Hencecontainsm(d+ 1) parameters in total.
Some Intuition
The neural network employs munits, each with their own
parameters Wkandbk, and these neurons are used to construct a
\hidden" representation h2Rm.
Matrix Form
We can for example replace the operation
zk=Wkx+b fork= 1:::m
with
z=Wx+b
where the dimensions are as follows (note that an m-dimensional
column vector is equivalent to a matrix of dimension m1):
z|{z}
m1=W|{z}
mdx|{z}
d1|{z}
m1+b|{z}
m1
Denition (Single-Layer Feedforward Representation
(Matrix Form))
A single-layer feedforward representation consists of the following:
IAn integerdspecifying the input dimension. Each input to
the network is a vector x2Rd.
IAn integermspecifying the number of hidden units.
IA matrix of parameters W2Rmd.
IA vector of bias parameters b2Rm
IA transfer function g:Rm!Rm. Common choices would be
to deneg(z)to be a vector with components
ReLU (z1);ReLU (z2);:::; ReLU (zm)or
tanh(z1);tanh(z2);:::; tanh(zm).
Denition (Single-Layer Feedforward Representation
(Matrix Form) (Continued))
We then dene the following:
IThe vector of inputs to the hidden layer z2Rmis dened as
z=Wx+b.
IThe vector of outputs from the hidden layer h2Rmis
dened ash=g(z)
IFinally, dene (x;) =h. Here the parameters contain
the matrixWand the vector b.
IIt follows that
(x;) =g(Wx+b)
Overview
IBasic denitions
IStochastic gradient descent
IDening the input to a neural network
IA single neuron
IA single-layer feedforward network
IMotivation: the XOR problem
A Motivating Example: the XOR Problem (from Deep
Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville)
We will assume a training set where each label is in the set
Y=f 1;+1g, and there are 4 training examples, as follows:
x1= [0;0]; y1= 1
x2= [0;1]; y2= 1
x3= [1;0]; y3= 1
x4= [1;1]; y4= 1
A Useful Lemma
Assume we have a model of the form
p(yjx;v) =expfv(y)x+
ygP
yexpfv(y)x+
yg
and the set of possible labels is Y=f 1;+1g. Then for any x,
p(+1jx;v)>0:5
if and only if
ux+
 >0
whereu=v(+1) v( 1)and
=
+1 
 1. Similarly for any x,
p( 1jx;v)>0:5
if and only if ux+
 <0
Proof: We have
p(+1jx;v) =expfv(+1)x+
+1g
expfv(+1)x+
+1g+ expfv( 1)x+
 1g
=1
1 + expf (ux+
)g
It follows that p(+1jx;v)>0:5if and only if
expf (ux+
)g<1from which it follows that ux+
 >0.
A similar proof applies to the condition p( 1jx;v)>0:5.
Theorem
Assume we have examples (xi;yi)fori= 1:::4as dened above.
Assume we have a model of the form
p(yjx;v) =expfv(y)x+
ygP
yexpfv(y)x+
yg
Then there are no parameter settings for v(+1);v( 1);
+1;
 1
such that
p(yijxi;v)>0:5fori= 1:::4
Proof Sketch:
From the previous lemma, p(yi= 1jxi;v)>0:5if and only if
uxi+
 >0
whereu=v(+1) v( 1)and
=
+1 
 1.
Similarlyp(yi= 1jxi;v)>0:5if and only if
uxi+
 <0
whereu=v(+1) v( 1)and
=
+1 
 1.
Hence to satisfy p(yijxi;v)>0:5fori= 1:::4, there must exist
parameters vand
such that
u[0;0] +
 < 0 (5)
u[0;1] +
 > 0 (6)
u[1;0] +
 > 0 (7)
u[1;1] +
 < 0 (8)
The Constraints can not be Satised
u[0;0] +
 < 0
u[0;1] +
 > 0
u[1;0] +
 > 0
u[1;1] +
 < 0
The Constraints can not be Satised
u[0;0] +
 < 0
u[0;1] +
 > 0
u[1;0] +
 > 0
u[1;1] +
 < 0
Theorem
Assume we have examples (xi;yi)fori= 1:::4as dened above.
Assume we have a model of the form
p(yjx;;v) =expfv(y)(x;) +
ygP
yexpfv(y)(x;) +
yg
where(x;)is dened by a single layer neural network with
m= 2 hidden units, and the ReLU (z)activation function. Then
there are parameter settings for v(0);v(1);
0;
1;such that
p(yijxi;v)>0:5fori= 1:::4
Proof Sketch: DeneW1= [1;1],W2= [1;1],b1= 0,b2= 1.
Then for each input xwe can calculate the value for the vectors z
andhcorresponding to the inputs and the outputs from the
hidden layer:
x= [0;0])z= [0; 1])h= [0;0]
x= [1;0])z= [1;0])h= [1;0]
x= [0;1])z= [1;0])h= [1;0]
x= [1;1])z= [2;1])h= [2;1]
Proof Sketch (continued)
Hence to satisfy p(yijxi;v)>0:5fori= 1:::4, there must exist
parameters u=v(+1) v( 1)and
=
+1 
 1such that
u[0;0] +
 < 0 (9)
u[1;0] +
 > 0 (10)
u[1;0] +
 > 0 (11)
u[2;1] +
 < 0 (12)
It can be veried that u= [1; 2];
= 0:5satisies these
contraints.
The  Probabilistic  Approach  to  Learning  from  Data
1Prob.  Readings:Lecture  notes  from  10-­‐600  (See  Piazza  post  for  the  pointers)Murphy  2Bishop  2HTF  -­‐-­‐Mitchell  -­‐-­‐10-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  4January  30,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University

Reminders•Websitescheduleupdated•BackgroundExercises(Homework1)–Released:  Wed,  Jan.  25–Due:  Wed,  Feb.  1  at  5:30pm(Thedeadlinewasextended!)•Homework2:  NaiveBayes–Released:  Wed,  Feb.  1–Due:  Mon,  Feb.  13  at  5:30pm2
Outline•Generating  Data–Natural  (stochastic)  data–Synthetic  data–Why  synthetic  data?–Examples:  Multinomial,  Bernoulli,  Gaussian•Data  Likelihood–Independent  and  Identically  Distributed  (i.i.d.)–Example:  Dice  Rolls•Learning  from  Data  (Frequentist)–Principle  of  Maximum  Likelihood  Estimation  (MLE)–Optimization  for  MLE–Examples:  1D  and  2D  optimization–Example:  MLE  of  Multinomial–Aside:  Method  of  LangrangeMultipliers•Learning  from  Data  (Bayesian)–maximum  a  posteriori  (MAP)  estimation–Optimization  for  MAP–Example:  MAP  of  Bernoulli—Beta  3
Generating  DataWhiteboard–Natural  (stochastic)  data–Synthetic  data–Why  synthetic  data?–Examples:  Multinomial,  Bernoulli,  Gaussian
4
In-­‐Class  Exercise1.With  your  neighbor,  write  a  function  which  returns  samples  from  a  Categorical–Assume  access  to  the  rand()function–Function  signature  should  be:categorical_sample(phi)where  phi  is  the  array  of  parameters–Make  your  implementation  as  efficientas  possible!2.What  is  the  expected  runtimeof  your  function?5
Data  LikelihoodWhiteboard–Independent  and  Identically  Distributed  (i.i.d.)–Example:  Dice  Rolls
6
Learning  from  Data  (Frequentist)Whiteboard–Principle  of  Maximum  Likelihood  Estimation  (MLE)–Optimization  for  MLE–Examples:  1D  and  2D  optimization–Example:  MLE  of  Multinomial–Aside:  Method  of  LangrangeMultipliers
7
Learning  from  Data  (Bayesian)Whiteboard–maximum  a  posteriori  (MAP)  estimation–Optimization  for  MAP–Example:  MAP  of  Bernoulli—Beta  
8
Takeaways•One  view  of  what  ML  is  trying  to  accomplish  is  function  approximation•The  principle  of  maximum  likelihood  estimation  provides  an  alternate  view  of  learning•Synthetic  data  can  help  debugML  algorithms•Probability  distributions  can  be  used  to  modelreal  data  that  occurs  in  the  world(don’t  worry  we’ll  make  our  distributions  more  interesting  soon!)9
The  remaining  slides  are  extra  slidesfor  your  reference.Since  they  are  background  material  they  were  not  (explicitly)  covered  in  class.10
Outline  of  Extra  Slides•Probability  Theory–Sample  space,  Outcomes,  Events–Kolmogorov’s  Axioms  of  Probability•Random  Variables–Random  variables,  Probability  mass  function  (pmf),  Probability  density  function  (pdf),  Cumulative  distribution  function  (cdf)–Examples–Notation–Expectation  and  Variance–Joint,  conditional,  marginal  probabilities–Independence–Bayes’  Rule•Common  Probability  Distributions–Beta,  Dirichlet,  etc.11
PROBABILITY  THEORY
12
Probability  Theory:  Definitions
13
Sample  Space{Heads,Tails}OutcomeExample:  HeadsEventExample:  {Heads}ProbabilityP({Heads}) = 0.5P({Tails}) = 0.5 E  P(E)   Example  1:  Flipping  a  coin

Probability  Theory:  DefinitionsProbability  provides  a  science  for  inference  about  interesting  events
14Sample  SpaceThe  set  of  all  possible  outcomesOutcomePossibleresult  of  an  experimentEventAny  subset  of  the  sample  spaceProbabilityThe  non-­‐negative  number  assigned  to  each  event  in  the  sample  space E  P(E)•Each  outcome  is  unique•Only  one  outcome  can  occur  per  experiment•An  outcome  can  be  in  multiple  events•An  elementary  event  consists  of  exactly  one  outcome   
Probability  Theory:  Definitions
15Sample  Space{1,2,3,4,5,6}OutcomeExample:  3EventExample:  {3}  (theevent  “the  diecame  up  3”)ProbabilityP({3}) = 1/6P({4}) = 1/6 E  P(E)   Example  2:  Rolling  a  6-­‐sided  die

Probability  Theory:  Definitions
16Sample  Space{1,2,3,4,5,6}OutcomeExample:  3EventExample:  {2,4,6}  (theevent  “the  roll  waseven”)ProbabilityP({2,4,6}) = 0.5P({1,3,5}) = 0.5 E  P(E)   Example  2:  Rolling  a  6-­‐sided  die

Probability  Theory:  Definitions
17Sample  Space[0,  +∞)OutcomeExample:  1,433,600  hoursEventExample:  [1,  6]  hoursProbabilityP([1,6]) = 0.000000000001P([1,433,600,  +∞)) = 0.99 E  P(E)   Example  3:  Timing  how  long  it  takes  a  monkey  to  reproduce  Shakespeare

Kolmogorov’s  Axioms
181.P(E) 0,foralleventsE2.P( )=13.IfE1,E2,...aredisjoint,thenP(E1orE2or...)=P(E1)+P(E2)+...
Kolmogorov’s  Axioms
In  words:1.Each  event  has  non-­‐negative  probability.2.The  probability  that  someevent  will  occur  is  one.3.The  probability  of  the  union  of  many  disjoint  sets  is  the  sum  of  their  probabilities191.P(E) 0,foralleventsE2.P( )=13.IfE1,E2,...aredisjoint,thenP   i=1Ei =  i=1P(Ei)All  of  probability  can  be  derived  from  just  these!
Probability  Theory:  Definitions•The  complementof  an  event  E,  denoted  ~E,  is  the  event  that  Edoes  not  occur.
20 E~E
RANDOM  VARIABLES
21
Random  Variables:  Definitions
22RandomVariable(capitalletters)Def1:  Variablewhose  possible  values  are  the  outcomes  of  a  random  experimentValue  of  a  RandomVariable(lowercaseletters)The  value  taken  by  a  random  variableXx
Random  Variables:  Definitions
23RandomVariableDef1:  Variablewhose  possible  values  are  the  outcomes  of  a  random  experimentDiscreteRandom  VariableRandom  variable  whose  values  come  from  a  countable  set  (e.g.  the  natural  numbers  or  {True,  False})Continuous  RandomVariableRandom  variable  whose  values  come  from  aninterval  or  collection  of  intervals  (e.g.  the  real  numbers  or  the  range  (3,  5))XXX
Random  Variables:  Definitions
24RandomVariableDef1:  Variablewhose  possible  values  are  the  outcomes  of  a  random  experimentDef2:  A  measureable  function  from  the  sample  space  to  the  real  numbers:DiscreteRandom  VariableRandom  variable  whose  values  come  from  a  countable  set  (e.g.  the  natural  numbers  or  {True,  False})Continuous  RandomVariableRandom  variable  whose  values  come  from  aninterval  or  collection  of  intervals  (e.g.  the  real  numbers  or  the  range  (3,  5))XX:  EXX
Random  Variables:  Definitions
25Discrete  RandomVariableRandom  variable  whose  values  come  from  a  countable  set  (e.g.  the  natural  numbers  or  {True,  False})Probability  mass  function  (pmf)Function  giving  the  probabilitythat  discrete  r.v.  X  takes  value  x.Xp(x): =P(X=x)p(x)
Random  Variables:  Definitions
26Sample  Space{1,2,3,4,5,6}OutcomeExample:  3EventExample:  {3}  (theevent  “the  diecame  up  3”)ProbabilityP({3}) = 1/6P({4}) = 1/6 E  P(E)   Example  2:  Rolling  a  6-­‐sided  die

Random  Variables:  Definitions
27Sample  Space{1,2,3,4,5,6}OutcomeExample:  3EventExample:  {3}  (theevent  “the  diecame  up  3”)ProbabilityP({3}) = 1/6P({4}) = 1/6Discrete  Ran-­‐domVariableExample:  The  value  on  the  top  faceof  the  die.Prob.Mass  Function  (pmf)p(3)  =  1/6p(4)  =  1/6 E  P(E)   Example  2:  Rolling  a  6-­‐sided  die
Xp(x)

Random  Variables:  Definitions
28Sample  Space{1,2,3,4,5,6}OutcomeExample:  3EventExample:  {2,4,6}  (theevent  “the  roll  waseven”)ProbabilityP({2,4,6}) = 0.5P({1,3,5}) = 0.5Discrete  Ran-­‐domVariableExample:  1  if  thedie  landed  on  an  even  number  and  0  otherwiseProb.Mass  Function  (pmf)p(1)  =  0.5p(0)  =  0.5 E  P(E)   Example  2:  Rolling  a  6-­‐sided  die
Xp(x)

Random  Variables:  Definitions
29Discrete  RandomVariableRandom  variable  whose  values  come  from  a  countable  set  (e.g.  the  natural  numbers  or  {True,  False})Probability  mass  function  (pmf)Function  giving  the  probabilitythat  discrete  r.v.  X  takes  value  x.Xp(x): =P(X=x)p(x)
Random  Variables:  Definitions
30Continuous  RandomVariableRandom  variable  whose  values  come  from  aninterval  or  collection  of  intervals  (e.g.  the  real  numbers  or  the  range  (3,  5))Probability  density  function  (pdf)Function  thereturns  a  nonnegative  realindicating  the  relative  likelihood  that  a  continuous  r.v.  X  takes  value  xXf(x)•For  any  continuous  random  variable:  P(X = x) = 0•Non-­‐zero  probabilities  are  only  available  to  intervals:  P(a X b)= baf(x)dx
Random  Variables:  Definitions
31Sample  Space[0,  +∞)OutcomeExample:  1,433,600  hoursEventExample:  [1,  6]  hoursProbabilityP([1,6]) = 0.000000000001P([1,433,600,  +∞)) = 0.99Continuous  RandomVar.Example:  Represents  time  to  reproduce  (notaninterval!)Prob.  Density  FunctionExample:  Gamma  distribution E  P(E)   Example  3:  Timing  how  long  it  takes  a  monkey  to  reproduce  Shakespeare
Xf(x)
Random  Variables:  Definitions
32X=1X=2X=3X=4X=5Sample  SpaceΩ{1,2,3,4,5}EventsxThe  sub-­‐regions  1,  2,  3,  4,  or  5Discrete  Ran-­‐domVariableXRepresents  a  random  selection  of  a  sub-­‐regionProb.  MassFn.P(X=x)Proportionalto  size  of  sub-­‐region“Region”-­‐valued  Random  Variables

Random  Variables:  Definitions
33X=1X=2X=3X=4X=5Sample  SpaceΩAllpoints  in  the  region:  EventsxThe  sub-­‐regions  1,  2,  3,  4,  or  5Discrete  Ran-­‐domVariableXRepresents  a  random  selection  of  a  sub-­‐regionProb.  MassFn.P(X=x)Proportionalto  size  of  sub-­‐region“Region”-­‐valued  Random  Variables
Recallthat  an  event  is  any  subset  of  the  sample  space.So  both  definitions  of  the  sample  space  here  are  valid.

Random  Variables:  Definitions
34Sample  SpaceΩAll  Korean  sentences  (an  infinitely  large  set)EventxTranslationof  an  English  sentence  into  Korean  (i.e.  elementary  events)Discrete  Ran-­‐domVariableXRepresents  a  translationProbabilityP(X=x)Given  by  a  modelString-­‐valued  Random  Variables
machine  learning  requires  probability  and  statistics기계학습은확률과통계를필요머신러닝은확률통계를필요머신러닝은확률통계를이필요합니다P( X =                                                                    )P( X =                                                                    )P( X =                                                                    )…English:Korean:

Random  Variables:  Definitions
35Cumulativedistribution  functionFunctionthat  returns  the  probability  that  a  random  variable  X  is  less  than  or  equal  to  x:F(x)F(x)=P(X x)•For  discreterandom  variables:•For  continuousrandom  variables:F(x)=P(X x)= x <xP(X=x )= x <xp(x )F(x)=P(X x)= x  f(x )dx 
Answer:  P(X=x)is  just  shorthand!Example  1:Example  2:  Random  Variables  and  EventsQuestion:  Something  seems  wrong…•We  defined  P(E)(the  capital  ‘P’)  as  a  function  mapping  eventsto  probabilities•So  why  do  we  write  P(X=x)?•A  good  guess:  X=xis  an  event…
36RandomVariableDef2:  A  measureable  function  from  the  sample  space  to  the  real  numbers:X:  RP(X 7) P({   :X( ) 7})P(X=x) P({   :X( )=x})These  sets  are  events!
Notational  Shortcuts
37P(A|B)=P(A, B)P(B) Forallvaluesofaandb:P(A=a|B=b)=P(A=a, B=b)P(B=b)A  convenient  shorthand:
Notational  ShortcutsBut  then  how  do  we  tell  P(E)apart  from  P(X)?
38
Event
RandomVariable
P(A|B)=P(A, B)P(B)Instead  of  writing:We  should  write:PA|B(A|B)=PA,B(A, B)PB(B)…but  only  probability  theory  textbooks  go  to  such  lengths.
Expectation  and  Variance
39•Discrete  random  variables:E[X]= x Xxp(x)SupposeXcantakeanyvalueinthesetX.•Continuous  random  variables:E[X]= +   xf(x)dxThe  expected  value  of  Xis  E[X].  Also  called  the  mean.
Expectation  and  Variance
40The  varianceof  Xis  Var(X).Va r(X)=E[(X E[X])2]•Discrete  random  variables:Va r(X)= x X(x µ)2p(x)•Continuous  random  variables:Va r(X)= +   (x µ)2f(x)dxµ=E[X]
MULTIPLE  RANDOM  VARIABLESJoint  probabilityMarginal  probabilityConditional  probability
41
Joint  Probability
42Means, Variances and Covariances•Remember the deﬁnition of the mean and covariance of a vectorrandom variable:E[x]=/integraldisplayxxp(x)dx=mCov[x]=E[(x−m)(x−m)⊤]=/integraldisplayx(x−m)(x−m)⊤p(x)dx=Vwhich is the expected value of the outer product of the variablewith itself, after subtracting the mean.•Also, the covariance between two variables:Cov[x,y]=E[(x−mx)(y−my)⊤]=C=/integraldisplayxy(x−mx)(y−my)⊤p(x,y)dxdy=Cwhich is the expected value of the outer product of one variablewith another, after subtracting their means.Note:Cis not symmetric.Joint Probability•Key concept: two or more random variables may interact.Thus, the probability of one taking on a certain value dependso nwhich value(s) the others are taking.•We call this a joint ensemble and writep(x, y)=p r o b (X=xandY=y)
xyzp(x,y,z)Marginal Probabilities•We can ”sum out” part of a joint distribution to get themarginaldistributionof a subset of variables:p(x)=/summationdisplayyp(x, y)•This is like adding slices of the table together.
xyzxyzΣp(x,y)•Another equivalent deﬁnition:p(x)=/summationtextyp(x|y)p(y).Conditional Probability•If we know that some event has occurred, it changes our beliefabout the probability of other events.•This is like taking a ”slice” through the joint table.p(x|y)=p(x, y)/p(y)
xyzp(x,y|z)Slide  from  Sam  Roweis(MLSS,  2005)
Marginal  Probabilities
43Means, Variances and Covariances•Remember the deﬁnition of the mean and covariance of a vectorrandom variable:E[x]=/integraldisplayxxp(x)dx=mCov[x]=E[(x−m)(x−m)⊤]=/integraldisplayx(x−m)(x−m)⊤p(x)dx=Vwhich is the expected value of the outer product of the variablewith itself, after subtracting the mean.•Also, the covariance between two variables:Cov[x,y]=E[(x−mx)(y−my)⊤]=C=/integraldisplayxy(x−mx)(y−my)⊤p(x,y)dxdy=Cwhich is the expected value of the outer product of one variablewith another, after subtracting their means.Note:Cis not symmetric.Joint Probability•Key concept: two or more random variables may interact.Thus, the probability of one taking on a certain value dependso nwhich value(s) the others are taking.•We call this a joint ensemble and writep(x, y)=p r o b (X=xandY=y)
xyzp(x,y,z)Marginal Probabilities•We can ”sum out” part of a joint distribution to get themarginaldistributionof a subset of variables:p(x)=/summationdisplayyp(x, y)•This is like adding slices of the table together.
xyzxyzΣp(x,y)•Another equivalent deﬁnition:p(x)=/summationtextyp(x|y)p(y).Conditional Probability•If we know that some event has occurred, it changes our beliefabout the probability of other events.•This is like taking a ”slice” through the joint table.p(x|y)=p(x, y)/p(y)
xyzp(x,y|z)Slide  from  Sam  Roweis(MLSS,  2005)
Conditional  Probability
44Slide  from  Sam  Roweis(MLSS,  2005)Means, Variances and Covariances•Remember the deﬁnition of the mean and covariance of a vectorrandom variable:E[x]=/integraldisplayxxp(x)dx=mCov[x]=E[(x−m)(x−m)⊤]=/integraldisplayx(x−m)(x−m)⊤p(x)dx=Vwhich is the expected value of the outer product of the variablewith itself, after subtracting the mean.•Also, the covariance between two variables:Cov[x,y]=E[(x−mx)(y−my)⊤]=C=/integraldisplayxy(x−mx)(y−my)⊤p(x,y)dxdy=Cwhich is the expected value of the outer product of one variablewith another, after subtracting their means.Note:Cis not symmetric.Joint Probability•Key concept: two or more random variables may interact.Thus, the probability of one taking on a certain value dependso nwhich value(s) the others are taking.•We call this a joint ensemble and writep(x, y)=p r o b (X=xandY=y)
xyzp(x,y,z)Marginal Probabilities•We can ”sum out” part of a joint distribution to get themarginaldistributionof a subset of variables:p(x)=/summationdisplayyp(x, y)•This is like adding slices of the table together.
xyzxyzΣp(x,y)•Another equivalent deﬁnition:p(x)=/summationtextyp(x|y)p(y).Conditional Probability•If we know that some event has occurred, it changes our beliefabout the probability of other events.•This is like taking a ”slice” through the joint table.p(x|y)=p(x, y)/p(y)
xyzp(x,y|z)
Independence  and  Conditional  Independence
45Bayes’ Rule•Manipulating the basic deﬁnition of conditional probability givesone of the most important formulas in probability theory:p(x|y)=p(y|x)p(x)p(y)=p(y|x)p(x)/summationtextx′p(y|x′)p(x′)•This gives us a way of ”reversing”conditional probabilities.•Thus, all joint probabilities can be factored by selecting ano r d e r i n gfor the random variables and using the ”chain rule”:p(x, y, z, . . .)=p(x)p(y|x)p(z|x, y)p(...|x, y, z)
Independence & Conditional Independence•Two variables are independent iﬀtheir joint factors:p(x, y)=p(x)p(y)p(x,y)=xp(y)p(x)•Two variables are conditionally independent given a third one if forall values of the conditioning variable, the resulting slicef a c t o r s :p(x, y|z)=p(x|z)p(y|z)∀zEntropy•Measures the amount of ambiguity or uncertainty in a distribution:H(p)=−/summationdisplayxp(x)l o gp(x)•Expected value of−logp(x)(a function which depends on p(x)!).•H(p)>0unless only one possible outcomein which caseH(p)=0.•Maximal value when p is uniform.•Tells you the expected ”cost” if each event costs−logp(event)
Cross Entropy (KL Divergence)•An assymetric measure of the distancebetween two distributions:KL[p∥q]=/summationdisplayxp(x)[logp(x)−logq(x)]•KL >0unlessp=qthenKL=0•Tells you the extra cost if events were generated byp(x)butinstead of charging underp(x)you charged underq(x).
Slide  from  Sam  Roweis(MLSS,  2005)
MLE  AND  MAP
46
MLEWhat  does  maximizing  likelihood  accomplish?•There  is  only  a  finite  amount  of  probability  mass  (i.e.  sum-­‐to-­‐one  constraint)•MLE  tries  to  allocate  as  much  probability  mass  as  possible  to  the  things  we  have  observed……at  the  expenseof  the  things  we  have  notobserved47
MLE  vs.  MAP
48SupposewehavedataD={x(i)}Ni=1 MLE=`;Kt N i=1p(t(i)| ) MAP=`;Kt N i=1p(t(i)| )p( ) MLE=`;Kt N i=1p(t(i)| )Maximum  Likelihood  Estimate  (MLE)
Background:  MLEExample:  MLE  of  Exponential  Distribution
49•pdfofExponential( ):f(x)= e  x•SupposeXi Exponential( )for1 i N.•FindMLEfordataD={x(i)}Ni=1•Firstwritedownlog-likelihoodofsample.•Computeﬁrstderivative,settozero,solvefor .•Compute second derivative and check that it isconcavedownat MLE.
Background:  MLEExample:  MLE  of  Exponential  Distribution
50•Firstwritedownlog-likelihoodofsample. ( )=N i=1HQ;f(x(i))(1)=N i=1HQ;( 2tT(  x(i)))(2)=N i=1HQ;( )+  x(i)(3)=NHQ;( )  N i=1x(i)(4)
Background:  MLEExample:  MLE  of  Exponential  Distribution
51•Computeﬁrstderivative,settozero,solvefor .d ( )d =dd NHQ;( )  N i=1x(i)(1)=N  N i=1x(i)=0(2)  MLE=N Ni=1x(i)(3)
Background:  MLEExample:  MLE  of  Exponential  Distribution
52•pdfofExponential( ):f(x)= e  x•SupposeXi Exponential( )for1 i N.•FindMLEfordataD={x(i)}Ni=1•Firstwritedownlog-likelihoodofsample.•Computeﬁrstderivative,settozero,solvefor .•Compute second derivative and check that it isconcavedownat MLE.
MLE  vs.  MAP
53SupposewehavedataD={x(i)}Ni=1 MLE=`;Kt N i=1p(t(i)| ) MAP=`;Kt N i=1p(t(i)| )p( ) MAP=`;Kt N i=1p(t(i)| )p( )
Prior MLE=`;Kt N i=1p(t(i)| )Maximum  Likelihood  Estimate  (MLE)Maximum  a  posteriori(MAP)  estimate
COMMON  PROBABILITY  DISTRIBUTIONS54
Common  Probability  Distributions•For  Discrete  Random  Variables:–Bernoulli–Binomial–Multinomial–Categorical–Poisson•For  Continuous  Random  Variables:–Exponential–Gamma–Beta–Dirichlet–Laplace–Gaussian  (1D)–Multivariate  Gaussian55
Common  Probability  DistributionsBeta  Distribution000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049050051052053Shared Components Topic ModelsAnonymous Author(s)AfﬁliationAddressemail1 Distributionsf(⌅| ,⇥)=1B( ,⇥)x  1(1 x)⇥ 12 SCTMAProduct of Experts(PoE) [1] modelp(x|⇥1,...,⇥C)=QCc=1⌅cxPVv=1QCc=1⌅cv,where there areCcomponents, and the summation in the denominator is over all possible feature types.Latent Dirichlet allocation generative processFor each topick⇤{1,...,K}: k⇥Dir( )[draw distribution over words]For each documentm⇤{1,...,M}✓m⇥Dir(↵)[draw distribution over topics]For each wordn⇤{1,...,Nm}zmn⇥Mult(1,✓m)[draw topic]xmn⇥ zmi[draw word]The Finite IBP model generative processFor each componentc⇤{1,...,C}:[columns]⇤c⇥Beta( C,1)[draw probability of componentc]For each topick⇤{1,...,K}:[rows]bkc⇥Bernoulli(⇤c)[draw whether topic includescth component in its PoE]2.1 PoEp(x|⇥1,...,⇥C)=⇥Cc=1⌅cx Vv=1⇥Cc=1⌅cv(1)2.2 IBPLatent Dirichlet allocation generative processFor each topick⇤{1,...,K}: k⇥Dir( )[draw distribution over words]For each documentm⇤{1,...,M}✓m⇥Dir(↵)[draw distribution over topics]For each wordn⇤{1,...,Nm}zmn⇥Mult(1,✓m)[draw topic]xmn⇥ zmi[draw word]The Beta-Bernoulli model generative processFor each featurec⇤{1,...,C}:[columns]⇤c⇥Beta( C,1)For each classk⇤{1,...,K}:[rows]bkc⇥Bernoulli(⇤c)2.3 Shared Components Topic ModelsGenerative processWe can now present the formal generative process for the SCTM. For eachof theCshared components, we generate a distribution⇥cover theVwords from a Dirichletparametrized by . Next, we generate aK⇥Cbinary matrix using the ﬁnite IBP prior. We selectthe probability⇤cof each componentcbeing on (bkc=1) from a Beta distribution parametrized101234f( |↵, )
00.20.40.60.81 ↵=0.1, =0.9↵=0.5, =0.5↵=1.0, =1.0↵=5.0, =5.0↵=10.0, =5.0probability  density  function:
Common  Probability  DistributionsDirichletDistribution000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049050051052053Shared Components Topic ModelsAnonymous Author(s)AfﬁliationAddressemail1 Distributionsf(⌅| ,⇥)=1B( ,⇥)x  1(1 x)⇥ 12 SCTMAProduct of Experts(PoE) [1] modelp(x|⇥1,...,⇥C)=QCc=1⌅cxPVv=1QCc=1⌅cv,where there areCcomponents, and the summation in the denominator is over all possible feature types.Latent Dirichlet allocation generative processFor each topick⇤{1,...,K}: k⇥Dir( )[draw distribution over words]For each documentm⇤{1,...,M}✓m⇥Dir(↵)[draw distribution over topics]For each wordn⇤{1,...,Nm}zmn⇥Mult(1,✓m)[draw topic]xmn⇥ zmi[draw word]The Finite IBP model generative processFor each componentc⇤{1,...,C}:[columns]⇤c⇥Beta( C,1)[draw probability of componentc]For each topick⇤{1,...,K}:[rows]bkc⇥Bernoulli(⇤c)[draw whether topic includescth component in its PoE]2.1 PoEp(x|⇥1,...,⇥C)=⇥Cc=1⌅cx Vv=1⇥Cc=1⌅cv(1)2.2 IBPLatent Dirichlet allocation generative processFor each topick⇤{1,...,K}: k⇥Dir( )[draw distribution over words]For each documentm⇤{1,...,M}✓m⇥Dir(↵)[draw distribution over topics]For each wordn⇤{1,...,Nm}zmn⇥Mult(1,✓m)[draw topic]xmn⇥ zmi[draw word]The Beta-Bernoulli model generative processFor each featurec⇤{1,...,C}:[columns]⇤c⇥Beta( C,1)For each classk⇤{1,...,K}:[rows]bkc⇥Bernoulli(⇤c)2.3 Shared Components Topic ModelsGenerative processWe can now present the formal generative process for the SCTM. For eachof theCshared components, we generate a distribution⇥cover theVwords from a Dirichletparametrized by . Next, we generate aK⇥Cbinary matrix using the ﬁnite IBP prior. We selectthe probability⇤cof each componentcbeing on (bkc=1) from a Beta distribution parametrized101234f( |↵, )
00.20.40.60.81 ↵=0.1, =0.9↵=0.5, =0.5↵=1.0, =1.0↵=5.0, =5.0↵=10.0, =5.0probability  density  function:
Common  Probability  DistributionsDirichletDistribution000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049050051052053Shared Components Topic ModelsAnonymous Author(s)AfﬁliationAddressemail1 DistributionsBetaf(⇤| ,⇥)=1B( ,⇥)x  1(1 x)⇥ 1Dirichletp(⌅⇤| )=1B( )K⇤k=1⇤ k 1kwhereB( )=⇥Kk=1 ( k) ( Kk=1 k)(1)2 SCTMAProduct of Experts(PoE) [1] modelp(x|⇥1,...,⇥C)=QCc=1⌅cxPVv=1QCc=1⌅cv,where there areCcomponents, and the summation in the denominator is over all possible feature types.Latent Dirichlet allocation generative processFor each topick⇤{1,...,K}: k⇥Dir( )[draw distribution over words]For each documentm⇤{1,...,M}✓m⇥Dir(↵)[draw distribution over topics]For each wordn⇤{1,...,Nm}zmn⇥Mult(1,✓m)[draw topic]xmn⇥ zmi[draw word]The Finite IBP model generative processFor each componentc⇤{1,...,C}:[columns]⇤c⇥Beta( C,1)[draw probability of componentc]For each topick⇤{1,...,K}:[rows]bkc⇥Bernoulli(⇤c)[draw whether topic includescth component in its PoE]2.1 PoEp(x|⇥1,...,⇥C)=⇥Cc=1⇤cx Vv=1⇥Cc=1⇤cv(2)2.2 IBPLatent Dirichlet allocation generative processFor each topick⇤{1,...,K}: k⇥Dir( )[draw distribution over words]For each documentm⇤{1,...,M}✓m⇥Dir(↵)[draw distribution over topics]For each wordn⇤{1,...,Nm}zmn⇥Mult(1,✓m)[draw topic]xmn⇥ zmi[draw word]The Beta-Bernoulli model generative processFor each featurec⇤{1,...,C}:[columns]⇤c⇥Beta( C,1)For each classk⇤{1,...,K}:[rows]bkc⇥Bernoulli(⇤c)100.20.40.60.81 200.250.50.751 11.522.53p(~ |~↵)
00.20.40.60.81 200.250.50.751 1051015p(~ |~↵)
probability  density  function:
K-­‐Means+GMMs
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  16March  20,  2017Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Clustering  Readings:Murphy  25.5Bishop  12.1,  12.3HTF  14.3.0Mitchell  -­‐-­‐EM,  GMM  Readings:Murphy  11.4.1,  11.4.2,  11.4.4Bishop  9HTF  8.5  -­‐8.5.3Mitchell  6.12  -­‐6.12.2
Reminders•Homework5:  Readings/  Applicationof  ML–Release:  Wed,  Mar.  08–Due:  Wed,  Mar.  22  at  11:59pm
2
Peer  Tutoring
3TutorTutee

K-­‐MEANS
5
K-­‐Means  Outline•Clustering:  Motivation  /  Applications•Optimization  Background–Coordinate  Descent–Block  Coordinate  Descent•Clustering–Inputs  and  Outputs–Objective-­‐based  Clustering•K-­‐Means–K-­‐Means  Objective–Computational  Complexity–K-­‐Means  Algorithm  /  Lloyd’s  Method•K-­‐Means  Initialization–Random–Farthest  Point–K-­‐Means++6
This  Lecture
Last  Lecture
Clustering,  Informal  GoalsGoal:  Automatically  partition  unlabeleddata  into  groups  of  similar  datapoints.Question:  When  and  why  would  we  want  to  do  this?•  Automatically  organizing  data.Useful  for:•  Representing  high-­‐dimensional  data  in  a  low-­‐dimensional  space  (e.g.,  for  visualization  purposes).•  Understanding  hidden  structure  in  data.•  Preprocessing  for  further  analysis.
Slide  courtesy  of  Nina  Balcan
•Cluster  news  articles  or  web  pages  or  search  results  by  topic.
Applications
(Clustering  
comes  up  
everywhere…)
•Cluster  protein  sequences  by  function  or  genes  according  to  expression  profile.•Cluster  users  of  social  networks  by  interest  (community  detection).
Facebook network
Twitter NetworkSlide  courtesy  of  Nina  Balcan
•Cluster  customers  according  to  purchase  history.
Applications  
(Clustering  comes  up  everywhere…)
•Cluster  galaxies  or  nearby  stars(e.g.  Sloan  Digital  Sky  Survey)
•And  many  manymore  applications….Slide  courtesy  of  Nina  Balcan
Optimization  BackgroundWhiteboard:–Coordinate  Descent–Block  Coordinate  Descent
10
ClusteringQuestion:  Which  of  these  partitions  is  “better”?
11
ClusteringWhiteboard:–Inputs  and  Outputs–Objective-­‐based  Clustering
12
K-­‐MeansWhiteboard:–K-­‐Means  Objective–Computational  Complexity–K-­‐Means  Algorithm  /  Lloyd’s  Method
13
K-­‐Means  InitializationWhiteboard:–Random–Furthest  Traversal–K-­‐Means++
14
Lloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Example:  Given  a  set  of  datapointsLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Select  initial  centers  at  randomLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Assign  each  point  to  its  nearest  centerLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Recomputeoptimal  centers  given  a  fixed  clusteringLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Assign  each  point  to  its  nearest  centerLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Recomputeoptimal  centers  given  a  fixed  clusteringLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Assign  each  point  to  its  nearest  centerLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Recomputeoptimal  centers  given  a  fixed  clusteringLloyd’s  method:  Random  Initialization
Get  a  good    quality  solution  in  this  example.Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
It  always  converges,  but  it  may  converge  at  a  local  optimum  that  is  different  from  the  global  optimum,  and  in  fact  could  be  arbitrarily  worse  in  terms  of  its  score.Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
Local  optimum:  every  point  is  assigned  to  its  nearest  center  and  every  center  is  the  mean  value  of  its  points.Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
.It  is  arbitrarily  worse  than  optimum  solution….
Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
This  bad  performance,  can  happen  even  with  well  separated  Gaussian  clusters.Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
This  bad  performance,  can  happen  even  with  well  separated  Gaussian  clusters.
Some  Gaussian  are  combined…..Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
•For  k  equal-­‐sized  Gaussians,  Pr[each  initial  center  is  in  a  different  Gaussian]  ≈"!"$≈%&$•Becomes  unlikely  as  k  gets  large.  •If  we  do  random  initialization,  as  kincreases,  it  becomes  more  likely  we  won’t  have  perfectly  picked  one  center  per  Gaussian  in  our  initialization  (so  Lloyd’s  method  will  output  a  bad  solution).
Slide  courtesy  of  Nina  Balcan
Another  Initialization  Idea:  Furthest  Point  HeuristicChoose  𝐜𝟏arbitrarily  (or  at  random).•Pick  𝐜𝐣among  datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝐧that  is  farthest  from  previously  chosen  𝐜𝟏,𝐜𝟐,…,𝐜𝒋0𝟏•For  j=2,…,kFixes  the  Gaussian  problem.  But  it  can  be  thrown  off  by  outliers….Slide  courtesy  of  Nina  Balcan
Furthest  point  heuristic  does  well  on  previous  example
Slide  courtesy  of  Nina  Balcan
(0,1)(0,-­‐1)(-­‐2,0)(3,0)Furthest  point  initialization  heuristic  sensitive  to  outliersAssume  k=3
Slide  courtesy  of  Nina  Balcan
(0,1)(0,-­‐1)(-­‐2,0)(3,0)Furthest  point  initialization  heuristic  sensitive  to  outliersAssume  k=3
Slide  courtesy  of  Nina  Balcan
K-­‐means++  Initialization:  D6sampling  [AV07]•Choose  𝐜𝟏at  random.•Pick  𝐜𝐣among  𝐱𝟏,𝐱𝟐,…,𝐱𝒏according  to  the  distribution•For  j=2,…,k•Interpolate  between  random  and  furthest  point  initialization
𝐏𝐫(𝐜𝐣=𝐱𝐢)∝𝐦𝐢𝐧𝐣?@𝐣	
  𝐱𝐢−𝐜𝐣?𝟐•Let  D(x)be  the  distance  between  a  point  𝑥and  its  nearest  center.  Chose  the  next  center  proportional  to  D6(𝐱).D6(𝐱𝐢)Theorem:  K-­‐means++  always  attains  an  O(log  k)  approximation  to  optimal  k-­‐means  solution  in  expectation.Running  Lloyd’scan  only  further  improve  the  cost.Slide  courtesy  of  Nina  Balcan
K-­‐means++  Idea:  D6sampling•Interpolate  between  random  and  furthest  point  initialization•Let  D(x)be  the  distance  between  a  point  𝑥and  its  nearest  center.Chose  the  next  center  proportional  to  DD𝐱=𝐦𝐢𝐧𝐣?@𝐣	
  𝐱𝐢−𝐜𝐣?𝜶.•𝛼=0,  random  sampling•𝛼=∞,  furthest  point  (Side  note:  it  actually  works  well  for  k-­‐center)•𝛼=2,  k-­‐means++Side  note:  𝛼=1,  works  well  for  k-­‐median  Slide  adapted  from  Nina  Balcan
(0,1)(0,-­‐1)(-­‐2,0)(3,0)K-­‐means  ++  Fix
Slide  courtesy  of  Nina  Balcan
K-­‐means++/Lloyd’sRunning  Time
Repeatuntil  there  is  no  change  in  the  cost.•For  each  j:    CK←{𝑥∈𝑆whose  closest  center  is  𝐜𝐣}•For  each  j:  𝐜𝐣←mean  of  CKEach  round  takes  time  O(nkd).•K-­‐means  ++  initialization:  O(nd)  and  one  pass  over  data  to  select  next  center.  So  O(nkd)  time  in  total.•Lloyd’s  method•Exponential  #  of  rounds  in  the  worst  case  [AV07].•Expected  polynomial  time  in  the  smoothed  analysis  (non  worst-­‐case)  model!Slide  courtesy  of  Nina  Balcan
K-­‐means++/Lloyd’s  Summary•Exponential  #  of  rounds  in  the  worst  case  [AV07].•Expected  polynomial  time  in  the  smoothed  analysis  model!•K-­‐means++  always  attains  an  O(log  k)  approximation  to  optimal  k-­‐means  solution  in  expectation.•Running  Lloyd’s  can  only  further  improve  the  cost.•Does  well  in  practice.
Slide  courtesy  of  Nina  Balcan
What  value  of  k?•Hold-­‐out  validation/cross-­‐validation  on  auxiliary  task  (e.g.,  supervised  learning  task).•Heuristic:  Find  large  gap  between  (k  -­‐1)-­‐means  cost  and  k-­‐means  cost.•Try  hierarchical  clustering.
Slide  courtesy  of  Nina  Balcan
EM  AND  GMMS
43
Expectation-­‐Maximization  Outline•Background–Multivariate  Gaussian  Distribution–Marginal  Probabilities•Building  up  to  GMMs–Distinction  #1:  Model  vs.  Objective  Function–Gaussian  Naïve  Bayes  (GNB)–Gaussian  Discriminant  Analysis–Gaussian  Mixture  Model  (GMM)•Expectation-­‐Maximization–Distinction  #2:  Data  Likelihood  vs.  Marginal  Data  Likelihood–Distinction  #3:  Latent  Variables  vs.  Parameters–Objective  Functions  for  EM–Hard  (Viterbi)  EM  Algorithm•Example:  K-­‐Means  as  Hard  EM–Soft  (Standard)  EM  Algorithm•Example:  Soft  EM  for  GMM•Properties  of  EM–Nonconvexity/  Local  Optimization–Example:  Grammar  Induction–Variants  of  EM44
BackgroundWhiteboard–Multivariate  Gaussian  Distribution–Marginal  Probabilities
46
GAUSSIAN  MIXTURE  MODEL  (GMM)47
Building  up  to  GMMsWhiteboard–Distinction  #1:  Model  vs.  Objective  Function–Gaussian  Naïve  Bayes  (GNB)–Gaussian  Discriminant  Analysis–Gaussian  Mixture  Model  (GMM)
48
Gaussian  Discriminant  Analysis
49Data:  Model:Joint:Log-­‐likelihood:Generative  Story:z Categorical( )t Gaussian(µz, z)p(t,z; ,µ, )=p(t|z;µ, )p(z; )D={(t(i),x(i))}Ni=1wheret(i) RMandz(i) {1,...,K}
 ( ,µ, )=HQ;N i=1p(t(i),z(i); ,µ, )=N i=1HQ;p(t(i)|z(i);µ, )+HQ;p(z(i); )
Gaussian  Discriminant  Analysis
50Data:  Log-­‐likelihood:D={(t(i),x(i))}Ni=1wheret(i) RMandz(i) {1,...,K}Maximum  Likelihood  Estimates:Take  the  derivative  of  the  Lagrangian,  set  it  equal  to  zero  and  solve. k=1NN i=1I(z(i)=k), kµk= Ni=1I(z(i)=k)t(i) Ni=1I(z(i)=k), k k= Ni=1I(z(i)=k)(t(i) µk)(t(i) µk)T Ni=1I(z(i)=k), k ( ,µ, )=N i=1HQ;p(t(i)|z(i);µ, )+HQ;p(z(i); )Implementation:  Just  counting  
Gaussian  Mixture-­‐Model
51Data:  Assumewearegivendata,D,consistingofNfullyunsupervisedex-amplesinMdimensions:D={t(i)}Ni=1wheret(i) RMModel:Joint:Marginal:(Marginal)  Log-­‐likelihood:Generative  Story:z Categorical( )t Gaussian(µz, z)p(t; ,µ, )=K z=1p(t|z;µ, )p(z; )p(t,z; ,µ, )=p(t|z;µ, )p(z; ) ( ,µ, )=HQ;N i=1p(t(i); ,µ, )=N i=1HQ;K z=1p(t(i)|z;µ, )p(z; )
Mixture-­‐Model
52Data:  Assumewearegivendata,D,consistingofNfullyunsupervisedex-amplesinMdimensions:D={t(i)}Ni=1wheret(i) RMModel:p , (t,z)=p (t|z)p (z)p , (t)=K z=1p (t|z)p (z)Joint:Marginal:(Marginal)  Log-­‐likelihood: ( )=HQ;N i=1p , (t(i))=N i=1HQ;K z=1p (t(i)|z)p (z)Generative  Story:z Multinomial( )t p (·|z)z Categorical( )t Gaussian(µz, z)
Mixture-­‐Model
53Data:  Assumewearegivendata,D,consistingofNfullyunsupervisedex-amplesinMdimensions:D={t(i)}Ni=1wheret(i) RMModel:p , (t,z)=p (t|z)p (z)p , (t)=K z=1p (t|z)p (z)Joint:Marginal:(Marginal)  Log-­‐likelihood: ( )=HQ;N i=1p , (t(i))=N i=1HQ;K z=1p (t(i)|z)p (z)Generative  Story:z Multinomial( )t p (·|z)z Categorical( )t Gaussian(µz, z)
This  could  be  any  arbitrary  distribution  parameterized  by  θ.Today  we’re  thinking  about  the  case  where  it  is  a  Multivariate  Gaussian.

Unsupervised  Learning:  Parameters  are  coupled  by  marginalization.Supervised  Learning:  The  parameters  decouple!Learning  a  Mixture  Model
54
X1
XM
X2
Z
…  ,  =`;Kt , N i=1HQ;p (t(i)|z(i))p (z(i))  =`;Kt N i=1HQ;p (t(i)|z(i))  =`;Kt N i=1HQ;p (z(i))
X1
XM
X2
Z
…D={t(i)}Ni=1
  ,  =`;Kt , N i=1HQ;K z=1p (t(i)|z)p (z)D={(t(i),x(i))}Ni=1
Unsupervised  Learning:  Parameters  are  coupled  by  marginalization.Supervised  Learning:  The  parameters  decouple!Learning  a  Mixture  Model
55
X1
XM
X2
Z
…  ,  =`;Kt , N i=1HQ;p (t(i)|z(i))p (z(i))  =`;Kt N i=1HQ;p (t(i)|z(i))  =`;Kt N i=1HQ;p (z(i))
X1
XM
X2
Z
…D={t(i)}Ni=1
  ,  =`;Kt , N i=1HQ;K z=1p (t(i)|z)p (z)
Training  certainly  isn’t  as  simple  as  the  supervised  case.In  many  cases,  we  could  still  use  some  black-­‐box  optimization  method  (e.g.  Newton-­‐Raphson)  to  solve  this  coupledoptimization  problem.This  lecture  is  about  a  more  problem-­‐specific  method:  EM.
D={(t(i),x(i))}Ni=1
EXAMPLE:  K-­‐MEANS  VS  GMM
56
Example:  K-­‐Means
57
Example:  K-­‐Means
58
Example:  K-­‐Means
59
Example:  K-­‐Means
60
Example:  K-­‐Means
61
Example:  K-­‐Means
62
Example:  K-­‐Means
63
Example:  K-­‐Means
64
Example:  GMM
65
Example:  GMM
66
Example:  GMM
67
Example:  GMM
68
Example:  GMM
69
Example:  GMM
70
Example:  GMM
71
Example:  GMM
72
Example:  GMM
73
Example:  GMM
74
Example:  GMM
75
Example:  GMM
76
Example:  GMM
77
Example:  GMM
78
Example:  GMM
79
Example:  GMM
80
Example:  GMM
81
Example:  GMM
82
Example:  GMM
83
Example:  GMM
84
Example:  GMM
85
Example:  GMM
86
K-­‐Means  vs.  GMMConvergence:  K-­‐Meanstends  to  convergemuch  faster  than  a  GMMSpeed:  Each  iteration  of  K-­‐Meansis  computationally  less  intensivethan  each  iteration  of  a  GMMInitialization:  To  initializea  GMM,  we  typically  first  run  K-­‐Meansand  use  the  resulting  cluster  centers  as  the  means  of  the  Gaussian  componentsOutput:  A  GMM  yields  a  probability  distribution  over  the  cluster  assignment  for  each  point;  whereas  K-­‐Meansgives  a  single  hard  assignment87
Perceptron+Kernels
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  11February  22,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Perceptron  Readings:Murphy  8.5.4Bishop  4.1.7HTF  -­‐-­‐Mitchell  4.4.0Kernel  Readings:Murphy  14.1  –14.2.4Bishop  6.1  –6.2HTF  -­‐-­‐Mitchell  -­‐-­‐
Reminders•Homework3:  Linear  /  LogisticRegression–Release:  Mon,  Feb.  13–Due:  Wed,  Feb.  22  at  11:59pm•CourseSurvey[5  pts]–dueFri,  Feb  24  at  11:59pm•Homework4:  Perceptron/  Kernels/  SVM–Release:  Wed,  Feb.  22–Due:  Fri,  Mar.  03  at  11:59pm3New  due  date(9  days  for  HW4)
Outline•Perceptron–Online  Learning–Perceptron  Algorithm–Margin  Definitions–Perceptron  Mistake  Bound•Kernels–Kernel  Perceptron–Kernel  as  a  dot  product–Gram  matrix–Examples:  Polynomial,  RBF•Support  Vector  Machine  (SVM)4
This  Lecture
Last  Lecture
PERCEPTRON
5
Why  don’t  we  drop  the  generative  model  and  try  to  learn  this  hyperplanedirectly?
Background:  Hyperplanes
Background:  HyperplanesH={x:wTx=b}Hyperplane(Definition  1):  
w
Half-­‐spaces:  H+={x:wTx>0 andx1=1}H ={x:wTx<0 andx1=1}Hyperplane(Definition  2):  H={x:wTx=0andx1=1}x0x0x0
Why  don’t  we  drop  the  generative  model  and  try  to  learn  this  hyperplanedirectly?
Background:  Hyperplanes
Directly  modeling  the  hyperplanewould  use  a  decision  function:for:h(t)=sign( Tt)y { 1,+1}
Online  LearningFor  i=  1,  2,  3,  …:•Receivean  unlabeled  instance  x(i)•Predicty’  =  h(x(i))•Receivetrue  label  y(i)Checkfor  correctness  (y’  ==  y(i))Goal:•Minimizethe  number  of  mistakes10
Online  Learning:  MotivationExamples1.Email  classification  (distribution  of  both  spam  and  regular  mail  changes  over  time,  but  the  target  function  stays  fixed  -­‐last  year's  spam  still  looks  like  spam).2.Recommendation  systems.  Recommending  movies,  etc.3.Predicting  whether  a  user  will  be  interested  in  a  new  news  article  or  not.4.Ad  placement  in  a  new  market.11Slide  from  Nina  Balcan
Perceptron  Algorithm
12Learning:  Iterative  procedure:•while  not  converged•receivenext  example  (x(i),  y(i))•predicty’  =  h(x(i))•ifpositive  mistake:  addx(i)to  parameters•ifnegative  mistake:  subtractx(i)from  parametersData:  Inputs  are  continuous  vectors  of  length  K.  Outputs  are  discrete.D={t(i),y(i)}Ni=1wheret RKandy {+1, 1}Prediction:  Output  determined  by  hyperplane.ˆy=h (x) = sign( Tx)sign(a)= 1,ifa 0 1,otherwise

Perceptron  Algorithm
13Learning:Data:  Inputs  are  continuous  vectors  of  length  K.  Outputs  are  discrete.D={t(i),y(i)}Ni=1wheret RKandy {+1, 1}Prediction:  Output  determined  by  hyperplane.ˆy=h (x) = sign( Tx)sign(a)= 1,ifa 0 1,otherwise

Perceptron  Algorithm:  ExampleExample:−1,2−-++𝜃&=(0,0)𝜃+=𝜃&−−1,2=(1,−2)𝜃,=𝜃++1,1=(2,−1)𝜃.=𝜃,−−1,−2=(3,1)+--Algorithm:§Set  t=1,  start  with  all-­‐zeroes  weight  vector  𝑤&.§Given  example  𝑥,  predict  positive  iff𝜃2⋅𝑥≥0.§On  a  mistake,  update  as  follows:  •Mistake  on  positive,  update  𝜃26&←𝜃2+𝑥•Mistake  on  negative,  update  𝜃26&←𝜃2−𝑥1,0+1,1+−1,0−−1,−2−1,−1+XaXaXa
Slide  adapted  from  Nina  Balcan
Geometric  MarginDefinition:The  marginof  example  𝑥w.r.t.a  linear  sep.𝑤is  the  distance  from  𝑥	
  to  the  plane  𝜃	
  ⋅𝑥=0(or  the  negativeif  on  wrong  side)𝑥&wMargin  of  positive  example  𝑥&
𝑥+Margin  of  negative  example  𝑥+Slide  from  Nina  Balcan
Geometric  MarginDefinition:The  margin  𝛾:of  a  set  of  examples  𝑆wrta  linear  separator  𝑤is  the  smallest  margin  over  points  𝑥∈𝑆.++++++-----𝛾:𝛾:+----+wDefinition:The  marginof  example  𝑥w.r.t.a  linear  sep.𝑤is  the  distance  from  𝑥	
  to  the  plane  𝑤⋅𝑥=0(or  the  negativeif  on  wrong  side)
Slide  from  Nina  Balcan
++++-----𝛾𝛾+----wDefinition:The  margin  𝛾of  a  set  of  examples  𝑆is  the  maximum𝛾:over  all  linear  separators  𝑤.Geometric  MarginDefinition:The  margin  𝛾:of  a  set  of  examples  𝑆wrta  linear  separator  𝑤is  the  smallest  margin  over  points  𝑥∈𝑆.Definition:The  marginof  example  𝑥w.r.t.a  linear  sep.𝑤is  the  distance  from  𝑥	
  to  the  plane  𝑤⋅𝑥=0(or  the  negativeif  on  wrong  side)
Slide  from  Nina  Balcan
Analysis:  Perceptron
18Slide  adapted  from  Nina  Balcan(Normalized  margin:  multiplyingall  points  by  100,  or  dividing  all  points  by  100,  doesn’t  change  the  number  of  mistakes;  algois  invariant  to  scaling.)Perceptron  Mistake  BoundGuarantee:Ifdatahasmargin andallpointsinsideaballofradiusR,thenPerceptronmakes (R/ )2mistakes.+++++++-----gg----+
R  
Analysis:  Perceptron
19Figure  from  Nina  BalcanPerceptron  Mistake  Bound
+++++++-----gg----+R  Theorem0.1(Block(1962),Novikoﬀ(1962)).Givendataset:D={(t(i),y(i))}Ni=1.Suppose:1.Finitesizeinputs:||x(i)|| R2.Linearly separable data:   s.t.||  ||=1andy(i)(  ·t(i))  , iThen: ThenumberofmistakesmadebythePerceptronalgorithmonthisdatasetisk (R/ )2
Analysis:  Perceptron
20Proof  of  Perceptron  Mistake  Bound:We  will  show  that  there  exist  constants  A  and  B  s.t.Ak || (k+1)|| B kAk || (k+1)|| B kAk || (k+1)|| B kAk || (k+1)|| B kAk || (k+1)|| B k
Analysis:  Perceptron
21+++++++-----gg----+R  Theorem0.1(Block(1962),Novikoﬀ(1962)).Givendataset:D={(t(i),y(i))}Ni=1.Suppose:1.Finitesizeinputs:||x(i)|| R2.Linearly separable data:   s.t.||  ||=1andy(i)(  ·t(i))  , iThen: ThenumberofmistakesmadebythePerceptronalgorithmonthisdatasetisk (R/ )2Algorithm1PerceptronLearningAlgorithm(Online)1:procedurePĊėĈĊĕęėĔē(D={(t(1),y(1)),(t(2),y(2)),...})2:  0,k=1 Initializeparameters3:fori {1,2,...}do Foreachexample4:ify(i)( (k)·t(i)) 0then Ifmistake5: (k+1)  (k)+y(i)t(i) Updateparameters6:k k+17:return 
Analysis:  PerceptronWhiteboard:  Proof  of  Perceptron  Mistake  Bound
22
Analysis:  Perceptron
23Proof  of  Perceptron  Mistake  Bound:Part  1:  for  some  A,  Ak || (k+1)|| B k (k+1)·  =( (k)+y(i)t(i))  byPerceptronalgorithmupdate= (k)·  +y(i)(  ·t(i))  (k)·  + byassumption  (k+1)·   k byinductiononksince (1)=0 || (k+1)|| k since||r|| ||m|| r·mand||  ||=1
Cauchy-­‐Schwartz  inequality
Analysis:  Perceptron
24Proof  of  Perceptron  Mistake  Bound:Part  2:  for  some  B,  Ak || (k+1)|| B k|| (k+1)||2=|| (k)+y(i)t(i)||2byPerceptronalgorithmupdate=|| (k)||2+(y(i))2||t(i)||2+2y(i)( (k)·t(i)) || (k)||2+(y(i))2||t(i)||2sincekthmistake y(i)( (k)·t(i)) 0=|| (k)||2+R2since(y(i))2||t(i)||2=||t(i)||2=R2byassumptionand(y(i))2=1 || (k+1)||2 kR2byinductiononksince( (1))2=0 || (k+1)||  kR
Analysis:  Perceptron
25Proof  of  Perceptron  Mistake  Bound:Part  3:  Combining  the  bounds  finishes  the  proof.k  || (k+1)||  kR k (R/ )2
The  total  number  of  mistakes  must  be  less  than  this
(Batch)  Perceptron  Algorithm
26Learningfor  Perceptron  also  works  if  we  have  a  fixed  training  dataset,  D.  We  call  this  the  “batch”  setting  in  contrast  to  the  “online”  setting  that  we’ve  discussed  so  far.Algorithm1PerceptronLearningAlgorithm(Batch)1:procedurePĊėĈĊĕęėĔē(D={(t(1),y(1)),...,(t(N),y(N))})2:  0 Initializeparameters3:whilenotconvergeddo4:fori {1,2,...,N}do Foreachexample5:ˆy sign( Tt(i)) Predict6:ifˆy =y(i)then Ifmistake7:   +y(i)t(i) Updateparameters8:return 
(Batch)  Perceptron  Algorithm
27Learningfor  Perceptron  also  works  if  we  have  a  fixed  training  dataset,  D.  We  call  this  the  “batch”  setting  in  contrast  to  the  “online”  setting  that  we’ve  discussed  so  far.Discussion:The  Batch  Perceptron  Algorithm  can  be  derived  by  applying  Stochastic  Gradient  Descent  (SGD)  to  minimize  a  so-­‐called  Hinge  Loss  on  a  linear  separator
Extensions  of  Perceptron•Kernel  Perceptron–Choose  a  kernel  K(x’,  x)–Apply  the  kernel  trick  to  Perceptron–Resulting  algorithm  is  still  very  simple•Structured  Perceptron–Basic  idea  can  also  be  applied  when  yranges  over  an  exponentially  large  set–Mistake  bound  does  notdepend  on  the  size  of  that  set28
Matching  GameGoal:  Match  the  Algorithm  to  its  Update  Rule
291.  SGD  for  Logistic  Regression2.  Least  Mean  Squares3.  Perceptron4.5.6.A.  1=5,  2=4,  3=6B.  1=5,  2=6,  3=4C.  1=6,  2=4,  3=4D.  1=5,  2=6,  3=6E.  1=6,  2=6,  3=6 k  k+11 + exp (h (x(i)) y(i)) k  k+(h (x(i)) y(i)) k  k+ (h (x(i)) y(i))x(i)kh (x)=p(y|x)h (x)= Txh (x) = sign( Tx)
Summary:  Perceptron•Perceptron  is  a  linear  classifier•Simplelearning  algorithm:  when  a  mistake  is  made,  add  /  subtract  the  features•For  linearly  separable  and  inseparable  data,  we  can  bound  the  number  of  mistakes  (geometric  argument)•Extensions  support  nonlinear  separators  and  structured  prediction30
KERNELS
31
Kernels:  MotivationMost  real-­‐world  problems  exhibit  data  that  is  not  linearly  separable.
32Q:  When  your  data  is  not  linearly  separable,  how  can  you  still  use  a  linear  classifier?A:Preprocess  the  data  to  produce  nonlinearfeaturesExample:  pixel  representation  for  Facial  Recognition:

Kernels:  Motivation•Motivation  #1:  Inefficient  Features–Non-­‐linearly  separable  data  requires  high  dimensional  representation–Might  be  prohibitively  expensive  to  compute  or  store•Motivation  #2:  Memory-­‐based  Methods–k-­‐Nearest  Neighbors  (KNN)  for  facial  recognition  allows  a  distance  metricbetween  images  -­‐-­‐no  need  to  worry  about  linearity  restriction  at  all33
KernelsWhiteboard–Kernel  Perceptron–Kernel  as  a  dot  product–Gram  matrix–Examples:  RBF  kernel,  string  kernel
34
Kernel  Methods•Key  idea:  1.Rewritethe  algorithm  so  that  we  only  work  with  dot  productsxTzof  feature  vectors2.Replacethe  dot  products  xTzwith  a  kernel  function  k(x,  z)•The  kernel  k(x,z)  can  be  anylegal  definition  of  a  dot  product:  k(x,  z)  =  φ(x)Tφ(z)  for  any  function  φ:  X àRDSo  we  only  compute  the  φ  dot  product  implicitly•This  “kernel  trick”can  be  applied  to  many  algorithms:–classification:  perceptron,  SVM,  …–regression:  ridge  regression,  …–clustering:  k-­‐means,  …35

Kernel  Methods
36Q:  These  are  just  non-­‐linear  features,  right?A:Yes,  but…Q:  Can’t  we  just  compute  the  feature  transformation  φexplicitly?A:That  depends...Q:  So,  why  all  the  hype  about  the  kernel  trick?A:Because  the  explicit  features  might  either  be  prohibitively  expensive  to  compute  or  infinite  length  vectors
Example:  Polynomial  Kernel
37Slide  from  Nina  BalcanExample For n=2, d=2, the kernel Kx,z=x⋅zd corresponds to  𝑥1,𝑥2→Φ𝑥=(𝑥12,𝑥22,2𝑥1𝑥2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space Example For n=2, d=2, the kernel Kx,z=x⋅zd corresponds to  𝑥1,𝑥2→Φ𝑥=(𝑥12,𝑥22,2𝑥1𝑥2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space Example For n=2, d=2, the kernel Kx,z=x⋅zd corresponds to  𝑥1,𝑥2→Φ𝑥=(𝑥12,𝑥22,2𝑥1𝑥2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space Example ϕ:R2→R3, x1,x2→Φx=(x12,x22,2x1x2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space ϕx⋅ϕ𝑧=x12,x22,2x1x2⋅(𝑧12,𝑧22,2𝑧1𝑧2) =x1𝑧1+x2𝑧22=x⋅𝑧2=K(x,z) Example ϕ:R2→R3, x1,x2→Φx=(x12,x22,2x1x2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space ϕx⋅ϕ𝑧=x12,x22,2x1x2⋅(𝑧12,𝑧22,2𝑧1𝑧2) =x1𝑧1+x2𝑧22=x⋅𝑧2=K(x,z) 
Example:  Polynomial  Kernel
38Slide  from  Nina  BalcanAvoid explicitly expanding the features Feature space can grow really large and really quickly…. Crucial to think of ϕ as implicit, not explicit!!!! –𝑥1𝑑, 𝑥1𝑥2…𝑥𝑑, 𝑥12𝑥2…𝑥𝑑−1 –Total number of such feature is 𝑑+𝑛−1𝑑=𝑑+𝑛−1!𝑑!𝑛−1! –𝑑=6,𝑛=100, there are 1.6 billion terms 
•Polynomial kernel degreee 𝑑, 𝑘𝑥,𝑧=𝑥⊤𝑧𝑑=𝜙𝑥⋅𝜙𝑧 
𝑘𝑥,𝑧=𝑥⊤𝑧𝑑=𝜙𝑥⋅𝜙𝑧 
𝑂𝑛  𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛! 
Kernel  ExamplesSide  Note:  The  feature  space  might  not  be  unique!
39Example Note:  feature space might not be unique. 
ϕ:R2→R4, x1,x2→Φx=(x12,x22,x1x2,x2x1) ϕx⋅ϕ𝑧=(x12,x22,x1x2,x2x1)⋅(z12,z22,z1z2,z2z1) =x⋅𝑧2=K(x,z) ϕ:R2→R3, x1,x2→Φx=(x12,x22,2x1x2) ϕx⋅ϕ𝑧=x12,x22,2x1x2⋅(𝑧12,𝑧22,2𝑧1𝑧2) =x1𝑧1+x2𝑧22=x⋅𝑧2=K(x,z) 
Slide  from  Nina  BalcanExplicit  representation  #1:Explicit  representation  #2:These  two  different  feature  representations  correspond  to  the  same  kernel  function!
Kernel  Examples
40NameKernel  Function(implicitdot  product)Feature  Space(explicit  dot  product)LinearSame  as  original  input  spacePolynomial  (v1)All  polynomialsofdegree  dPolynomial(v2)All  polynomialsup  to  degree  dGaussianInfinite  dimensional  spaceHyperbolicTangent  (Sigmoid)  Kernel(WithSVM,  this  is  equivalent  to  a  2-­‐layer  neural  network)

RBF  Kernel  Example
41
RBF  Kernel:
RBF  Kernel  Example
42
RBF  Kernel:
RBF  Kernel  Example
43
RBF  Kernel:
RBF  Kernel  Example
44
RBF  Kernel:
RBF  Kernel  Example
45
RBF  Kernel:
RBF  Kernel  Example
46
RBF  Kernel:
RBF  Kernel  Example
47
RBF  Kernel:
RBF  Kernel  Example
48
RBF  Kernel:
RBF  Kernel  Example
49
RBF  Kernel:
RBF  Kernel  Example
50
RBF  Kernel:
RBF  Kernel  Example
51
RBF  Kernel:
RBF  Kernel  Example
52
RBF  Kernel:
RBF  Kernel  Example
53
RBF  Kernel:KNN  vs.  SVM
RBF  Kernel  Example
54
RBF  Kernel:KNN  vs.  SVM
RBF  Kernel  Example
55
RBF  Kernel:KNN  vs.  SVM
RBF  Kernel  Example
56
RBF  Kernel:KNN  vs.  SVM
Kernels, Discussion •If  all computations involving instances are in terms of inner products then: Conceptually, work in a very high diml space and the alg’s performance depends only on linear separability in that extended space.  Computationally, only need to modify the algo by replacing each x⋅z with a Kx,z. How to choose a kernel: •Use Cross-Validation to choose the parameters, e.g., 𝜎  for Gaussian Kernel  Kx,𝑧=exp−𝑥−𝑧22 𝜎2  •Learn a good kernel; e.g.,  [Lanckriet-Cristianini-Bartlett-El Ghaoui-Jordan’04] •Kernels often encode domain knowledge (e.g., string kernels) Kernels:  Discussion
58Slide  from  Nina  Balcan
Experimental  Design+k-­‐Nearest  Neighbors
1KNN  Readings:Mitchell  8.2HTF  13.3Murphy  -­‐-­‐-­‐Bishop  2.5.2Prob.  Readings:  (next  lecture)Lecture  notes  from  10-­‐600  (See  Piazza  post  for  the  pointers)Murphy  2Bishop  2HTF  -­‐-­‐Mitchell  -­‐-­‐10-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  3January  25,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University

Reminders•BackgroundExercises(Homework1)–Released:  Wed,  Jan.  25–Due:  Mon,  Jan.  30  at  5:30pm•Websiteupdates–Office  hoursGoogle  calendar  on“People”–Readingson“Schedule”•MeetAIs:  Sarah,  Daniel,  Brynn
2
Outline•k-­‐Nearest  Neighbors  (KNN)–Special  cases–Choosing  k–Case  Study:  KNN  on  Fisher  Iris  Data–Case  Study:  KNN  on  2D  Gaussian  Data•Experimental  Design–Train  error  vs.  test  error–Train  /  validation  /  test  splits–Cross-­‐validation•Function  Approximation  View  of  ML3
K-­‐NEAREST  NEIGHBORS
4
k-­‐Nearest  NeighborsWhiteboard:–Special  cases–Choosing  k
5
KNN  ON  FISHER  IRIS  DATA
6
Fisher  Iris  DatasetFisher  (1936)  used  150  measurements  of  flowers  from  3  different  species:  Iris  setosa(0),  Iris  virginica(1),  Iris  versicolor(2)  collected  by  Anderson  (1936)
7Full  dataset:  https://en.wikipedia.org/wiki/Iris_flower_data_setSpeciesSepal  LengthSepal  WidthPetal  LengthPetal  Width04.33.01.10.104.93.61.40.105.33.71.50.214.92.43.31.015.72.84.11.316.33.34.71.616.73.05.01.7
KNN  on  Fisher  Iris  Data
8
KNN  on  Fisher  Iris  Data
9Special  Case:  Nearest  Neighbor
KNN  on  Fisher  Iris  Data
10Special  Case:  Majority  Vote
KNN  on  Fisher  Iris  Data
11
KNN  on  Fisher  Iris  Data
12Special  Case:  Nearest  Neighbor
KNN  on  Fisher  Iris  Data
13
KNN  on  Fisher  Iris  Data
14
KNN  on  Fisher  Iris  Data
15
KNN  on  Fisher  Iris  Data
16
KNN  on  Fisher  Iris  Data
17
KNN  on  Fisher  Iris  Data
18
KNN  on  Fisher  Iris  Data
19
KNN  on  Fisher  Iris  Data
20
KNN  on  Fisher  Iris  Data
21
KNN  on  Fisher  Iris  Data
22
KNN  on  Fisher  Iris  Data
23
KNN  on  Fisher  Iris  Data
24
KNN  on  Fisher  Iris  Data
25
KNN  on  Fisher  Iris  Data
26
KNN  on  Fisher  Iris  Data
27
KNN  on  Fisher  Iris  Data
28
KNN  on  Fisher  Iris  Data
29
KNN  on  Fisher  Iris  Data
30
KNN  on  Fisher  Iris  Data
31
KNN  on  Fisher  Iris  Data
32Special  Case:  Majority  Vote
KNN  ON  GAUSSIAN  DATA
33
KNN  on  Gaussian  Data
34
KNN  on  Gaussian  Data
35
KNN  on  Gaussian  Data
36
KNN  on  Gaussian  Data
37
KNN  on  Gaussian  Data
38
KNN  on  Gaussian  Data
39
KNN  on  Gaussian  Data
40
KNN  on  Gaussian  Data
41
KNN  on  Gaussian  Data
42
KNN  on  Gaussian  Data
43
KNN  on  Gaussian  Data
44
KNN  on  Gaussian  Data
45
KNN  on  Gaussian  Data
46
KNN  on  Gaussian  Data
47
KNN  on  Gaussian  Data
48
KNN  on  Gaussian  Data
49
KNN  on  Gaussian  Data
50
KNN  on  Gaussian  Data
51
KNN  on  Gaussian  Data
52
KNN  on  Gaussian  Data
53
KNN  on  Gaussian  Data
54
KNN  on  Gaussian  Data
55
KNN  on  Gaussian  Data
56
KNN  on  Gaussian  Data
57
KNN  on  Gaussian  Data
58
CHOOSING  THE  NUMBER  OF  NEIGHBORS59
F-­‐Fold  Cross-­‐ValidationKey  idea:  rather  than  just  a  single  “validation”  set,  use  many!  (Error  is  more  stable.  Slower  computation.)
60(Name  changed  from  K-­‐Fold  Cross-­‐Validation  to  avoid  confusion  with  KNN)
D=
y(1)y(2)y(N)x(1)x(2)x(N)
Fold  1
Fold  2
Fold  3
Fold  4Divide  data  into  folds  (e.g.  4)1.Train  on  folds  {1,2,3}  and  predict  on  {4}2.Train  on  folds  {1,2,4}  and  predict  on  {3}3.Train  on  folds  {1,3,4}  and  predict  on  {2}4.Train  on  folds  {2,3,4}  and  predict  on  {1}Concatenate  all  the  predictions  and  evaluate  error
Math  as  CodeIt  depends  on  how  large  the  set  Y  is!If  it’s  a  small  enumerable  set  Y  =  {1,2,…,77},  then:
61ymax=  argmaxf(y)y  ∈Y  How  to  implement?ymax= -infforyin{1,2,…77}:iff(y) > ymax:ymax= yreturnymax
Math  as  CodeIt  depends  on  how  large  the  set  Y  is!If  it’s  a  small  enumerable  set  Y  =  {1,2,…,77},  then:
62vmax=  maxf(y)y  ∈Y  How  to  implement?vmax= -infforyin{1,2,…77}:iff(y) > vmax:vmax= f(y)returnvmax
Function  Approximation  View  of  MLWhiteboard
63
Beyond  the  Scope  of  This  Lecture•k-­‐Nearest  Neighbors  (KNN)  for  Regression•Distance-­‐weightedKNN•Cover  &  Hart  (1967)  Bayes  error  rate  bound•KNN  for  Facial  Recognition  (see  Eigenfacesin  PCA  lecture)
64
Takeaways•k-­‐Nearest  Neighbors–Requires  careful  choice  of  k  (#  of  neighbors)–Experimental  design  can  be  just  as  important  as  the  learning  algorithm  itself•Function  Approximation  View–Assumption:  inputs  are  sampled  from  some  unknown  distributions–Assumption:  outputs  come  from  a  fixed  unknown  function  (e.g.  human  annotator)–Goal:  Learn  a  hypothesis  which  closely  approximates  that  function65
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  January 28, 2015 
Today: • Naïve Bayes • discrete-valued Xi’s • Document classification • Gaussian Naïve Bayes • real-valued Xi’s • Brain image classification   Readings:  Required: • Mitchell: “Naïve Bayes and Logistic Regression”      (available on class website)  Optional • Bishop 1.2.4 • Bishop 4.2  
Recently: • Bayes classifiers to learn P(Y|X) • MLE and MAP estimates for parameters of P • Conditional independence • Naïve Bayes à make Bayesian learning practical Next: • Text classification • Naïve Bayes and continuous variables Xi: • Gaussian Naïve Bayes classifier • Learn P(Y|X) directly • Logistic regression, Regularization, Gradient ascent • Naïve Bayes or Logistic Regression? • Generative vs. Discriminative classifiers 
Naïve Bayes in a Nutshell Bayes rule: Assuming conditional independence among Xi’s:   So, classification rule for Xnew = < X1, …, Xn > is:  

Example: Live in Sq Hill?  P(S|G,D,B) • S=1 iff live in Squirrel Hill • G=1 iff shop at SH Giant Eagle • D=1 iff Drive or Carpool to CMU • B=1 iff Birthday is before July 1   P(S=1) : P(D=1 | S=1) : P(D=1 | S=0) : P(G=1 | S=1) : P(G=1 | S=0) : P(B=1 | S=1) : P(B=1 | S=0) : P(S=0) : P(D=0 | S=1) : P(D=0 | S=0) : P(G=0 | S=1) : P(G=0 | S=0) : P(B=0 | S=1) : P(B=0 | S=0) :              Tom: D=1, G=0, B=0 P(S=1|D=1,G=0,B=0) =                                          P(S=1) P(D=1|S=1) P(G=0|S=1) P(B=0|S=1)      _____________________________________________________________________________   [P(S=1) P(D=1|S=1) P(G=0|S=1) P(B=0|S=1) + P(S=0) P(D=1|S=0) P(G=0|S=0) P(B=0|S=0)]      
Another way to view Naïve Bayes (Boolean Y): Decision rule: is this quantity greater or less than 1?  

Naïve Bayes: classifying text documents • Classify which emails are spam? • Classify which emails promise an attachment?  How shall we represent text documents for Naïve Bayes? 

Learning to classify documents: P(Y|X)  • Y discrete valued.   – e.g., Spam or not • X = <X1, X2, … Xn> = document   • Xi is a random variable describing… 

Learning to classify documents: P(Y|X)  • Y discrete valued.   – e.g., Spam or not • X = <X1, X2, … Xn> = document   • Xi is a random variable describing… Answer 1: Xi is boolean, 1 if word i is in document, else 0                  e.g., Xpleased = 1    Issues? 

Learning to classify documents: P(Y|X)  • Y discrete valued.   – e.g., Spam or not • X = <X1, X2, … Xn> = document   • Xi is a random variable describing… Answer 2:  • Xi represents the ith word position in document • X1 = “I”,  X2 = “am”, X3 = “pleased” • and, let’s assume the Xi are iid (indep, identically distributed) 

Learning to classify document: P(Y|X) the “Bag of Words” model • Y discrete valued.  e.g., Spam or not • X = <X1, X2, … Xn> = document  • Xi are iid random variables.  Each represents the word at its position i in the document • Generating a document according to this distribution = rolling a 50,000 sided die, once for each word position in the document • The observed counts for each word follow a ??? distribution  
Multinomial Distribution 

Multinomial Bag of Words 
aardvark 0 about 2 all 2 Africa 1 apple 0 anxious 0 ... gas 1 ... oil 1 … Zaire 0 
MAP estimates for bag of words  Map estimate for multinomial     What β’s should we choose?  

Naïve Bayes Algorithm – discrete Xi  • Train Naïve Bayes (examples)    for each value yk  estimate   for each value xij of each attribute Xi   estimate   • Classify (Xnew)    
prob that word xij appears in position i, given Y=yk  * Additional assumption:  word probabilities are position independent 


For code and data, see www.cs.cmu.edu/~tom/mlbook.html  click on “Software and Data” 
What if we have continuous Xi ? Eg., image classification: Xi is real-valued ith pixel  

What if we have continuous Xi ? Eg., image classification: Xi is real-valued ith pixel  Naïve Bayes requires P(Xi | Y=yk), but Xi is real (continuous)     Common approach: assume P(Xi | Y=yk) follows a Normal (Gaussian) distribution    

What if we have continuous Xi ? Eg., image classification: Xi is real-valued ith pixel  Naïve Bayes requires P(Xi | Y=yk), but Xi is real (continuous)     Common approach: assume P(Xi | Y=yk) follows a Normal (Gaussian) distribution    

Gaussian Distribution (also called “Normal”)  p(x) is a probability density function, whose  integral (not sum) is 1 
What if we have continuous Xi ? Gaussian Naïve Bayes (GNB): assume     Sometimes assume variance • is independent of Y (i.e., σi),  • or independent of Xi (i.e., σk) • or both (i.e., σ) 
• Train Naïve Bayes (examples)    for each value yk  estimate*   for each attribute Xi estimate  • class conditional mean        , variance         • Classify (Xnew)    
Gaussian Naïve Bayes Algorithm – continuous Xi   (but still discrete Y) 
 * probabilities must sum to 1, so need estimate only n-1 parameters... 

Estimating Parameters: Y discrete, Xi continuous  Maximum likelihood estimates: 
jth training example δ()=1 if (Yj=yk) else 0 ith feature kth class 
How many parameters must we estimate for Gaussian Naïve Bayes if Y has k possible values, X=<X1, … Xn>?  

GNB Example: Classify a person’s cognitive state, based on brain image •  reading a sentence or viewing a picture? •  reading the word describing a “Tool” or “Building”?   •  answering the question, or getting confused? 

Y is the mental state (reading “house” or “bottle”) Xi are the voxel activities,   this is a plot of the µ’s defining P(Xi | Y=“bottle”) 
fMRI activation  high 
below average average Mean activations over all training examples for Y=“bottle” 
Classification task: is person viewing a “tool” or “building”? 
p4p8p6p11p5p7p10p9p2p12p3p100.10.20.30.40.50.60.70.80.91
ParticipantsClassification accuracystatistically significant p<0.05 Classification accuracy 
Where is information encoded in the brain? 
Accuracies of  cubical 27-voxel  classifiers centered at each significant voxel [0.7-0.8] 

Naïve Bayes: What you should know • Designing classifiers based on Bayes rule • Conditional independence – What it is – Why it’s important • Naïve Bayes assumption and its consequences – Which (and how many) parameters must be estimated under different generative models (different forms for P(X|Y) ) • and why this matters • How to train Naïve Bayes classifiers – MLE and MAP estimates  – with discrete and/or continuous inputs Xi 
Questions to think about: • Can you use Naïve Bayes for a combination of discrete and real-valued Xi?  • How can we easily model just 2 of n attributes as dependent? • What does the decision surface of a Naïve Bayes classifier look like? • How would you select a subset of Xi’s? 
Simple Picture for GNB for P(Y|X1) 
Recurrent Networks, and Attention, for
Statistical Machine Translation
Michael Collins, Columbia University
Mapping Sequences to Sequences
ILearn to map input sequences x1: : : x nto output sequences
y1: : : y mwhere ym=STOP.
ICan decompose this as
p(y1: : : y mjx1: : : x n) =mY
j=1p(yjjy1: : : y j 1; x1: : : x n)
IEncoder/decoder framework: use an LSTM to map x1: : : x n
to a vector h(n), then model
p(yjjy1: : : y j 1; x1: : : x n) =p(yjjy1: : : y j 1; h(n))
using a \decoding" LSTM
The Computational Graph
Training A Recurrent Network for Translation
Inputs: A sequence of source language words x1:::x nwhere each
xj2Rd. A sequence of target language words y1:::y mwhere
ym=STOP.
Denitions: F=parameters of an \encoding" LSTM. D=
parameters of a \decoding" LSTM. LSTM (x(t);h(t 1);)maps an
inputx(t)together with a hidden state h(t 1)to a new hidden state
h(t). Hereare the parameters of the LSTM
Training A Recurrent Network for Translation
(continued)
Computational Graph:
IInitializeh(0)to some values (e.g. vector of all zeros)
I(Encoding step: ) Fort= 1:::n
Ih(t)=LSTM (x(t);h(t 1);F)
IInitialize(0)to some values (e.g., vector of all zeros)
I(Decoding step: ) Forj= 1:::m
I(j)=LSTM (CONCAT (yj 1;h(n));(j 1);D)
Il(j)=VCONCAT ((j);yj 1;h(n)) +
; q(j)=LS(l(j)),
o(j)= q(j)
yj
I(Final loss is sum of losses: )
o=mX
j=1o(j)
The Computational Graph
Greedy Decoding with A Recurrent Network for
Translation
IEncoding step: Calculate h(n)from the input x1: : : x n
Ij= 1.Do:
Iyj= arg max yp(yjy1:::y j 1;h(n))
Ij=j+ 1
IUntil:yj 1=STOP
Greedy Decoding with A Recurrent Network for
Translation
Computational Graph:
IInitializeh(0)to some values (e.g. vector of all zeros)
I(Encoding step: ) Fort= 1:::n
Ih(t)=LSTM (x(t);h(t 1);F)
IInitialize(0)to some values (e.g., vector of all zeros)
I(Decoding step: )j= 1.Do:
I(j)=LSTM (CONCAT (yj 1;h(n));(j 1);D)
Il(j)=VCONCAT ((j);yj 1;h(n)) +
Iyj= arg max yl(j)
y
Ij=j+ 1
IUntilyj 1=STOP
IReturny1:::y j 1
A bi-directional LSTM (bi-LSTM) for Encoding
Inputs: A sequence x1:::x nwhere each xj2Rd.
Denitions: FandBare parameters of a forward and backward
LSTM.
Computational Graph:
Ih(0);(n+1)are set to some inital values.
IFort= 1:::n
Ih(t)=LSTM (x(t);h(t 1);F)
IFort=n::: 1
I(t)=LSTM (x(t);(t+1);B)
IFort= 1:::n
Iu(t)=CONCAT (h(t);(t))(encoding for position t
The Computational Graph
Incorporating Attention
IOld decoder:
Ic(j)=h(n)(context used in decoding at j'th step
I(j)=LSTM (CONCAT (yj 1;c(j));(j 1);D)
Il(j)=VCONCAT ((j);yj 1;c(j)) +
Iyj= arg max yl(j)
y
Incorporating Attention
INew decoder:
IDene
c(j)=nX
i=1ai;ju(i)
where
ai;j=expfsi;jgPn
i=1si;j
and
si;j=A((j 1);u(i);A)
whereA(:::)is a non-linear function (e.g., a feedforward
network) with parameters A
Greedy Decoding with Attention
I(Decoding step: )j= 1.Do:
IFori= 1:::n ,
si;j=A((j 1);u(i);A)
IFori= 1:::n ,
ai;j=expfsi;jgPn
i=1si;j
ISetc(j)=Pn
i=1ai;ju(i)
I(j)=LSTM (CONCAT (yj 1;c(j));(j 1);D)
Il(j)=VCONCAT ((j);yj 1;c(j)) +
Iyj= arg max yl(j)
y
Ij=j+ 1
IUntilyj 1=STOP
IReturny1:::y j 1
Training with Attention
I(Decoding step: ) Forj= 1:::m
IFori= 1:::n ,
si;j=A((j 1);u(i);A)
IFori= 1:::n ,
ai;j=expfsi;jgPn
i=1si;j
ISetc(j)=Pn
i=1ai;ju(i)
I(j)=LSTM (CONCAT (yj 1;c(j));(j 1);D)
Il(j)=VCONCAT ((j);yj 1;c(j)) +
; q(j)=LS(l(j)),
o(j)= q(j)
yj
I(Final loss is sum of losses: )
o=mX
j=1o(j)
The Computational Graph
Results from Wu et al. 2016
IFrom Google's Neural Machine Translation System: Bridging the Gap
between Human and Machine Translation , Wu et al. 2016. Human
evaluations are on a 1-6 scale (6 is best). PBMT is a phrase-based
translation system, using IBM alignment models as a starting point.
Results from Wu et al. 2016 (continued)

Conclusions
IDirectly model
p(y1: : : y mjx1: : : x n) =mY
j=1p(yjjy1: : : y j 1; x1: : : x n)
IEncoding step: map x1: : : x ntou(1): : : u(n)using a
bidirectional LSTM
IDecoding step: use an LSTM in decoding together with
attention
Log-Linear Models for History-Based Parsing
Michael Collins, Columbia University
Log-Linear Taggers: Summary
IThe input sentence is w[1:n]=w1: : : w n
IEach tag sequence t[1:n]has a conditional probability
p(t[1:n]jw[1:n]) =Qn
j=1p(tjjw1: : : w n; t1: : : t j 1) Chain rule
=Qn
j=1p(tjjw1: : : w n; tj 2; tj 1) Independence
assumptions
IEstimate p(tjjw1: : : w n; tj 2; tj 1)using log-linear models
IUse the Viterbi algorithm to compute
argmaxt[1:n]logp(t[1:n]jw[1:n])
A General Approach:
(Conditional) History-Based Models
IWe've shown how to dene p(t[1:n]jw[1:n])where t[1:n]is a
tag sequence
IHow do we dene p(TjS)ifTis a parse tree (or another
structure)? (We use the notation S=w[1:n])
A General Approach:
(Conditional) History-Based Models
IStep 1: represent a tree as a sequence of decisions d1: : : d m
T=hd1; d2; : : : d mi
misnot necessarily the length of the sentence
IStep 2: the probability of a tree is
p(TjS) =mY
i=1p(dijd1: : : d i 1; S)
IStep 3: Use a log-linear model to estimate
p(dijd1: : : d i 1; S)
IStep 4: Search?? (answer we'll get to later: beam or
heuristic search)
An Example Tree
S(questioned)
NP(lawyer)
DT
theNN
lawyerVP(questioned)
Vt
questionedNP(witness)
DT
theNN
witnessPP(about)
IN
aboutNP(revolver)
DT
theNN
revolver
Ratnaparkhi's Parser: Three Layers of Structure
1. Part-of-speech tags
2. Chunks
3. Remaining structure
Layer 1: Part-of-Speech Tags
DT
theNN
lawyerVt
questionedDT
theNN
witnessIN
aboutDT
theNN
revolver
IStep 1: represent a tree as a sequence of decisions d1: : : d m
T=hd1; d2; : : : d mi
IFirstndecisions are tagging decisions
hd1: : : d ni=hDT, NN, Vt, DT, NN, IN, DT, NN i
Layer 2: Chunks
NP
DT
theNN
lawyerVt
questionedNP
DT
theNN
witnessIN
aboutNP
DT
theNN
revolver
Chunks are dened as any phrase where all children are
part-of-speech tags
(Other common chunks are ADJP ,QP)
Layer 2: Chunks
Start(NP)
DT
theJoin(NP)
NN
lawyerOther
Vt
questionedStart(NP)
DT
theJoin(NP)
NN
witnessOther
IN
aboutStart(NP)
DT
theJoin(NP)
NN
revolver
IStep 1: represent a tree as a sequence of decisions d1: : : dm
T=hd1; d2; : : : d mi
IFirstndecisions are tagging decisions
Nextndecisions are chunk tagging decisions
hd1: : : d 2ni=hDT, NN, Vt, DT, NN, IN, DT, NN,
Start(NP), Join(NP), Other, Start(NP), Join(NP),
Other, Start(NP), Join(NP) i
Layer 3: Remaining Structure
Alternate Between Two Classes of Actions:
IJoin(X) or Start(X), where X is a label (NP, S, VP etc.)
ICheck=YES or Check=NO
Meaning of these actions:
IStart(X) starts a new constituent with label X
(always acts on leftmost constituent with no start or join
label above it)
IJoin(X) continues a constituent with label X
(always acts on leftmost constituent with no start or join
label above it)
ICheck=NO does nothing
ICheck=YES takes previous Join or Start action, and converts
it into a completed constituent
NP
DT
theNN
lawyerVt
questionedNP
DT
theNN
witnessIN
aboutNP
DT
theNN
revolver
Start(S)
NP
DT
theNN
lawyerVt
questionedNP
DT
theNN
witnessIN
aboutNP
DT
theNN
revolver
Start(S)
NP
DT
theNN
lawyerVt
questionedNP
DT
theNN
witnessIN
aboutNP
DT
theNN
revolver
Check=NO
Start(S)
NP
DT
theNN
lawyerStart(VP)
Vt
questionedNP
DT
theNN
witnessIN
aboutNP
DT
theNN
revolver
Start(S)
NP
DT
theNN
lawyerStart(VP)
Vt
questionedNP
DT
theNN
witnessIN
aboutNP
DT
theNN
revolver
Check=NO
Start(S)
NP
DT
theNN
lawyerStart(VP)
Vt
questionedJoin(VP)
NP
DT
theNN
witnessIN
aboutNP
DT
theNN
revolver
Start(S)
NP
DT
theNN
lawyerStart(VP)
Vt
questionedJoin(VP)
NP
DT
theNN
witnessIN
aboutNP
DT
theNN
revolver
Check=NO
Start(S)
NP
DT
theNN
lawyerStart(VP)
Vt
questionedJoin(VP)
NP
DT
theNN
witnessStart(PP)
IN
aboutNP
DT
theNN
revolver
Start(S)
NP
DT
theNN
lawyerStart(VP)
Vt
questionedJoin(VP)
NP
DT
theNN
witnessStart(PP)
IN
aboutNP
DT
theNN
revolver
Check=NO
Start(S)
NP
DT
theNN
lawyerStart(VP)
Vt
questionedJoin(VP)
NP
DT
theNN
witnessStart(PP)
IN
aboutJoin(PP)
NP
DT
theNN
revolver
Start(S)
NP
DT
theNN
lawyerStart(VP)
Vt
questionedJoin(VP)
NP
DT
theNN
witnessPP
IN
aboutNP
DT
theNN
revolver
Check=YES
Start(S)
NP
DT
theNN
lawyerStart(VP)
Vt
questionedJoin(VP)
NP
DT
theNN
witnessJoin(VP)
PP
IN
aboutNP
DT
theNN
revolver
Start(S)
NP
DT
theNN
lawyerVP
Vt
questionedNP
DT
theNN
witnessPP
IN
aboutNP
DT
theNN
revolver
Check=YES
Start(S)
NP
DT
theNN
lawyerJoin(S)
VP
Vt
questionedNP
DT
theNN
witnessPP
IN
aboutNP
DT
theNN
revolver
S
NP
DT
theNN
lawyerVP
Vt
questionedNP
DT
theNN
witnessPP
IN
aboutNP
DT
theNN
revolver
Check=YES
The Final Sequence of decisions
hd1: : : d mi=hDT, NN, Vt, DT, NN, IN, DT, NN,
Start(NP), Join(NP), Other, Start(NP), Join(NP),
Other, Start(NP), Join(NP),
Start(S), Check=NO, Start(VP), Check=NO,
Join(VP), Check=NO, Start(PP), Check=NO,
Join(PP), Check=YES, Join(VP), Check=YES,
Join(S), Check=YES i
A General Approach:
(Conditional) History-Based Models
IStep 1: represent a tree as a sequence of decisions d1: : : d m
T=hd1; d2; : : : d mi
misnotnecessarily the length of the sentence
IStep 2: the probability of a tree is
p(TjS) =Qm
i=1p(dijd1: : : d i 1; S)
IStep 3: Use a log-linear model to estimate
p(dijd1: : : d i 1; S)
IStep 4: Search?? (answer we'll get to later: beam or heuristic
search)
Applying a Log-Linear Model
IStep 3: Use a log-linear model to estimate
p(dijd1: : : d i 1; S)
IA reminder:
p(dijd1: : : d i 1; S) =ef(hd1:::di 1;Si;di)v
P
d2Aef(hd1:::di 1;Si;d)v
where:
hd1: : : d i 1; Si is the history
di is the outcome
f maps a history/outcome pair to a feature vector
v is a parameter vector
A is set of possible actions
Applying a Log-Linear Model
IStep 3: Use a log-linear model to estimate
p(dijd1: : : d i 1; S) =ef(hd1:::di 1;Si;di)v
P
d2Aef(hd1:::di 1;Si;d)v
IThe big question: how do we dene f?
IRatnaparkhi's method denes fdierently depending on
whether next decision is:
IA tagging decision
(same features as before for POS tagging!)
IA chunking decision
IA start/join decision after chunking
IA check=no/check=yes decision
Layer 3: Join or Start
ILooks at head word, constituent (or POS) label, and
start/join annotation of n'th tree relative to the decision,
where n= 2; 1
ILooks at head word, constituent (or POS) label of n'th tree
relative to the decision, where n= 0;1;2
ILooks at bigram features of the above for (-1,0) and (0,1)
ILooks at trigram features of the above for (-2,-1,0), (-1,0,1)
and (0, 1, 2)
IThe above features with all combinations of head words
excluded
IVarious punctuation features
Layer 3: Check=NO or Check=YES
IA variety of questions concerning the proposed constituent
The Search Problem
IIn POS tagging, we could use the Viterbi algorithm because
p(tjjw1: : : w n; j; t 1: : : t j 1) =p(tjjw1: : : w n; j; t j 2: : : t j 1)
INow: Decision dicould depend on arbitrary decisions in the
\past")no chance for dynamic programming
IInstead, Ratnaparkhi uses a beam search method
CS224dDeep	NLPLecture	5:Project	Information+Neural	Networks	&	BackpropRichard	Socherrichard@metamind.io
Overview	Today:•Organizational	Stuff•Project	Tips•From	one-layer	to	multi	layer	neural	network!•Max-Margin	loss	and	backprop!	(This	is	the	hardest	lecture	of	the	quarter)
4/12/16Richard	SocherLecture	5,	Slide	2
Announcement:	•1%	extra	credit	for	Piazza	participation!•Hint	for	PSet1:	Understand	math	and	dimensionality,	then	add	print	statements,	e.g.
•Student	survey	sent	out	last	night,	please	give	us	feedback	to	improve	the	class	:)4/12/16Richard	SocherLecture	5,	Slide	3

Class	Project•Most	important	(40%)	and	lasting	result	of	the	class•PSet3	a	little	easier	to	have	more	time•Start	early	and	clearly	define	your	task	and	dataset•Project	types:1.Apply	existing	neural	network	model	to	a	new	task2.Implement	a	complex	neural	architecture3.Come	up	with	a	new	neural	network	model4.Theory4/12/16Richard	SocherLecture	5,	Slide	4
Class	Project:	Apply	Existing	NNetsto	Tasks1.Define	Task:	•Example:	Summarization2.Define	Dataset1.Search	for	academic	datasets	•They	already	have	baselines•E.g.:	Document	Understanding	Conference	(DUC)	2.Define	your	own	(harder,	need	more	new	baselines)•If	you’re	a	graduate	student:	connect	to	your	research•Summarization,	Wikipedia:	Intro	paragraph	and	rest	of	large	article•Be	creative:	Twitter,	Blogs,	News4/12/16Richard	SocherLecture	5,	Slide	5
Class	Project:	Apply	Existing	NNetsto	Tasks3.Define	your	metric•Search	online	for	well	established	metrics	on	this	task•Summarization:	Rouge	(Recall-Oriented	Understudy	for	GistingEvaluation)	which	defines	n-gram	overlap	to	human	summaries4.Split	your	dataset!•Train/Dev/Test•Academic	dataset	often	come	pre-split•Don’t	look	at	the	test	split	until	~1	week	before	deadline!4/12/16Richard	SocherLecture	5,	Slide	6
Class	Project:	Apply	Existing	NNetsto	Tasks5.Establish	a	baseline•Implement	the	simplest	model	(often	logistic	regression	on	unigrams	and	bigrams)	first•Compute	metrics	on	train	AND	dev•Analyze	errors•If	metrics	are	amazing	and	no	errors:	done,	problem	was	too	easy,	restart	:)	6.Implement	existing	neural	net	model	•Compute	metric	on	train	and	dev•Analyze	output	and	errors•Minimum	 bar	for	this	class4/12/16Richard	SocherLecture	5,	Slide	7
Class	Project:	Apply	Existing	NNetsto	Tasks7.Always	be	close	to	your	data!•Visualize	the	dataset•Collect	summary	statistics•Look	at	errors•Analyze	how	different	hyperparametersaffect	performance8.Try	out	different	model	variants•Soon	you	will	have	more	options•Word	vector	averaging	model	(neural	bag	of	words)•Fixed	window	neural	model•Recurrent	neural	network•Recursive	neural	network•Convolutional	neural	network4/12/16Richard	SocherLecture	5,	Slide	8
Class	Project:	A	New	Model	--Advanced	Option•Do	all	other	steps	first	(Start	early!)•Gain	intuition	of	why	existing	models	are	flawed•Talk	to	other	researchers,	come	to	my	office	hours	a	lot•Implement	new	models	and	iterate	quickly	over	ideas•Set	up	efficient	experimental	framework•Build	simpler	new	models	first•Example	Summarization:•Average	word	vectors	per	paragraph,	then	greedy	search•Implement	language	model	or	autoencoder(introduced	later)•Stretch	goal	for	potential	paper:	Generate	summary!4/12/16Richard	SocherLecture	5,	Slide	9
Project	Ideas•Summarization•NER,	like	PSet2	but	with	larger	dataNatural	Language	Processing	 (almost)	 from	Scratch,	Ronan	Collobert,	 Jason	Weston,	 Leon	Bottou,	Michael	Karlen,	KorayKavukcuoglu,	PavelKuksa,	http://arxiv.org/abs/1103.0398•Simple	question	answering,ANeural	Network	for	Factoid	Question	 Answering	over	Paragraphs,	MohitIyyer,	Jordan	 Boyd-Graber,	Leonardo	Claudino,	 Richard	Socher	and	Hal	DauméIII	(EMNLP	2014)•Image	to	text	mapping	or	generation,Grounded	 Compositional	 Semantics	for	Finding	 and	Describing	Images	with	Sentences,	 Richard	Socher,	 Andrej	Karpathy,	QuocV.	Le,	Christopher	 D.	Manning,	Andrew	Y.	Ng.	(TACL	2014)orDeep	Visual-Semantic	 Alignments	 for	Generating	Image	Descriptions,	 Andrej	Karpathy,	Li	Fei-Fei•Entity	level	sentiment•Use	DL	to	solve	an	NLP	challenge	on	kaggle,Develop	a	scoring	algorithm	for	student-written	 short-answer	responses,	https://www.kaggle.com/c/asap-sas4/12/16Richard	SocherLecture	5,	Slide	10
Default	project:	sentiment	classification•Sentiment	on	movie	reviews:	http://nlp.stanford.edu/sentiment/•Lots	of	deep	learning	baselines	and	methods	have	been	tried
4/12/16Richard	SocherLecture	1,	Slide	11

A	more	powerful	window	classifier•Revisiting	•Xwindow=	[		xmuseumsxinxParisxarexamazing]•Assume	we	want	to	classify	whether	the	center	word	is	a	location	or	not
4/12/16Richard	SocherLecture	5,	Slide	12
A	Single	Layer	Neural	Network•A	single	layer	is	a	combination	of	a	linear	layer	and	a	nonlinearity:•The	neural	activations	a	can	thenbe	used	to	compute	some	function•For	instance,	an	unnormalizedscore	or	a	softmaxprobability	we	care	about:
13
Summary:	Feed-forward	Computation
14Computing	a	window’s	score	with	a	3-layer	neural	net:	s	=	score(museums	in	Paris	are	amazing	)
Xwindow=	[		xmuseumsxinxParisxarexamazing]
Main	intuition	for	extra	layer
15The	layer	learns	non-linearinteractions	between	the	input	word	vectors.Example:only	if	“museums”isfirst	vector	shouldit	matter	that	“in”isin	the	second	positionXwindow=	[		xmuseumsxinxParisxarexamazing]
Summary:	Feed-forward	Computation•s=	score(museums	in	Paris	are	amazing)•sc=	score(Not	all	museums	in	Paris)•Idea	for	training	objective:	make	score	of	true	window	larger	and	corrupt	window’s	score	lower	(until	they’re	good	enough):	minimize•This	is	continuous,	can	perform	SGD
16
Max-margin	Objective	function•Objective	for	a	single	window:•Each	window	with	a	location	at	its	center	should	have	a	score	+1	higher	than	any	window	without	a	location	at	its	center•xxx		|ß1				à|			ooo•For	full	objective	function:	Sum	over	all	training	windows
17
Training	with	BackpropagationAssuming	cost	J	is	>	0,	compute	the	derivatives	of	sand	scwrtall	the	involved	variables:	U,	W,	b,	x
18

Training	with	Backpropagation•Let’s	consider	the	derivative	of	a	single	weight	Wij•This	only	appears	inside	ai•For	example:	W23is	only	used	to	compute	a2
x1x2																	x3+1a1a2s		U2W2319b2
Training	with	BackpropagationDerivative	of	weight	Wij:
20x1x2																	x3+1a1a2s		U2W23
where																																																		for	logistic	fTraining	with	BackpropagationDerivative	of	single	weight	Wij:
Local	error	signalLocal	input	signal21x1x2																	x3+1a1a2s		U2W23

•We	want	all	combinations	ofi=	1,	2andj	=	1,	2,	3	à?•Solution:	Outer	product:where																		is	the	“responsibility”	or	error	messagecoming	from	each	activation	aTraining	with	Backpropagation•From	single	weight	Wijto	full	W:
22x1x2																	x3+1a1a2s		U2W23
S
Training	with	Backpropagation•For	biases	b,	we	get:
23x1x2																	x3+1a1a2s		U2W23

Training	with	Backpropagation
24That’s	almost	backpropagationIt’s	simply	taking	derivatives	and	using	the	chain	rule!Remaining	trick:	we	can	re-use	derivatives	computed	for	higher	layers	in	computing	derivatives	for	lower	layers!Example:	last	derivatives	of	model,	the	word	vectors	in	x
Training	with	Backpropagation•Take	derivative	of	score	with	respect	to	single	element	of	word	vector•Now,	we	cannot	just	take	into	consideration	one	aibecause	each	xjis	connected	to	all	the	neurons	above	and	hence	xjinfluences	the	overall	score	through	all	of	these,	hence:
Re-used	part	of	previous	derivative25

Training	with	Backpropagation•With																								,what	is	the	full	gradient?	à•Observations:		The	error	message	±that	arrives	at	a	hidden	layer	has	the	same	dimensionality	 as	that	hidden	layer
26

Putting	all	gradients	together:•Remember:	Full	objective	function	for	each	window	was:	•For	example:	gradient	for	U:
27

Two	layer	neural	nets	and	full	backprop•Let’s	look	at	a	2	layer	neural	network•Same	window	definition	for	x•Same	scoring	function	•2	hidden	layers	(carefully	not	superscripts	now!)
4/12/16Richard	SocherLecture	5,	Slide	28
W(1)W(2)a(2)a(3)xUs
Two	layer	neural	nets	and	full	backprop•Fully	written	out	as	one	function:
•Same	derivation	as	before	for	W(2)	(now	sitting	on	a(1))
4/12/16Richard	SocherLecture	5,	Slide	29
W(1)W(2)
Sa(2)a(3)
Two	layer	neural	nets	and	full	backprop•Same	derivation	as	before	for	top	W(2)	:•In	matrix	notation:where																																									and	±is	the	element-wise	product			also	called	Hadamardproduct•Last	missing	piece	for	understanding		general	backprop:4/12/16Richard	SocherLecture	5,	Slide	30

Two	layer	neural	nets	and	full	backprop•Last	missing	piece:	•What’s	the	bottom	layer’s	error	message	±(2)?•Similar	derivation	to	single	layer	model•Main	difference,	we	already	have																				and	need	to	apply	the	chain	rule	again	on	4/12/16Richard	SocherLecture	5,	Slide	31

Two	layer	neural	nets	and	full	backprop•Chain	rule	for:•Get	intuition	by	deriving														as	if	it	was	a	scalar•Intuitively,	we	have	to	sum	over	all	the	nodes	coming	into	layer	•Putting	it	all	together:4/12/16Richard	SocherLecture	5,	Slide	32
The second derivative in eq. 28 for output units is simply@a(nl)i@W(nl 1)ij=@@W(nl 1)ijz(nl)i=@@W(nl 1)ij⇣W(nl 1)i·a(nl 1)⌘=a(nl 1)j.(46)We adopt standard notation and introduce the error related to an output unit:@En@W(nl 1)ij=(yi ti)a(nl 1)j= (nl)ia(nl 1)j.(47)So far, we only computed errors for output units, now we will derive ’s for normal hidden units andshow how these errors are backpropagated to compute weight derivatives of lower levels. We will start withsecond to top layer weights from which a generalization to arbitrarily deep layers will become obvious.Similar to eq. 28, we start with the error derivative:@E@W(nl 2)ij=Xn@En@a(nl)|{z} (nl)@a(nl)@W(nl 2)ij+ W(nl 2)ji.(48)Now,( (nl))T@a(nl)@W(nl 2)ij=( (nl))T@z(nl)@W(nl 2)ij(49)=( (nl))T@@W(nl 2)ijW(nl 1)a(nl 1)(50)=( (nl))T@@W(nl 2)ijW(nl 1)·ia(nl 1)i(51)=( (nl))TW(nl 1)·i@@W(nl 2)ija(nl 1)i(52)=( (nl))TW(nl 1)·i@@W(nl 2)ijf(z(nl 1)i) (53)=( (nl))TW(nl 1)·i@@W(nl 2)ijf(W(nl 2)i·a(nl 2)) (54)=( (nl))TW(nl 1)·if0(z(nl 1)i)a(nl 2)j(55)=⇣( (nl))TW(nl 1)·i⌘f0(z(nl 1)i)a(nl 2)j(56)=0@sl+1Xj=1W(nl 1)ji (nl)j)1Af0(z(nl 1)i)|{z}a(nl 2)j(57)= (nl 1)ia(nl 2)j(58)where we used in the ﬁrst line that the top layer is linear. This is a very detailed account of essentiallyjust the chain rule.So, we can write the errors of all layersl(except the top layer) (in vector format, using the Hadamardproduct ): (l)=⇣(W(l))T (l+1)⌘ f0(z(l)),(59)7

Two	layer	neural	nets	and	full	backprop•Last	missing	piece:	•In	general	for	any	matrix	W(l)	at	internal	layer	land	any	error	with	regularization	ERall	backpropin	standard	multilayer	neural	networks	boils	down	to	2	equations:•Top	and	bottom	layers	have	simpler	±4/12/16Richard	SocherLecture	5,	Slide	33
The second derivative in eq. 28 for output units is simply@a(nl)i@W(nl 1)ij=@@W(nl 1)ijz(nl)i=@@W(nl 1)ij⇣W(nl 1)i·a(nl 1)⌘=a(nl 1)j.(46)We adopt standard notation and introduce the error related to an output unit:@En@W(nl 1)ij=(yi ti)a(nl 1)j= (nl)ia(nl 1)j.(47)So far, we only computed errors for output units, now we will derive ’s for normal hidden units andshow how these errors are backpropagated to compute weight derivatives of lower levels. We will start withsecond to top layer weights from which a generalization to arbitrarily deep layers will become obvious.Similar to eq. 28, we start with the error derivative:@E@W(nl 2)ij=Xn@En@a(nl)|{z} (nl)@a(nl)@W(nl 2)ij+ W(nl 2)ji.(48)Now,( (nl))T@a(nl)@W(nl 2)ij=( (nl))T@z(nl)@W(nl 2)ij(49)=( (nl))T@@W(nl 2)ijW(nl 1)a(nl 1)(50)=( (nl))T@@W(nl 2)ijW(nl 1)·ia(nl 1)i(51)=( (nl))TW(nl 1)·i@@W(nl 2)ija(nl 1)i(52)=( (nl))TW(nl 1)·i@@W(nl 2)ijf(z(nl 1)i) (53)=( (nl))TW(nl 1)·i@@W(nl 2)ijf(W(nl 2)i·a(nl 2)) (54)=( (nl))TW(nl 1)·if0(z(nl 1)i)a(nl 2)j(55)=⇣( (nl))TW(nl 1)·i⌘f0(z(nl 1)i)a(nl 2)j(56)=0@sl+1Xj=1W(nl 1)ji (nl)j)1Af0(z(nl 1)i)|{z}a(nl 2)j(57)= (nl 1)ia(nl 2)j(58)where we used in the ﬁrst line that the top layer is linear. This is a very detailed account of essentiallyjust the chain rule.So, we can write the errors of all layersl(except the top layer) (in vector format, using the Hadamardproduct ): (l)=⇣(W(l))T (l+1)⌘ f0(z(l)),(59)7where the sigmoid derivative from eq. 14 givesf0(z(l))=( 1 a(l))a(l). Using that deﬁnition, we get thehidden layer backprop derivatives:@@W(l)ijER=a(l)j (l+1)i+ W(l)ij(60)(61)Which in one simpliﬁed vector notation becomes:@@W(l)ER= (l+1)(a(l))T+ W(l).(62)In summary, the backprop procedure consists of four steps:1. Apply an inputxnand forward propagate it through the network to get the hidden and outputactivations using eq. 18.2. Evaluate (nl)for output units using eq. 42.3. Backpropagate the ’s to obtain a (l)for each hidden layer in the network using eq. 59.4. Evaluate the required derivatives with eq. 62 and update all the weights using an optimizationprocedure such as conjugate gradient or L-BFGS. CG seems to be faster and work better whenusing mini-batches of training data to estimate the derivatives.If you have any further questions or found errors, please send an email torichard@socher.org5 Recursive Neural NetworksSame as backprop in previous section but splitting error derivatives and noting that the derivatives of thesameWat each node can all be added up. Lastly, the delta’s from the parent node and possible delta’sfrom a softmax classiﬁer at each node are just added.References[Ben07] Yoshua Bengio. Learning deep architectures for ai. Technical report, Dept. IRO, Universite deMontreal, 2007.
8

Visualization	of	intuition•Let’s	say	we	want	with	previous	layer	and	f	=	¾
4/12/16Richard	SocherLecture	1,	Slide	34Our first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 31 
1 σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s δ(3) Gradient w.r.t W(2) = δ(3)a(2)T 

Visualization	of	intuition
4/12/16Richard	SocherLecture	5,	Slide	35Our first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 32 
1 σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s δ(3) W(2)T δ(3) --Reusing the δ(3) for downstream updates. --Moving error vector across affine transformation simply requires multiplication with the transpose of forward matrix --Notice that the dimensions will line up perfectly too! 
Visualization	of	intuitionOur first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 33 
1 σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s W(2)T δ(3) σ’(z(2))!W(2)T δ(3)  = δ(2)  --Moving error vector across point-wise non-linearity requires point-wise multiplication with local gradient of the non-linearity 4/12/16Richard	SocherLecture	5,	Slide	36
Visualization	of	intuitionOur first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 34 
1 σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s δ(2) Gradient w.r.t W(1) = δ(2)a(1)T W(1)T δ(2) 
4/12/16Richard	SocherLecture	5,	Slide	37
Backpropagation(Another	explanation)•Compute	gradient	of	example-wise	loss	wrtparameters	•Simply	applying	the	derivative	chain	rule	wisely•If	computing	the	loss(example,	parameters)	is	O(n)	computation,	then	so	is	computing	the	gradient
38
Simple Chain Rule
39
Multiple Paths Chain Rule
40
Multiple	Paths	Chain	Rule	-General
…
41
Chain Rule in Flow Graph
…
……Flow	graph:	any	directed	acyclic	graphnode	=	computation	resultarc	=	computation	dependency=	successors	of	
42
Back-Prop in Multi-Layer Net……
43h=sigmoid(Vx)
Back-Prop in General Flow Graph
………
=	successors	of	
1.Fprop:	visit	nodes	in	topo-sort	order	-Compute	value	of	node	given	predecessors2.Bprop:-initialize	output	gradient	=	1	-visit	nodes	in	reverse	order:Compute	gradient	wrteach	node	using	gradient	wrtsuccessorsSingle	 scalar	output
44
Automatic Differentiation•The	gradient	computation	can	be	automatically	inferred	from	the	symbolic	expression	of	the	fprop.•Each	node	type	needs	to	know	how	to	compute	its	output	and	how	to	compute	the	gradient	wrtits	inputs	given	the	gradient	wrtits	output.•Easy	and	fast	prototyping45……
Summary
4/12/16Richard	Socher46•Congrats!•You	survived	the	hardest	part	of	this	class.•Everything	else	from	now	on	is	just	more	matrix	multiplications	and	backprop:)•Next	up:	•Recurrent	Neural	Networks
Sequence to Sequence Models
for Machine Translation (2)
CMSC 723 / LING 723 / INST 725
Marine Carpuat
Slides & figure credits: Graham 
Neubig
Introduction to Neural Machine Translation
•Neural language models review
•Sequence to sequence models for MT
•Encoder -Decoder
•Sampling and search (greedy vs beam search)
•Practical tricks
•Sequence to sequence models for other NLP tasks
•Attention mechanism
A recurrent language model

A recurrent language model

Encoder -decoder model

Encoder -decoder model

Generating Output
•We have a model P(E|F), how can we generate translations?
•2 methods
•Sampling : generate a random sentence according to probability distribution
•Argmax : generate sentence with highest probability
Training
•Same as for RNN language modeling
•Loss function
•Negative log -likelihood of training data
•Total loss for one example (sentence) = sum of loss at each time step (word)
•BackPropagation Through Time (BPTT)
•Gradient of loss at time step t is propagated through the network all the way 
back to 1sttime step
Note that training loss differs from 
evaluation metric (BLEU)

Other encoder structures:
Bidirectional encoder
•
Motivation:
-Help bootstrap learning
-By shortening length of 
dependencies
Motivation: 
-Take 2 hidden vectors from source 
encoder
-Combine them into a vector of size 
required by decoder
A few more tricks: addressing length bias
•Default models tend to generate short sentences
•Solutions:
•Prior probability on sentence length
•Normalize by sentence length

A few more tricks: ensembling
•Combine predictions from 
multiple models
•Methods
•Linear or log -linear interpolation
•Parameter averaging
Introduction to Neural Machine Translation
•Neural language models review
•Sequence to sequence models for MT
•Encoder -Decoder
•Sampling and search (greedy vs beam search)
•Practical tricks
•Sequence to sequence models for other NLP tasks
•Attention mechanism
Beyond MT: Encoder -Decoder can be used as 
Conditioned Language Models to generate text Y 
according to some specification X

Introduction to Neural Machine Translation
•Neural language models review
•Sequence to sequence models for MT
•Encoder -Decoder
•Sampling and search (greedy vs beam search)
•Practical tricks
•Sequence to sequence models for other NLP tasks
•Attention mechanism
Problem with previous encoder -decoder 
model
•Long -distance dependencies remain a problem
•A single vector represents the entire source sentence
•No matter its length
•Solution: attention mechanism
•An example of incorporating inductive bias in model architecture 
Attention model intuition
•Encode each word in source sentence into a vector
•When decoding, perform a linear combination of these vectors, 
weighted by “attention weights”
•Use this combination when predicting next word
[Bahdanau et al. 2015]
Attention model
Source word representations 
•We can use representations 
from bidirectional RNN encoder
•And concatenate them in a 
matrix

Attention model
Create a source context vector
•Attention vector:
•Entries between 0 and 1
•Interpreted as weight given to 
each source word when 
generating output at time step t
Attention vector Context vector
Attention model
Illustrating attention weights

Attention model
How to calculate attention scores

Attention model
Various ways of calculating attention score
•Dot product
•Bilinear function
•Multi -layer perceptron (original 
formulation in Bahdanau et al.)

Advantages of attention
•Helps illustrate/interpret translation decisions
•Can help insert translations for OOV
•By copying or look up in external dictionary
•Can incorporate linguistically motivated priors in model
Attention extensions
An active area of research
•Attend to multiple sentences ( Zoph et al. 2015)
•Attend to a sentence and an image (Huang et al. 2016)
•Incoprorate bias from alignment models
Introduction to Neural Machine Translation
•Neural language models review
•Sequence to sequence models for MT
•Encoder -Decoder
•Sampling and search (greedy vs beam search)
•Practical tricks
•Sequence to sequence models for other NLP tasks
•Attention mechanism
Maria -Florina  Balcan  
04/01/2015  Active Learning  
•HWK #6 due on Friday.  Logistics  
•Midway Project Review due on Monday.  
Make sure to talk to your  mentor TA!  
Classic Fully Supervised Learning 
Paradigm Insufficient Nowadays  
Modern applications: massive amounts of raw data.  
Only a tiny fraction can be annotated by human experts.  
Billions of webpages  
 Images  
 Protein sequences  
Modern applications: massive amounts of raw data.  Modern ML: New Learning Approaches  
Expert  
•Semi -supervised Learning, (Inter)active Learning . Techniques that best utilize data,  minimizing need for 
expert/human intervention . 
Paradigms where there has been great progress.  
                 Labeled Examples   Semi -Supervised  Learning  
Learning 
Algorithm  
Expert / Oracle  
Data Source  
Unlabeled 
examples  
Algorithm outputs a classifier  Unlabeled 
examples  
 Sl={(x1,y1), …,(xml,yml)}  
 Su={x1, …,xmu} drawn i.i.d from  D xi drawn i.i.d from  D, yi=c∗(xi)  Goal:  h has small error over D. 
 errDh=Pr
x~ D(hx≠c∗(x)) 
Unlabeled data useful if we have a bias/belief not only about 
the form of the target, but also about its relationship with 
the underlying data distribution . Key Insight/ Underlying Fundamental Principle  
E.g., “large margin separator ” 
[Joachims  ’99] 
 Similarity based  
(“small cut”) 
[B&C01], [ZGL03]  
 “self-consistent rules ” [Blum & Mitchell ’98]  
 
+ 
+ _ 
_ + h1(x1)=h2(x2) 
•Unlabeled data can help reduce search space or re -order the fns 
in the search space according to our belief, biasing the search 
towards fns satisfying the belief (which becomes concrete once 
we see unlabeled data).  
My Advisor  Prof. Avrim 
x1- Text info  x2- Link info  x = h x1, x2 i Semi -supervised L earning  
As in PAC/SLT, discuss algorithmic and sample complexity issues.  
•How much unlabeled data is needed.  
• depends both on complexity of H and of compatibility notion.  
•Ability of unlabeled data to reduce #of labeled examples.  
• compatibility of the target, helpfulness of the distrib . Analyze fundamental sample complexity aspects:  A General Discriminative Model for SSL  
[BalcanBlum , COLT 2005; JACM 2010]  
• Note: the mixture method that Tom talked about on Feb 25th can 
be explained from this point of view too. See the Zhu survey.  •Survey on “Semi -Supervised Learning” (Jerry Zhu, 2010) 
explains the SSL techniques from this point of view.  
Active Learning  
Additional resources:  
•Two faces of active learning. Sanjoy  Dasgupta . 2011 .  
•Active Learning. Bur Settles. 2012.  
•Active Learning. Balcan -Urner . Encyclopedia of Algorithms. 2015  
Batch Active Learning  
          A Label for that Example  Request for the Label of an Example            A Label for that Example  Request for the Label of an Example  
Data Source  
Unlabeled 
examples  . . .  
Algorithm outputs a classifier w.r.t D Learning 
Algorithm  
Expert  
•Learner can choose specific examples to be labeled.  
 •Goal:  use fewer labeled examples  
 [pick informative  examples to be labeled].  Underlying data 
distr. D. 
Unlabeled 
example 𝑥3 Unlabeled 
example 𝑥1 Unlabeled 
example 𝑥2 Selective Sampling Active Learning  
Request for 
label or let it 
go? 
Data Source  
Learning 
Algorithm  
Expert  
Request label  A label 𝑦1for example 𝑥1 
Let it 
go 
Algorithm outputs a classifier w.r.t D Request label  A label 𝑦3for example 𝑥3 
•Selective sampling AL (Online AL) : stream of unlabeled examples, 
when each arrives make a decision to ask for label or not.  
 •Goal:  use fewer labeled examples  
 [pick informative  examples to be labeled].  Underlying data 
distr. D. 
•Need to choose the label requests carefully, to get 
informative  labels.  What Makes a Good Active Learning 
Algorithm?  
•Guaranteed to output a relatively good classifier 
for most learning problems.  
• Doesn’t make too many label requests.  
     Hopefully  a lot less than passive learning and SSL . 
Can adaptive querying really do better than 
passive/random sampling?  
•YES! (sometimes)  
 
•We often need far fewer labels for active 
learning than for passive.  
•This is predicted by theory and has been 
observed in practice . 
Can adaptive querying help? [CAL92, Dasgupta04]  
•Threshold fns on the real line:  
w + - 
Exponential improvement.   hw(x) = 1(x ¸ w),  C = {hw: w 2 R} 
•How can we recover the correct labels with  ≪N queries?  
- •   Do binary search!  
Active : only O(log1/ϵ) labels . Passive supervised : Ω(1/ϵ) labels to find an -accurate threshold.  + - Active Algorithm  
Just need O(log N) labels!  
• N=O(1/ϵ)  we are guaranteed to get a classifier of error ≤ϵ.  • Get N unlabeled  examples  
• Output a classifier consistent with the N inferred labels.  

Common Technique in Practice  
Uncertainty sampling in SVMs common and quite useful 
in practice.  
•At any time during the alg., we have a “current guess” 
wt of the separator: the max -margin separator of all 
labeled points so far.  
•Request the label of the example closest to the current 
separator.  E.g., [Tong & Koller , ICML 2000; Jain, Vijayanarasimhan  & Grauman , NIPS 2010; 
Schohon  Cohn, ICML 2000]  
Active SVM Algorithm  
Common Technique in Practice  
Active SVM  seems to be quite useful in practice.  
•Find 𝑤𝑡 the max -margin 
separator of all labeled 
points so far.  
•Request the label of the 
example closest to the current 
separator: minimizing 𝑥𝑖⋅𝑤𝑡. [Tong & Koller , ICML 2000; Jain, Vijayanarasimhan  & Grauman , NIPS 2010]  
Algorithm (batch version)  
Input  Su={x1, …,xmu} drawn i.i.d from  the underlying source D  
Start: query for the labels of a few random 𝑥𝑖s. 
For 𝒕=𝟏, …., 
(highest uncertainty)  
Common Technique in Practice  
Active SVM  seems to be quite useful in practice.  
E.g., Jain, Vijayanarasimhan  & Grauman , NIPS 2010  
Newsgroups dataset (20.000  documents from 20 categories)  
Common Technique in Practice  
Active SVM  seems to be quite useful in practice.  
E.g., Jain, Vijayanarasimhan  & Grauman , NIPS 2010  
CIFAR -10 image dataset (60.000  images from 10 categories)  

Active SVM/ Uncertainty Sampling  
•Works sometimes….  
•However, we need to be very very very careful!!!  
•Myopic, greedy technique can suffer from sampling bias . 
•A bias created because of the querying strategy; as time 
goes on the sample is less and less representative of the true 
data source.  
[Dasgupta10]  
Active SVM/ Uncertainty Sampling  
•Works sometimes….  
•However, we need to be very very careful!!!  
Active SVM/ Uncertainty Sampling  
•Works sometimes….  
•However, we need to be very very careful!!!  
•Myopic, greedy technique can suffer from sampling bias . 
•Bias created because of the querying strategy; as time goes on 
the sample is less and less representative of the true source.  
•Main tension : want to choose informative points, but also 
want to guarantee that the classifier we output does  well on 
true random examples from the underlying distribution.  •Observed in practice too!!!!  

Safe Active Learning Schemes  
Disagreement Based Active Learning  
Hypothesis Space Search  
[CAL92]  [BBL06 ]  
 [Hanneke’07, DHM’07, Wang’09 , Fridman’09, Kolt10, BHW’08, BHLZ’10, H’10, Ailon’12, …]  
Version Spaces 
I.e., h∈ VS(H) iff hxi=c∗xi ∀i∈{1,…,ml}. •X – feature/instance space; distr. D over X; 𝑐∗ target fnc 
•Fix hypothesis space  H. 
Assume realizable case: c∗∈H.  Definition (Mitchell’82)  
Version space of H : part of H consistent with labels so far .  Given a set of labeled examples (x1,y1), …,(xml,yml),  yi=c∗(xi) 
Version Spaces 
Version space of H : part of H consistent with labels so far .  Given a set of labeled examples (x1,y1), …,(xml,yml),  yi=c∗(xi) •X – feature/instance space; distr. D over X; 𝑐∗ target fnc 
•Fix hypothesis space  H. 
Assume realizable case: c∗∈H.  Definition (Mitchell’82)  
E.g.,: data lies on 
circle in R2, H = 
homogeneous  
linear seps. current version space  
region of disagreement 
in data space  + + 
Version Spaces. Region of Disagreement 
current version space  Version space : part of H consistent with labels so far . 
+ + E.g.,: data lies on 
circle in R2, H = 
homogeneous  
linear seps. x∈X,x∈DIS(VSH) iff ∃h1,h2∈VS(H), h1x≠h2(x) Region of disagreement  = part of data space about which there 
is still some uncertainty (i.e. disagreement within version space)  Definition (CAL’92)  
region of disagreement 
in data space  
Pick a few points at random from the current 
region of uncertainty and query their labels.  current version space  
region of 
uncertainy  
Algorithm:   Disagreement Based Active Learning [CAL92]  
Note : it is active since we do not waste labels by querying 
in regions of space we are certain about the labels.  Stop when region of uncertainty is small.  
Disagreement Based Active Learning [CAL92]  
Pick a few points at random from the current region of 
disagreement DIS(Ht) and query their labels .  current version space  
region of 
uncertainy  
Algorithm:   
Query for the labels of a few random 𝑥𝑖s. 
Let  H1  be the current version space.  
For 𝒕=𝟏, …., 
Let Ht+1 be the new version space.  
Region of uncertainty [CAL92] 
•  Current version space : part of C consistent with labels so far.  
•  “Region of uncertainty”  = part of data space about which 
there is still some uncertainty (i.e. disagreement within version 
space)  
current version space  
 region of uncertainty 
in data space  + + 
Region of uncertainty [CAL92] 
•  Current version space : part of C consistent with labels so far.  
•  “Region of uncertainty”  = part of data space about which 
there is still some uncertainty (i.e. disagreement within version 
space)  
new version space  
New region of 
disagreement in 
data space  + + 
How about the agnostic case 
where the target might not 
belong the H? 

A2 Agnostic Active Learner [BBL’06]  
•Pick a few points at random from the current region 
of disagreement DIS(Ht) and query their labels .  current version space  
region of 
disagreement  Algorithm:   
For 𝒕=𝟏, …., 
•Throw out hypothesis if you are statistically 
confident they are suboptimal .  Careful use of generalization bounds; 
Avoid the sampling bias !!!! Let  H1=H. 
When Active Learning Helps. Agnostic case  
“Region of disagreement” style:   Pick a few points at random from the 
current region of disagreement, query their labels, throw out 
hypothesis if you are statistically confident they are suboptimal.  
•  C - homogeneous linear separators in Rd, 
         D - uniform ,  low  noise, only d2 log (1/) labels.  [Balcan, Beygelzimer, Langford, ICML’06]  A2 the first algorithm which is robust to noise.  
•  C – thresholds, low noise, exponential improvement.  [Balcan, Beygelzimer, Langford, JCSS’08]  
•  It is safe (never worse than passive learning) & exponential improvements.  Guarantees for A2 [BBL’06,’08] : 
c* 
A lot of subsequent work.  
 [Hanneke’07, DHM’07, Wang’09 , Fridman’09, Kolt10, BHW’08, BHLZ’10, H’10, Ailon’12, …]  
General guarantees for A2 Agnostic Active Learner  
“Disagreement based”:   Pick a few points at random from the current region of 
uncertainty, query their labels, throw out hypothesis if you are statistically 
confident  they are suboptimal.  
Guarantees for A2 [Hanneke’07]:  
Disagreement coefficient  
[BBL’06]  
Realizable case:  
c* Linear Separators, uniform distr.:  
How quickly the region of disagreement 
collapses as we get closer and closer to 
optimal classifier  
Disagreement Based Active Learning  
“Disagreement based ” algos :  query points from current 
region of disagreement, throw out hypotheses when 
statistically confident they are suboptimal.  
Lots of subsequent work trying to make is more efficient computationally 
and more aggressive too: [Hanneke07, DasguptaHsuMontleoni’07 , Wang’09 , 
Fridman’09 ,  Koltchinskii10, BHW’08, BeygelzimerHsuLangfordZhang’10 , Hsu’10 , Ailon’12, …]  •Generic (any class), adversarial label noise.  
Still, could be suboptimal in label complex & computationally 
inefficient in general.  
 •Computationally efficient for classes of small VC -dimension  
Other Interesting ALTechniques  
used in Practice  
Interesting open question to analyze 
under what conditions they are successful.  
Density -Based Sampling  
Centroid of largest unsampled cluster  
[Jaime G. Carbonell ] 
Uncertainty Sampling  
Closest to decision boundary (Active SVM)  
[Jaime G. Carbonell ] 
Maximal Diversity Sampling  
Maximally distant from labeled x’s  
[Jaime G. Carbonell ] 
Ensemble -Based Possibilities  
Uncertainty + Diversity criteria  
Density + uncertainty criteria  
[Jaime G. Carbonell ] 
Graph -based Active and 
Semi -Supervised Methods  
Graph -based Methods  
E.g., handwritten digits [Zhu07]:  •Assume we are given a pairwise similarity fnc and that 
very similar examples probably have the same label.  
•If we have a lot of labeled data, this suggests a 
Nearest -Neighbor type of algorithm.  
•If you have a lot of unlabeled data, perhaps can use 
them as “stepping stones ”. 
Graph -based Methods  
Idea : construct a graph with edges between very similar 
examples.  
Unlabeled data can help “glue” the objects of the same 
class together.  
Several methods:  
–Minimum/ Multiway  cut [Blum&Chawla01 ] 
–Minimum “soft -cut” [ZhuGhahramaniLafferty’03]  
–Spectral partitioning  
–… Main Idea : 
•Might have also glued together in G 
examples of different classes.  Often, transductive  approach .  (Given L + U, output predictions on 
U). Are alllowed  to output any labeling of 𝐿∪𝑈. 
•Construct graph G with edges 
between very similar examples.  
•Run a graph partitioning algorithm to 
separate the graph into pieces.  Graph -based Methods  
SSL using soft cuts 
[ZhuGhahramaniLafferty’03]  
 
Similar nodes get 
similar labels 
(weighted similarity)  Agreement with labels  
(agreement not strictly enforces)  Solve for label function 𝑓𝑥∈0,1 to minimize:  
𝐽𝑓= 𝑤𝑖𝑗𝑓𝑥𝑖−𝑓(𝑥𝑗)2+
𝑒𝑑𝑔𝑒𝑠 (𝑖,𝑗) 𝜆𝑓𝑥𝑖−𝑦𝑖2
𝑥𝑖∈𝐿 
Active learning with label propagation  
 
44 
(using soft -cuts)  
How to choose 
which node to 
query?  
Active learning with label propagation  
45 
(using soft -cuts)  One natural idea: query the most uncertain point.  
But this has only one edge.  Query won’t have 
much impact!  
(even worse: a completely isolated node)  
Active learning with label propagation  
46 
(using soft -cuts)  Instead, use a 1 -step-lookahead heuristic:  
•For a node with label 𝑝, assume that querying will have prob 
𝑝 of returning answer 1, 1−𝑝 of returning answer 0.  
•Compute “average confidence” after running soft -cut in each case:  
𝑝1
𝑛 max𝑓1𝑥𝑖,1−𝑓1𝑥𝑖+(1−𝑝)1
𝑛 max𝑓0𝑥𝑖,1−𝑓0𝑥𝑖 𝑥𝑖 𝑥𝑖  
•Query node s.t. this quantity is highest (you want to be more 
confident on average).  
Active Learning with Label Propagation  in 
Practice  
•Does well for Video Segmentation (Fathi -Balcan -Ren-Regh , BMVC 11) . 

What You Should Know  
•Active learning could be really helpful, could provide 
exponential improvements in label complexity (both 
theoretically and practically)!  
•Common heuristics (e.g., those based on uncertainty 
sampling). Need to be very careful due to  sampling bias.  
•Safe Disagreement Based Active Learning Schemes.  
•Understand how they operate precisely in noise 
free scenarios.  
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  January 26, 2015 
Today: • Bayes Classifiers • Conditional Independence • Naïve Bayes Readings: Mitchell:     “Naïve Bayes and Logistic Regression”      (available on class website) 
Two Principles for Estimating Parameters • Maximum Likelihood Estimate (MLE): choose θ that maximizes probability of observed data 
• Maximum a Posteriori (MAP) estimate: choose θ that is most probable given prior probability and the data 

Maximum Likelihood Estimate 
X=1 X=0 
P(X=1) = θ P(X=0) = 1-θ (Bernoulli)  

Maximum A Posteriori (MAP) Estimate 
X=1 X=0 

Let’s learn classifiers by learning P(Y|X) Consider Y=Wealth,  X=<Gender, HoursWorked>          Gender HrsWorked P(rich | G,HW) P(poor | G,HW) F <40.5 .09 .91 F >40.5 .21 .79 M <40.5 .23 .77 M >40.5 .38 .62 

How many parameters must we estimate?  Suppose X =<X1,… Xn>  where Xi and Y are boolean RV’s  To estimate P(Y| X1, X2, … Xn)     If we have 30 boolean Xi’s:  P(Y | X1, X2, … X30)             

Bayes Rule  
Which is shorthand for: 
Equivalently: 

Can we reduce params using Bayes Rule? Suppose X =<X1,… Xn>  where Xi and Y are boolean RV’s  How many parameters to define P(X1,… Xn | Y)?     How many parameters to define P(Y)? 

Naïve Bayes Naïve Bayes assumes    i.e., that Xi and Xj are conditionally independent given Y, for all i≠j 

Conditional Independence  Definition: X is conditionally independent of Y  given Z, if the probability distribution governing X is independent of the value of Y, given the value of Z    Which we often write    E.g.,   

Naïve Bayes uses assumption that the Xi are conditionally independent, given Y.   E.g.,  Given this assumption, then:    

Naïve Bayes uses assumption that the Xi are conditionally independent, given Y.   E.g.,  Given this assumption, then:     in general:   

Naïve Bayes uses assumption that the Xi are conditionally independent, given Y.   E.g.,  Given this assumption, then:     in general:  How many parameters to describe P(X1…Xn|Y)?  P(Y)? • Without conditional indep assumption? • With conditional indep assumption? 

Bayes rule: Assuming conditional independence among Xi’s:    So, to pick most probable Y for Xnew = < X1, …, Xn >   
Naïve Bayes in a Nutshell 

Naïve Bayes Algorithm – discrete Xi  • Train Naïve Bayes (examples)    for each* value yk  estimate   for each* value xij of each attribute Xi   estimate  • Classify (Xnew)    
 * probabilities must sum to 1, so need estimate only n-1 of these... 

Estimating Parameters: Y, Xi discrete-valued  Maximum likelihood estimates (MLE’s): 
Number of items in dataset D for which Y=yk 
Example: Live in Sq Hill?  P(S|G,D,B) • S=1 iff live in Squirrel Hill • G=1 iff shop at SH Giant Eagle • D=1 iff Drive or carpool to CMU • B=1 iff Birthday is before July 1  What probability parameters must we estimate? 
Example: Live in Sq Hill?  P(S|G,D,E) • S=1 iff live in Squirrel Hill • G=1 iff shop at SH Giant Eagle • D=1 iff Drive or Carpool to CMU • B=1 iff Birthday is before July 1   P(S=1) : P(D=1 | S=1) : P(D=1 | S=0) : P(G=1 | S=1) : P(G=1 | S=0) : P(B=1 | S=1) : P(B=1 | S=0) : P(S=0) : P(D=0 | S=1) : P(D=0 | S=0) : P(G=0 | S=1) : P(G=0 | S=0) : P(B=0 | S=1) : P(B=0 | S=0) : 
Naïve Bayes: Subtlety #1 Often the Xi are not really conditionally independent  • We use Naïve Bayes in many cases anyway, and it often works pretty well – often the right classification, even when not the right probability (see [Domingos&Pazzani, 1996])  • What is effect on estimated P(Y|X)? – Extreme case: what if we add two copies: Xi  = Xk   
Extreme case: what if we add two copies: Xi  = Xk  
Naïve Bayes: Subtlety #2 If unlucky, our MLE estimate for P(Xi | Y) might be zero.  (for example, Xi = birthdate.  Xi = Jan_25_1992)  • Why worry about just one parameter out of many? • What can be done to address this? 
Estimating Parameters • Maximum Likelihood Estimate (MLE): choose θ that maximizes probability of observed data 
• Maximum a Posteriori (MAP) estimate: choose θ that is most probable given prior probability and the data 

Maximum likelihood estimates: 
Estimating Parameters: Y, Xi discrete-valued  
MAP estimates (Beta, Dirichlet priors):  Only difference: “imaginary” examples 

Learning to classify text documents • Classify which emails are spam? • Classify which emails promise an attachment? • Classify which web pages are student home pages? How shall we represent text documents for Naïve Bayes? 
Baseline: Bag of Words Approach 
aardvark 0 about 2 all 2 Africa 1 apple 0 anxious 0 ... gas 1 ... oil 1 … Zaire 0 
Learning to classify document: P(Y|X) the “Bag of Words” model • Y discrete valued.  e.g., Spam or not • X = <X1, X2, … Xn> = document  • Xi is a random variable describing the word at position i in the document • possible values for Xi : any word wk in English  • Document = bag of words: the vector of counts for all wk’s – like #heads, #tails, but we have many more than 2 values – assume word probabilities are position independent  (i.i.d. rolls of a 50,000-sided die) 
Naïve Bayes Algorithm – discrete Xi  • Train Naïve Bayes (examples)    for each value yk  estimate   for each value xj of each attribute Xi   estimate   • Classify (Xnew)    
prob that word xj appears in position i, given Y=yk  * Additional assumption:  word probabilities are position independent 
MAP estimates for bag of words  Map estimate for multinomial     What β’s should we choose?  


For code and data, see www.cs.cmu.edu/~tom/mlbook.html  click on “Software and Data” 
What you should know: • Training and using classifiers based on Bayes rule • Conditional independence – What it is – Why it’s important • Naïve Bayes – What it is – Why we use it so much – Training using MLE, MAP estimates – Discrete variables and continuous (Gaussian) 
Questions:  • How can we extend Naïve Bayes if just 2 of the Xi‘s are dependent? • What does the decision surface of a Naïve Bayes classifier look like? • What error will the classifier achieve if Naïve Bayes assumption is satisfied and we have infinite training data? • Can you use Naïve Bayes for a combination of discrete and real-valued Xi?  



COMS 4721: Machine Learning for Data Science
Lecture 9, 2/16/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
LOGISTIC REGRESSION
BINARY CLASSIFICATION
Linear classiﬁers
Given: Data (x1;y1);:::; (xn;yn), where xi2Rdandyi2f  1;+1g
Alinear classiﬁer takes a vector w2Rdand scalar w02Rand predicts
yi=f(xi;w;w0) =sign(xT
iw+w0):
We discussed two methods last time:
ILeast squares: Sensitive to outliers
IPerceptron: Convergence issues, assumes linear separability
Can we combine the separating hyperplane idea with probability to ﬁx this?
BAYES LINEAR CLASSIFICATION
Linear discriminant analysis
We saw an example of a linear classiﬁcation rule using a Bayes classiﬁer.
For the model yBern()andxjyN(y;), declare y=1 given xif
lnp(xjy=1)p(y=1)
p(xjy=0)p(y=0)>0:
In this case, the log odds is equal to
lnp(xjy=1)p(y=1)
p(xjy=0)p(y=0)=ln1
0 1
2(1+0)T 1(1 0)
| {z }
a constant w0
+xT 1(1 0)|{z}
a vector w
LOG ODDS AND BAYES CLASSIFICATION
Original formulation
Recall that originally we wanted to declare y=1 given xif
lnp(y=1jx)
p(y=0jx)>0
We didn’t have a way to deﬁne p(yjx), so we used Bayes rule:
IUsep(yjx) =p(xjy)p(y)
p(x)and let the p(x)cancel each other in the fraction
IDeﬁne p(y)to be a Bernoulli distribution (coin ﬂip distribution)
IDeﬁne p(xjy)however we want (e.g., a single Gaussian)
Now, we want to directly deﬁne p(yjx). We’ll use the log odds to do this.
LOG ODDS AND BAYES CLASSIFICATION
Log odds and hyperplanes
Classifying xbased on the log odds
L=lnp(y= +1jx)
p(y= 1jx);
we notice that
1.L0 : more conﬁdent y= +1,
2.L0 : more conﬁdent y= 1,
3.L=0 : can go either wayx1x2H
w
 w0=kwk2x
The linear function xTw+w0captures these three objectives:
IThe distance of xto a hyperplane Hdeﬁned by (w;w0)isxTw
kwk2+w0
kwk2.
IThe sign of the function captures which side xis on.
IAsxmoves away/towards H, we become more/less conﬁdent.
LOG ODDS AND HYPERPLANES
Logistic link function
We can directly plug in the hyperplane representation for the log odds:
lnp(y= +1jx)
p(y= 1jx)=xTw+w0
Question : What is different from the previous Bayes classiﬁer?
Answer : There was a formula for calculating wandw0based on the prior
model and data x. Now, we put no restrictions on these values.
Setting p(y= 1jx) =1 p(y= +1jx), solve for p(y= +1jx)to ﬁnd
p(y= +1jx) =expfxTw+w0g
1+expfxTw+w0g=(xTw+w0):
IThis is called the sigmoid function .
IWe have chosen xTw+w0as the link function for the log odds.
LOGISTIC SIGMOID FUNCTION
−5 0 500.51
IRed line: Sigmoid function (xTw+w0), which maps xtop(y= +1jx).
IThe function ()captures our desire to be more conﬁdent as we move
away from the separating hyperplane, deﬁned by the x-axis.
I(Blue dashed line: Not discussed.)
LOGISTIC REGRESSION
As with regression, absorb the offset: w w0
w
andx 1
x
.
Deﬁnition
Let(x1;y1);:::; (xn;yn)be a set of binary labeled data with y2f  1;+1g.
Logistic regression models each yias independently generated, with
P(yi= +1jxi;w) =(xT
iw); (xi;w) =exT
iw
1+exT
iw:
Discriminative vs Generative classiﬁers
IThis is a discriminative classiﬁer because xis not directly modeled.
IBayes classiﬁers are known as generative because xis modeled.
Discriminative: p(yjx) Generative: p(xjy)p(y).
LOGISTIC REGRESSION LIKELIHOOD
Data likelihood
Deﬁnei(w) =(xT
iw). The joint likelihood of y1;:::; ynis
p(y1;:::; ynjx1;:::; xn;w) =nY
i=1p(yijxi;w)
=nY
i=1i(w)1(yi=+1)(1 i(w))1(yi= 1)
INotice that each ximodiﬁes the probability of a ‘ +1’ for its respective yi.
IPredicting new data is the same:
IIfxTw>0, then(xTw)>1=2 and predict y= + 1, and vice versa.
IWe now get a conﬁdence in our prediction via the probability (xTw).
LOGISTIC REGRESSION AND MAXIMUM LIKELIHOOD
More notation changes
Use the following fact to condense the notation:
eyixT
iw
1+eyixT
iw
|{z}
i(yiw)=exT
iw
1+exT
iw
|{z}
i(w)1(yi=+1)
1 exT
iw
1+exT
iw
|{z}
1 i(w)1(yi= 1)
therefore, the data likelihood can be written compactly as
p(y1;:::; ynjx1;:::; xn;w) =nY
i=1i(yiw)
We want to maximize this over w.
LOGISTIC REGRESSION AND MAXIMUM LIKELIHOOD
Maximum likelihood
The maximum likelihood solution for wcan be written
wML=arg max
wnX
i=1lni(yiw)
=arg max
wL
As with the Perceptron, we can’t directly set rwL=0, and so we need an
iterative algorithm. Since we want to maximizeL, at step twe can update
w(t+1)=w(t)+rwL;rwL=nX
i=1(1 i(yiw))yixi:
We will see that this results in an algorithm similar to the Perceptron.
LOGISTIC REGRESSION ALGORITHM (STEEPEST ASCENT )
Input : Training data (x1;yi);:::; (xn;yn)and step size >0
1.Setw(1)=~0
2.For iteration t=1;2;::: do
Update w(t+1)=w(t)+nX
i=1
1 i(yiw(t))
yixi
Perceptron : Search for misclassiﬁed (xi;yi), update w(t+1)=w(t)+yixi:
Logistic regression : Something similar except we sum over all data.
IRecall thati(yiw)picks out the probability model gives to the observed yi.
ITherefore 1  i(yiw)is the probability the model picks the wrong value.
IPerceptron is “all-or-nothing.” Either it’s correctly or incorrectly classiﬁed.
ILogistic regression has a probabilistic “fudge-factor.”
BAYESIAN LOGISTIC REGRESSION
Problem : If a hyperplane can separate all training data, then kwMLk2!1 .
This drivesi(yiw)!1 for each (xi;yi).
Even for nearly separable data it might get a few very wrong in order to be
more conﬁdent about the rest. This is a case of “over-ﬁtting.”
A solution : Regularize wwithwTw:
wMAP=arg max wPn
i=1lni(yiw) wTw
We’ve seen how this corresponds to a
Gaussian prior distribution on w.
How about the posterior p(wjx;y)?
−4−202468−8−6−4−2024
LAPLACE APPROXIMATION
BAYESIAN LOGISTIC REGRESSION
Posterior calculation
Deﬁne the prior distribution on wto be wN(0; 1I). The posterior is
p(wjx;y) =p(w)Qn
i=1i(yiw)R
p(w)Qn
i=1i(yiw)dw
This is not a “standard” distribution and we can’t calculate the denominator.
Therefore we can’t actually say what p(wjx;y)is.
Can we approximate p(wjx;y)?
LAPLACE APPROXIMATION
One strategy
Pick a distribution to approximate p(wjx;y). We will say
p(wjx;y)Normal (;):
Now we need a method for setting and.
Laplace approximations
Using a condensed notation, notice from Bayes rule that
p(wjx;y) =elnp(y;wjx)
R
elnp(y;wjx)dw:
We will approximate ln p(y;wjx)in the numerator and denominator.
LAPLACE APPROXIMATION
Let’s deﬁne f(w) =lnp(y;wjx).
Taylor expansions
We can approximate f(w)with a second order Taylor expansion .
Recall that w2Rd+1. For any point z2Rd+1,
f(w)f(z) + ( w z)Trf(z) +1
2(w z)T 
r2f(z)
(w z)
The notationrf(z)is short forrwf(w)jz, and similarly for the matrix of
second derivatives. We just need to pick z.
The Laplace approximation deﬁnes z=wMAP.
LAPLACE APPROXIMATION (SOLVING )
Recall f(w) =lnp(y;wjx)andz=wMAP. From Bayes rule and the Laplace
approximation we now have
p(wjx;y) =ef(w)
R
ef(w)dw
ef(z)+(w z)Trf(z)+1
2(w z)T(r2f(z))(w z)
R
ef(z)+(w z)Trf(z)+1
2(w z)T(r2f(z))(w z)dw
This can be simpliﬁed in two ways,
1. The term ef(wMAP)in the numerator and denominator can be viewed as a
multiplicative constant since it doesn’t vary in w. They therefore cancel.
2. By deﬁnition of how we ﬁnd wMAP, the vectorrwlnp(y;wjx)jwMAP=0.
LAPLACE APPROXIMATION (SOLVING )
We’re therefore left with the approximation
p(wjx;y)e 1
2(w wMAP)T( r2lnp(y;wMAPjx))(w wMAP)
R
e 1
2(w wMAP)T( r2lnp(y;wMAPjx))(w wMAP)dw
The solution comes by observing that this is a multivariate normal,
p(wjx;y)Normal (;);
where
=wMAP; = 
 r2lnp(y;wMAPjx) 1
We can take the second derivative (Hessian) of the log joint likelihood to ﬁnd
r2lnp(y;wMAPjx) = I nX
i=1i(yiwMAP) (1 i(yiwMAP))xixT
i
BAYESIAN LOGISTIC REGRESSION
Laplace approximation for logistic regression
Given labeled data (x1;y1);:::; (xn;yn)and the model
P(yijxi;w) =i(yiw);wN(0; 1I);  i(yiw) =eyixT
iw
1+eyixT
iw
1. Find: wMAP=arg max
wnX
i=1lni(yiw) 
2wTw
2. Set:  1= I nX
i=1i(yiwMAP) (1 i(yiwMAP))xixT
i
3. Approximate: p(wjx;y) =N(wMAP;).
CMSC 422 Introduction to Machine LearningLecture 13 Binary Classification with Linear ModelsFurong Huang / furongh@cs.umd.eduSlides adapted from Prof Carpuatand Duraiswami
Project 1 –regrading requests on piazza•all grade requests submitted by Friday will be handled by Friday evening•requests submitted during spring break or the week after will be handled daily starting the Monday after spring break•All grade requests must be submitted by the Thursday after Spring Break.
Commonmisunderstandings -I
Misunderstanding I:The classifier is best when performance on test and training are equal. (KNN problem)
The best classifier here is for K = 5?(Reasoning, "Since the training and test are equal we are generalizing perfectly." ?) NOIn this case, it may not matter much as the difference in accuracy is only .02, but if you had training 95 & test 90, you wouldn't want to choose training 60 test 60 just because they're equal.
Common misunderstandings-II
MisunderstandingII:Generally misunderstanding underfitting(NN eps problem)
MisunderstandingII(a):“Weareunderfittingforeps<.35.Becausethetestaccuracyincreaseswitheps,itisclearwehavenotlearnedeverythingwecanlearnfromourdataset,thereforeweareunderfitting.”NO
Common misunderstandings-II
MisunderstandingII:Generally misunderstanding underfitting(NN eps problem)
“Testaccuracyshowshowwellyougeneralize,trainingaccuracyshowshowmuchyou'velearned."With100%trainingand50%test,we'velearnedtoomuch,cannotgeneralize.Therefore,we'reoverfittingnotunderfitting.
Common misunderstandings-II
MisunderstandingII:Generally misunderstanding underfitting(NN eps problem)
Misunderstanding II(b):"Although training accuracy is decreasing, test accuracy is increasing, therefore we can not be overfitting". 

Common misunderstandings -performance on low examples*Case 1* Why does the training accuracy decrease? "there is too much noise in the data" or "it's too hard to learn 1200 examples" ? NOIt was because the DT had a fixed max-depth of 9. Also, this example has 100% accuracy early on. XOR example : DT with a depth of 2, we can get 100% accuracy, but with a depth of 1 we cannot. By limiting the max-depth we limit what we can learn.
Common misunderstandings -performance on low examples
DTKNN*Case 2* Why is there jaggedness on the left?
The behavior can be very random when you've only seen a few examples.
What we learned last time•Ranking•Bias and Fairness•Unsupervised adaptation
Supervised adaptationGoal:learn a classifier f that achieves low expected loss under new distributionGiven labeled training data from old distribution And labeled examples from new distribution

One solution: feature augmentationMap inputs to a new augmented representation

One solution: feature augmentation•Transform Doldand Dnewtraining examples•Train a classifier on new representations•Done!
One solution: feature augmentation•Adding instance weighting might be useful if N >> M•Most effective when distributions are “not too close but not too far”•In practice, always try “old only”, “new only”, “union of old and new” as well!
Bias and how to deal with it•Train/test mismatch•Unsupervised adaptation•Supervised adaptation
TopicsLinear ModelsLoss functionsRegularizationGradient DescentCalculus refresherConvexityGradients[CIML Chapter 6]
Binary classificationvia hyperplanesA classifier is a hyperplane (w,b)At test time, we check on what side of the hyperplane examples fall!"=$%&'()*++-)This is a linear classifierBecause the prediction is a linear combination of feature values x


Learning a Linear Classifieras an Optimization Problem
Indicator function: 1 if (.) is true, 0 otherwiseThe loss function above is called the 0-1 loss
Loss functionmeasures how well classifier fits training dataRegularizerprefers solutions that generalize wellObjective function
Learning a Linear Classifieras an Optimization Problem
•Problem:The 0-1 loss above is NP-hard to optimize exactly/approximately in general•Solution:Different loss function approximations and regularizerslead to specific algorithms(e.g., perceptron, support vector machines, logistic regression, etc.)
The 0-1 LossSmall changes in w,bcan lead to big changes in the loss value0-1 loss is non-smooth, non-convex

Calculus refresher:Smooth functions, convex functions
Approximating the 0-1 loss with surrogate loss functionsExamples (with b = 0)Hinge lossLog lossExponential lossAll are convex upper-bounds on the 0-1 loss

Approximating the 0-1 loss with surrogate loss functionsExamples (with b = 0)Hinge lossLog lossExponential lossQ: Which of these loss functions is not smooth?

Approximating the 0-1 loss with surrogate loss functionsExamples (with b = 0)Hinge lossLog lossExponential lossQ: Which of these loss functions is most sensitive to outliers?

Casting Linear Classificationas an Optimization Problem
Indicator function: 1 if (.) is true, 0 otherwiseThe loss function above is called the 0-1 loss
Loss functionmeasures how well classifier fits training dataRegularizerprefers solutions that generalize wellObjective function
The regularizertermGoal: find simple solutions  (inductive bias)Ideally, we want most entries of w to be zero, so prediction depends only on a small number of features.Formally, we want to minimize:That’s NP-hard, so we use approximations instead. E.g., we encourage wd’sto be small

Norm-based Regularizers!"norms can be used as regularizers
Contourplots forp = 2p = 1p < 1
Norm-based Regularizers!"norms can be used as regularizersSmaller p favors sparse vectors wi.e. most entries of w are close or equal to 0!#norm:convex,smooth,easytooptimize!$norm:  encourages sparse w, convex, but not smooth at axis points%<1∶norm becomes non convex and hard to optimize
Casting Linear Classificationas an Optimization Problem
Indicator function: 1 if (.) is true, 0 otherwiseThe loss function above is called the 0-1 loss
Loss functionmeasures how well classifier fits training dataRegularizerprefers solutions that generalize wellObjective function
What is the perceptron optimizing?
Loss function is a variant of the hinge loss

Recap: Linear ModelsGeneral framework for binary classificationCast learning as optimization problemOptimization objective combines 2 termsloss function: measures how well classifier fits training data Regularizer: measures how simple classifier is•Does not assume data is linearly separableLets us separate model definition from training algorithm
Calculus refresher:Gradients
Gradient descentA general solution for our optimization problemIdea: take iterative steps to update parameters in the direction of the gradient

Gradient descent algorithm

Recap: Linear ModelsGeneral framework for binary classificationCast learning as optimization problemOptimization objective combines 2 termsloss function: measures how well classifier fits training data Regularizer: measures how simple classifier is•Does not assume data is linearly separableLets us separate model definition from training algorithm (Gradient Descent)
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
Question	Answering(and	Textual	Entailment)Prof.	Sameer	SinghCS	295:	STATISTICAL	NLPWINTER	2017March	14,	2017Based	on	slides	from	Dan	Jurafsky,	Yejin	Choi,	Stephen	Clark,	Dan	Klein,	NiranjanBalasubramanian,	and	everyone	else	they	copied	from.
Upcoming…•Homework	4	was	due	on	last	night•Lowest	grade	of	the	homeworkswill	be	droppedHomework
•Final	report	due	in	a	week:	March	20,	2017•Instructions	coming	soon:	ACL	style,	5	pages	(+references)Project•Paper	summaries	due	tonight•Summary	2	gradedSummaries
CS	295:	STATISTICAL	NLP	(WINTER	2017)2TA/Instructor	Evaluations	are	available!
OutlineQuestion	Answering
Textual	EntailmentCS	295:	STATISTICAL	NLP	(WINTER	2017)3IR-Based	QA	SystemOther	Extensions
OutlineQuestion	Answering
Textual	EntailmentCS	295:	STATISTICAL	NLP	(WINTER	2017)4IR-Based	QA	SystemOther	Extensions
Questions	in	Modern	SystemsFactoid	questions◦Who	wrote	“The	Universal	Declaration	of	Human	Rights”?◦How	many	calories	are	there	in	two	slices	of	apple	pie?◦What	is	the	average	age	of	the	onset	of	autism?◦Where	is	Apple	Computer	based?Complex	(narrative)	questions:◦In	children	with	an	acute	febrile	illness,	what	is	the	efficacy	of	acetaminophen	in	reducing	fever?◦What	do	scholars	think	about	Jefferson’s	position	on	dealing	with	pirates?
CS	295:	STATISTICAL	NLP	(WINTER	2017)5
Commercial	systems:	mainly	factoid	questionsWhere	is	the	Louvre	Museum	located?In	Paris,	FranceWhat’s	the	abbreviationfor	limited	partnership?L.P .Whatare	the	names	of	Odin’s	ravens?Huginnand	MuninnWhatcurrency	is	used	in	China?The	yuanWhat	kind	of	nuts	are	used	in	marzipan?almondsWhat	instrument	does	Max	Roach	play?drumsWhat	is	the	telephone	number	for	Stanford	University?650-723-2300CS	295:	STATISTICAL	NLP	(WINTER	2017)6
Paradigms	for	QAIR-based	approaches◦TREC;		IBM	Watson;	GoogleKnowledge-based	and	Hybrid	approaches◦IBM	Watson;	Apple	Siri;	Wolfram	Alpha;	True	Knowledge	Evi
CS	295:	STATISTICAL	NLP	(WINTER	2017)7
Many	questions	can	already	be	answered	by	web	search
CS	295:	STATISTICAL	NLP	(WINTER	2017)8

IR-based	Question	Answering
CS	295:	STATISTICAL	NLP	(WINTER	2017)9

IR-based	Factoid	QA
CS	295:	STATISTICAL	NLP	(WINTER	2017)10
DocumentDocumentDocument
Document
Document
Document
Document
Document
Question ProcessingPassageRetrievalQuery FormulationAnswer Type DetectionQuestionPassage RetrievalDocument RetrievalAnswer ProcessingAnswerpassagesIndexingRelevantDocsDocument
DocumentDocument
Knowledge-based	QA	(Siri)Build	a	semantic	representation	of	the	query◦Times,	dates,	locations,	entities,	numeric	quantitiesMap	from	this	semantics	to	query	structured	data		or	resources◦Geospatial	databases◦Ontologies	(Wikipedia	infoboxes,	dbPedia,	WordNet,	Yago)◦Restaurant	review	sources	and	reservation	services◦Scientific	databases
CS	295:	STATISTICAL	NLP	(WINTER	2017)11
Hybrid	approaches	(Watson)Build	a	shallow	semantic	representation	of	the	queryGenerate	answer	candidates	using	IR	methods◦Augmented	with	ontologies	and	semi-structured	dataScore	each	candidate	using	richer	knowledge	sources◦Geospatial	databases◦Temporal	reasoning◦Taxonomical	classification
CS	295:	STATISTICAL	NLP	(WINTER	2017)12
IBM’s	Watson
Won	Jeopardyon	February	16,	2011!13WILLIAM	WILKINSON’S	“AN	ACCOUNT	OF	THE	PRINCIPALITIES	OFWALLACHIA	AND	MOLDOVIA”INSPIRED	THIS	AUTHOR’SMOST	FAMOUS	NOVELBram	Stoker
CS	295:	STATISTICAL	NLP	(WINTER	2017)
Motivation	for	Watson
CS	295:	STATISTICAL	NLP	(WINTER	2017)14
Single	Source	is	not	Sufficient
CS	295:	STATISTICAL	NLP	(WINTER	2017)15

Watson	Architecture
CS	295:	STATISTICAL	NLP	(WINTER	2017)16
. . .
Models
Answer & Confidence
Question
Evidence Sources
Models
Models
Models
Models
Models
PrimarySearch
CandidateAnswerGeneration
Answer Sources
EvidenceRetrievalEvidence ScoringLearned Modelshelp combine and weigh the Evidence
Hypothesis and Evidence Scoring
Merging &Ranking
Synthesis
Watson	Performance
CS	295:	STATISTICAL	NLP	(WINTER	2017)17

OutlineQuestion	Answering
Textual	EntailmentCS	295:	STATISTICAL	NLP	(WINTER	2017)18IR-Based	QA	SystemOther	Extensions
IR-based	Factoid	QA
CS	295:	STATISTICAL	NLP	(WINTER	2017)19
DocumentDocumentDocument
Document
Document
Document
Document
Document
Question ProcessingPassageRetrievalQuery FormulationAnswer Type DetectionQuestionPassage RetrievalDocument RetrievalAnswer ProcessingAnswerpassagesIndexingRelevantDocsDocument
DocumentDocument
IR-based	Factoid	QAQUESTION	PROCESSING◦Detect	question	type,	answer	type,	focus,	relations◦Formulate	queries	to	send	to	a	search	enginePASSAGE	RETRIEVAL◦Retrieve	ranked	documents◦Break	into	suitable	passages	and	rerankANSWER	PROCESSING◦Extract	candidate	answers◦Rank	candidates	◦using	evidence	from	the	text	and	external	sources
CS	295:	STATISTICAL	NLP	(WINTER	2017)20
Factoid	Q/A
CS	295:	STATISTICAL	NLP	(WINTER	2017)21
DocumentDocumentDocument
Document
Document
Document
Document
Document
Question ProcessingPassageRetrievalQuery FormulationAnswer Type DetectionQuestionPassage RetrievalDocument RetrievalAnswer ProcessingAnswerpassagesIndexingRelevantDocsDocument
DocumentDocument
Question	ProcessingAnswer	Type	Detection◦Decide	the	named	entity	type	(person,	place)	of	the	answerQuery	Formulation◦Choose	query	keywords	for	the	IR	systemQuestion	Type	classification◦Is	this	a	definition	question,	a	math	question,	a	list	question?Focus	Detection◦Find	the	question	words	that	are	replaced	by	the	answerRelation	Extraction◦Find	relations	between	entities	in	the	questionCS	295:	STATISTICAL	NLP	(WINTER	2017)22
Question	Processing
Answer	Type:		US	stateQuery:		two	states,	border,	Florida,	northFocus:	the	two	statesRelations:		borders(Florida,	?x,	north)CS	295:	STATISTICAL	NLP	(WINTER	2017)23They’re	the	two	states	you	could	be	reentering	if	you’re	crossing	Florida’s	northern	border
Answer	Types:	Named	EntitiesWho	founded	Virgin	Airlines?PERSONWhat	Canadian	city	has	the	largest	population?CITY
CS	295:	STATISTICAL	NLP	(WINTER	2017)24
Part	of	Answer	Type	Taxonomy
CS	295:	STATISTICAL	NLP	(WINTER	2017)25
LOCATIONNUMERICENTITYHUMANABBREVIATIONDESCRIPTIONcountrycitystate
datepercentmoneysizedistanceindividualtitlegroupfoodcurrencyanimaldefinitionreasonexpressionabbreviation
Li,	Roth.	Learning	Question	Classifiers.	COLING	(2002)
Answer	Types
CS	295:	STATISTICAL	NLP	(WINTER	2017)26

More	Answer	Types
CS	295:	STATISTICAL	NLP	(WINTER	2017)27

Answer	types	in	Watson2500	answer	types	in	20,000	Jeopardy	question	sample◦The	most	frequent	200	answer	types	cover	<	50%	of	dataThe	40	most	frequent	Jeopardy	answer	typeshe,	country,	city,	man,	film,	state,	she,	author,	group,	here,	company,	president,	capital,	star,	novel,	character,	woman,	river,	island,	king,	song,	part,	series,	sport,	singer,	actor,	play,	team,		show,	actress,	animal,	presidential,	composer,	musical,	nation,	book,	title,	leader,	game
CS	295:	STATISTICAL	NLP	(WINTER	2017)28Ferrucciet	al.	Building	Watson:	An	Overview	of	the	DeepQAProject.	AI	Magazine.	2010
Answer	Type	Detection
CS	295:	STATISTICAL	NLP	(WINTER	2017)29Hand-written	RulesRegular	expression-based	rules	can	get	some	cases:◦Who	{is|was|are|were}	PERSONOther	rules	use	the	question	headword:◦(the	headword	of	the	first	noun	phrase	after	the	wh-word)◦Which	city	in	China	has	most	foreign	financial	companies?◦What	is	the	state	flower	of	California?Machine	LearningQuestion	words	and	phrasesPart-of-speech	tagsParse	features	(headwords)Named	EntitiesSemantically	related	words
Factoid	Q/A
CS	295:	STATISTICAL	NLP	(WINTER	2017)30
DocumentDocumentDocument
Document
Document
Document
Document
Document
Question ProcessingPassageRetrievalQuery FormulationAnswer Type DetectionQuestionPassage RetrievalDocument RetrievalAnswer ProcessingAnswerpassagesIndexingRelevantDocsDocument
DocumentDocument
Keyword	Selection	Algorithm1.	Select	all	non-stop	words	in	quotations2.	Select	all	NNP	words	in	recognized	named	entities3.	Select	all	complex	nominalswith	their	adjectival	modifiers4.	Select	all	other	complex	nominals5.	Select	all	nouns	with	their	adjectival	modifiers6.	Select	all	other	nouns7.	Select	all	verbs	8.	Select	all	adverbs	9.	Select	the	QFW	word	(skipped	in	all	previous	steps)	10.	Select	all	other	wordsCS	295:	STATISTICAL	NLP	(WINTER	2017)31Moldovan,	Harabagiu,	Pasca,	Mihalcea,	Goodrum,	Girjuand	Rus.	TREC	(1999)
Choosing	keywords
CS	295:	STATISTICAL	NLP	(WINTER	2017)32Who	coined	the	term	“cyberspace”	in	his	novel	“Neuromancer”?11447cyberspace/1	Neuromancer/1	term/4	novel/4	coined/7
Factoid	Q/A
CS	295:	STATISTICAL	NLP	(WINTER	2017)33
DocumentDocumentDocument
Document
Document
Document
Document
Document
Question ProcessingPassageRetrievalQuery FormulationAnswer Type DetectionQuestionPassage RetrievalDocument RetrievalAnswer ProcessingAnswerpassagesIndexingRelevantDocsDocument
DocumentDocument
Passage	RetrievalRetrieve	documents	using	IR◦query	terms	as	keywords
CS	295:	STATISTICAL	NLP	(WINTER	2017)34Step	1Step	2Segment	the	documents	into	shorter	units◦something	like	paragraphsStep	3Passage	ranking◦Use	answer	type	to	help	rerankpassages
Features	for	Passage	RankingNumber	of	Named	Entities	of	the	right	type	in	passageNumber	of	query	words	in	passageNumber	of	question	N-grams	also	in	passageProximity	of	query	keywords	to	each	other	in	passageLongest	sequence	of	question	wordsRank	of	the	document	containing	passageCS	295:	STATISTICAL	NLP	(WINTER	2017)35Either	in	rule-based	classifiers	or	with	supervised	machine	learning
Factoid	Q/A
CS	295:	STATISTICAL	NLP	(WINTER	2017)36
DocumentDocumentDocument
Document
Document
Document
Document
Document
Question ProcessingPassageRetrievalQuery FormulationAnswer Type DetectionQuestionPassage RetrievalDocument RetrievalAnswer ProcessingAnswerpassagesIndexingRelevantDocsDocument
DocumentDocument
Answer	ExtractionRun	an	answer-type	named-entity	tagger	on	the	passages◦Each	answer	type	requires	a	named-entity	tagger	that	detects	it◦If	answer	type	is	CITY ,	tagger	has	to	tag	CITY◦Can	be	full	NER,	simple	regular	expressions,	or	hybridReturn	the	string	with	the	right	type:◦Who	is	the	prime	minister	of	India	(PERSON)◦Manmohan	Singh,	Prime	Minister	of	India,	had	told	left	leaders	that	the	deal	would	not	be	renegotiated.◦How	tall	is	Mt.	Everest?	(LENGTH)◦The	official	height	of	Mount	Everest	is	29035	feetCS	295:	STATISTICAL	NLP	(WINTER	2017)37
Ranking	Candidate	AnswersBut	what	if	there	are	multiple	candidate	answers!Q:	Who	was	Queen	Victoria’s	second	son?Answer	Type:		Person
CS	295:	STATISTICAL	NLP	(WINTER	2017)38•Passage:The	Marie	biscuit	is	named	after	Marie	Alexandrovna,	the	daughter	of	Czar	Alexander	II	of	Russia	and	wife	of	Alfred,	the	second	son	of	Queen	Victoria	and	Prince	Albert
Ranking	Candidate	AnswersBut	what	if	there	are	multiple	candidate	answers!Q:	Who	was	Queen	Victoria’s	second	son?Answer	Type:		Person
CS	295:	STATISTICAL	NLP	(WINTER	2017)39•Passage:The	Marie	biscuit	is	named	after	Marie	Alexandrovna,	the	daughter	of	Czar	Alexander	II	of	Russia	and	wife	of	Alfred,	the	second	son	of	Queen	Victoria	and	PrinceAlbert
Features	for	MLAnswer	type	match:		Candidate	contains	a	phrase	with	the	correct	answer	type.Pattern	match:	Regular	expression	pattern	matches	the	candidate.Question	keywords:	#	of	question	keywords	in	the	candidate.Keyword	distance:	Distance	in	words	between	the	candidate	and	query	keywords	Novelty	factor:	A	word	in	the	candidate	is	not	in	the	query.Apposition	features:	The	candidate	is	an	appositive	to	question	termsPunctuation	location:	The	candidate	is	immediately	followed	by	a	comma,	period,	quotation	marks,	semicolon,	or	exclamation	mark.Sequences	of	question	terms:	The	length	of	the	longest	sequence	of	question	terms	that	occurs	in	the	candidate	answer.CS	295:	STATISTICAL	NLP	(WINTER	2017)40
Scoring	Candidates	in	WatsonEach	candidate	answer	gets	scores	from	>50	components◦(from	unstructured	text,	semi-structured	text,	triple	stores)◦logical	form	(parse)	match	between	question	and	candidate◦passage	source	reliability	◦geospatial	location◦California		is		”southwest	of	Montana”◦temporal	relationships◦taxonomic	classification
CS	295:	STATISTICAL	NLP	(WINTER	2017)41
OutlineQuestion	Answering
Textual	EntailmentCS	295:	STATISTICAL	NLP	(WINTER	2017)42IR-Based	QA	SystemOther	Extensions
AskMSR
CS	295:	STATISTICAL	NLP	(WINTER	2017)43
12345Question	ProcessingSearch
Answer	ExtractionAnswer	ScoringDumais,Banko,Brill,Lin,Ng,	SIGIR	(2002)
Step	1:	Rewrite	Queries
CS	295:	STATISTICAL	NLP	(WINTER	2017)44Intuition:Questionsareoftensyntacticallyquiteclose	tosentenceswiththeanswer•Whereis	the	Louvre	Museum	located?•The	Louvre	Museum	is	locatedinParis•Whocreated	the	character	of	Scrooge?•CharlesDickens	created	the	character	of	Scrooge

Feedback	Loops:	FALCON
CS	295:	STATISTICAL	NLP	(WINTER	2017)45
Allen	AI	Science	Challenge
CS	295:	STATISTICAL	NLP	(WINTER	2017)46
Which	object	is	the	best	conductor	of	electricity?(A)a	wax	crayon		(B)	a	plastic	spoon	(C)	a	rubber	eraser		(D)	an	iron	nail
Allen	AI	Science	Challenge
CS	295:	STATISTICAL	NLP	(WINTER	2017)47
Which	object	is	the	best	conductor	of	electricity?(A)a	wax	crayon		(B)	a	plastic	spoon	(C)	a	rubber	eraser		(D)	an	iron	nail

Allen	AI	Science	Challenge
CS	295:	STATISTICAL	NLP	(WINTER	2017)48
Fourth	graders	are	planning	a	roller-skate	race.	Which	surface	would	be	the	best	for	this	race?(A)	gravel		(B)	sand		(C)	blacktop		(D)	grass
Allen	AI	Science	Challenge
CS	295:	STATISTICAL	NLP	(WINTER	2017)49
Fourth	graders	are	planning	a	roller-skate	race.	Which	surface	would	be	the	best	for	this	race?(A)	gravel		(B)	sand		(C)	blacktop(D)	grass§Information	retrieval	methods	fail§Word	co-occurrence	methods	struggle
Gradersare commonly used in the construction and maintenance of dirt roads and gravel roadsAlso strong correlations between:grass «racegravel «surface
Allen	AI	Science	Challenge
CS	295:	STATISTICAL	NLP	(WINTER	2017)50
A	student	puts	two	identical	plants	in	the	same	type	and	amount	of	soil.	She	gives	them	the	same	amount	of	water.	She	puts	one	of	these	plants	near	a	sunny	window	and	the	other	in	a	dark	room.	This	experiment	tests	how	the	plants	respond	to	(A)	light	(B)	air	(C)	water	(D)	soil	
Allen	AI	Science	Challenge
CS	295:	STATISTICAL	NLP	(WINTER	2017)51
A	student	puts	two	identical	plants	in	the	same	type	and	amount	of	soil.	She	gives	them	the	same	amount	of	water.	She	puts	one	of	these	plants	near	a	sunny	window	and	the	other	in	a	dark	room.	This	experiment	tests	how	the	plants	respond	to	(A)	light(B)	air	(C)	water	(D)	soil	Knowledgeneeded	(for	example):§near	sunny	window	®receive	light§in	a	dark	room	®no	light§test	X’s	response	to	Y	®compare	X+Y	with	X+not(Y)
OutlineQuestion	Answering
Textual	EntailmentCS	295:	STATISTICAL	NLP	(WINTER	2017)52IR-Based	QA	SystemOther	Extensions
Natural	Language	&	Meaning
CS	295:	STATISTICAL	NLP	(WINTER	2017)53Meaning
LanguageAmbiguityVariabilityinterpretationexpression
Inference	vs	Entailment
CS	295:	STATISTICAL	NLP	(WINTER	2017)54MeaningRepresentationNatural LanguageInferenceTextual	Entailment
Textual	Entailment
CS	295:	STATISTICAL	NLP	(WINTER	2017)55•A	directional	relation	between	two	text	fragments:		Text	(t)and	Hypothesis	(h):t entailsh (t Þh) if humansreading t  will infer that h is most likely true•Assuming	“common	background	knowledge”	–which	is	indeed	expected	from	applications
Example
CS	295:	STATISTICAL	NLP	(WINTER	2017)56https://cogcomp.cs.illinois.edu/page/resource_view/9

More	Sentence	Pairs
CS	295:	STATISTICAL	NLP	(WINTER	2017)571.Some students came to school by car.Some students came to school.2.No students came to school by car. Some students came to school.3.John drove legally. John drove.4.John drove predictably.John drove. 5.Legally, John could drive.John drove.
Entailment	with	Knowledge
CS	295:	STATISTICAL	NLP	(WINTER	2017)58For	textual	entailment	to	hold	we	require:◦textAND	knowledgeÞh,	but	◦knowledge	should	not	entail	haloneSystems	arenot	supposed	to	validate	h’s	truth	regardless	of	t(e.g.	by	searching	h	on	the	web)t entailsh (t Þh) if humansreading t  will infer that h is most likelytrue
Example
CS	295:	STATISTICAL	NLP	(WINTER	2017)59TEXT:	…While	no	one	accuses	Madonna	of	doing	anything	illegal	in	adopting	the	4-year-old	girl,	reportedly	named	Mercy,	there	are	questions	nonetheless	about	how	Madonna	is	able	to	navigate	Malawi's	18-to-24	month	vetting	period	in	just	a	matter	of	days	or	weeks…HYPOTHESIS:Madonna	is	50	years	old.

Third	Label:	Contradictions
CS	295:	STATISTICAL	NLP	(WINTER	2017)60t contradictsh (t ^h) if humansreading t will find the relations/events described by hto be highly unlikely given t.
Example
CS	295:	STATISTICAL	NLP	(WINTER	2017)61

More	Examples
CS	295:	STATISTICAL	NLP	(WINTER	2017)62

Applications
CS	295:	STATISTICAL	NLP	(WINTER	2017)63Question	AnsweringInformation	ExtractionMachine	TranslationInformation	RetrievalQuestionExpected answer formWhoboughtOverture?     >>    XboughtOvertureOverture’s acquisitionby YahooYahoobought Overturetexthypothesized answerentailsHe	abhorred	the	men’s	unctuous	ways.He	disliked	the	men’s	flattering	ways.Similarity	based	on	whether	document	entails	the	query.
Information	ExtractionProf.	Sameer	SinghCS	295:	STATISTICAL	NLPWINTER	2017February	21,	2017Based	on	slides	from	Dan	Jurafski,	Chris	Manning,	Jay	Pujara,	and	everyone	else	they	copied	from.
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)2What	is	Information	ExtractionNamed	Entity	RecognitionHomework	3
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)3What	is	Information	ExtractionNamed	Entity	RecognitionHomework	3
Making	Sense	of	Text
4Massive	Corpus	of	Unstructured	Text
StructuredRepresentationQuery?
Search(IR)SearchDB	QueryQueryInformationExtractionDatabaseor	GraphDocumentsDocumentsDocumentsDocumentsDocumentsDocumentsDocuments
News	Articles
5StructuredRepresentationQuery
Which	AI	startups	have	been	acquired	by	Tech	companies?
InformationExtractionMassive	Corpus	of	News	ArticlesCompanyIndustryPeopleacquiredbelongsTofoundedemployeeexpertIn
Fiction
6Query
Which	two	characters	are	not	related	by	blood?
Collection	ofBooksStructuredRepresentationInformationExtraction

Academic	ResearchQuery
What	is	the	interaction	pathway	between	YY1	and	TIP60?
Massive	Corpus	of	Scientific	Papers
StructuredRepresentationInformationExtraction

Applications
8
InformationExtractionDatabase	or	Graph?
Question	Answering050100150200250AprilJuneVisualization	&	StatisticsDocumentsDocumentsDocumentsDocumentsDocumentsDocumentsDocuments
Downstream	AI	applications
Low-level	Info.	Extraction
CS	295:	STATISTICAL	NLP	(WINTER	2017)9

Slightly	better…
CS	295:	STATISTICAL	NLP	(WINTER	2017)10

Slightly	better?
CS	295:	STATISTICAL	NLP	(WINTER	2017)11
The	headquarters	of	BHP	Billiton	Limited,	and	the	global	headquarters	of	the	combined	BHP	Billiton	Group,	are	located	in	Melbourne,	Australia.headquarters(“BHP	BilitonLimited”,	“Melbourne,	Australia”)
In	the	industry…Google	Knowledge	Graph◦Google	Knowledge	VaultAmazon	Product	GraphFacebook	Graph	APIIBM	WatsonMicrosoft	Satori◦Project	Hanover/LiteromeLinkedIn	Knowledge	GraphYandexObject	AnswerDiffbot,	GraphIQ,	Maana,	ParseHub,	Reactor	Labs,	SpazioDatiCS	295:	STATISTICAL	NLP	(WINTER	2017)12

Knowledge	Extraction
13John was born in Liverpool, to Julia and Alfred Lennon.Text
John	LennonAlfred	LennonJulia	LennonLiverpoolbirthplacechildOfchildOfLiteral	Facts
Role	of	NLP?John was born in Liverpool, to Julia and Alfred Lennon.Natural	LanguageProcessing
NNPVBDVBDINNNPTONNPCCNNPNNP
John was born in Liverpool, to Julia and Alfred Lennon.PersonLocationPersonPersonLennon..John	Lennon...Mrs.	Lennon....	his	mother	..his	fatherAlfredhethe	Pool
14
Information	Extraction
John	LennonAlfred	LennonJulia	LennonLiverpoolbirthplacechildOfchildOfspouseInformation	ExtractionNNPVBDVBDINNNPTONNPCCNNPNNP
John was born in Liverpool, to Julia and Alfred Lennon.PersonLocationPersonPersonLennon..John	Lennon...Mrs.	Lennon....	his	mother	..his	fatherAlfredhethe	Pool
15
Breaking	it	Down
John was born in Liverpool, to Julia and Alfred Lennon.NNPVBDVBDINNNPTONNPCCNNPNNP
John was born in Liverpool, to Julia and Alfred Lennon.PersonLocationPersonPersonLennon..John	Lennon...Mrs.	Lennon....	his	mother	..his	fatherAlfredhethe	PoolSentenceDependency	Parsing,Part	of	speech	tagging,Named	entity	recognition…DocumentCoreferenceResolution...John	LennonAlfred	LennonJulia	LennonLiverpoolbirthplacechildOfchildOfspouseInformationExtractionEntity	resolution,Entity	linking,Relation	extraction…
16
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)17What	is	Information	ExtractionNamed	Entity	RecognitionHomework	3Relation	Extraction
An	important	sub-task:	find	and	classify	names	in	text,	for	example:◦The	decision	by	the	independent	MP	Andrew	Wilkieto	withdraw	his	support	for	the	minority	Labor	government	sounded	dramatic	but	it	should	not	further	threaten	its	stability.	When,	after	the	2010	election,	Wilkie,	Rob	Oakeshott,	Tony	Windsor	and	the	Greens	agreed	to	support	Labor,	they	gave	just	two	guarantees:	confidence	and	supply.Named	Entity	Recognition
An	important	sub-task:	findand	classifynames	in	text,	for	example:◦The	decision	by	the	independent	MP	Andrew	Wilkieto	withdraw	his	support	for	the	minority	Laborgovernment	sounded	dramatic	but	it	should	not	further	threaten	its	stability.	When,	after	the	2010	election,	Wilkie,	Rob	Oakeshott,	Tony	Windsor	and	the	Greens	agreed	to	support	Labor,	they	gave	just	two	guarantees:	confidence	and	supply.Named	Entity	Recognition
An	important	sub-task:	findand	classifynames	in	text,	for	example:◦The	decision	by	the	independent	MP	Andrew	Wilkieto	withdraw	his	support	for	the	minority	Laborgovernment	sounded	dramatic	but	it	should	not	further	threaten	its	stability.	When,	after	the	2010election,	Wilkie,	Rob	Oakeshott,	Tony	Windsor	and	the	Greensagreed	to	support	Labor,	they	gave	just	two	guarantees:	confidence	and	supply.Named	Entity	RecognitionPersonDateLocationOrgani-zation
Detecting	Named	Entities
Uses	in	Knowledge	Extraction:•Mentions	describes	the	nodes•Types	are	incredibly	important!•Often	restrict	relations•Fine-grained	types	are	informative!•Brooklyn:	city•Sanders:	politician,	senatorHow	it	is	done:•Context	is	important!•Georgia,	Washington,	…•John	Deere,	Thomas	Cook,	…•Princeton,	Amazon,	…•Label	whole	sentence	together•Structured	prediction	againJohn was born in Liverpool, to Julia and Alfred Lennon.PersonLocationPersonPerson
21
NER:	Entity	Types3	class:Location,	Person,	Organization4	class:Location,	Person,	Organization,	Misc7	class:Location,	Person,	Organization,	Money,	Percent,	Date,	Time
From	Stanford	CoreNLP(http://nlp.stanford.edu/software/CRF-NER.shtml)PERSONPeople,	including	fictional.NORPNationalities	or	religious	or	political	groups.FACILITYBuildings,	airports,	highways,	bridges,	etc.ORGCompanies,	agencies,	institutions,	etc.GPECountries,	cities,	states.LOCNon-GPE	locations,	mountain	ranges,	bodies	of	water.PRODUCTObjects,	vehicles,	foods,	etc.	(Not	services.)EVENTNamed	hurricanes,	battles,	wars,	sports	events,	etc.WORK_OF_ARTTitles	of	books,	songs,	etc.LANGUAGEAny	named	language.Stanford	CoreNLP
spaCy.io
22
NER:	Entity	Types
From	Ling	&	Weld.	AAAI	2012	(http://aiweb.cs.washington.edu/ai/pubs/ling-aaai12.pdf)Fine-grained	Types
23
CS	295:	STATISTICAL	NLP	(WINTER	2017)24

Sequence	Labeling	for	NER
CS	295:	STATISTICAL	NLP	(WINTER	2017)25
Features:	Words	and	Lexicons
CS	295:	STATISTICAL	NLP	(WINTER	2017)26WordsLexicons
Features:	Prefixes/Suffixes
CS	295:	STATISTICAL	NLP	(WINTER	2017)27
Features:	Substrings	of	Words
CS	295:	STATISTICAL	NLP	(WINTER	2017)28417144
241drugcompanymovieplacepersonCotrimoxazoleWethersfieldAlien	Fury:	Countdown	to	Invasion000
180oxa
7080006:086
6814fieldWethersfieldCotrimoxazoleAlien	Fury:	Countdown	to	Invasion
Features:	Word	Shapes
CS	295:	STATISTICAL	NLP	(WINTER	2017)29Short	shapesJohnDC-100CamelCaseShape(c)=if	A-Zif	a-zif	0-9o.w.Word	shapesXxdcXxxxXX-dddXxxxxXxxxXxX-dXxXx
Features:	Surrounding	Context
CS	295:	STATISTICAL	NLP	(WINTER	2017)30BIASWORD=DeereLWORD=deereFIRSTCAP=TrueSSHAPE=XxLEXICON=company…ii-1i+1BIASWORD=DeereLWORD=deereFIRSTCAP=TrueSSHAPE=XxLEXICON=company…BIASWORD=DeereLWORD=deereFIRSTCAP=TrueSSHAPE=XxLEXICON=company…NEXT_NEXT_NEXT_NEXT_NEXT_NEXT_PREV_PREV_PREV_PREV_PREV_PREV_JohnDeereannounced
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)31What	is	Information	ExtractionNamed	Entity	RecognitionHomework	3Relation	Extraction
Sequence	Tagging	on	Twitter
CS	295:	STATISTICAL	NLP	(WINTER	2017)32Steedman,	2000What a productive day . Not .PRONDETADJNOUN. ADV.Parts	of	Speech
‘ Breaking Dawn ’ Returnsto VancouveronJanuary 11thO B-MOVIEI-MOVIEO     OOB-GEO-LOCO     OONamed	Entity	Recognition
Sequence	Tagging	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)33Steedman,	2000Logistic	Regression
Conditional	Random	Fields
What	do	you	have	to	do?
CS	295:	STATISTICAL	NLP	(WINTER	2017)34Steedman,	2000Feature	EngineeringViterbi	AlgorithmTest	data	will	be	released	very	close	to	the	deadline!
Upcoming…
CS	295:	STATISTICAL	NLP	(WINTER	2017)35•Homework	3	is	due	on	February	27•Write-up	and	data	has	been	released.Homework•Status	report	due	in	1.5	weeks:	March	2,	2017•Instructions	coming	soon•Only	5	pagesProject•Paper	summaries:	February	28,	March	14•Only	1page	eachSummaries
CMSC 422 Introduction to Machine LearningLecture 10 Multiclass Classification and ReductionsFurong Huang / furongh@cs.umd.eduSlides adapted from Prof Carpuatand Duraiswami
TopicsGiven an arbitrary method for binary classification, how can we learn to make multiclass predictions?Fundamental ML concept:  reductions
One Example of Reduction:Learning with Imbalanced Data
Subsampling Optimality Theorem:If the binary classifier achieves a binary error rate of ε, then the error rate of the α-weighted classifier is α ε
Another Example of Reduction:Multiclass Classification

Reduction 1: OVA•“One versus all” (aka “one versus rest”)•Train K-many binary classifiers•classifier k predicts whether an example belong to class k or not•At test time, •If only one classifier predicts positive, predict that class•Break ties randomly

Time complexity•Suppose you have N training examples, in K classes. How long does it take to train an OVA classifier•if the base binary classifier takes O(N) time to learn?•if the base binary classifier takes O(N^2) time to learn?
Error bound•Theorem:Suppose that the average error of the K binary classifiers is ε, then the error rate of the OVA multiclass classifier is at most (K-1)ε•To prove this: how do different errors affect the maximum ratio of the probability of a multiclass error to the number of binary errors (“efficiency”)?
Error bound proof1. If we have a false negativeon one of the binary classifiers (assuming all other classifiers correctly output negative) What is the probability that we will make an incorrect multiclass prediction?(K –1) / KEfficiency: [( K –1) / K] / 1 = (K –1 ) / K
Error bound proof2. If we have m false positiveswith the binary classifiersWhat is the probability that we will make an incorrect multiclass prediction?If there is also a false negative: 1Efficiency =1 / (m + 1)Otherwise  m / ( m + 1)Efficiency = [m / (m + 1)] / m = 1 / ( m + 1)
Error bound proof3. What is the worst case scenario?False negative case: efficiency is (K-1)/KLarger than false positive efficiencies There are K-many opportunities to get false negative, overall error bound is (K-1)ε
Reduction 2:  AVAAll versus all (aka all pairs)How many binary classifiers does this require?

Time complexity•Suppose you have N training examples, in K classes. How long does it take to train an AVA classifier•if the base binary classifier takes O(N) time to learn?•if the base binary classifier takes O(N^2) time to learn?
Error boundTheorem:Suppose that the average error of the K binary classifiers is ε, then the error rate of the AVA multiclass classifier is at most 2(K-1)εQuestion: Does this mean that AVA is always worse than OVA?
Extensions•Divide and conquer•Organize classes into binary tree structures•Use confidence to weight predictions of binary classifiers•Instead of using majority vote
Topics•Given an arbitrary method for binary classification, how can we learn to make multiclass predictions?•OVA, AVA•Fundamental ML concept:  reductions
RankingCanonical example: web searchGiven all the documents on the webFor a user query, retrieve relevant documents, ranked from most relevant to least relevant
How can we reduce ranking to binary classification?
Preference function•Given a query q and documents diand dj, the preference function outputs whether•dishould be preferred to dj•Or djshould be preferred to di•That’s a binary classification problem!
Specifying the reduction from ranking to binary classification•How to train classifier that predicts preferences?•How to turn the predicted preferences into a ranking?
Features associated with comparing document j and document j for query n
Naïve approachWorks well for bipartite problems“is this document relevant or not?”Not ideal for full ranking problems, becauseBinary preference problems are not all equally importantSeparates preference function and sorting
Improving on naïve approach

Example of cost functions

Resulting Ranking Algorithms


RankTestA probabilistic version of the quicksort algorithmOnly O(Mlog2M) calls to f in expectationBetter error bound than naïve algorithm(see CIML for theorem)
What you should know•What are reductions and why they are useful•Implement, analyze and prove error bounds of algorithms for•Weighted binary classification•Multiclass classification (OVA, AVA)•Understand algorithms for•!−ranking
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
COMS 4721: Machine Learning for Data Science
Lecture 12, 2/28/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
DECISION TREES
DECISION TREES
Adecision tree maps input x2Rdto output yusing binary decision rules:
IEach node in the tree has a splitting rule .
IEach leaf node is associated with an output value (outputs can repeat).
Each splitting rule is of the form
h(x) =1fxj>tg
for some dimension jofxandt2R.
Using these transition rules, a path
to aleaf node gives the prediction.
(One-level tree =decision stump )
x1>1.7
x2>2.8 ˆy= 1
ˆy= 2 ˆy= 3
REGRESSION TREES
Motivation : Partition the space so that data in a region have same prediction
Left: Difﬁcult to deﬁne a “rule”.
Right: Easy to deﬁne a recursive splitting rule.
REGRESSION TREES
 !
If we think in terms of trees, we can deﬁne a simple rule for partitioning the
space. The left and right ﬁgures represent the same regression function.
REGRESSION TREES
 !
Adding an output dimension to the ﬁgure (right), we can see how regression
trees can learn a step function approximation to the data.
CLASSIFICATION TREES (EXAMPLE )
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
Classifying irises using sepal and petal
measurements:
Ix2R2,y2f1;2;3g
Ix1=ratio of sepal length to width
Ix2=ratio of petal length to width

CLASSIFICATION TREES (EXAMPLE )
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
Classifying irises using sepal and petal
measurements:
Ix2R2,y2f1;2;3g
Ix1=ratio of sepal length to width
Ix2=ratio of petal length to width
ˆy= 2
CLASSIFICATION TREES (EXAMPLE )
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
Classifying irises using sepal and petal
measurements:
Ix2R2,y2f1;2;3g
Ix1=ratio of sepal length to width
Ix2=ratio of petal length to width
x1>1.7
CLASSIFICATION TREES (EXAMPLE )
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
Classifying irises using sepal and petal
measurements:
Ix2R2,y2f1;2;3g
Ix1=ratio of sepal length to width
Ix2=ratio of petal length to width
x1>1.7
ˆy= 1 ˆy= 3
CLASSIFICATION TREES (EXAMPLE )
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
Classifying irises using sepal and petal
measurements:
Ix2R2,y2f1;2;3g
Ix1=ratio of sepal length to width
Ix2=ratio of petal length to width
x1>1.7
x2>2.8 ˆy= 1
CLASSIFICATION TREES (EXAMPLE )
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
Classifying irises using sepal and petal
measurements:
Ix2R2,y2f1;2;3g
Ix1=ratio of sepal length to width
Ix2=ratio of petal length to width
x1>1.7
x2>2.8 ˆy= 1
ˆy= 2 ˆy= 3
BASIC DECISION TREE LEARNING ALGORITHM
ˆy= 2
 !
x1>1.7
ˆy= 1 ˆy= 3  !
x1>1.7
x2>2.8 ˆy= 1
ˆy= 2 ˆy= 3
The basic method for learning trees is with a top-down greedy algorithm.
IStart with a single leaf node containing all data
ILoop through the following steps:
IPick the leaf to split that reduces uncertainty the most.
IFigure out the 7decision rule on one of the dimensions.
IStopping rule discussed later.
Label/response of the leaf is majority-vote/average of data assigned to it.
GROWING A REGRESSION TREE
How do we grow a regression tree?
IForMregions of the space, R1;:::; RM,
the prediction function is
f(x) =MX
m=1cm1fx2Rmg:
So for a ﬁxed M, we need Rmandcm.
Goal: Try to minimizeP
i(yi f(xi))2.
1. Find cmgiven Rm: Simply the average of all yifor which xi2Rm.
2. How do we ﬁnd regions? Consider splitting region Rat value sof dim j:
IDeﬁne R (j;s) =fxi2Rjxi(j)sgandR+(j;s) =fxi2Rjxi(j)>sg
IFor each dimension j, calculate the best splitting point sfor that dimension.
IDo this for each region (leaf node). Pick the one that reduces the objective most.
GROWING A CLASSIFICATION TREE
For regression : Squared error is a natural way to deﬁne the splitting rule.
For classiﬁcation : Need some measure of how badly a region classiﬁes data
and how much it can improve if it’s split.
K-class problem : For all x2Rm, letpkbe empirical fraction labeled k.
Measures of quality of Rminclude
1. Classiﬁcation error: 1  max kpk
2. Gini index: 1 P
kp2
k
3. Entropy: P
kpklnpk
IThese are all maximized when pkis uniform on the Kclasses in Rm.
IThese are minimized when pk=1 for some k(Rmonly contains one class)
GROWING A CLASSIFICATION TREE
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
x1>1.7
ˆy= 1 ˆy= 3
Search R1andR2for splitting options.
1.R1:y=1 leaf classiﬁes perfectly
2.R2:y=3 leaf has Gini index
u(R2) =1 1
1012
 50
1012
 50
1012
=0:5098
Gini improvement from split RmtoR 
m&R+
m:
u(Rm) 
pR 
mu(R 
m) +pR+
mu(R+
m)
pR+
m: Fraction of data in Rmsplit into R+
m.
u(R+
m): New quality measure in region R+
m.
GROWING A CLASSIFICATION TREE
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
x1>1.7
ˆy= 1 ˆy= 3
Search R1andR2for splitting options.
1.R1:y=1 leaf classiﬁes perfectly
2.R2:y=3 leaf has Gini index
u(R2) =1 1
1012
 50
1012
 50
1012
=0:5098
Check split R2with1fx1>tg
t1.6 1.8 2 2.2 2.4 2.6 2.8 3reduction in uncertainty
00.0050.010.0150.02
GROWING A CLASSIFICATION TREE
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
x1>1.7
ˆy= 1 ˆy= 3
Search R1andR2for splitting options.
1.R1:y=1 leaf classiﬁes perfectly
2.R2:y=3 leaf has Gini index
u(R2) =1 1
1012
 50
1012
 50
1012
=0:5098
Check split R2with1fx2>tg
t2 2.5 3 3.5 4 4.5reduction in uncertainty
00.050.10.150.20.25
GROWING A CLASSIFICATION TREE
sepal length/width1.5 2 2.5 3petal length/width
22.533.544.555.56
x1>1.7
x2>2.8 ˆy= 1
ˆy= 2 ˆy= 3
Search R1andR2for splitting options.
1.R1:y=1 leaf classiﬁes perfectly
2.R2:y=3 leaf has Gini index
u(R2) =1 1
1012
 50
1012
 50
1012
=0:5098
Check split R2with1fx2>tg
t2 2.5 3 3.5 4 4.5reduction in uncertainty
00.050.10.150.20.25
PRUNING A TREE
Q: When should we stop growing a tree?
A: Uncertainty reduction is not best way.
Example : Any split of x1orx2at right
will show zero reduction in uncertainty.
However, we can learn a perfect tree on
this data by partitioning in quadrants.
x1x2
Pruning is the method most often used. Grow the tree to a very large size.
Then use an algorithm to trim it back.
(We won’t cover the algorithm, but mention that it’s non-trivial.)
OVERFITTING
number of nodes in treeerror
training errortrue error
ITraining error goes to zero as size of tree increases.
ITesting error decreases, but then increases because of overﬁtting .
THEBOOTSTRAP
THEBOOTSTRAP : A R ESAMPLING TECHNIQUE
We brieﬂy present a technique called the bootstrap . This statistical technique
is used as the basis for learning ensemble classiﬁers .
Bootstrap
Bootstrap (i.e., resampling) is a technique for improving estimators.
Resampling = Sampling from the empirical distribution of the data
Application to ensemble methods
IWe will use resampling to generate many “mediocre” classiﬁers.
IWe then discuss how “bagging” these classiﬁers improves performance.
IFirst, we cover the bootstrap in a simpler context.
BOOTSTRAP : BASIC ALGORITHM
Input
IA sample of data x1;:::; xn.
IAn estimation rule ^Sof a statistic S. For example, ^S=med(x1:n)
estimates the true median Sof the unknown distribution on x.
Bootstrap algorithm
1. Generate bootstrap samples B1;:::;BB.
CreateBbby picking points from fx1; : : : ; xngrandomly ntimes.
A particular xican appear inBbmany times (it’s simply duplicated).
2. Evaluate the estimator on each Bbby pretending it’s the data set:
^Sb:=^S(Bb)
3. Estimate the mean and variance of ^S:
B=1
BBX
b=1^Sb; 2
B=1
BBX
b=1(^Sb B)2
EXAMPLE : VARIANCE ESTIMATION OF THE MEDIAN
IThe median of x1;:::; xn(forx2R) is found by simply sorting them
and taking the middle one, or the average of the two middle ones.
IHow conﬁdent can we be in the estimate median (x1;:::; xn)?
IFind it’s variance.
IBut how? Answer: By bootstrapping the data.
1. Generate bootstrap data sets B1;:::;BB.
2. Calculate: (notice that ^Smeanis the mean of the median)
^Smean=1
BBX
b=1median (Bb);^Svar=1
BBX
b=1
median (Bb) ^Smean2
IThe procedure is remarkably simple, but has a lot of theory behind it.
BAGGING AND RANDOM FORESTS
BAGGING
Bagging uses the bootstrap for regression or classiﬁcation:
Bagging =Bootstrap aggregation
Algorithm
Forb=1;:::; B:
1. Draw a bootstrap sample Bbof size nfrom training data.
2. Train a classiﬁer or regression model fbonBb.
IFor a new point x0, compute:
favg(x0) =1
BBX
b=1fb(x0)
IFor regression, favg(x0)is the prediction.
IFor classiﬁcation, view favg(x0)as an average over Bvotes. Pick the majority.
EXAMPLE : BAGGING TREES
IBinary classiﬁcation, x2R5.
INote the variation among
bootstrapped trees.
ITake-home message:
With bagging, each tree doesn’t
have to be great, just “ok”.
IBagging often improves results
when the function is non-linear .
Elements of Statistical Learning (2nd Ed.)c/circlecopyrtHastie, Tibshirani & Friedman 2009 Chap 8
|x.1 < 0.395
01010110Original Tree|x.1 < 0.555
01001b = 1|x.2 < 0.205
010101b = 2
|x.2 < 0.285
11010b = 3|x.3 < 0.985010111b = 4|x.4 < −1.3601101010b = 5
|x.1 < 0.395
11001b = 6|x.1 < 0.395
01011b = 7|x.3 < 0.985
010010b = 8
|x.1 < 0.395010110b = 9|x.1 < 0.555
10101b = 10|x.1 < 0.555
0101b = 11FIGURE 8.9.Bagging trees on simulated dataset.Th l f l h h i i l El
RANDOM FORESTS
Drawbacks of Bagging
IBagging works on trees because of the
bias-variance tradeoff ( "bias,#variance).
IHowever, the bagged trees are correlated.
IIn general, when bootstrap samples are
correlated, the beneﬁt of bagging decreases.
Random Forests
Modiﬁcation of bagging where trees are designed to reduce correlation.
IA very simple modiﬁcation.
IStill learn a tree on each bootstrap set, Bb.
ITo split a region, only consider random subset of dimensions of x2Rd.
RANDOM FORESTS : ALGORITHM
Training
Input parameter: m— a positive integer with m<d, often mp
d
Forb=1;:::; B:
1. Draw a bootstrap sample Bbof size nfrom the training data.
2. Train a tree classiﬁer on Bb, where each split is computed as follows:
IRandomly select mdimensions of x2Rd, newly chosen for each b.
IMake the best split restricted to that subset of dimensions.
IBagging for trees: Bag trees learned using the original algorithm.
IRandom forests: Bag trees learned using algorithm on this slide.
RANDOM FORESTS
Example problem
IRandom forest classiﬁcation.
IForest size: A few hundred trees.
INotice there is a tendency to align
decision boundary with the axis.
Elements of Statistical Learning (2nd Ed.)c/circlecopyrtHastie, Tibshirani & Friedman 2009 Chap 15
Random Forest Classifier
ooooooooooooooooooooooo
oooooooooooooooooooooo
ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo
Training Error: 0.000Test Error:       0.238Bayes Error:    0.2103−Nearest Neighbors
ooooooooooooooooooooooo
oooooooooooooooooooooo
ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo
Training Error: 0.130Test Error:       0.242Bayes Error:    0.210FIGURE 15.11.Random forests versus 3-NN on themixture data. The axis-oriented nature of the individ-ual trees in a random forest lead to decision regionswith an axis-oriented ﬂavor.
Lagrangian	
  Relaxa,on	
  	
  for	
  MAP	
  Inference	
  SPFLODD	
  October	
  8,	
  2013	
  
Outline	
  • An	
  elegant	
  example	
  of	
  a	
  relaxa,on	
  to	
  TSP	
  • A	
  common	
  problem	
  in	
  NLP:	
  	
  ﬁnding	
  consensus	
  • Basic	
  Lagrangian	
  relaxa,on	
  • Solving	
  the	
  problem	
  with	
  subgradient	
  • AD3:	
  	
  an	
  alterna,ve	
  approach	
  to	
  decomposi,on	
  and	
  op,miza,on	
  using	
  the	
  augmented	
  Lagrangian	
  
Traveling	
  Salesman	
  Problem	
  • Given:	
  	
  a	
  graph	
  (V,	
  E)	
  with	
  edge	
  weight	
  func,on	
  θ	
  • Tour:	
  	
  a	
  subset	
  of	
  E	
  corresponding	
  to	
  a	
  path	
  that	
  starts	
  and	
  ends	
  in	
  the	
  same	
  place,	
  and	
  visits	
  every	
  other	
  node	
  exactly	
  once.	
  • TSP:	
  	
  Find	
  the	
  maximum-­‐scoring	
  tour.	
  – NP-­‐hard	
  maxy2YtourXe2Eye✓e
Another	
  Problem	
  • 1-­‐tree:	
  	
  a	
  tree	
  on	
  edges	
  for	
  {2,	
  ...,	
  |V|},	
  plus	
  two	
  edges	
  from	
  E	
  that	
  link	
  the	
  tree	
  to	
  vertex	
  1.	
  – All	
  tours	
  are	
  1-­‐trees.	
  – All	
  1-­‐trees	
  where	
  every	
  vertex	
  has	
  degree	
  2	
  are	
  tours.	
  – Easy	
  to	
  solve.	
  
Held	
  and	
  Karp	
  (1971)	
  Ytour=(y:y2Y1-tree^8i2{1,...,|V|},Xe:i2eye=2)maxy2YtourXe2Eye✓e
Lagrangian	
  dual	
  transforming	
  the	
  constraints	
  maxy2Y1-treeXe2Eye✓es.t.8i,Xe:i2eye=2L(u) = maxy2Y1-treeXe2Eye✓e+|V|Xi=1ui Xe:i2eye 2!
LR	
  Algorithm	
  for	
  TSP	
  1. Ini,alize	
  u(0)	
  =	
  0	
  2. Repeat	
  for	
  k	
  =	
  1,	
  2,	
  ...:	
  If	
  this	
  converges	
  to	
  a	
  solu,on	
  that	
  sa,sﬁes	
  the	
  constraints,	
  it	
  is	
  a	
  solu,on	
  to	
  the	
  TSP.	
  y(k) arg maxy2Y1-treeXe2Eye✓e+|V|Xi=1u(k 1)i Xe:i2eye 2!8i, u(k)i u(k 1)i  k Xe:i2eye 2!
Lagrangian	
  Relaxa,on,	
  More	
  Generally	
  • Assume	
  a	
  linear	
  scoring	
  func,on	
  that	
  is	
  “hard”	
  to	
  maximize.	
  • Rewrite	
  the	
  problem	
  as	
  something	
  easier,	
  with	
  linear	
  constraints	
  (relaxa,on):	
  	
  • Tackle	
  the	
  dual	
  problem:	
  maxy2Y✓>ymaxy2Y0✓>ys.t.Ay=bY={y2Y0:Ay=b}minumaxy2Y0✓>y+u>(Ay b)
Theory	
  • The	
  dual	
  func,on	
  (of	
  u)	
  upper	
  bounds	
  the	
  MAP	
  problem.	
  • A	
  subgradient	
  algorithm	
  can	
  be	
  applied	
  to	
  minimize	
  the	
  dual;	
  it	
  will	
  converge	
  in	
  the	
  limit.	
  • If	
  the	
  solu,on	
  to	
  the	
  dual	
  problem	
  sa,sﬁes	
  the	
  constraints,	
  it	
  is	
  also	
  a	
  solu,on	
  to	
  the	
  primal	
  (relaxed)	
  problem	
  (Y’).	
  – If	
  the	
  relaxa,on	
  is	
  !ght,	
  we	
  also	
  have	
  a	
  solu,on	
  to	
  the	
  original	
  primal	
  problem	
  (Y).	
  
Dual	
  Decomposi,on	
  	
  (A	
  Special	
  Case	
  of	
  LR)	
  • Assume	
  the	
  objec,ve	
  decomposes	
  into	
  two	
  parts,	
  coupled	
  only	
  through	
  the	
  linear	
  constraints:	
  • The	
  relaxa,on:	
  maxy2Y,z2Z✓>y+ >zs.t.Ay+Cz=bmaxy2Y,z2Z✓>y+ >z⌘✓maxy2Y✓>y,maxz2Z >z◆
Dual	
  Decomposi,on	
  minumaxy2Y,z2Z✓>y+ >z+u>(Ay+Cz b)1. Ini,alize	
  u(0)	
  =	
  0	
  2. Repeat	
  for	
  k	
  =	
  1,	
  2,	
  ...:	
  y(k) maxy2Y✓>y+u(k 1)>Ayz(k) maxz2Z >z+u(k 1)>Czu(k) u(k 1)  k⇣Ay(k)+Cz(k) b⌘
Consensus	
  Problems	
  in	
  NLP	
  • Key	
  example:	
  – Find	
  the	
  jointly-­‐best	
  parse	
  (under	
  a	
  WCFG)	
  and	
  sequence	
  labeling	
  (under	
  an	
  HMM);	
  see	
  Rush	
  et	
  al.	
  (2010)	
  • Other	
  examples:	
  – Finding	
  a	
  lexicalized	
  phrase	
  structure	
  parse	
  that	
  is	
  jointly-­‐best	
  under	
  a	
  WCFG	
  and	
  a	
  dependency	
  model	
  (Rush	
  et	
  al.,	
  2010)	
  – Decoding	
  in	
  phrase-­‐based	
  transla,on	
  (Chang	
  and	
  Collins,	
  2011).	
  
Example	
  Run	
  (k	
  =	
  1)	
  
8i2{1,...,n},8N2N,y[N,i,i]=z[N,i]u[A,1] = 1u[N,2] = 1u[V,5] = 1u[N,1] = 1u[V,2] = 1u[N,5] = 1u[N,i](1)=u[N,i](0)  k⇣y[N,i,i](1) z[N,i](1)⌘
Example	
  Run	
  (k	
  =	
  2)	
  
8i2{1,...,n},8N2N,y[N,i,i]=z[N,i]
u[N,i](2)=u[N,i](1)  k⇣y[N,i,i](2) z[N,i](2)⌘#u[N,1]#u[V,1]"u[A,1]"u[N,1]
Example	
  Run	
  (k	
  =	
  3)	
  8i2{1,...,n},8N2N,y[N,i,i]=z[N,i]
“Cer,ﬁcate”	
  • Proof	
  that	
  we	
  have	
  solved	
  the	
  original	
  problem:	
  	
  constraints	
  hold.	
  – This	
  is	
  easy	
  to	
  check	
  given	
  y	
  and	
  z.	
  • In	
  published	
  NLP	
  papers	
  so	
  far,	
  this	
  happens	
  most	
  of	
  the	
  ,me	
  (beher	
  than	
  98%).	
  
What	
  can	
  go	
  wrong?	
  • It	
  can	
  take	
  many	
  itera,ons	
  to	
  converge.	
  • Oscilla,on	
  between	
  diﬀerent	
  solu,ons;	
  failure	
  to	
  agree.	
  – Suggested	
  solu,on:	
  	
  add	
  more	
  variables	
  for	
  “bigger	
  parts”	
  and	
  enforce	
  agreement	
  among	
  them	
  with	
  more	
  constraints.	
  
What	
  does	
  this	
  have	
  to	
  do	
  with	
  ILP?	
  • The	
  linear	
  constraints	
  are	
  expressed	
  in	
  terms	
  of	
  an	
  integer-­‐vector	
  representa,on	
  of	
  the	
  output	
  space.	
  – Just	
  like	
  when	
  we	
  treated	
  decoding	
  as	
  an	
  ILP.	
  • The	
  subproblems	
  could	
  be	
  expressed	
  as	
  ILPs,	
  though	
  we’d	
  prefer	
  to	
  use	
  poly-­‐,me	
  combinatorial	
  algorithms	
  to	
  solve	
  them	
  if	
  we	
  can.	
  
Consensus	
  Problems,	
  Revisited	
  • What	
  if	
  we	
  just	
  have	
  a	
  hard	
  combinatorial	
  op,miza,on	
  problem?	
  – There	
  isn’t	
  always	
  a	
  straighmorward	
  decomposi,on.	
  • Mar,ns	
  et	
  al.	
  (2011):	
  	
  shaher	
  a	
  decoding	
  problem	
  into	
  many	
  “small”	
  subproblems	
  (instead	
  of	
  two	
  “big”	
  ones).	
  – Instead	
  of	
  dynamic	
  programming	
  as	
  a	
  subrou,ne,	
  LP	
  relaxa,ons	
  of	
  “small”	
  subproblems.	
  – Extra	
  LP	
  relaxa,on	
  step.	
  
Mar,ns’	
  Alterna,ve	
  Formula,on	
  • Original	
  problem:	
  	
  • Convex	
  relaxa,on:	
  • Dual:	
  maxy12Y1,...,yS2YS,w2RDSXs=1✓>syss.t.8s,Asw=ysmaxy12conv(Y1),...,yS2conv(YS),w2RDSXs=1✓>syss.t.8s,Asw=ysminu1,...,uSmaxy12conv(Y1),...,yS2conv(YS),w2RDSXs=1✓>sys+Xsu>s(ys Asw)
Augmented	
  Lagrangian	
  (Hestenes,	
  1969;	
  Powell,	
  1969)	
  
minu1,...,uSmaxy12conv(Y1),...,yS2conv(YS),w2RDSXs=1✓>sys+Xsu>s(ys Asw)minu1,...,uSmaxy12conv(Y1),...,yS2conv(YS),w2RDSXs=1✓>sys+Xsu>s(ys Asw)+⇢2Xskys Aswk22
Alterna,ng	
  Direc,ons	
  Method	
  of	
  Mul,pliers	
  	
  (Gabay	
  and	
  Mercier,	
  1976;	
  Glowinski	
  and	
  Marroco,	
  1975)	
  Dual	
  Decomposi,on	
  (AD3)	
  • Alternate	
  between	
  upda,ng	
  y	
  and	
  w:	
  	
  • Subgradient	
  step	
  for	
  dual	
  variables	
  u	
  is	
  similar	
  to	
  before:	
  8s,ys arg maxys2conv(Ys)✓>sys+u>sys+⇢2kys Aswk22w arg maxwXsu>sAsw+⇢2Xskys Aswk228s,u(k)s u(k 1)s  k(ys Asw)
Massive	
  Decomposi,on	
  8s,ys arg maxys2conv(Ys)✓>sys+u>sys+⇢2kys Aswk22w arg maxwXsu>sAsw+⇢2Xskys Aswk22• Most	
  extreme:	
  	
  every	
  factor	
  (MN)	
  or	
  “part”	
  is	
  a	
  separate	
  subproblem.	
  • Some	
  kinds	
  of	
  MN	
  factors	
  can	
  be	
  solved	
  very	
  eﬃciently	
  ...	
  
XOR,	
  OR,	
  OR-­‐with-­‐Output	
  	
  Solvable	
  in	
  O(K	
  log	
  K)	
  

8s,ys arg maxys2conv(Ys)✓>sys+u>sys+⇢2kys Aswk22w arg maxwXsu>sAsw+⇢2Xskys Aswk22AD3	
  and	
  “Big”	
  Subproblems?	
  • Return	
  to	
  Rush	
  and	
  Collins’	
  example.	
  – One	
  subproblem	
  is	
  “WCFG”	
  and	
  one	
  is	
  “HMM	
  tagger.”	
  	
  	
  – In	
  dependency	
  parsing,	
  “max	
  arborescence”	
  might	
  be	
  a	
  subproblem.	
  – Why	
  can’t	
  we	
  use	
  AD3?	
  
Pros	
  and	
  Cons	
  • Con:	
  	
  Subproblems	
  are	
  now	
  quadra!c.	
  – Linear	
  decoders	
  as	
  subrou,nes?	
  • Con:	
  	
  Frac,onal	
  solu,ons.	
  • Pro:	
  	
  Beher	
  stopping	
  criteria:	
  	
  residuals.	
  – Primal	
  residuals	
  measure	
  amount	
  by	
  which	
  primal	
  constraints	
  are	
  violated.	
  – Dual	
  residuals	
  measure	
  amount	
  by	
  which	
  dual	
  op,mality	
  is	
  violated.	
  • Pro:	
  	
  Cer,ﬁcates	
  as	
  before	
  (for	
  each	
  s,	
  Asw	
  =	
  ys)	
  
Convergence	
  of	
  AD3	
  vs.	
  Subgradient	
  
Dependency	
  parsing:	
  • ADMM	
  =	
  AD3	
  • Sec	
  Ord	
  =	
  Second	
  order	
  model	
  for	
  which	
  subgradient	
  op,miza,on	
  is	
  possible	
  • Full	
  =	
  second	
  order	
  model	
  with	
  all-­‐siblings,	
  directed	
  paths,	
  and	
  non-­‐projec,ve	
  arcs	
  
Take-­‐Home	
  Messages	
  • Dual	
  decomposi,on	
  is	
  useful	
  for	
  consensus	
  problems.	
  – Subgradient	
  DD	
  when	
  there	
  are	
  a	
  few	
  subproblems	
  with	
  good	
  specialized	
  solvers.	
  – AD3	
  when	
  you’ve	
  got	
  a	
  big	
  problem	
  with	
  lots	
  of	
  hard	
  and	
  sor	
  constraints.	
  	
  (There	
  is	
  a	
  library.)	
  • Ahrac,ve	
  guarantees	
  (cf.	
  beam	
  search).	
  • Only	
  MAP	
  inference.	
  
References	
  • “A	
  tutorial	
  on	
  dual	
  decomposi,on	
  and	
  Lagrangian	
  relaxa,on	
  for	
  inference	
  in	
  natural	
  language	
  processing,”	
  by	
  A.	
  Rush	
  and	
  M.	
  Collins,	
  JAIR	
  45:305-­‐362,	
  2013.	
  • “Alterna,ng	
  direc,ons	
  dual	
  decomposi,on”	
  by	
  A.	
  Mar,ns	
  et	
  al.,	
  arXiv	
  1212.6550.	
  
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  February 2, 2015 
Today: • Logistic regression • Generative/Discriminative classifiers Readings: (see class website)  Required: • Mitchell: “Naïve Bayes and Logistic Regression”   Optional • Ng & Jordan 
Announcements  • HW3 due Wednesday Feb 4 • HW4 will be handed out next Monday Feb 9 • new reading available:    – Estimating Probabilities: MLE and MAP (Mitchell) – see Lecture tab of class website • required reading for today: – Naïve Bayes and Logistic Regression (Mitchell) 
Gaussian Naïve Bayes – Big Picture Example:  Y= PlayBasketball (boolean), X1=Height,  X2=MLgrade assume P(Y=1) = 0.5 
Logistic Regression Idea: • Naïve Bayes allows computing P(Y|X) by learning P(Y) and P(X|Y)  • Why not learn P(Y|X) directly? 
•  Consider learning f: X à Y, where •  X is a vector of real-valued features, < X1 … Xn > •  Y is boolean •  assume all Xi are conditionally independent given Y •  model P(Xi | Y = yk) as Gaussian N(µik,σi) •  model P(Y) as Bernoulli (π) •  What does that imply about the form of P(Y|X)? 

Derive form for P(Y|X) for Gaussian P(Xi|Y=yk) assuming σik = σi 

Very convenient! implies 
implies 
implies 

Very convenient! implies 
implies 
implies 
linear classification rule! 
Logistic function 

Logistic regression more generally• Logistic regression when Y not boolean (but still discrete-valued).  • Now y ∈ {y1 ... yR} : learn R-1 sets of weights  for k<R    for k=R 

Training Logistic Regression: MCLE • we have L training examples: • maximum likelihood estimate for parameters W • maximum conditional likelihood estimate  

Training Logistic Regression: MCLE • Choose parameters W=<w0, ... wn> to maximize conditional likelihood of training data • Training data D =  • Data likelihood =  • Data conditional likelihood =  
where 

Expressing Conditional Log Likelihood 

Maximizing Conditional Log Likelihood 
Good news: l(W) is concave function of WBad news: no closed-form solution to maximize l(W)

 Gradient Descent:  Batch gradient: use error           over entire training set D Do until satisfied:     1. Compute the gradient      2. Update the vector of parameters:   
Stochastic gradient: use error          over single examples Do until satisfied:     1. Choose (with replacement) a random training example      2. Compute the gradient just for    :     3. Update the vector of parameters:   Stochastic approximates Batch arbitrarily closely as Stochastic can be much faster when D is very large Intermediate approach: use error over subsets of D  

Maximize Conditional Log Likelihood:          Gradient Ascent  

Maximize Conditional Log Likelihood:          Gradient Ascent  
Gradient ascent algorithm: iterate until change < ε   For all i, repeat      

That’s all for M(C)LE.  How about MAP? • One common approach is to define priors on W – Normal distribution, zero mean, identity covariance • Helps avoid very large weights and overfitting • MAP estimate • let’s assume Gaussian prior: W ~ N(0, σ) 

MLE vs MAP  • Maximum conditional likelihood estimate • Maximum a posteriori estimate with prior W~N(0,σI) 

MAP estimates and Regularization • Maximum a posteriori estimate with prior W~N(0,σI) 
called a “regularization” term •  helps reduce overfitting •  keep weights nearer to zero (if P(W) is zero mean           Gaussian prior), or whatever the prior suggests •  used very frequently in Logistic Regression 
•  Consider learning f: X à Y, where •  X is a vector of real-valued features, < X1 … Xn > •  Y is boolean •  assume all Xi are conditionally independent given Y •  model P(Xi | Y = yk) as Gaussian N(µik,σi) •  model P(Y) as Bernoulli (π) •  Then P(Y|X) is of this form, and we can directly estimate W •  Furthermore, same holds if the Xi are boolean •  trying proving that to yourself 
The Bottom Line 
Generative vs. Discriminative Classifiers Training classifiers involves estimating f: X à Y, or P(Y|X)  Generative classifiers (e.g., Naïve Bayes) • Assume some functional form for P(X|Y), P(X) • Estimate parameters of P(X|Y), P(X) directly from training data • Use Bayes rule to calculate P(Y|X= xi) Discriminative classifiers (e.g., Logistic regression)  • Assume some functional form for P(Y|X) • Estimate parameters of P(Y|X) directly from training data 
Use Naïve Bayes or Logisitic Regression? Consider • Restrictiveness of modeling assumptions  • Rate of convergence (in amount of training data) toward asymptotic hypothesis 
Naïve Bayes vs Logistic Regression Consider Y boolean, Xi continuous, X=<X1 ... Xn>  Number of parameters to estimate: • NB:   • LR:    

Naïve Bayes vs Logistic Regression Consider Y boolean, Xi continuous, X=<X1 ... Xn>  Number of parameters: • NB: 4n +1 • LR: n+1 Estimation method: • NB parameter estimates are uncoupled • LR parameter estimates are coupled  
G.Naïve Bayes vs. Logistic Regression Recall two assumptions deriving form of LR from GNBayes: 1.   Xi conditionally independent of Xk given Y 2.   P(Xi | Y = yk)  =  N(µik,σi),   ß not N(µik,σik) Consider three learning methods: •  GNB (assumption 1 only) •  GNB2 (assumption 1 and 2) •  LR   Which method works better if we have infinite training data, and… •  Both (1) and (2) are satisfied •  Neither (1) nor (2) is satisfied •  (1) is satisfied, but not (2)  
G.Naïve Bayes vs. Logistic Regression Recall two assumptions deriving form of LR from GNBayes: 1.   Xi conditionally independent of Xk given Y 2.   P(Xi | Y = yk)  =  N(µik,σi),   ß not N(µik,σik) Consider three learning methods: • GNB (assumption 1 only) • GNB2 (assumption 1 and 2) • LR   Which method works better if we have infinite training data, and...  • Both (1) and (2) are satisfied • Neither (1) nor (2) is satisfied • (1) is satisfied, but not (2)  [Ng & Jordan, 2002] 
G.Naïve Bayes vs. Logistic Regression Recall two assumptions deriving form of LR from GNBayes: 1.   Xi conditionally independent of Xk given Y 2.   P(Xi | Y = yk)  =  N(µik,σi),   ß not N(µik,σik) Consider three learning methods: • GNB (assumption 1 only)     -- decision surface can be non-linear • GNB2 (assumption 1 and 2) – decision surface linear • LR                                         -- decision surface linear, trained without                assumption 1.  Which method works better if we have infinite training data, and...  • Both (1) and (2) are satisfied: LR = GNB2 = GNB • (1) is satisfied, but not (2) :     GNB > GNB2, GNB > LR,  LR > GNB2  • Neither (1) nor (2) is satisfied:   GNB>GNB2,  LR > GNB2, LR><GNB [Ng & Jordan, 2002] 
G.Naïve Bayes vs. Logistic Regression What if we have only finite training data?  They converge at different rates to their asymptotic (∞ data) error  Let          refer to expected error of learning algorithm A after n training examples  Let d be the number of features: <X1 … Xd>          So, GNB requires n = O(log d) to converge, but LR requires n = O(d) [Ng & Jordan, 2002] 

Some experiments from UCI data sets 
[Ng & Jordan, 2002]  
Naïve Bayes vs. Logistic Regression The bottom line:  GNB2 and LR both use linear decision surfaces, GNB need not  Given infinite data, LR is better or equal to GNB2 because training procedure does not make assumptions 1 or 2 (though our derivation of the form of P(Y|X) did).  But GNB2 converges more quickly to its perhaps-less-accurate asymptotic error  And GNB is both more biased (assumption1) and less (no assumption 2) than LR, so either might outperform the other 
What you should know: • Logistic regression – Functional form follows from Naïve Bayes assumptions • For Gaussian Naïve Bayes assuming variance σi,k = σi • For discrete-valued Naïve Bayes too – But training procedure picks parameters without making conditional independence assumption – MLE training: pick W to maximize P(Y | X, W) – MAP training: pick W to maximize P(W | X,Y) • ‘regularization’  • helps reduce overfitting  • Gradient ascent/descent – General approach when closed-form solutions unavailable • Generative vs. Discriminative classifiers – Bias vs. variance tradeoff 
extra slides 
What is the minimum possible error? Best case: • conditional independence assumption is satistied • we know P(Y), P(X|Y) perfectly (e.g., infinite training data) 
Questions to think about: • Can you use Naïve Bayes for a combination of discrete and real-valued Xi?  • How can we easily model the assumption that just 2 of the n attributes as dependent? • What does the decision surface of a Naïve Bayes classifier look like? • How would you select a subset of Xi’s?  
Maria -Florina  Balcan  
03/23/2015  Kernels Methods in Machine Learning  
•Perceptron. Geometric Margins.  
•Support Vector Machines (SVMs ).  
Quick Recap  about 
Perceptron and Margins  
Mistake bound model  •Example arrive sequentially . The Online Learning Model  
•We need to make a prediction.  
Afterwards observe the outcome.  
•Analysis wise, make no distributional assumptions . 
•Goal: Minimize  the number of mistakes . Online Algorithm  Example 𝑥𝑖 
Prediction ℎ(𝑥𝑖) Phase i:  
Observe c∗(𝑥𝑖) For i=1, 2, …, :  
•Set t=1, start with the all zero vector  𝑤1. Perceptron Algorithm in Online Model  
•Given example 𝑥, predict + iff 𝑤𝑡⋅𝑥≥0 
•On a mistake, update as follows:  
•Mistake on positive, 𝑤𝑡+1←𝑤𝑡+𝑥 
•Mistake on negative, 𝑤𝑡+1←𝑤𝑡−𝑥 
Note 1:  wt is weighted sum of incorrectly classified examples  
𝑤𝑡=𝑎𝑖1𝑥𝑖1+⋯+𝑎𝑖𝑘𝑥𝑖𝑘 So, 𝑤𝑡⋅𝑥=𝑎𝑖1𝑥𝑖1⋅𝑥+⋯+𝑎𝑖𝑘𝑥𝑖𝑘⋅𝑥 X 
X X X 
X 
X X X 
X X O 
O 
O O 
O O 
O O w 
Note 2:  Number of mistakes ever made depends only on the 
geometric margin of examples seen.  WLOG  homogeneou s linear separators  [w0=0]. 
•No matter how long the sequence is or how high dimension n is!  X=Rn 
Geometric Margin  
Definition:  The margin  of example 𝑥 w.r.t.  a linear sep. 𝑤 is the 
distance from 𝑥 to the plane 𝑤⋅𝑥=0. 
𝑥1 
w Margin of example 𝑥1 
𝑥2 Margin of example 𝑥2 If 𝑤=1,  margin of x 
w.r.t. w is |𝑥⋅𝑤|. 
+ + 
+ + - 
- - 
- - 𝛾 𝛾 
+ 
- - - - w Definition:  The margin 𝛾 of a set of examples 𝑆 is the maximum  
𝛾𝑤 over all linear separators 𝑤. Geometric Margin  
Definition:  The margin 𝛾𝑤 of a set of examples 𝑆 wrt a linear 
separator 𝑤 is the smallest margin over points 𝑥∈𝑆. Definition:  The margin  of example 𝑥 w.r.t.  a linear sep. 𝑤 is the 
distance from 𝑥 to the plane 𝑤⋅𝑥=0. 
Perceptron: Mistake  Bound  
Theorem : If data linearly separable by margin 𝛾 and points inside 
a ball of radius 𝑅, then Perceptron makes ≤𝑅/𝛾2 mistakes.  
(Normalized margin: multiplying  all points by 100, or dividing all points by 100, 
doesn’t change the number of mistakes; algo is invariant to scaling.)  + + 
+ + 
+ + + 
- 
- - 
- -   
- - - 
- + 
w* 
R Margin : the amount of 
wiggle -room available for 
a solution.  •No matter how long the sequence is how high dimension n is!  
Perceptron Extensions  
•Can use it to find a consistent separator with a given set 
S linearly separable by margin 𝛾 (by cycling through the data) . 
•Can convert the mistake bound guarantee into a distributional 
guarantee too (for the case where the 𝑥𝑖s come from a fixed 
distribution) . 
•Can be adapted to the case where there is no perfect 
separator as long as the so called hinge loss (i.e., the total 
distance needed to move the points to classify them correctly large 
margin)  is small.  
•Can be kernelized  to handle non -linear decision boundaries!  
Theorem : If data linearly separable by margin 𝛾 and points inside 
a ball of radius 𝑅, then Perceptron makes ≤𝑅/𝛾2 mistakes.  
Implies that large margin classifiers have 
smaller complexity!  
Complexity of Large Margin Linear Sep.  
•Know that in Rn we can shatter n+1 points with linear 
separators, but not n+2 points (VC-dim of linear sep is n+1). 
What if we require that the points be 
linearly separated by margin 𝛾?  
Can have at most 𝑅
𝛾2
 points inside ball of radius R 
that can be shattered at margin 𝛾 (meaning that every 
labeling is achievable by a separator of margin 𝛾). 
•So, large margin classifiers have smaller complexity!  
•Less prone to overfitting !!!! 
•Less classifiers to worry about that will look good over 
the sample, but bad over all….  X 
X X X 
X 
X X X 
X X O 
O 
O O 
O O 
O O w 
•Nice implications for usual distributional learning setting.  
Both sample complexity and algorithmic implications.  Margin Important Theme in ML.  
Sample/Mistake Bound complexity : 
•If large  margin, # mistakes Peceptron  makes 
is small  (independent on the dim of the space)!  
•If large  margin  𝛾 and if alg. produces a large 
margin classifier, then amount of data needed 
depends only on R/𝛾 [Bartlett & Shawe -Taylor ’ 99].  
Algorithmic Implications : 
•Perceptron, Kernels, SVMs…  + + 
+ + - 
- - 
- - 𝛾 𝛾 
+ 
- - - - w 
•Suggests searching for a 
large margin classifier…  
So far, talked  about  margins  in 
the context  of (nearly)  linearly  
separable  datasets  
What if Not Linearly Separable  
Problem:  data not linearly separable in the most natural 
feature representation.  
Solutions:  
• “Learn a more complex class of functions ”  
•(e.g., decision trees, neural networks, boosting).  
• “Use a Kernel ”  
• “Use a Deep Network”  
Example:  vs No good linear 
separator in pixel 
representation.  
 
• “Combine  Kernels  and Deep Networks”  (a neat solution that attracted a lot of attention)  
Overview of Kernel Methods  
What is a Kernel?  
A kernel K is a legal def of dot -product : i.e. there exists an 
implicit mapping Φ s.t. K(    ,     ) =Φ(    )⋅Φ(    )  
Why Kernels matter?  
• Many algorithms interact with data only via dot -products.  
•So, if replace x⋅z with Kx,z they act implicitly as if data  
was in the higher -dimensional Φ-space.  
•If data is linearly separable by large margin in the Φ-space, 
then good sample complexity.   E.g., K( x,y) = (x ¢ y + 1)d 
: (n-dimensional space) ! nd-dimensional space  
[Or other regularity properties for controlling the capacity.]  
Kernels  
Definition  
K(⋅,⋅) is a  kernel if it can be viewed as a legal definition of 
inner product:  
•∃ ϕ:X→RN  s.t. Kx,z=ϕx⋅ϕ(z) 
•Range of ϕ is called the Φ-space . 
•N can be very large .  
•But think of ϕ as implicit , not explicit!!!!  
Example  
For n=2, d=2, the kernel Kx,z=x⋅zd corresponds to  
𝑥1,𝑥2→Φ𝑥=(𝑥12,𝑥22,2𝑥1𝑥2) 
x2 
x1 O 
O O O 
O 
O O O X 
X X X 
X 
X 
X X X 
X X X 
X X X 
X 
X X z1 
z3 O 
O O O 
O O O 
O O X X 
X X X 
X X 
X X X X 
X X 
X X 
X X X Φ-space  Original space  
Example  
ϕ:R2→R3, x1,x2→Φx=(x12,x22,2x1x2) 
x2 
x1 O 
O O O 
O 
O O O X 
X X X 
X 
X 
X X X 
X X X 
X X X 
X 
X X z1 
z3 O 
O O O 
O O O 
O O X X 
X X X 
X X 
X X X X 
X X 
X X 
X X X Φ-space  Original space  ϕx⋅ϕ𝑧=x12,x22,2x1x2⋅(𝑧12,𝑧22,2𝑧1𝑧2) 
=x1𝑧1+x2𝑧22=x⋅𝑧2=K(x,z) 
Kernels  
Definition  
K(⋅,⋅) is a  kernel if it can be viewed as a legal definition of 
inner product:  
•∃ ϕ:X→RN  s.t. Kx,z=ϕx⋅ϕ(z) 
•Range of ϕ is called the Φ-space . 
•N can be very large .  
•But think of ϕ as implicit , not explicit!!!!  
Example  
Note:   feature space might not be unique.  
ϕ:R2→R4, x1,x2→Φx=(x12,x22,x1x2,x2x1) 
ϕx⋅ϕ𝑧=(x12,x22,x1x2,x2x1)⋅(z12,z22,z1z2,z2z1) 
=x⋅𝑧2=K(x,z) ϕ:R2→R3, x1,x2→Φx=(x12,x22,2x1x2) 
ϕx⋅ϕ𝑧=x12,x22,2x1x2⋅(𝑧12,𝑧22,2𝑧1𝑧2) 
=x1𝑧1+x2𝑧22=x⋅𝑧2=K(x,z) 
Avoid explicitly expanding the features  
Feature space can grow really large and really quickly….  
Crucial to think of ϕ as implicit , not explicit!!!!  
–𝑥1𝑑, 𝑥1𝑥2…𝑥𝑑, 𝑥12𝑥2…𝑥𝑑−1 
–Total number of such feature is  
𝑑+𝑛−1
𝑑=𝑑+𝑛−1!
𝑑!𝑛−1! 
–𝑑=6,𝑛=100, there are 1.6 billion terms  
•Polynomial kernel degreee 𝑑, 𝑘𝑥,𝑧=𝑥⊤𝑧𝑑=𝜙𝑥⋅𝜙𝑧 
𝑘𝑥,𝑧=𝑥⊤𝑧𝑑=𝜙𝑥⋅𝜙𝑧 
𝑂𝑛  𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛 ! 
Kernelizing  a learning algorithm  
•If  all computations involving instances are in terms of 
inner products then : 
Conceptually, work in a very high diml space and the alg’s 
performance depends only on linear separability  in that 
extended space.  
 Computationally, only need to modify the algo by replacing 
each x⋅z with a Kx,z. 
• Examples of kernalizable  algos: 
• classification: Perceptron, SVM.  
• regression: linear, ridge regression.  
• clustering: k -means.  
•Set t=1, start with the all zero vector  𝑤1. Kernelizing   the Perceptron Algorithm  
•Given example 𝑥, predict + iff 𝑤𝑡⋅𝑥≥0 
•On a mistake, update as follows:  
•Mistake on positive, 𝑤𝑡+1←𝑤𝑡+𝑥 
•Mistake on negative, 𝑤𝑡+1←𝑤𝑡−𝑥 
Easy to kernelize  since 𝑤𝑡 is weighted sum of incorrectly 
classified examples  𝑤𝑡=𝑎𝑖1𝑥𝑖1+⋯+𝑎𝑖𝑘𝑥𝑖𝑘 
𝑤𝑡⋅𝑥=𝑎𝑖1𝑥𝑖1⋅𝑥+⋯+𝑎𝑖𝑘𝑥𝑖𝑘⋅𝑥 X 
X X X 
X 
X X X 
X X O 
O 
O O 
O O 
O O w 
Replace  
Note: need to store all the mistakes so far.  with 
𝑎𝑖1 𝐾(𝑥𝑖1,𝑥)+⋯+𝑎𝑖𝑘𝐾(𝑥𝑖𝑘,𝑥) 
Kernelizing   the Perceptron Algorithm  
•Given 𝑥, predict + iff 
•On the 𝑡 th  mistake, update as follows:  
•Mistake on positive, set  𝑎𝑖𝑡←1; store 𝑥𝑖𝑡 
•Mistake on negative, 𝑎𝑖𝑡←−1; store 𝑥𝑖𝑡 
Perceptron  𝑤𝑡=𝑎𝑖1𝑥𝑖1+⋯+𝑎𝑖𝑘𝑥𝑖𝑘 
𝑤𝑡⋅𝑥=𝑎𝑖1𝑥𝑖1⋅𝑥+⋯+𝑎𝑖𝑘𝑥𝑖𝑘⋅𝑥 X 
X X X 
X 
X X X 
X X O 
O 
O O 
O O 
O O w 
Exact same behavior/prediction rule as if mapped data in the 
𝜙-space and ran Perceptron there!  → 𝑎𝑖1 𝐾(𝑥𝑖1,𝑥)+⋯+𝑎𝑖𝑘𝐾(𝑥𝑖𝑘,𝑥) Φ-space  
𝑎𝑖1 𝐾(𝑥𝑖1,𝑥)+⋯+𝑎𝑖𝑡−1𝐾(𝑥𝑖𝑡−1,𝑥)≥0 
Do this implicitly, so computational savings!!!!!  𝜙(𝑥𝑖𝑡−1)⋅𝜙(𝑥) 
Generalize Well if Good Margin  
•If data is linearly separable by margin in the  𝜙-space, 
then small mistake bound.  
•If margin 𝛾 in 𝜙-space, then Perceptron makes 𝑅
𝛾2
  mistakes.  
+ 
w* + 
+ + 
+ + + 
- 
- - 
- -   
- - - 
- + 
R Φ-space  
Kernels: More E xamples  
•Polynomial: Kx,𝑧=x⋅𝑧d or Kx,𝑧=1+x⋅𝑧d 
•Gaussian: Kx,𝑧=exp−𝑥−𝑧2
2 𝜎2  •Linear: Kx,z=x⋅𝑧 
•Laplace Kernel: Kx,𝑧=exp−||𝑥−𝑧||
2 𝜎2  
•Kernel for non -vectorial  data, e.g., measuring similarity 
between sequences.  
Properties of Kernels  
Theorem (Mercer)  
K is a kernel if and only if:  
•K is symmetric  
•For any set of training points 𝑥1,𝑥2,…,𝑥𝑚 and for 
any 𝑎1,𝑎2,…,𝑎𝑚∈𝑅, we have:  
  𝑎𝑖𝑎𝑗𝐾𝑥𝑖,𝑥𝑗≥0𝑖,𝑗  
𝑎𝑇𝐾𝑎≥0 
I.e., 𝐾=(𝐾𝑥𝑖,𝑥𝑗)𝑖,𝑗=1,…,𝑛  is positive semi -definite.  
Kernel Methods  
•Offer great modularity . 
•No need to change the underlying learning 
algorithm to accommodate a particular choice 
of kernel function.  
•Also, we can substitute a different algorithm 
while maintaining the same kernel.  
Kernel, Closure Properties  
Easily create new kernels using basic ones!  
then Kx,z=c1K1x,z+c2K2x,z is a kernel . If  K1⋅,⋅ and  K2⋅,⋅ are kernels  c1≥0,𝑐2≥0, Fact: 
Key idea : concatenate the 𝜙 spaces.  
ϕx=(c1 ϕ1x,c2 ϕ2(x))  
ϕx⋅ϕ(z)=c1 ϕ1x⋅ϕ1z+ c2 ϕ2x⋅ϕ2z 
𝐾1(𝑥,𝑧) 𝐾2(𝑥,𝑧) 
Kernel, Closure Properties  
then Kx,z=K1x,zK2x,z is a kernel . If  K1⋅,⋅ and  K2⋅,⋅ are kernels,  Fact: 
Key idea : ϕx=ϕ1,ix ϕ2,jx𝑖∈1,…,𝑛,𝑗∈{1,…,𝑚}  
ϕx⋅ϕ(z)= ϕ1,ix ϕ2,jxϕ1,iz ϕ2,jz
𝑖,𝑗 
= ϕ1,ix ϕ1,𝑖z ϕ2,𝑗x ϕ2,jz
𝑗 
𝑖 
= ϕ1,ix ϕ1,𝑖zK2x,z𝑖 =K1x,z K2x,z Easily create new kernels using basic ones!  

Kernels, Discussion  
• Lots of Machine Learning algorithms are kernalizable : 
• classification: Perceptron, SVM.  
• regression: linear regression.  
• clustering: k -means.  •If  all computations involving instances are in terms 
of inner products then:  
Conceptually, work in a very high diml space and the alg’s 
performance depends only on linear separability  in that 
extended space.  
 Computationally, only need to modify the algo by replacing 
each x⋅z with a Kx,z. 
Kernels, Discussion  
•If  all computations involving instances are in terms 
of inner products then:  
Conceptually, work in a very high diml space and the alg’s 
performance depends only on linear separability  in that 
extended space.  
 Computationally, only need to modify the algo by replacing 
each x⋅z with a Kx,z. 
How to choose a kernel:  
•Use Cross -Validation to choose the parameters, e.g., 𝜎  for 
Gaussian Kernel   Kx,𝑧=exp−𝑥−𝑧2
2 𝜎2  
•Learn  a good kernel; e.g.,  [Lanckriet -Cristianini -Bartlett -El Ghaoui -
Jordan’ 04] •Kernels often encode domain knowledge (e.g., string kernels)  
Computational	Linguistics	ICMSC	723	/	LING	723	/	INST	725Marine	Carpuat
What	is	language?Wikipedia:“Language	is	the	ability	to	acquire	and	use	complex	systems	of	communication,	particularly	the	human	ability	to	do	so,	and	a	language	is	any	specific	example	of	such	a	system.	The	scientific	study	of	language	is	called	linguistics.”	
•Computational	Linguistics	(CL)•The	science	of	doing	what	linguists	do	with	language,	but	using	computers•Natural	Language	Processing	(NLP)•The	engineering	discipline	of	doing	what	people	do	with	language,	but	using	computers•Speech/Language/Text	processing•Human	Language	Technology
NLP	State	of	the	ArtStill	a	challenging	problem!AI’s	Language	Problem“Machines	that	truly	understand	language	would	be	incredibly	useful.	But	we	don’t	know	how	to	build	them.”MIT	Technology	ReviewWill	Knight,	Aug	9,	2016Many	useful	applications	already	exist

What	does	an	NLP	system	need	to	“know”?•Language	consists	of	many	levels	of	structure•Humans	fluently	integrate	all	of	these	in	producing	and	understanding	language•Ideally,	so	would	a	computer!
Example	from	Nathan	Schneider

Why	is	NLP	hard?
AmbiguityAt	the	word	level•Part	of	speech•[V	Duck]!•[N	Duck]	is	delicious	for	dinner.•Word	sense•I	went	to	the	bank	to	deposit	my	check.•I	went	to	the	bank	to	look	out	at	the	river
AmbiguityAt	the	syntactic	level•PP	Attachment	ambiguity•I	saw	the	man	on	the	hill	with	the	telescope•Structural	ambiguity•I	cooked	her	duck•Visiting	relatives	can	be	annoying•Time	flies	like	an	arrow
Ambiguity•Quantifier	scope•Everyone	on	the	island	speaks	two	languages.•Hard	cases	require	world	knowledge,	understanding	of	speaker	goals•The	city	council	denied	the	demonstrators	the	permit	because	they	advocated	violence•The	city	council	denied	the	demonstrators	the	permit	because	they	feared	violence
Ambiguity•NLP	challenge:	how	can	we	model	ambiguity,	and	choose	the	correct	analysis	in	context?•Approach:	learn	from	data

Word	counts•Most	frequent	words	in	the	English	Europarlcorpus•(out	of	24M	word	tokens)

Word	counts•But	also,	out	of	the	93,638	distinct	words	(word	types),	36,231	occur	only	once

Plotting	word	frequencies

Plotting	word	frequencies	(with	log-log	axes)

Zipf’slaw

Zipf’slaw:	implications•Even	in	a	very	large	corpus,	there	will	be	a	lot	of	infrequent	words•The	same	holds	for	many	other	levels	of	linguistic	structure•Core	NLP	challenge:	we	need	to	estimate	probabilities	or	to	be	able	to	make	predictions	for	things	we	have	rarely	or	never	seen
Variation	and	Expressivity•The	same	meaning	can	be	expressed	with	different	forms•I	saw	the	man•The	man	was	seen	by	me•She	needed	to	make	a	quick	decision	in	that	situation•The	scenario	required	her	to	make	a	split-second	judgment
6,800	living	languages600	with	written	tradition		100	spoken	by	95%	of	population
Social	Impact•NLP	experiments	and	applications	can	have	a	direct	effect	on	individual	users’	lives•Some	issues•Privacy•Exclusion•Overgeneralization•Dual-use	problems[Hovy&	SpruitACL	2016]
Today•Levels	of	linguistic	analysis	in	NLP•Morphology,	syntax,	semantics,	discourse•Why	is	NLP	hard?•Ambiguity•Sparse	data•Zipf’slaw,	corpus,	word	types	and	tokens•Variation	and	expressivity•Social	Impact
Course	Logisticshttp://www.cs.umd.edu/class/fall2017/cmsc723/
Before	next	class•Read	the	syllabus•Make	sure	you	have	access	to	piazza•Get	started	on	homework	1	–due	Thursday	Sep	7	by	12pm.

AWS	TutorialQiaojing	Yan
What	is	AWS	and	EC2•AWS	(Amazon	web	service)	:	cloud	service	platform	provide	by	amazon,	which	includes:1.	Compute:	e.g.	EC2	(Elastic	Compute	Cloud)2.	Storage:	e.g.	S3	&	EBS3.	Database4.	Networking…
Using	EC2•Just	like	sshintoa	myth/corn	machine•More	flexible:	can	install	new	packages•More	powerful:	e.g.	UseGPU	tomake	training	neural	networks	faster

EC2	and	TensorFlowsetup•We	provide	an	AMI	(amazon	machine	image)	that	already	has	TensorFlowinstalled.	•All	you	need	to	do	is	to	launch	your	instance	using	that	AMI.	•A	step-by-step	tutorial	can	be	found	on	the	class	website
Data	Storage	when	using	EC2•The	way	data	is	stored	is	different	than	myth/corn•When	you	terminate	an	instance,	all	the	data	on	that	instance	would	be	lost!•However,	for	EBS-backed	instance,	data	is	preservedwhenyouchoosetostopitinsteadofterminateit.EBS-backedinstanceisourdefaultsetup.•Youcanalsostoreyourdatasomewhereelse,e.g.S3
Useful	topics	for	doing	final	project	using	AWS•IAM	(Identity	and	Access	Management)	:	Standard	way	to	share	resources,	such	as	EC2	instance	and	S3Link:	http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html•Exchange	data	between	EC2	and	S3:	You	can	use	Amazon	S3	to	store	backup	copies	of	your	dataLink:	http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonS3.html
Probabilistic Context-Free Grammars
Michael Collins, Columbia University
Overview
IProbabilistic Context-Free Grammars (PCFGs)
IThe CKY Algorithm for parsing with PCFGs
A Probabilistic Context-Free Grammar (PCFG)
S) NP VP 1.0
VP) Vi 0.4
VP) Vt NP 0.4
VP) VP PP 0.2
NP) DT NN 0.3
NP) NP PP 0.7
PP) P NP 1.0Vi) sleeps 1.0
Vt) saw 1.0
NN) man 0.7
NN) woman 0.2
NN) telescope 0.1
DT) the 1.0
IN) with 0.5
IN) in 0.5
IProbability of a tree twith rules
1!1;2!2;:::; n!n
isp(t) =Qn
i=1q(i!i)whereq(!)is the probability
for rule!.
DERIVATION RULES USED PROBABILITY
SS!NP VP1.0
NP VPNP!DT NN0.3
DT NN VPDT!the1.0
the NN VPNN!dog0.1
the dog VPVP!Vi0.4
the dog ViVi!laughs0.5
the dog laughs
DERIVATION RULES USED PROBABILITY
SS!NP VP1.0
NP VPNP!DT NN0.3
DT NN VPDT!the1.0
the NN VPNN!dog0.1
the dog VPVP!Vi0.4
the dog ViVi!laughs0.5
the dog laughs
DERIVATION RULES USED PROBABILITY
SS!NP VP1.0
NP VPNP!DT NN0.3
DT NN VPDT!the1.0
the NN VPNN!dog0.1
the dog VPVP!Vi0.4
the dog ViVi!laughs0.5
the dog laughs
DERIVATION RULES USED PROBABILITY
SS!NP VP1.0
NP VPNP!DT NN0.3
DT NN VPDT!the1.0
the NN VPNN!dog0.1
the dog VPVP!Vi0.4
the dog ViVi!laughs0.5
the dog laughs
DERIVATION RULES USED PROBABILITY
SS!NP VP1.0
NP VPNP!DT NN0.3
DT NN VPDT!the1.0
the NN VPNN!dog0.1
the dog VPVP!Vi0.4
the dog ViVi!laughs0.5
the dog laughs
DERIVATION RULES USED PROBABILITY
SS!NP VP1.0
NP VPNP!DT NN0.3
DT NN VPDT!the1.0
the NN VPNN!dog0.1
the dog VPVP!Vi0.4
the dog ViVi!laughs0.5
the dog laughs
DERIVATION RULES USED PROBABILITY
SS!NP VP1.0
NP VPNP!DT NN0.3
DT NN VPDT!the1.0
the NN VPNN!dog0.1
the dog VPVP!Vi0.4
the dog ViVi!laughs0.5
the dog laughs
Properties of PCFGs
IAssigns a probability to each left-most derivation , or parse-tree,
allowed by the underlying CFGISay we have a sentence s, set of derivations for that sentence is
T(s). Then a PCFG assigns a probability p(t)to each member of
T(s). i.e., we now have a ranking in order of probability .
IThe most likely parse tree for a sentence sis
arg max
t2T(s)p(t)
Properties of PCFGs
IAssigns a probability to each left-most derivation , or parse-tree,
allowed by the underlying CFG
ISay we have a sentence s, set of derivations for that sentence is
T(s). Then a PCFG assigns a probability p(t)to each member of
T(s). i.e., we now have a ranking in order of probability .IThe most likely parse tree for a sentence sis
arg max
t2T(s)p(t)
Properties of PCFGs
IAssigns a probability to each left-most derivation , or parse-tree,
allowed by the underlying CFG
ISay we have a sentence s, set of derivations for that sentence is
T(s). Then a PCFG assigns a probability p(t)to each member of
T(s). i.e., we now have a ranking in order of probability .
IThe most likely parse tree for a sentence sis
arg max
t2T(s)p(t)
Data for Parsing Experiments: Treebanks
IPenn WSJ Treebank = 50,000 sentences with associated trees
IUsual set-up: 40,000 training sentences, 2400 test sentences
An example tree:
CanadianNNP
UtilitiesNNPSNP
hadVBD
1988CD
revenueNNNP
ofIN
C$$
1.16CD
billionCD
,PUNC,QPNPPPNP
mainlyRBADVP
fromIN
itsPRP$
naturalJJ
gasNN
andCC
electricJJ
utilityNN
businessesNNSNP
inIN
AlbertaNNP
,PUNC,NP
whereWRBWHADVP
theDT
companyNNNP
servesVBZ
aboutRB
800,000CDQP
customersNNS
.PUNC.NPVPSSBARNPPPNPPPVPSTOP
Canadian Utilities had 1988 revenue of C$ 1.16 billion ,
mainly from its natural gas and electric utility businesses in
Alberta , where the company serves about 800,000
customers .
Deriving a PCFG from a Treebank
IGiven a set of example trees (a treebank), the underlying
CFG can simply be all rules seen in the corpus
IMaximum Likelihood estimates:
qML(!) =Count (!)
Count ()
where the counts are taken from a training set of example
trees.
IIf the training data is generated by a PCFG , then as the
training data size goes to innity, the maximum-likelihood
PCFG will converge to the same distribution as the \true"
PCFG.
PCFGs
Booth and Thompson (1973) showed that a CFG with rule
probabilities correctly denes a distribution over the set of
derivations provided that:
1. The rule probabilities dene conditional distributions over the
dierent ways of rewriting each non-terminal.
2. A technical condition on the rule probabilities ensuring that
the probability of the derivation terminating in a nite
number of steps is 1. (This condition is not really a practical
concern.)
Parsing with a PCFG
IGiven a PCFG and a sentence s, deneT(s)to be the set of
trees withsas the yield.
IGiven a PCFG and a sentence s, how do we nd
arg max
t2T(s)p(t)
Chomsky Normal Form
A context free grammar G= (N;;R;S )in Chomsky
Normal Form is as follows
INis a set of non-terminal symbols
Iis a set of terminal symbols
IRis a set of rules which take one of two forms:
IX!Y1Y2forX2N, andY1;Y22N
IX!YforX2N, andY2
IS2Nis a distinguished start symbol
A Dynamic Programming Algorithm
IGiven a PCFG and a sentence s, how do we nd
max
t2T(s)p(t)
INotation:
n=number of words in the sentence
wi=i'th word in the sentence
N=the set of non-terminals in the grammar
S=the start symbol in the grammar
IDene a dynamic programming table
[i;j;X ] = maximum probability of a constituent with non-terminal X
spanning words i:::j inclusive
IOur goal is to calculate max t2T(s)p(t) =[1;n;S ]
An Example
the dog saw the man with the telescope
A Dynamic Programming Algorithm
IBase case denition: for all i= 1:::n , forX2N
[i;i;X ] =q(X!wi)
(note: dene q(X!wi) = 0 ifX!wiis not in the
grammar)
IRecursive denition: for all i= 1:::n ,j= (i+ 1):::n ,
X2N,
(i;j;X ) = max
X!Y Z2R;
s2fi:::(j 1)g(q(X!YZ)(i;s;Y )(s+ 1;j;Z))
An Example
(i;j;X ) = max
X!Y Z2R;
s2fi:::(j 1)g(q(X!YZ)(i;s;Y )(s+ 1;j;Z))
the dog saw the man with the telescope
The Full Dynamic Programming Algorithm
Input: a sentences=x1:::x n, a PCFGG= (N;;S;R;q ).
Initialization:
For alli2f1:::ng, for allX2N,
(i;i;X ) =q(X!xi)ifX!xi2R
0 otherwise
Algorithm:
IForl= 1:::(n 1)
IFori= 1:::(n l)
ISetj=i+l
IFor allX2N, calculate
(i;j;X ) = max
X!Y Z2R;
s2fi:::(j 1)g(q(X!YZ)(i;s;Y )(s+ 1;j;Z))
and
bp(i;j;X ) = arg max
X!Y Z2R;
s2fi:::(j 1)g(q(X!YZ)(i;s;Y )(s+ 1;j;Z))
Output: Return(1;n;S ) = max t2T(s)p(t), and backpointers bp
which allow recovery of arg max t2T(s)p(t).
A Dynamic Programming Algorithm for the Sum
IGiven a PCFG and a sentence s, how do we ndX
t2T(s)p(t)
INotation:
n=number of words in the sentence
wi=i'th word in the sentence
N=the set of non-terminals in the grammar
S=the start symbol in the grammar
IDene a dynamic programming table
[i;j;X ] = sum of probabilities for constituent with non-terminal X
spanning words i:::j inclusive
IOur goal is to calculateP
t2T(s)p(t) =[1;n;S ]
Summary
IPCFGs augments CFGs by including a probability for each
rule in the grammar.
IThe probability for a parse tree is the product of probabilities
for the rules in the tree
ITo build a PCFG-parsed parser:
1. Learn a PCFG from a treebank
2. Given a test data sentence, use the CKY algorithm to
compute the highest probability tree for the sentence under
the PCFG
Recurrent Networks, and LSTMs, for NLP
Michael Collins, Columbia University
Representing Sequences
IOften we want to map some sequence x[1:n]=x1:::xnto a
labelyor a distribution p(yjx[1:n])
IExamples:
ILanguage modeling: x[1:n]is rstnwords in a document, y
is the (n+ 1) 'th word
ISentiment analysis: x[1:n]is a sentence (or document), yis
label indicating whether the sentence is
positive/neutral/negative about a particular topic (e.g., a
particular restaurant)
IMachine translation: x[1:n]is a source-language sentence, y
is a target language sentence (or the rst word in the target
language sentence)
Representing Sequences (continued)
ISlightly more generally: map a sequence x[1:n]and a
positioni2f1:::ngto a labelyor a distribution
p(yjx[1:n];i)
IExamples:
ITagging:x[1:n]is a sentence, iis a position in the sentence,
yis the tag for position i
IDependency parsing: x[1:n]is a sentence, iis a position in
the sentence, y2f1:::ng;y6=iis the head for word xiin
the dependency parse
A Simple Recurrent Network
Inputs: A sequence x1:::xnwhere each xj2Rd. A label
y2f1:::Kg. An integer mdening size of hidden dimension.
Parameters Whh2Rmm,Whx2Rmd,bh2Rm,h02Rm,
V2RKm,
2RK. Transfer function g:Rm!Rm.
Denitions:
=fWhh;Whx;bh;h0g
R(x(t);h(t 1);) =g(Whxx(t)+Whhh(t 1)+bh)
Computational Graph:
IFort= 1:::n
Ih(t)=R(x(t);h(t 1);)
Il=Vh(n)+
; q =LS(l),o= qy
The Computational Graph
A Problem in Training: Exploding and Vanishing
Gradients
ICalculation of gradients involves multiplication of long chains
of Jacobians
IThis leads to exploding and vanishing gradients
LSTMs (Long Short-Term Memory units)
IOld denitions of the recurrent update:
=fWhh;Whx;bh;h0g
R(x(t);h(t 1);) =g(Whxx(t)+Whhh(t 1)+bh)
ILSTMs give an alternative denition of R(x(t);h(t 1);).
Denition of Sigmoid Function, Element-Wise
Product
IGiven any integer d1,d:Rd!Rdis the function that
maps a vector vto a vectord(v)such that for i= 1:::d ,
d
i(v) =evi
1 +evi
IGiven vectors a2Rdandb2Rd,c=abhas components
ci=aibi
fori= 1:::d
LSTM Equations (from Ilya Sutskever, PhD thesis)
Maintainst;~st;htas hidden state at position t.stismemory ,
intuitively allows long-term memory. The function
st;~st;ht=LSTM (xt;st 1;~st 1;ht 1;)is dened as:
ut=CONCAT (ht 1;xt;~st 1)
ht=g(Whut+bh)(hidden state)
it=g(Wiut+bi) (\input")
t=(Wut+b)(\input gate")
ot=(Wout+bo)(\output gate")
ft=(Wfut+bf)(\forget gate")
st=st 1ft+ittforget and input gates control update of memory
~st=stotoutput gate controls information that can leave the unit
An LSTM-based Recurrent Network
Inputs: A sequence x1:::xnwhere each xj2Rd. A label
y2f1:::Kg.
Computational Graph:
Ih(0);s(0);~s(0)are set to some inital values.
IFort= 1:::n
Is(t);~s(t);h(t)=LSTM (x(t);s(t 1);~s(t 1);h(t 1);)
Il=Vlhh(n)+Vls~s(n)+
; q =LS(l),o= qy
The Computational Graph
An LSTM-based Recurrent Network for Tagging
Inputs: A sequence x1:::xnwhere each xj2Rd. A sequence
y1:::ynof tags.
Computational Graph:
Ih(0);s(0);~s(0)are set to some inital values.
IFort= 1:::n
Is(t);~s(t);h(t)=LSTM (x(t);s(t 1);~s(t 1);h(t 1);)
IFort= 1:::n
Ilt=VCONCAT (h(t);~s(t)) +
; qt=LS(lt),ot= qyt
Io=Pn
t=1ot
The Computational Graph
A bi-directional LSTM (bi-LSTM) for tagging
Inputs: A sequence x1:::xnwhere each xj2Rd. A sequence
y1:::ynof tags.
Denitions: FandBare parameters of a forward and backward
LSTM.
Computational Graph:
Ih(0);s(0);~s(0);(n+1);(n+1);~(n+1)are set to some inital values.
IFort= 1:::n
Is(t);~s(t);h(t)=LSTM (x(t);s(t 1);~s(t 1);h(t 1);F)
IFort=n::: 1
I(t);~(t);(t)=LSTM (x(t);(t+1);~(t+1);(t+1);B)
IFort= 1:::n
Ilt=VCONCAT (h(t);~s(t);(t);~t) +
; qt=LS(lt),
ot= qyt
Io=Pn
t=1ot
The Computational Graph
Results on Language Modeling
IResults from One Billion Word Benchmark for Measuring
Progress in Statistical Language Modeling , Ciprian Chelba,
Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants.
Results on Dependency Parsing
IDeep Biane Attention for Neural Dependency Parsing ,
Dozat and Manning.
IUses a bidirectional LSTM to represent each word
IUses LSTM representations to predict head for each word in
the sentence
IUnlabeled dependency accuracy: 95.75%
Conclusions
IRecurrent units map input sequences x1:::xnto
representations h1:::hn. The vector hncan be used to
predict a label for the entire sentence. Each vector hifor
i= 1:::n can be used to make a prediction for position i
ILSTMs are recurrent units that make use of more involved
recurrent updates. They maintain a \memory" state.
Empirically they perform extremely well
IBi-directional LSTMs allow representation of both the
information before and after a position iin the sentence
IMany applications: language modeling, tagging, parsing,
speech recognition, we will soon see machine translation
Log-Linear Models
Michael Collins, Columbia University
The Language Modeling Problem
Iwiis thei'th word in a document
IEstimate a distribution p(wijw1;w2;:::wi 1)given previous
\history"w1;:::;wi 1.
IE.g.,w1;:::;wi 1=
Third, the notion \grammatical in English" cannot be
identied in any way with the notion \high order of
statistical approximation to English". It is fair to assume
that neither sentence (1) nor (2) (nor indeed any part of
these sentences) has ever occurred in an English
discourse. Hence, in any statistical
Trigram Models
IEstimate a distribution p(wijw1;w2;:::wi 1)given previous
\history"w1;:::;wi 1=
Third, the notion \grammatical in English" cannot be identied in any way
with the notion \high order of statistical approximation to English". It is fair
to assume that neither sentence (1) nor (2) (nor indeed any part of these
sentences) has ever occurred in an English discourse. Hence, in any statistical
ITrigram estimates:
q(modeljw1;:::wi 1) =1qML(modeljwi 2=any;wi 1=statistical ) +
2qML(modeljwi 1=statistical ) +
3qML(model )
wherei0,P
ii= 1,qML(yjx) =Count (x;y)
Count (x)
Trigram Models
q(modeljw1;:::wi 1) =1qML(modeljwi 2=any;wi 1=statistical ) +
2qML(modeljwi 1=statistical ) +
3qML(model )
IMakes use of only bigram, trigram, unigram estimates
IMany other \features" of w1;:::;wi 1may be useful, e.g.,:
qML(modeljwi 2=any)
qML(modeljwi 1is an adjective )
qML(modeljwi 1ends in \ical" )
qML(modeljauthor =Chomsky )
qML(modelj\model" does not occur somewhere in w1;:::wi 1)
qML(modelj\grammatical" occurs somewhere in w1;:::wi 1)
A Naive Approach
q(modeljw1;:::wi 1) =
1qML(modeljwi 2=any;wi 1=statistical ) +
2qML(modeljwi 1=statistical ) +
3qML(model ) +
4qML(modeljwi 2=any) +
5qML(modeljwi 1is an adjective ) +
6qML(modeljwi 1ends in \ical" ) +
7qML(modeljauthor =Chomsky ) +
8qML(modelj\model" does not occur somewhere in w1;:::wi 1) +
9qML(modelj\grammatical" occurs somewhere in w1;:::wi 1)
This quickly becomes very unwieldy...
A Second Example: Part-of-Speech Tagging
INPUT:
Prots soared at Boeing Co., easily topping forecasts on Wall Street,
as their CEO Alan Mulally announced rst quarter results.
OUTPUT:
Prots/N soared/V at/P Boeing/N Co./N ,/, easily/ADV topping/V
forecasts/N on/P Wall/N Street/N ,/, as/P their/POSS CEO/N
Alan/N Mulally/N announced/V rst/ADJ quarter/N results/N ./.
N = Noun
V = Verb
P = Preposition
Adv = Adverb
Adj = Adjective
: : :
A Second Example: Part-of-Speech Tagging
Hispaniola/NNP quickly/RB became/VB an/DT important/JJ
base/?? from which Spain expanded its empire into the rest of the
Western Hemisphere .
There are many possible tags in the position ??
fNN, NNS, Vt, Vi, IN, DT, . . . g
The task: model the distribution
p(tijt1;:::;ti 1;w1:::wn)
wheretiis thei'th tag in the sequence, wiis thei'th word
A Second Example: Part-of-Speech Tagging
Hispaniola/NNP quickly/RB became/VB an/DT important/JJ
base/?? from which Spain expanded its empire into the rest of the
Western Hemisphere .
The task: model the distribution
p(tijt1; : : : ; ti 1; w1: : : wn)
where tiis the i'th tag in the sequence, wiis the i'th word
Again: many \features" of t1; : : : ; ti 1; w1: : : wnmay be relevant
qML(NNjwi=base)
qML(NNjti 1isJJ)
qML(NNjwiends in \e" )
qML(NNjwiends in \se" )
qML(NNjwi 1is \important" )
qML(NNjwi+1is \from" )
Overview
ILog-linear models
IThe maximum-entropy property
ISmoothing, feature selection etc. in log-linear models
The General Problem
IWe have some input domainX
IHave a nite label setY
IAim is to provide a conditional probability p(yjx)
for anyx;y wherex2X ,y2Y
Language Modeling
Ixis a \history" w1;w2;:::wi 1, e.g.,
Third, the notion \grammatical in English" cannot be identied in any
way with the notion \high order of statistical approximation to English".
It is fair to assume that neither sentence (1) nor (2) (nor indeed any
part of these sentences) has ever occurred in an English discourse.
Hence, in any statistical
Iyis an \outcome" wi
Feature Vector Representations
IAim is to provide a conditional probability p(yjx)for
\decision"ygiven \history" x
IAfeature is a function fk(x;y)2R
(Often binary features orindicator functions
f(x;y)2f0;1g).
ISay we have mfeaturesfkfork= 1:::m
)Afeature vector f(x;y)2Rmfor anyx;y
Language Modeling
Ixis a \history" w1;w2;:::wi 1, e.g.,
Third, the notion \grammatical in English" cannot be identied in any
way with the notion \high order of statistical approximation to English".
It is fair to assume that neither sentence (1) nor (2) (nor indeed any
part of these sentences) has ever occurred in an English discourse.
Hence, in any statistical
Iyis an \outcome" wi
IExample features:
f1(x; y) =
1ify=model
0otherwise
f2(x; y) =1ify=model andwi 1=statistical
0otherwise
f3(x; y) =1ify=model ,wi 2=any,wi 1=statistical
0otherwise
f4(x;y) =1ify=model ,wi 2=any
0otherwise
f5(x;y) =1ify=model ,wi 1is an adjective
0otherwise
f6(x;y) =1ify=model ,wi 1ends in \ical"
0otherwise
f7(x;y) =1ify=model , author = Chomsky
0otherwise
f8(x;y) =1ify=model , \model" is not in w1;:::wi 1
0otherwise
f9(x;y) =1ify=model , \grammatical" is in w1;:::wi 1
0otherwise
Dening Features in Practice
IWe had the following \trigram" feature:
f3(x;y) =1ify=model ,wi 2=any,wi 1=statistical
0otherwise
IIn practice, we would probably introduce one trigram feature
for every trigram seen in the training data: i.e., for all
trigrams (u;v;w )seen in training data, create a feature
fN(u;v;w )(x;y) =1ify=w,wi 2=u,wi 1=v
0otherwise
whereN(u;v;w )is a function that maps each (u;v;w )
trigram to a dierent integer
The POS-Tagging Example
IEachxis a \history" of the form ht1;t2;:::;ti 1;w1:::wn;ii
IEachyis a POS tag, such as NN, NNS, Vt, Vi, IN, DT, . . .
IWe havemfeaturesfk(x;y)fork= 1:::m
For example:
f1(x;y) =1if current word wiisbase andy=Vt
0otherwise
f2(x;y) =1if current word wiends in ing andy=VBG
0otherwise
:::
The Full Set of Features in Ratnaparkhi, 1996
IWord/tag features for all word/tag pairs, e.g.,
f100(x;y) =1if current word wiisbase andy=Vt
0otherwise
ISpelling features for all prexes/suxes of length 4, e.g.,
f101(x;y) =1if current word wiends in ing andy=VBG
0otherwise
f102(h;t) =1if current word wistarts with pre andy=NN
0otherwise
The Full Set of Features in Ratnaparkhi, 1996
IContextual Features, e.g.,
f103(x;y) =1ifhti 2;ti 1;yi=hDT, JJ, Vti
0otherwise
f104(x;y) =1ifhti 1;yi=hJJ, Vti
0otherwise
f105(x;y) =1ifhyi=hVti
0otherwise
f106(x;y) =1if previous word wi 1=theandy=Vt
0otherwise
f107(x;y) =1if next word wi+1=theandy=Vt
0otherwise
The Final Result
IWe can come up with practically any questions ( features )
regarding history/tag pairs.
IFor a given history x2X , each label inYis mapped to a
dierent feature vector
f(hJJ, DT,hHispaniola, . . .i, 6i;Vt) = 1001011001001100110
f(hJJ, DT,hHispaniola, . . .i, 6i;JJ) = 0110010101011110010
f(hJJ, DT,hHispaniola, . . .i, 6i;NN) = 0001111101001100100
f(hJJ, DT,hHispaniola, . . .i, 6i;IN) = 0001011011000000010
:::
Parameter Vectors
IGiven features fk(x;y)fork= 1:::m ,
also dene a parameter vector v2Rm
IEach (x;y)pair is then mapped to a \score"
vf(x;y) =X
kvkfk(x;y)
Language Modeling
Ixis a \history" w1;w2;:::wi 1, e.g.,
Third, the notion \grammatical in English" cannot be identied in any
way with the notion \high order of statistical approximation to English".
It is fair to assume that neither sentence (1) nor (2) (nor indeed any
part of these sentences) has ever occurred in an English discourse.
Hence, in any statistical
IEach possible ygets a dierent score:
vf(x;model ) = 5:6vf(x;the ) = 3:2
vf(x;is) = 1:5vf(x;of ) = 1:3
vf(x;models ) = 4:5:::
Log-Linear Models
IWe have some input domain X, and a nite label set Y. Aim is
to provide a conditional probability p(yjx)for any x2X and
y2Y.
IA feature is a function f:XY! R
(Often binary features or indicator functions
fk:XY!f 0;1g).
ISay we have mfeaturesfkfork= 1:::m
)A feature vector f(x;y)2Rmfor anyx2X andy2Y.
IWe also have a parameter vector v2Rm
IWe dene
p(yjx;v) =evf(x;y)
P
y02Yevf(x;y0)
Why the name?
logp(yjx;v) =vf(x;y)|{z}
Linear term logX
y02Yevf(x;y0)
|{z}
Normalization term
Maximum-Likelihood Estimation
IMaximum-likelihood estimates given training sample (xi;yi)
fori= 1:::n , each (xi;yi)2XY :
vML = argmaxv2RmL(v)
where
L(v) =nX
i=1logp(yijxi;v) =nX
i=1vf(xi;yi) nX
i=1logX
y02Yevf(xi;y0)
Calculating the Maximum-Likelihood Estimates
INeed to maximize:
L(v) =nX
i=1vf(xi; yi) nX
i=1logX
y02Yevf(xi;y0)
ICalculating gradients:
dL(v)
dvk=nX
i=1fk(xi;yi) nX
i=1P
y02Yfk(xi;y0)evf(xi;y0)
P
z02Yevf(xi;z0)
=nX
i=1fk(xi;yi) nX
i=1X
y02Yfk(xi;y0)evf(xi;y0)
P
z02Yevf(xi;z0)
=nX
i=1fk(xi;yi)
|{z}
Empirical counts nX
i=1X
y02Yfk(xi;y0)p(y0jxi;v)
|{z}
Expected counts
Gradient Ascent Methods
INeed to maximize L(v)where
dL(v)
dv=nX
i=1f(xi;yi) nX
i=1X
y02Yf(xi;y0)p(y0jxi;v)
Initialization: v= 0
Iterate until convergence:
ICalculate  =dL(v)
dv
ICalculate= argmaxL(v+)(Line
Search)
ISetv v+
Conjugate Gradient Methods
I(Vanilla) gradient ascent can be very slow
IConjugate gradient methods require calculation of gradient at
each iteration, but do a line search in a direction which is a
function of the current gradient, and the previous step
taken .
IConjugate gradient packages are widely available
In general: they require a function
calc gradient (v)!
L(v);dL(v)
dv
and that's about it!
Overview
ILog-linear models
IThe maximum-entropy property
ISmoothing, feature selection etc. in log-linear models
Overview
ILog-linear models
IThe maximum-entropy property
ISmoothing, feature selection etc. in log-linear models
Smoothing in Maximum Entropy Models
ISay we have a feature:
f100(h;t) =1if current word wiisbase andt=Vt
0otherwise
IIn training data, base is seen 3 times, with Vtevery time
IMaximum likelihood solution satises
X
if100(xi;yi) =X
iX
yp(yjxi;v)f100(xi;y)
)p(Vtjxi;v) = 1 for any history xiwherewi=base
)v100!1 at maximum-likelihood solution (most likely)
)p(Vtjx;v) = 1 for any test data history xwherew=base
Regularization
IModied loss function
L(v) =nX
i=1vf(xi;yi) nX
i=1logX
y02Yevf(xi;y0) 
2mX
k=1v2
k
ICalculating gradients:
dL(v)
dvk=nX
i=1fk(xi;yi)
|{z}
Empirical counts nX
i=1X
y02Yfk(xi;y0)p(y0jxi;v)
|{z}
Expected counts vk
ICan run conjugate gradient methods as before
IAdds a penalty for large weights
Experiments with Gaussian Priors
I[Chen and Rosenfeld, 1998 ]: apply log-linear models to
language modeling: Estimate q(wijwi 2;wi 1)
IUnigram, bigram, trigram features, e.g.,
f1(wi 2;wi 1;wi) =1if trigram is (the,dog,laughs)
0otherwise
f2(wi 2;wi 1;wi) =1if bigram is (dog,laughs)
0otherwise
f3(wi 2;wi 1;wi) =1if unigram is (laughs)
0otherwise
q(wijwi 2;wi 1) =ef(wi 2;wi 1;wi)v
P
wef(wi 2;wi 1;w)v
Experiments with Gaussian Priors
IIn regular (unregularized) log-linear models, if all n-gram
features are included, then it's equivalent to
maximum-likelihood estimates!
q(wijwi 2;wi 1) =Count (wi 2;wi 1;wi)
Count (wi 2;wi 1)
I[Chen and Rosenfeld, 1998 ]: with gaussian priors, get very
good results. Performs as well as or better than standardly
used \discounting methods" (see lecture 2).
IDownside: computingP
wef(wi 2;wi 1;w)vis SLOW.
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  January 14, 2015  
Today: • The Big Picture • Overfitting • Review: probability Readings: Decision trees, overfiting • Mitchell, Chapter 3 Probability review • Bishop Ch. 1 thru 1.2.3 • Bishop, Ch. 2 thru 2.2 • Andrew Moore’s online tutorial 
Function Approximation:   Problem Setting: • Set of possible instances X  • Unknown target function f : XàY• Set of function hypotheses H={ h | h : XàY }Input: • Training examples {<x(i),y(i)>} of unknown target function fOutput: • Hypothesis h ∈ H that best approximates target function f
Function Approximation: Decision Tree Learning Problem Setting: • Set of possible instances X – each instance x in X is a feature vector  x = < x1, x2 … xn> • Unknown target function f : XàY– Y is discrete valued • Set of function hypotheses H={ h | h : XàY }– each hypothesis h is a decision tree Input: • Training examples {<x(i),y(i)>} of unknown target function fOutput: • Hypothesis h ∈ H that best approximates target function f
Information Gain (also called mutual information) between input attribute A and target variable Y  Information Gain is the expected reduction in entropy of target variable Y for data sample S, due to sorting on variable A   
Function approximation as Search for the best hypothesis • ID3 performs heuristic search through space of decision trees 

Function Approximation: The Big Picture 
Which Tree Should We Output? • ID3 performs heuristic search through space of decision trees • It stops at smallest acceptable tree. Why? 
Occam’s razor: prefer the simplest hypothesis that fits the data 
Why Prefer Short Hypotheses? (Occam’s Razor) Arguments in favor:      Arguments opposed:  
Why Prefer Short Hypotheses? (Occam’s Razor) Argument in favor: • Fewer short hypotheses than long ones à a short hypothesis that fits the data is less likely to be a statistical coincidence   Argument opposed: • Also fewer hypotheses containing a prime number of nodes and attributes beginning with “Z” • What’s so special about “short” hypotheses, instead of “prime number of nodes and edges”? 

Overfitting Consider a hypothesis h and its • Error rate over training data: • True error rate over all data:  

Overfitting Consider a hypothesis h and its • Error rate over training data: • True error rate over all data:   We say h overfits the training data if   Amount of overfitting =  



Split data into training and validation setCreate tree that classiﬁes training set correctly

Decision Tree Learning, Formal Guarantees  
   Labeled Examples   Supervised Learning or Function Approximation  Learning Algorithm  Expert / Oracle  Data Source  
Alg.outputs  Distribution D on X 
c* : X ! Y (x1,c*(x1)),…, (xm,c*(xm)) h : X ! Y x1 > 5 x6 > 2 +1 -1 +1 
   Labeled Examples   Learning Algorithm  Expert/Oracle  Data Source  
Alg.outputs  c* : X ! Y h : X ! Y (x1,c*(x1)),…, (xm,c*(xm)) • Algo sees training sample S: (x1,c*(x1)),…, (xm,c*(xm)), xi i.i.d. from D Distribution D on X 
  err(h)=Prx 2 D(h(x) ≠ c*(x)) •    Does optimization over S, finds hypothesis h (e.g., a decision tree). •    Goal:  h has small error over D. Supervised Learning or Function Approximation  
Two Core Aspects of Machine Learning  Algorithm Design. How to optimize? Automatically generate rules that do well on observed data. Confidence Bounds, Generalization Confidence for rule effectiveness on future data. Computation 
• Very well understood: Occam’s bound, VC theory, etc. (Labeled) Data • Decision trees: if we were able to find a small decision tree that explains data well, then good generalization guarantees.  • NP-hard [Hyafil-Rivest’76]   
Top Down Decision Trees Algorithms  • Decision trees: if we were able to find a small decision tree consistent with the data, then good generalization guarantees.  • NP-hard [Hyafil-Rivest’76]   • Very nice practical heuristics;  top down algorithms, e.g, ID3 • Natural greedy approaches where we grow the tree from the root to the leaves by repeatedly replacing an existing leaf with an internal node. • Key point: splitting criterion. • ID3: split the leaf that decreases the entropy the most. • Why not split according to error rate --- this is what we care about after all? • There are examples where we can get stuck in local minima!!! 
0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 Initial error rate is 1/4  (25% positive, 75% negative) Error rate after split is   (left leaf is 100% negative; right leaf is 50/50) Overall error doesn’t decrease! Entropy as a better splitting measure  
0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 Initial entropy is  Entropy after split is  Entropy decreases! Entropy as a better splitting measure  
• Natural greedy approaches where we grow the tree from the root to the leaves by repeatedly replacing an existing leaf with an internal node. • Key point: splitting criterion. • ID3: split the leaf that decreases the entropy the most. • Why not split according to error rate --- this is what we care about after all? • There are examples where you can get stuck!!! Top Down Decision Trees Algorithms  
• [Kearns-Mansour’96]: if measure of progress is entropy, we can always guarantees success under some formal relationships between the class of splits and the target (the class of splits can weakly approximate the target function).   • Provides a way to think about the effectiveness of various top down algos. 
Top Down Decision Trees Algorithms  • Key: strong concavity of the splitting crieterion h Pr[c*=1]=q 
Pr[c*=1| h=0]=p Pr[c*=1| h=1]=r 0 1 Pr[h=0]=u Pr[h=1]=1-u v v1 v2 • q=up + (1-u) r. p q r Want to lower bound: G(q) – [uG(p) + (1-u)G(r)] • If: G(q) =min(q,1-q) (error rate), then G(q) = uG(p) + (1-u)G(r)  • If: G(q) =H(q) (entropy), then G(q) – [uG(p) + (1-u)G(r)] >0 if r-p> 0 and u ≠1, u ≠0 (this happens under the weak learning assumption)
Two Core Aspects of Machine Learning  Algorithm Design. How to optimize? Automatically generate rules that do well on observed data. Confidence Bounds, Generalization Confidence for rule effectiveness on future data. Computation (Labeled) Data 
What you should know: • Well posed function approximation problems: – Instance space, X – Sample of labeled training data { <x(i), y(i)>} – Hypothesis space, H = { f: XàY } • Learning is a search/optimization problem over H – Various objective functions • minimize training error (0-1 loss)  • among hypotheses that minimize training error, select smallest (?) – But inductive learning without some bias is futile ! • Decision tree learning – Greedy top-down learning of decision trees (ID3, C4.5, ...) – Overfitting and tree post-pruning – Extensions… 
Extra slides extensions to decision tree learning  

  



Questions to think about (1) • ID3 and C4.5 are heuristic algorithms that search through the space of decision trees.  Why not just do an exhaustive search? 
Questions to think about (2) • Consider target function f: <x1,x2> à y, where x1 and x2 are real-valued, y is boolean.  What is the set of decision surfaces describable with decision trees that use each attribute at most once? 
Questions to think about (3) • Why use Information Gain to select attributes in decision trees?  What other criteria seem reasonable, and what are the tradeoffs in making this choice?   
Questions to think about (4) • What is the relationship between learning decision trees, and learning IF-THEN rules 

Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  January 14, 2015  
Today: • Review: probability Readings:  Probability review • Bishop Ch. 1 thru 1.2.3 • Bishop, Ch. 2 thru 2.2 • Andrew Moore’s online tutorial many of these slides are derived from William Cohen, Andrew Moore, Aarti Singh, Eric Xing. Thanks! 
Probability Overview • Events  – discrete random variables, continuous random variables, compound events • Axioms of probability – What defines a reasonable theory of uncertainty • Independent events • Conditional probabilities • Bayes rule and beliefs • Joint probability distribution • Expectations • Independence, Conditional independence 
Random Variables • Informally, A is a random variable if – A denotes something about which we are uncertain – perhaps the outcome of a randomized experiment  • Examples A = True if a randomly drawn person from our class is female A = The hometown of a randomly drawn person from our class A = True if two randomly drawn persons from our class have same birthday  • Define P(A) as “the fraction of possible worlds in which A is true” or       “the fraction of times A holds, in repeated runs of the random experiment” – the set of possible worlds is called the sample space, S – A random variable A is a function defined over S                         A: S à {0,1}  
A little formalism More formally, we have • a sample space S (e.g., set of students in our class) – aka the set of possible worlds • a random variable is a function defined over the sample space – Gender: S à { m, f } – Height: S à Reals • an event is a subset of S – e.g., the subset of S for which Gender=f – e.g., the subset of S for which (Gender=m) AND (eyeColor=blue) • we’re often interested in probabilities of specific events • and of specific events conditioned on other specific events  
Visualizing A Sample space of all possible worlds Its area is 1             Worlds in which A is False Worlds in which A is true P(A) = Area of reddish oval 
The Axioms of Probability • 0 <= P(A) <= 1 • P(True) = 1 • P(False) = 0 • P(A or B) = P(A) + P(B) - P(A and B)  [di Finetti 1931]:  when gambling based on “uncertainty formalism A” you can be exploited by an opponent  iff  your uncertainty formalism A violates these axioms 
Elementary Probability in Pictures • P(~A) + P(A) = 1  A  ~A 
A useful theorem • 0 <= P(A) <= 1, P(True) = 1, P(False) = 0,     P(A or B) = P(A) + P(B) - P(A and B)  è P(A) = P(A ^ B) + P(A ^ ~B)  A =  [A and (B or ~B)]  =  [(A and B) or (A and ~B)] P(A) = P(A and B) + P(A and ~B) – P((A and B) and (A and ~B)) P(A) = P(A and B) + P(A and ~B) – P(A and B and A and ~B)  
Elementary Probability in Pictures • P(A) = P(A ^ B) + P(A ^ ~B)  B  A ^ ~B A ^ B 
Definition of Conditional Probability                      P(A ^ B)  P(A|B)  =  -----------                     P(B)  A  B  
Definition of Conditional Probability                      P(A ^ B)  P(A|B)  =  -----------                     P(B)  Corollary: The Chain Rule P(A ^ B) = P(A|B) P(B)  
Bayes Rule • let’s write 2 expressions for P(A ^ B)   B  A A ^ B 
P(B|A) * P(A) P(B) P(A|B) = Bayes, Thomas (1763) An essay towards solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London, 53:370-418 
…by no means merely a curious speculation in the doctrine of chances, but necessary to be solved in order to a sure foundation for all our reasonings concerning past facts, and what is likely to be hereafter…. necessary to be considered by any that would give a clear account of the strength of analogical or inductive reasoning… Bayes’ rule we call P(A) the “prior”  and P(A|B) the “posterior” 
Other Forms of Bayes Rule )(~)|~()()|()()|()|(APABPAPABPAPABPBAP+=)()()|()|(XBPXAPXABPXBAP∧∧∧=∧
Applying Bayes Rule P(A|B)=P(B|A)P(A)P(B|A)P(A)+P(B|~A)P(~A)A = you have the flu,   B = you just coughed  Assume: P(A) = 0.05 P(B|A) = 0.80 P(B| ~A) = 0.2  what is P(flu | cough) = P(A|B)? 
what does all this have to do with function approximation? 
The Joint Distribution Recipe for making a joint distribution of M variables: Example: Boolean variables A, B, C 
The Joint Distribution Recipe for making a joint distribution of M variables:  1. Make a truth table listing all combinations of values of your variables (if there are M Boolean variables then the table will have 2M rows). Example: Boolean variables A, B, C A B C 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 
The Joint Distribution Recipe for making a joint distribution of M variables:  1. Make a truth table listing all combinations of values of your variables (if there are M Boolean variables then the table will have 2M rows). 2. For each combination of values, say how probable it is. Example: Boolean variables A, B, C A B C Prob 0 0 0 0.30 0 0 1 0.05 0 1 0 0.10 0 1 1 0.05 1 0 0 0.05 1 0 1 0.10 1 1 0 0.25 1 1 1 0.10 
The Joint Distribution Recipe for making a joint distribution of M variables:  1. Make a truth table listing all combinations of values of your variables (if there are M Boolean variables then the table will have 2M rows). 2. For each combination of values, say how probable it is. 3. If you subscribe to the axioms of probability, those numbers must sum to 1. A B C Prob 0 0 0 0.30 0 0 1 0.05 0 1 0 0.10 0 1 1 0.05 1 0 0 0.05 1 0 1 0.10 1 1 0 0.25 1 1 1 0.10    A B C 0.05 0.25 0.10 0.05 0.05 0.10 0.10 0.30 
Using the Joint Distribution 
One you have the JD you can ask for the probability of any logical expression involving your attribute ∑=EPEP matching rows)row()(
Using the Joint 
P(Poor Male) = 0.4654 ∑=EPEP matching rows)row()(
Using the Joint 
P(Poor) = 0.7604 ∑=EPEP matching rows)row()(
Inference with the Joint 
∑∑=∧=2 2 1 matching rows and matching rows22121)row()row()()()|(EEEPPEPEEPEEP
Inference with the Joint 
∑∑=∧=2 2 1 matching rows and matching rows22121)row()row()()()|(EEEPPEPEEPEEPP(Male | Poor) = 0.4654 / 0.7604 = 0.612   
You should know • Events  – discrete random variables, continuous random variables, compound events • Axioms of probability – What defines a reasonable theory of uncertainty • Conditional probabilities • Chain rule • Bayes rule • Joint distribution over multiple random variables – how to calculate other quantities from the joint distribution 
Expected values Given discrete random variable X, the expected value of  X, written E[X] is     We also can talk about the expected value of functions of X 

Covariance Given two discrete r.v.’s X and Y, we define the  covariance of X and Y as   e.g., X=gender, Y=playsFootball or     X=gender, Y=leftHanded   Remember: 

Linear  Regression
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  8February  13,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Linear  Regression  Readings:Murphy  7.1  –7.3Bishop  3.1HTF  3.1  –3.4Mitchell  4.1-­‐4.3Logistic  Regression  Readings:Murphy  8.1-­‐8.3,  8.6Bishop  4.3.2,  4.3.4HTF  4.1,  4.4Mitchell  –“Generative  …Logistic  Regression”  (Mitchell,  2016)“Maximum  …Gradient  Training”(Elkan,  2014)
Reminders•Homework2:  NaiveBayes–Release:  Wed,  Feb.  1–Due:  Mon,  Feb.  13  at  5:30pm•Homework3:  Linear  /  LogisticRegression–Release:  Mon,  Feb.  13–Due:  Wed,  Feb.  22  at  5:30pm
2
Linear  Regression  Outline•Regression  Problems–Definition–Linear  functions–Residuals–Notation  trick:  fold  in  the  intercept•Linear  Regression  as  Function  Approximation–Objective  function:  Mean  squared  error–Hypothesis  space:  Linear  Functions•Optimization  for  Linear  Regression–Normal  Equations  (Closed-­‐form  solution)•Computational  complexity•Stability–SGD  for  Linear  Regression•Partial  derivatives•Update  rule–Gradient  Descent  for  Linear  Regression•Probabilistic  Interpretation  of  Linear  Regression–Generative  vs.  Discriminative–Conditional  Likelihood–Background:  Gaussian  Distribution–Case  #1:  1D  Linear  Regression–Case  #2:  Multiple  Linear  Regression3
This  Lecture
Last  Lecture
Regression  ProblemsWhiteboard–Definition–Linear  functions–Residuals–Notation  trick:  fold  in  the  intercept
4
Linear  Regression  as  Function  ApproximationWhiteboard–Objective  function:  Mean  squared  error–Hypothesis  space:  Linear  Functions
5
Optimization  for  Linear  RegressionWhiteboard–Normal  Equations  (Closed-­‐form  solution)•Computational  complexity•Stability–SGD  for  Linear  Regression•Partial  derivatives•Update  rule–Gradient  Descent  for  Linear  Regression
6
Probabilistic  Interpretation  of  Linear  RegressionWhiteboard–Generative  vs.  Discriminative–Conditional  Likelihood–Background:  Gaussian  Distribution–Case  #1:  1D  Linear  Regression–Case  #2:  Multiple  Linear  Regression
7
Convergence  Curves
•For  the  batch  method,  the  training  MSE  is  initially  large  due  to  uninformed  initialization•In  the  online  update,  N  updates  for  every  epoch  reduces  MSE  to  a  much  smaller  value.
8©  Eric  Xing  @  CMU,  2006-­‐2011
Machine  Learning  in  Practice  +  k-­‐Nearest  Neighbors
1Intro  Readings:Mitchell  1HTF  1,  2Murphy  1Bishop  1KNN  Readings:Mitchell  8.2HTF  13.3Murphy  -­‐-­‐-­‐Bishop  2.5.210-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  2January  23,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University

Reminders•BackgroundTest–Tue,  Jan.  24  at  6:30pm–**Yourtest  locationdependsonyourregistrationstatus  –seePiazza  fordetails•BackgroundExercises(Homework1)–Released:  Tue,  Jan.  24  afterthetest–Due:  Mon,  Jan.  30  at  5:30pm
2
Machine  Learning  &  EthicsWhat  ethical  responsibilities  do  we  have  as  machine  learning  experts?
3
If  our  search  results  for  news  are  optimized  for  ad  revenue,  might  they  reflect  gender  /  racial  /  socio-­‐economic  biases?Should  restrictions  be  placed  on  intelligent  agents  that  are  capable  of  interacting  with  the  world?
How  do  autonomous  vehicles  make  decisions  when  all  of  the  outcomes  are  likely  to  be  negative?http://vizdoom.cs.put.edu.pl/http://bing.com/
http://arstechnica.com/
Some  topics  that  we  won’t  cover  are  probably  deserve  an  entire  course
Outline•Defining  Learning  Problems–Artificial  Intelligence  (AI)–Mitchell’s  definition  of  learning–Example  learning  problems–Data  annotation–The  Machine  Learning  framework•Classification–Binary  classification–2D  examples–Decision  rules  /  hypotheses•k-­‐Nearest  Neighbors  (KNN)–KNN  for  binary  classification–Distance  functions–Special  cases–Choosing  k4Covered  Next  Lecture
DEFINING  LEARNING  PROBLEMSThis  section  is  based  on  Chapter  1  of  (Mitchell,  1997)
5
Artificial  IntelligenceThe  basic  goal  of  AI  is  to  develop  intelligent  machines.  This  consists  of  many  sub-­‐goals:•Perception•Reasoning•Control  /  Motion  /  Manipulation•Planning•Communication•Creativity•Learning6
Artificial  Intelligence
Machine  Learning
Amazon  Gohttps://www.amazon.com/b?node=16008589011https://www.youtube.com/watch?v=NrmMk1Myrxc
7
Artificial  Intelligence  (AI):  Example  Tasks:–Identify  objects  in  an  image–Translate  from  one  human  language  to  another–Recognize  speech–Assess  risk  (e.g.  in  loan  application)–Make  decisions  (e.g.  in  loan  application)–Assess  potential  (e.g.  in  admission  decisions)–Categorize  a  complex  situation  (e.g.  medical  diagnosis)–Predict  outcome  (e.g.  medical  prognosis,  stock  prices,  inflation,  temperature)–Predict  events  (default  on  loans,  quitting  school,  war)–Plan  ahead  under  perfect  knowledge  (chess)–Plan  ahead  under  partial  knowledge  (Poker,  Bridge)©  Roni  Rosenfeld,  20168Slide  from  Roni  Rosenfeld
Well-­‐Posed  Learning  ProblemsThree  components:1.Task,  T2.Performance  measure,  P3.Experience,  EMitchell’s  definition  of  learning:A  computer  program  learnsif  its  performance  at  tasks  in  T,  as  measured  by  P,  improves  with  experience  E.9Definition  from  (Mitchell,  1997)
Example  Learning  Problems(historical  perspective)1.  Learning  to  recognize  spoken  words
10“…the SPHINX system (e.g. Lee 1989) learns speaker-specific strategies for recognizing the primitive sounds (phonemes) and words from the observed speech signal…neural network methods…hidden Markov models…”
(Mitchell, 1997)THEN
Source:  https://www.stonetemple.com/great-­‐knowledge-­‐box-­‐showdown/#VoiceStudyResults
NOW
Example  Learning  Problems(historical  perspective)2.  Learning  to  drive  an  autonomous  vehicle
11“…the ALVINN system (Pomerleau1989) has used its learned strategies to drive unassisted at 70 miles per hour for 90 miles on public highways among other cars…”
(Mitchell, 1997)THEN
waymo.comNOW
Example  Learning  Problems(historical  perspective)2.  Learning  to  drive  an  autonomous  vehicle
12“…the ALVINN system (Pomerleau1989) has used its learned strategies to drive unassisted at 70 miles per hour for 90 miles on public highways among other cars…”
(Mitchell, 1997)THEN
https://www.geek.com/wp-­‐content/uploads/2016/03/uber.jpgNOW

Example  Learning  Problems(historical  perspective)3.  Learning  to  beat  the  masters  at  board  games
13“…the world’s top computer program for backgammon, TD-GAMMON (Tesauro, 1992, 1995), learned its strategy by playing over one million practice games against itself…”
(Mitchell, 1997)THENNOW

Example  Learning  Problems3.  Learning  to  beat  the  masters  at  chess1.Task,  T:2.Performance  measure,  P:  3.Experience,  E:  
14
Example  Learning  Problems4.  Learning  to  respond  to  voice  commands  (Siri)1.Task,  T:2.Performance  measure,  P:  3.Experience,  E:
15
Capturing  the  Knowledge  of  Experts
16Solution  #1:  Expert  Systems•Over  20  years  ago,  we  had  rule  based  systems•Ask  the  expert  to1.Obtain  a  PhD  in  Linguistics2.Introspect  about  the  structure  of  their  native  language3.Write  down  the  rules  they  deviseGive me directions to StarbucksIf: “give me directions to X”Then: directions(here, nearest(X))How do I get to Starbucks?If: “how do iget to X”Then: directions(here, nearest(X))Where is the nearest Starbucks?If: “where is the nearest X”Then: directions(here, nearest(X))199020001980
2010

Capturing  the  Knowledge  of  Experts
17Solution  #1:  Expert  Systems•Over  20  years  ago,  we  had  rule  based  systems•Ask  the  expert  to1.Obtain  a  PhD  in  Linguistics2.Introspect  about  the  structure  of  their  native  language3.Write  down  the  rules  they  deviseGive me directions to StarbucksIf: “give me directions to X”Then: directions(here, nearest(X))How do I get to Starbucks?If: “how do iget to X”Then: directions(here, nearest(X))Where is the nearest Starbucks?If: “where is the nearest X”Then: directions(here, nearest(X))
I need directions to StarbucksIf: “I need directions to X”Then: directions(here, nearest(X))
Is there a Starbucks nearby?If: “Is there an X nearby”Then: directions(here, nearest(X))
Starbucks directionsIf: “X directions”Then: directions(here, nearest(X))199020001980
2010

Capturing  the  Knowledge  of  Experts
18Solution  #2:  Annotate  Data  and  Learn•Experts:–Very  good  at  answering  questions  about  specific  cases–Not  very  good  at  telling  HOWthey  do  it•1990s:  So  why  not  just  have  them  tell  you  what  they  do  on  SPECIFIC  CASES  and  then  let  MACHINE  LEARNING  tell  you  how  to  come  to  the  same  decisions  that  they  did199020001980
2010

Capturing  the  Knowledge  of  Experts
19Solution  #2:  Annotate  Data  and  Learn1.Collect  raw  sentences  {x1, …, xn}2.Experts  annotate  their  meaning  {y1, …, yn}x2: Show me the closest Starbucksy2: map(nearest(Starbucks))x3: Send a text to John that I’ll be latey3: txtmsg(John, I’ll be late)x1: How do I get to Starbucks?y1: directions(here,nearest(Starbucks))x4: Set an alarm for seven in the morningy4: setalarm(7:00AM)199020001980
2010

Example  Learning  Problems4.  Learning  to  respond  to  voice  commands  (Siri)1.Task,  T:  predicting  action  from  speech2.Performance  measure,  P:  percent  of  correct  actions  taken  in  user  pilot  study3.Experience,  E:  examples  of  (speech,  action)  pairs
20
The  Machine  Learning  Framework•Formulate  a  task  as  a  mapping  from  input  to  output–Task  examples  will  usually  be  pairs:  (input,  correct_output)•Formulate  performance  as  an  error  measure–or  more  generally,  as  an  objective  function  (aka  Loss  function)•Examples:–Medical  Diagnosis•mapping  input  to  one  of  several  classes/categories  èClassification–Predict  tomorrow’s  Temperature•mapping  input  to  a  number    èRegression–Chance  of  Survival:From  patient  data  to  p(survive  >=  5  years)•mapping  input  to  probability  èDensity  estimation–Driving  recommendation•mapping  input  into  a  plan  èPlanning©  Roni  Rosenfeld,  201621Slide  from  Roni  Rosenfeld
Often,  the  same  task  can  be  formulated  in  more  than  one  way:•Ex.  1:  Loan  applications  –creditworthiness/score  (regression)–probability  of  default  (density  estimation)–loan  decision  (classification)•Ex.  2:  Chess–Nature  of  available  training  examples/experience:•expert  advice  (painful  to  experts)•games  against  experts  (less  painful  but  limited,  and  not  much  control)•experts’  games  (almost  unlimited,  but  only  ”found  data”  –no  control)•games  against  self  (unlimited,  flexible,  but  can  you  learn  this  way?)–Choice  of  target  function:  boardàmovevs.  boardàscore©  Roni  Rosenfeld,  201622Slide  from  Roni  RosenfeldChoices  in  ML  Formulation
How  to  Approach  a  Machine  Learning  Problem1.Consider  your  goal  àdefinition  of  task  T–E.g.  make  good  loan  decisions,  win  chess  competitions,  …2.Consider  the  nature  of  available  (or  potential)  experience  E–How  much  data  can  you  get?    What  would  it  cost  (in  money,  time  or  effort)?3.Choose  type  of  output  O  to  learn–(Numerical?  Category?  Probability?  Plan?)  4.Choose  the  Performance  measure  P(error/loss  function)5.Choose  a  representation  for  the  input  X6.Choose  a  set  of  possible  solutions  H(hypothesis  space)–set  of  functions  h:  X  èO–(often,  by  choosing  a  representation  for  them)7.Choose  or  design  a  learning  algorithm–for  using  examples  (E)  to  converge  on  a  member  of  Hthat  optimizes  P©  Roni  Rosenfeld,  201623Slide  from  Roni  Rosenfeld
CLASSIFICATION
24

Fisher  Iris  DatasetFisher  (1936)  used  150  measurements  of  flowers  from  3  different  species:  Iris  setosa(0),  Iris  virginica(1),  Iris  versicolor(2)  collected  by  Anderson  (1936)
26Full  dataset:  https://en.wikipedia.org/wiki/Iris_flower_data_setSpeciesSepal  LengthSepal  WidthPetal  LengthPetal  Width04.33.01.10.104.93.61.40.105.33.71.50.214.92.43.31.015.72.84.11.316.33.34.71.616.73.05.01.7
Fisher  Iris  Dataset
ClassificationWhiteboard:–Binary  classification–2D  examples–Decision  rules  /  hypotheses
28
K-­‐NEAREST  NEIGHBORS
29
k-­‐Nearest  NeighborsWhiteboard:–KNN  for  binary  classification–Distance  functions
30
Takeaways•Learning  Problems–Defining  a  learning  problem  is  tricky–Formalizing  exposes  the  many  possibilities•k-­‐Nearest  Neighbors–KNN  is  an  extremely  simple  algorithm  for  classification
31
Neural  NetworksandBackpropagation
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  20April  3,  2017Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Neural  Net  Readings:Murphy  -­‐-­‐Bishop  5HTF  11Mitchell  4
Reminders•Homework6:  UnsupervisedLearning–Release:  Wed,  Mar.  22–Due:  Mon,  Apr.  03  at  11:59pm•Homework5(PartII):  Peer  Review–Release:  Wed,  Mar.  29–Due:  Wed,  Apr.  05  at  11:59pm•Peer  Tutoring
2
Expectation:  You  should  spend  at  most  1  hour  on  your  reviews
Neural  Networks  Outline•Logistic  Regression  (Recap)–Data,  Model,  Learning,  Prediction•Neural  Networks–A  Recipe  for  Machine  Learning–Visual  Notation  for  Neural  Networks–Example:  Logistic  Regression  Output  Surface–2-­‐Layer  Neural  Network–3-­‐Layer  Neural  Network•Neural  Net  Architectures–Objective  Functions–Activation  Functions•Backpropagation–Basic  Chain  Rule  (of  calculus)–Chain  Rule  for  Arbitrary  Computation  Graph–Backpropagation  Algorithm–Module-­‐based  Automatic  Differentiation  (Autodiff)3
This  Lecture
Last  Lecture
DECISION  BOUNDARY  EXAMPLES
4
Example  #1:  Diagonal  Band
5
Example  #2:  One  Pocket
6
Example  #3:  Four  Gaussians
7
Example  #4:  Two  Pockets
8
Example  #1:  Diagonal  Band
9
Example  #1:  Diagonal  Band
10
Example  #1:  Diagonal  Band
11
Error  in  slides:  “layers”  should  read  “number  of  hidden  units”All  the  neural  networks  in  this  section  used  1  hidden  layer.

Example  #1:  Diagonal  Band
12
Example  #1:  Diagonal  Band
13
Example  #1:  Diagonal  Band
14
Example  #1:  Diagonal  Band
15
Example  #2:  One  Pocket
16
Example  #2:  One  Pocket
17
Example  #2:  One  Pocket
18
Example  #2:  One  Pocket
19
Example  #2:  One  Pocket
20
Example  #2:  One  Pocket
21
Example  #2:  One  Pocket
22
Example  #2:  One  Pocket
23
Example  #3:  Four  Gaussians
24
Example  #3:  Four  Gaussians
25
Example  #3:  Four  Gaussians
26
Example  #3:  Four  Gaussians
27
Example  #3:  Four  Gaussians
28
Example  #3:  Four  Gaussians
29
Example  #3:  Four  Gaussians
36
Example  #3:  Four  Gaussians
37
Example  #3:  Four  Gaussians
38
Example  #4:  Two  Pockets
39
Example  #4:  Two  Pockets
40
Example  #4:  Two  Pockets
41
Example  #4:  Two  Pockets
42
Example  #4:  Two  Pockets
43
Example  #4:  Two  Pockets
44
Example  #4:  Two  Pockets
45
Example  #4:  Two  Pockets
46
Example  #4:  Two  Pockets
47
ARCHITECTURES
54
Neural  Network  ArchitecturesEven  for  a  basic  Neural  Network,  there  are  many  design  decisions  to  make:1.#  of  hidden  layers  (depth)2.#  of  units  per  hidden  layer  (width)3.Type  of  activation  function  (nonlinearity)4.Form  of  objective  function
55
Activation  Functions
56
……
Output
InputHidden  LayerNeural  Network  with  sigmoid  activation  functions(F)LossJ=12(y y )2(E)Output(sigmoid)y=11+2tT( b)(D)Output(linear)b= Dj=0 jzj(C)Hidden(sigmoid)zj=11+2tT( aj), j(B)Hidden(linear)aj= Mi=0 jixi, j(A)InputGivenxi, i
Activation  Functions
57
……
Output
InputHidden  LayerNeural  Network  with  arbitrary  nonlinear  activation  functions(F)LossJ=12(y y )2(E)Output(nonlinear)y= (b)(D)Output(linear)b= Dj=0 jzj(C)Hidden(nonlinear)zj= (aj), j(B)Hidden(linear)aj= Mi=0 jixi, j(A)InputGivenxi, i
Activation  FunctionsSo  far,  we’ve  assumed  that  the  activation  function  (nonlinearity)  is  always  the  sigmoid  function…
58Sigmoid  /  Logistic  Function
logistic(u)≡11+e−u
Activation  Functions•A  new  change:  modifying  the  nonlinearity–The  logistic  is  not  widely  used  in  modern  ANNs
Alternate  1:  tanhLike  logistic  function  but  shifted  to  range  [-­‐1,  +1]
Slide  from  William  Cohen

AI  Stats  2010
sigmoid  vs.  tanhdepth  4?
Figure  from  Glorot&  Bentio(2010)
Activation  Functions•A  new  change:  modifying  the  nonlinearity–reLUoften  used  in  vision  tasksAlternate  2:  rectified  linear  unitLinear  with  a  cutoff  at  zero(Implementation:  clip  the  gradient  when  you  pass  zero)
Slide  from  William  Cohen

Activation  Functions•A  new  change:  modifying  the  nonlinearity–reLUoften  used  in  vision  tasks
Alternate  2:  rectified  linear  unitSoft  version:  log(exp(x)+1)Doesn’t  saturate  (at  one  end)SparsifiesoutputsHelps  with  vanishing  gradient  
Slide  from  William  Cohen
Objective  Functions  for  NNs•Regression:–Use  the  same  objective  as  Linear  Regression–Quadratic  loss  (i.e.  mean  squared  error)•Classification:–Use  the  same  objective  as  Logistic  Regression–Cross-­‐entropy  (i.e.  negative  log  likelihood)–This  requires  probabilities,  so  we  add  an  additional  “softmax”  layer  at  the  end  of  our  network
63ForwardBackwardQuadraticJ=12(y y )2dJdy=y y CrossEntropyJ=y HQ;(y)+( 1 y )HQ;(1 y)dJdy=y 1y+( 1 y )1y 1
Cross-­‐entropy  vs.  Quadratic  loss
Figure  from  Glorot&  Bentio(2010)
A  Recipe  for  Machine  Learning1.  Given  training  data:3.  Define  goal:
67Background2.  Choose  each  of  these:–Decision  function–Loss  function4.  Train  with  SGD:(take  small  steps  opposite  the  gradient)
Objective  FunctionsMatching  Quiz:Suppose  you  are  given  a  neural  net  with  a  single  output,  y,  and  one  hidden  layer.
681)  Minimizing  sum  of  squared  errors…2)  Minimizing  sum  of  squared  errors  plus  squaredEuclidean  norm  of  weights…3)  Minimizingcross-­‐entropy…4)  Minimizing  hingeloss…5)  …MLE  estimates  of  weights  assuming  targetfollows  a  Bernoulli  with  parameter  given  by  the  output  value6)  …MAP  estimates  ofweightsassuming  weight  priors  are  zero  mean  Gaussian7)  …estimates  with  a  largemargin  on  the  training  data8)  …MLE  estimates  of  weights  assuming  zero  mean  Gaussian  noise  on  the  output  value…gives…
A.1=5,  2=7,  3=6,  4=8B.1=5,  2=7,  3=8,  4=6C.1=7,  2=5,  3=5,  4=7D.1=7,  2=5,  3=6,  4=8E.1=8,  2=6,  3=5,  4=7F.1=8,  2=6,  3=8,  4=6
BACKPROPAGATION
69
A  Recipe  for  Machine  Learning1.  Given  training  data:3.  Define  goal:
70Background2.  Choose  each  of  these:–Decision  function–Loss  function4.  Train  with  SGD:(take  small  steps  opposite  the  gradient)
Approaches  to  Differentiation•Question  1:When  can  we  compute  the  gradients  of  the  parameters  of  an  arbitrary  neural  network?•Question  2:When  can  we  make  the  gradient  computation  efficient?71Training
Approaches  to  Differentiation1.Finite  Difference  Method–Pro:  Great  for  testing  implementations  of  backpropagation–Con:  Slow  for  high  dimensional  inputs  /  outputs–Required:  Ability  to  call  the  function  f(x)  on  any  input  x2.Symbolic  Differentiation–Note:  The  method  you  learned  in  high-­‐school–Note:  Used  by  Mathematica  /  Wolfram  Alpha  /  Maple–Pro:  Yields  easily  interpretable  derivatives–Con:  Leads  to  exponential  computation  time  if  not  carefully  implemented–Required:  Mathematical  expression  that  defines  f(x)3.Automatic  Differentiation  -­‐Reverse  Mode–Note:  Called  Backpropagationwhen  applied  to  Neural  Nets–Pro:  Computes  partial  derivatives  of  one  output  f(x)iwith  respect  to  all  inputs  xjin  time  proportional  to  computation  of  f(x)–Con:  Slow  for  high  dimensional  outputs  (e.g.  vector-­‐valued  functions)–Required:  Algorithm  for  computing  f(x)4.Automatic  Differentiation  -­‐Forward  Mode–Note:  Easy  to  implement.  Uses  dual  numbers.–Pro:  Computes  partial  derivatives  of  all  outputs  f(x)iwith  respect  to  one  input  xjin  time  proportional  to  computation  of  f(x)–Con:  Slow  for  high  dimensional  inputs  (e.g.  vector-­‐valued  x)–Required:  Algorithm  for  computing  f(x)72Training

Finite  Difference  Method
Notes:•Suffers  from  issues  of  floating  point  precision,  in  practice•Typically  only  appropriate  to  use  on  small  examples  with  an  appropriately  chosen  epsilon73Training

Symbolic  DifferentiationCalculus  Quiz  #1:Suppose  x  =  2  and  z  =  3,  what  are  dy/dx  and  dy/dzfor  the  function  below?
74Training

Symbolic  DifferentiationCalculus  Quiz  #2:
75Training
……
…

Chain  RuleWhiteboard–Chain  Rule  of  Calculus
76Training
Chain  Rule
77Training2.2. NEURAL NETWORKS AND BACKPROPAGATIONxtoJ, but also a manner of carrying out that computation in terms of the intermediatequantitiesa,z,b,y. Which intermediate quantities to use is a design decision. In thisway, the arithmetic circuit diagram of Figure2.1is differentiated from the standard neuralnetwork diagram in two ways. A standard diagram for a neural network does not show thischoice of intermediate quantities nor the form of the computations.The topologies presented in this section are very simple. However, we will later (Chap-ter5) how an entire algorithm can deﬁne an arithmetic circuit.2.2.2 BackpropagationThe backpropagation algorithm (Rumelhart et al., 1986) is a general method for computingthe gradient of a neural network. Here we generalize the concept of a neural network toinclude any arithmetic circuit. Applying the backpropagation algorithm on these circuitsamounts to repeated application of the chain rule. This general algorithm goes under manyother names: automatic differentiation (AD) in the reverse mode (Griewank and Corliss,1991), analytic differentiation, module-based AD, autodiff, etc. Below we deﬁne a forwardpass, which computes the output bottom-up, and a backward pass, which computes thederivatives of all intermediate quantities top-down.Chain RuleAt the core of the backpropagation algorithm is the chain rule. The chainrule allows us to differentiate a functionfdeﬁned as the composition of two functionsgandhsuch thatf=(g h). If the inputs and outputs ofgandhare vector-valued variablesthenfis as well:h:RK!RJandg:RJ!RI)f:RK!RI. Given an inputvectorx={x1,x2,...,xK}, we compute the outputy={y1,y2,...,yI}, in terms of anintermediate vectoru={u1,u2,...,uJ}. That is, the computationy=f(x)=g(h(x))can be described in a feed-forward manner:y=g(u)andu=h(x). Then thechain rulemust sum over all the intermediate quantities.dyidxk=JXj=1dyidujdujdxk,8i, k(2.3)If the inputs and outputs off,g, andhare all scalars, then we obtain the familiar formof the chain rule:dydx=dydududx(2.4)Binary Logistic RegressionBinary logistic regression can be interpreted as a arithmeticcircuit. To compute the derivative of some loss function (below we use regression) withrespect to the model parameters✓, we can repeatedly apply the chain rule (i.e. backprop-agation). Note that the outputqbelow is the probability that the output label takes on thevalue1.y⇤is the true output label. The forward pass computes the following:J=y⇤logq+( 1 y⇤)l o g ( 1 q)(2.5)whereq=P✓(Yi=1|x)=11+e x p ( PDj=0✓jxj)(2.6)132.2. NEURAL NETWORKS AND BACKPROPAGATIONxtoJ, but also a manner of carrying out that computation in terms of the intermediatequantitiesa,z,b,y. Which intermediate quantities to use is a design decision. In thisway, the arithmetic circuit diagram of Figure2.1is differentiated from the standard neuralnetwork diagram in two ways. A standard diagram for a neural network does not show thischoice of intermediate quantities nor the form of the computations.The topologies presented in this section are very simple. However, we will later (Chap-ter5) how an entire algorithm can deﬁne an arithmetic circuit.2.2.2 BackpropagationThe backpropagation algorithm (Rumelhart et al., 1986) is a general method for computingthe gradient of a neural network. Here we generalize the concept of a neural network toinclude any arithmetic circuit. Applying the backpropagation algorithm on these circuitsamounts to repeated application of the chain rule. This general algorithm goes under manyother names: automatic differentiation (AD) in the reverse mode (Griewank and Corliss,1991), analytic differentiation, module-based AD, autodiff, etc. Below we deﬁne a forwardpass, which computes the output bottom-up, and a backward pass, which computes thederivatives of all intermediate quantities top-down.Chain RuleAt the core of the backpropagation algorithm is the chain rule. The chainrule allows us to differentiate a functionfdeﬁned as the composition of two functionsgandhsuch thatf=(g h). If the inputs and outputs ofgandhare vector-valued variablesthenfis as well:h:RK!RJandg:RJ!RI)f:RK!RI. Given an inputvectorx={x1,x2,...,xK}, we compute the outputy={y1,y2,...,yI}, in terms of anintermediate vectoru={u1,u2,...,uJ}. That is, the computationy=f(x)=g(h(x))can be described in a feed-forward manner:y=g(u)andu=h(x). Then thechain rulemust sum over all the intermediate quantities.dyidxk=JXj=1dyidujdujdxk,8i, k(2.3)If the inputs and outputs off,g, andhare all scalars, then we obtain the familiar formof the chain rule:dydx=dydududx(2.4)Binary Logistic RegressionBinary logistic regression can be interpreted as a arithmeticcircuit. To compute the derivative of some loss function (below we use regression) withrespect to the model parameters✓, we can repeatedly apply the chain rule (i.e. backprop-agation). Note that the outputqbelow is the probability that the output label takes on thevalue1.y⇤is the true output label. The forward pass computes the following:J=y⇤logq+( 1 y⇤)l o g ( 1 q)(2.5)whereq=P✓(Yi=1|x)=11+e x p ( PDj=0✓jxj)(2.6)13Chain  Rule:Given:  
…

Chain  Rule
78Training2.2. NEURAL NETWORKS AND BACKPROPAGATIONxtoJ, but also a manner of carrying out that computation in terms of the intermediatequantitiesa,z,b,y. Which intermediate quantities to use is a design decision. In thisway, the arithmetic circuit diagram of Figure2.1is differentiated from the standard neuralnetwork diagram in two ways. A standard diagram for a neural network does not show thischoice of intermediate quantities nor the form of the computations.The topologies presented in this section are very simple. However, we will later (Chap-ter5) how an entire algorithm can deﬁne an arithmetic circuit.2.2.2 BackpropagationThe backpropagation algorithm (Rumelhart et al., 1986) is a general method for computingthe gradient of a neural network. Here we generalize the concept of a neural network toinclude any arithmetic circuit. Applying the backpropagation algorithm on these circuitsamounts to repeated application of the chain rule. This general algorithm goes under manyother names: automatic differentiation (AD) in the reverse mode (Griewank and Corliss,1991), analytic differentiation, module-based AD, autodiff, etc. Below we deﬁne a forwardpass, which computes the output bottom-up, and a backward pass, which computes thederivatives of all intermediate quantities top-down.Chain RuleAt the core of the backpropagation algorithm is the chain rule. The chainrule allows us to differentiate a functionfdeﬁned as the composition of two functionsgandhsuch thatf=(g h). If the inputs and outputs ofgandhare vector-valued variablesthenfis as well:h:RK!RJandg:RJ!RI)f:RK!RI. Given an inputvectorx={x1,x2,...,xK}, we compute the outputy={y1,y2,...,yI}, in terms of anintermediate vectoru={u1,u2,...,uJ}. That is, the computationy=f(x)=g(h(x))can be described in a feed-forward manner:y=g(u)andu=h(x). Then thechain rulemust sum over all the intermediate quantities.dyidxk=JXj=1dyidujdujdxk,8i, k(2.3)If the inputs and outputs off,g, andhare all scalars, then we obtain the familiar formof the chain rule:dydx=dydududx(2.4)Binary Logistic RegressionBinary logistic regression can be interpreted as a arithmeticcircuit. To compute the derivative of some loss function (below we use regression) withrespect to the model parameters✓, we can repeatedly apply the chain rule (i.e. backprop-agation). Note that the outputqbelow is the probability that the output label takes on thevalue1.y⇤is the true output label. The forward pass computes the following:J=y⇤logq+( 1 y⇤)l o g ( 1 q)(2.5)whereq=P✓(Yi=1|x)=11+e x p ( PDj=0✓jxj)(2.6)132.2. NEURAL NETWORKS AND BACKPROPAGATIONxtoJ, but also a manner of carrying out that computation in terms of the intermediatequantitiesa,z,b,y. Which intermediate quantities to use is a design decision. In thisway, the arithmetic circuit diagram of Figure2.1is differentiated from the standard neuralnetwork diagram in two ways. A standard diagram for a neural network does not show thischoice of intermediate quantities nor the form of the computations.The topologies presented in this section are very simple. However, we will later (Chap-ter5) how an entire algorithm can deﬁne an arithmetic circuit.2.2.2 BackpropagationThe backpropagation algorithm (Rumelhart et al., 1986) is a general method for computingthe gradient of a neural network. Here we generalize the concept of a neural network toinclude any arithmetic circuit. Applying the backpropagation algorithm on these circuitsamounts to repeated application of the chain rule. This general algorithm goes under manyother names: automatic differentiation (AD) in the reverse mode (Griewank and Corliss,1991), analytic differentiation, module-based AD, autodiff, etc. Below we deﬁne a forwardpass, which computes the output bottom-up, and a backward pass, which computes thederivatives of all intermediate quantities top-down.Chain RuleAt the core of the backpropagation algorithm is the chain rule. The chainrule allows us to differentiate a functionfdeﬁned as the composition of two functionsgandhsuch thatf=(g h). If the inputs and outputs ofgandhare vector-valued variablesthenfis as well:h:RK!RJandg:RJ!RI)f:RK!RI. Given an inputvectorx={x1,x2,...,xK}, we compute the outputy={y1,y2,...,yI}, in terms of anintermediate vectoru={u1,u2,...,uJ}. That is, the computationy=f(x)=g(h(x))can be described in a feed-forward manner:y=g(u)andu=h(x). Then thechain rulemust sum over all the intermediate quantities.dyidxk=JXj=1dyidujdujdxk,8i, k(2.3)If the inputs and outputs off,g, andhare all scalars, then we obtain the familiar formof the chain rule:dydx=dydududx(2.4)Binary Logistic RegressionBinary logistic regression can be interpreted as a arithmeticcircuit. To compute the derivative of some loss function (below we use regression) withrespect to the model parameters✓, we can repeatedly apply the chain rule (i.e. backprop-agation). Note that the outputqbelow is the probability that the output label takes on thevalue1.y⇤is the true output label. The forward pass computes the following:J=y⇤logq+( 1 y⇤)l o g ( 1 q)(2.5)whereq=P✓(Yi=1|x)=11+e x p ( PDj=0✓jxj)(2.6)13Chain  Rule:Given:  
…
Backpropagationis  just  repeated  application  of  the  chain  rule  from  Calculus  101.
BackpropagationWhiteboard–Example:  Backpropagation  for  Calculus  Quiz  #1
79TrainingCalculus  Quiz  #1:Suppose  x  =  2  and  z  =  3,  what  are  dy/dx  and  dy/dz  for  the  function  below?

Backpropagation
80TrainingAutomatic  Differentiation  –Reverse  Mode  (aka.  Backpropagation)Forward  Computation1.Write  an  algorithmfor  evaluating  the  function  y  =  f(x).  The  algorithm  defines  a  directed  acyclic  graph,  where  each  variable  is  a  node  (i.e.  the  “computation  graph”)2.Visit  each  node  in  topological  order.  For  variable  uiwith  inputs  v1,…,  vNa.Compute  ui=  gi(v1,…,  vN)b.Store  the  result  at  the  nodeBackward  Computation1.Initializeall  partial  derivatives  dy/dujto  0  and  dy/dy=  1.2.Visit  each  node  in  reverse  topological  order.  For  variable  ui=  gi(v1,…,  vN)a.We  already  know  dy/duib.Increment  dy/dvjby  (dy/dui)(dui/dvj)(Choice  of  algorithm  ensures  computing  (dui/dvj)  is  easy)Return  partial  derivatives  dy/dui  for  all  variables
Backpropagation
81TrainingForward BackwardJ=cos(u)dJduY= sin(u)u=u1+u2dJdu1Y=dJdududu1,dudu1=1dJdu2Y=dJdududu2,dudu2=1u1=sin(t)dJdtY=dJdu1du1dt,du1dt=+Qb(t)u2=3tdJdtY=dJdu2du2dt,du2dt=3t=x2dJdxY=dJdtdtdx,dtdx=2xSimple Example:The goal is to computeJ=+Qb(bBM(x2)+3x2)ontheforwardpassandthederivativedJdxonthebackwardpass.
Backpropagation
82TrainingForward BackwardJ=cos(u)dJduY= sin(u)u=u1+u2dJdu1Y=dJdududu1,dudu1=1dJdu2Y=dJdududu2,dudu2=1u1=sin(t)dJdtY=dJdu1du1dt,du1dt=+Qb(t)u2=3tdJdtY=dJdu2du2dt,du2dt=3t=x2dJdxY=dJdtdtdx,dtdx=2xSimple Example:The goal is to computeJ=+Qb(bBM(x2)+3x2)ontheforwardpassandthederivativedJdxonthebackwardpass.
BackpropagationWhiteboard–SGD  for  Neural  Network–Example:  Backpropagation  for  Neural  Network
83Training
Backpropagation
84Training
…
OutputInputθ1θ2θ3θMCase  1:Logistic  RegressionForwardBackwardJ=y HQ;y+( 1 y )HQ;(1 y)dJdy=y y+(1 y )y 1y=11+2tT( a)dJda=dJdydyda,dyda=2tT( a)(2tT( a) + 1)2a=D j=0 jxjdJd j=dJdadad j,dad j=xjdJdxj=dJdadadxj,dadxj= j
Backpropagation
85Training
……
Output
InputHidden  Layer(F)LossJ=12(y y(d))2(E)Output(sigmoid)y=11+2tT( b)(D)Output(linear)b= Dj=0 jzj(C)Hidden(sigmoid)zj=11+2tT( aj), j(B)Hidden(linear)aj= Mi=0 jixi, j(A)InputGivenxi, i
Backpropagation
86Training
……
Output
InputHidden  Layer(F)LossJ=12(y y )2(E)Output(sigmoid)y=11+2tT( b)(D)Output(linear)b= Dj=0 jzj(C)Hidden(sigmoid)zj=11+2tT( aj), j(B)Hidden(linear)aj= Mi=0 jixi, j(A)InputGivenxi, i
Backpropagation
87TrainingCase  2:Neural  Network
……
ForwardBackwardJ=y HQ;y+( 1 y )HQ;(1 y)dJdy=y y+(1 y )y 1y=11+2tT( b)dJdb=dJdydydb,dydb=2tT( b)(2tT( b) + 1)2b=D j=0 jzjdJd j=dJdbdbd j,dbd j=zjdJdzj=dJdbdbdzj,dbdzj= jzj=11+2tT( aj)dJdaj=dJdzjdzjdaj,dzjdaj=2tT( aj)(2tT( aj) + 1)2aj=M i=0 jixidJd ji=dJdajdajd ji,dajd ji=xidJdxi=dJdajdajdxi,dajdxi=D j=0 ji
Backpropagation
88TrainingBackpropagation  (Auto.Diff.  -­‐Reverse  Mode)Forward  Computation1.Write  an  algorithmfor  evaluating  the  function  y  =  f(x).  The  algorithm  defines  a  directed  acyclic  graph,  where  each  variable  is  a  node  (i.e.  the  “computation  graph”)2.Visit  each  node  in  topological  order.  a.Compute  the  corresponding  variable’s  valueb.Store  the  result  at  the  nodeBackward  Computation3.Initializeall  partial  derivatives  dy/dujto  0  and  dy/dy=  1.4.Visit  each  node  in  reverse  topological  order.  For  variable  ui=  gi(v1,…,  vN)a.We  already  know  dy/duib.Increment  dy/dvjby  (dy/dui)(dui/dvj)(Choice  of  algorithm  ensures  computing  (dui/dvj)  is  easy)Return  partial  derivatives  dy/dui  for  all  variables
Case  2:Neural  Network
……
Module  1Module  2Module  3Module  4Module  5Backpropagation
89TrainingForwardBackwardJ=y HQ;y+( 1 y )HQ;(1 y)dJdy=y y+(1 y )y 1y=11+2tT( b)dJdb=dJdydydb,dydb=2tT( b)(2tT( b) + 1)2b=D j=0 jzjdJd j=dJdbdbd j,dbd j=zjdJdzj=dJdbdbdzj,dbdzj= jzj=11+2tT( aj)dJdaj=dJdzjdzjdaj,dzjdaj=2tT( aj)(2tT( aj) + 1)2aj=M i=0 jixidJd ji=dJdajdajd ji,dajd ji=xidJdxi=dJdajdajdxi,dajdxi=D j=0 ji
A  Recipe  for  Machine  Learning1.  Given  training  data:3.  Define  goal:
90Background2.  Choose  each  of  these:–Decision  function–Loss  function4.  Train  with  SGD:(take  small  steps  opposite  the  gradient)
Gradients
Backpropagationcan  compute  this  gradient!  And  it’s  a  special  case  of  a  more  general  algorithm  called  reverse-­‐mode  automatic  differentiation  that  can  compute  the  gradient  of  any  differentiable  function  efficiently!

Summary1.Neural  Networks…–provide  a  way  of  learning  features–are  highly  nonlinear  prediction  functions–(can  be)  a  highly  parallel  network  of  logistic  regression  classifiers–discover  useful  hidden  representations  of  the  input2.Backpropagation…–provides  an  efficient  way  to  compute  gradients–is  a  special  case  of  reverse-­‐mode  automatic  differentiation91
PCA+Neural  Networks
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  18March  27,  2017Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
PCA  Readings:Murphy  12Bishop  12HTF  14.5Mitchell  -­‐-­‐Neural  Net  Readings:Murphy  -­‐-­‐Bishop  5HTF  11Mitchell  4
Reminders•Homework6:  UnsupervisedLearning–Release:  Wed,  Mar.  22–Due:  Wed,  Mar.  22  at  11:59pm
2
DIMENSIONALITY  REDUCTION
3
PCA  Outline•Dimensionality  Reduction–High-­‐dimensional  data–Learning  (low  dimensional)  representations•Principal  Component  Analysis  (PCA)–Examples:  2D  and  3D–Data  for  PCA–PCA  Definition–Objective  functions  for  PCA–PCA,  Eigenvectors,  and  Eigenvalues–Algorithms  for  finding  Eigenvectors  /  Eigenvalues•PCA  Examples–Face  Recognition–Image  Compression4
This  Lecture
Last  Lecture
•High-­‐Dimensions  =  Lot  of  FeaturesDocument  classificationFeatures  per  document  =  thousands  of  words/unigramsmillions  of  bigrams,    contextual  informationSurveys  -­‐Netflix480189  users  x  17770  moviesBig  &  High-­‐Dimensional  Data
Slide  from  Nina  Balcan
•High-­‐Dimensions  =  Lot  of  FeaturesMEG  Brain  Imaging120  locations  x  500  time  points  x  20  objects
Big  &  High-­‐Dimensional  Data
Or  any  high-­‐dimensional  image  data
Slide  from  Nina  Balcan
•Useful  to  learn  lower  dimensional  representations  of  the  data.•Big  &  High-­‐Dimensional  Data.
Slide  from  Nina  Balcan
PCA,  Kernel  PCA,  ICA:  Powerful  unsupervised  learning  techniques  for  extracting  hidden  (potentially  lower  dimensional)  structure  from  high  dimensional  datasets.Learning  RepresentationsUseful  for:•Visualization  •Further  processing  by  machine  learning  algorithms•More  efficient  use  of  resources  (e.g.,  time,  memory,  communication)•Statistical:  fewer  dimensions  àbetter  generalization•Noise  removal  (improving  data  quality)Slide  from  Nina  Balcan
PRINCIPAL  COMPONENT  ANALYSIS  (PCA)11
PCA  Outline•Dimensionality  Reduction–High-­‐dimensional  data–Learning  (low  dimensional)  representations•Principal  Component  Analysis  (PCA)–Examples:  2D  and  3D–Data  for  PCA–PCA  Definition–Objective  functions  for  PCA–PCA,  Eigenvectors,  and  Eigenvalues–Algorithms  for  finding  Eigenvectors  /  Eigenvalues•PCA  Examples–Face  Recognition–Image  Compression12
Principal  Component  Analysis  (PCA)
In  case  where  data    lies  on  or  near  a  low  d-­‐dimensional  linear  subspace,  axes  of  this  subspace  are  an  effective  representation  of  the  data.
Identifying  the  axes  is  known  as  Principal  Components  Analysis,  and  can  be  obtained  by  using  classic  matrix  computation  tools  (Eigen  or  Singular  Value  Decomposition).Slide  from  Nina  Balcan
2D  Gaussian  dataset
Slide  from  Barnabas  Poczos
1stPCA  axis
Slide  from  Barnabas  Poczos
2ndPCA  axis
Slide  from  Barnabas  Poczos
Principal  Component  Analysis  (PCA)Whiteboard–Data  for  PCA–PCA  Definition–Objective  functions  for  PCA
17
Maximizing  the  Variance•Consider  the  two  projections  below•Which  maximizes  the  variance?
204
We see that the projected data still has a fairly large variance, andthepoints tend to be far from zero. In contrast, suppose had instead picked thefollowing direction:
Here, the projections have a signiﬁcantly smaller variance, and aremuchcloser to the origin.We would like to automatically select the directionucorresponding tothe ﬁrst of the two ﬁgures shown above. To formalize this, note that given aFigures  from  Andrew  Ng  (CS229  Lecture  Notes)4
We see that the projected data still has a fairly large variance, andthepoints tend to be far from zero. In contrast, suppose had instead picked thefollowing direction:
Here, the projections have a signiﬁcantly smaller variance, and aremuchcloser to the origin.We would like to automatically select the directionucorresponding tothe ﬁrst of the two ﬁgures shown above. To formalize this, note that given aOption  AOption  B
Principal  Component  Analysis  (PCA)Whiteboard–PCA,  Eigenvectors,  and  Eigenvalues–Algorithms  for  finding  Eigenvectors  /  Eigenvalues
21
Principal  Component  Analysis  (PCA)X	
  X#v=λv	
  ,  so  v  (the  first  PC)  is  the  eigenvector  of  sample  correlation/covariance  matrix  𝑋	
  𝑋(
Sample  variance  of  projection  v(𝑋	
  𝑋(v=𝜆v(v=𝜆Thus,  the  eigenvalue  𝜆	
  denotes  the  amount  of  variability  captured  along  that  dimension(aka  amount  of  energy  along  that  dimension).Eigenvalues  𝜆*≥𝜆,≥𝜆-≥⋯•The  1stPC  𝑣*is  the  theeigenvector  of  the  sample  covariance  matrix  𝑋	
  𝑋(associated  with  the  largest  eigenvalue  •The  2nd  PC  𝑣,is  the  theeigenvector  of  the  sample  covariance  matrix  𝑋	
  𝑋(associated  with  the  second  largest  eigenvalue  •And  so  on  …Slide  from  Nina  Balcan
•For  Moriginal  dimensions,  sample  covariance  matrix  is  MxM,  and  has  up  to  Meigenvectors.  So  MPCs.•Where  does  dimensionality  reduction  come  from?Can  ignore  the  components  of  lesser  significance.  
0510152025
PC1PC2PC3PC4PC5PC6PC7PC8PC9PC10Variance  (%)How  Many  PCs?
©  Eric  Xing  @  CMU,  2006-­‐201123•You  do  lose  some  information,  but  if  the  eigenvalues  are  small,  you  don’t  lose  much–Mdimensions  in  original  data  –calculateMeigenvectors  and  eigenvalues–choose  only  the  first  Deigenvectors,  based  on  their  eigenvalues–final  data  set  has  only  Ddimensions
PCA  EXAMPLESSlides  from  Barnabas  PoczosOriginal  sources  include:  •Karl  BookshResearch  group•Tom  Mitchell•Ron  Parr
24
Face  recognition
Slide  from  Barnabas  Poczos
Challenge:  Facial  Recognition•Want  to  identify  specific  person,  based  on  facial  image•Robust  to  glasses,  lighting,…ÞCan’t  just  use  the  given  256  x  256  pixels
Slide  from  Barnabas  Poczos
Applying  PCA:  Eigenfaces•Example  data  set:    Images  of  faces  –Famous  Eigenfaceapproach[Turk  &  Pentland],  [Sirovich&  Kirby]•Each  face  xis  …–256  ´256  values  (luminance  at  location)–xin  Â256´256        (view  as  64K  dim  vector)
256  x  256  real  values
m  facesX  =x1,  …,  xmMethod:Build  one  PCA  database  for  the  whole  dataset  and  then  classify  based  on  the  weights.
Slide  from  Barnabas  Poczos
Principle  Components
Slide  from  Barnabas  Poczos
Reconstructing…
•…  faster  if  train  with…–only  people  w/out  glasses–same  lighting  conditions
Slide  from  Barnabas  Poczos
Shortcomings•Requires  carefully  controlled  data:–All  faces  centered  in  frame–Same  size–Some  sensitivity  to  angle•Alternative:–“Learn”  one  set  of  PCA  vectors  for  each  angle–Use  the  one  with  lowest  error•Method  is  completely  knowledge  free–(sometimes  this  is  good!)–Doesn’t  know  that  faces  are  wrapped  around  3D  objects  (heads)–Makes  no  effort  to  preserve  class  distinctionsSlide  from  Barnabas  Poczos
Image  Compression
Slide  from  Barnabas  Poczos
Original  Image
•Divide  the  original  372x492image  into  patches:•Each  patch  is  an  instance  that  contains  12x12  pixels  on  a  grid•View  each  as  a  144-­‐D  vector
Slide  from  Barnabas  Poczos
L2error  and  PCA  dim
Slide  from  Barnabas  Poczos
PCA  compression:  144D  à60D
Slide  from  Barnabas  Poczos
PCA  compression:  144D  à16D
Slide  from  Barnabas  Poczos
16  most  important  eigenvectors
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012Slide  from  Barnabas  Poczos
PCA  compression:  144D  à6D
Slide  from  Barnabas  Poczos
2468101224681012
2468101224681012
2468101224681012
2468101224681012
2468101224681012
24681012246810126  most  important  eigenvectors
Slide  from  Barnabas  Poczos
PCA  compression:  144D  à3D
Slide  from  Barnabas  Poczos
2468101224681012
2468101224681012
24681012246810123  most  important  eigenvectors
Slide  from  Barnabas  Poczos
PCA  compression:  144D  à1D
Slide  from  Barnabas  Poczos
60  most  important  eigenvectors
Looks  like  the  discrete  cosine  bases  of  JPG!...Slide  from  Barnabas  Poczos
2D  Discrete  Cosine  Basis
http://en.wikipedia.org/wiki/Discrete_cosine_transformSlide  from  Barnabas  Poczos
Neural  Networks  Outline•Logistic  Regression  (Recap)–Data,  Model,  Learning,  Prediction•Neural  Networks–A  Recipe  for  Machine  Learning–Visual  Notation  for  Neural  Networks–Example:  Logistic  Regression  Output  Surface–1-­‐Layer  Neural  Network–2-­‐Layer  Neural  Network•Neural  Net  Architectures–Objective  Functions–Activation  Functions•Backpropagation–Basic  Chain  Rule  (of  calculus)–Chain  Rule  for  Arbitrary  Computation  Graph–Backpropagation  Algorithm–Module-­‐based  Automatic  Differentiation  (Autodiff)44
RECALL:  LOGISTIC  REGRESSION
45
Using  gradient  ascent  for  linear  classifiersKey  idea  behind  today’s  lecture:1.Define  a  linear  classifier  (logistic  regression)2.Define  an  objective  function  (likelihood)3.Optimize  it  with  gradient  descent  to  learn  parameters4.Predict  the  class  with  highest  probability  under  the  model
46
Using  gradient  ascent  for  linear  classifiers
47Use  a  differentiable  function  instead:
logistic(u)≡11+e−up (y=1|t)=11+2tT(  Tt)This  decision  function  isn’t  differentiable:
sign(x)h(t)=sign( Tt)
Using  gradient  ascent  for  linear  classifiers
48Use  a  differentiable  function  instead:
logistic(u)≡11+e−up (y=1|t)=11+2tT(  Tt)This  decision  function  isn’t  differentiable:
sign(x)h(t)=sign( Tt)
Logistic  Regression
49Learning:  finds  the  parameters  that  minimize  some  objective  function.  = argmin J( )Data:  Inputs  are  continuous  vectors  of  length  K.  Outputs  are  discrete.D={t(i),y(i)}Ni=1wheret RKandy {0,1}
Prediction:  Output  is  the  most  probable  class.ˆy=`;Kty {0,1}p (y|t)Model:  Logistic  function  applied  to  dot  product  of  parameters  with  input  vector.p (y=1|t)=11+2tT(  Tt)
NEURAL  NETWORKS
50
A  Recipe  for  Machine  Learning1.  Given  training  data:
56Background2.  Choose  each  of  these:–Decision  function–Loss  function
FaceFaceNot  a  face
Examples:  Linear  regression,  Logistic  regression,  Neural  NetworkExamples:  Mean-­‐squared  error,  Cross  Entropy
A  Recipe  for  Machine  Learning1.  Given  training  data:3.  Define  goal:
57Background2.  Choose  each  of  these:–Decision  function–Loss  function4.  Train  with  SGD:(take  small  steps  opposite  the  gradient)
A  Recipe  for  Machine  Learning1.  Given  training  data:3.  Define  goal:
58Background2.  Choose  each  of  these:–Decision  function–Loss  function4.  Train  with  SGD:(take  small  steps  opposite  the  gradient)
Gradients
Backpropagationcan  compute  this  gradient!  And  it’s  a  special  case  of  a  more  general  algorithm  called  reverse-­‐mode  automatic  differentiation  that  can  compute  the  gradient  of  any  differentiable  function  efficiently!

A  Recipe  for  Machine  Learning1.  Given  training  data:3.  Define  goal:
59Background2.  Choose  each  of  these:–Decision  function–Loss  function4.  Train  with  SGD:(take  small  steps  opposite  the  gradient)
Goals  for  Today’s  Lecture
1.Explore  a  new  class  of  decision  functions  (Neural  Networks)2.Consider  variants  of  this  recipe  for  training

Linear  Regression
60Decision  Functions
…
Output
Inputθ1θ2θ3θMy=h (x)= ( Tx)where (a)=a
Logistic  Regression
61Decision  Functions
…
Output
Inputθ1θ2θ3θMy=h (x)= ( Tx)where (a)=11+2tT( a)
y=h (x)= ( Tx)where (a)=11+2tT( a)Logistic  Regression
62Decision  Functions
…
Output
Inputθ1θ2θ3θM
FaceFaceNot  a  face
y=h (x)= ( Tx)where (a)=11+2tT( a)Logistic  Regression
63Decision  Functions
…
Output
Inputθ1θ2θ3θM
110
x1x2y
In-­‐Class  Example
Boosting Approach to ML  
Maria -Florina  Balcan  
03/18/2015  Perceptron, Margins, Kernels  
Recap from last time: Boosting  
•Works by creating a series of challenge datasets s.t. even 
modest performance on these can be used to produce an 
overall high -accuracy predictor.  
•Works amazingly well in practice.  •Adaboost  one of the top 10 ML algorithms.  •General method for improving the accuracy of any given 
learning algorithm.  
•Backed up by solid foundations.  
Adaboost  (Adaptive Boosting)  
•For t=1,2, … ,T  
 •Construct Dt on {x1, …, xm} 
•Run A on Dt producing ht:𝑋→{−1,1} xi∈𝑋, 𝑦𝑖∈𝑌={−1,1} 
+ + + 
+ + + + 
+ 
- - - 
- 
- - - 
- ht 
Output  Hfinal𝑥=sign 𝛼𝑡ℎ𝑡𝑥𝑡=1  Input : S={(x1,𝑦1), …,(xm,𝑦m)};  
weak learning algo A (e.g., Naïve Bayes, decision stumps)  
Dt+1 puts half of weight  on examples  
xi where  ht is incorrect  & half on 
examples  where  ht is correct   𝐷𝑡+1𝑖=𝐷𝑡𝑖
𝑍𝑡 e−𝛼𝑡   if 𝑦𝑖=ℎ𝑡𝑥𝑖  
 𝐷𝑡+1𝑖=𝐷𝑡𝑖
𝑍𝑡 e𝛼𝑡   if 𝑦𝑖≠ℎ𝑡𝑥𝑖  
 [i.e., D1𝑖=1
𝑚] 
•Given Dt and ht set 
𝛼𝑡=1
2ln1−𝜖𝑡
𝜖𝑡>0 • D1 uniform on {x1, …, xm} 
𝐷𝑡+1𝑖=𝐷𝑡𝑖
𝑍𝑡 e−𝛼𝑡𝑦𝑖 ℎ𝑡𝑥𝑖   
Nice Features of Adaboost  
•Very general : a meta -procedure, it can use any weak 
learning algorithm!!!  
•Very fast (single pass through data each round) & simple 
to code , no parameters to tune . 
•Grounded in rich theory.  (e.g., Naïve Bayes, decision stumps)  
Analyzing Training Error  
Theorem  𝜖𝑡=1/2−𝛾𝑡 (error of ℎ𝑡 over 𝐷𝑡) 
𝑒𝑟𝑟𝑆𝐻𝑓𝑖𝑛𝑎𝑙≤exp −2  𝛾𝑡2
𝑡 
So, if  ∀𝑡,𝛾𝑡≥𝛾>0, then 𝑒𝑟𝑟𝑆𝐻𝑓𝑖𝑛𝑎𝑙≤exp −2 𝛾2𝑇 
Adaboost  is adaptive  
•Does not need to know 𝛾 or T a priori  
•Can exploit  𝛾𝑡≫ 𝛾 The training error drops exponentially in T!!!  
To get 𝑒𝑟𝑟𝑆𝐻𝑓𝑖𝑛𝑎𝑙≤𝜖, need only 𝑇=𝑂1
𝛾2log1
𝜖 rounds  
Generalization Guarantees  
G={all fns of the form sign( 𝛼𝑡ℎ𝑡(𝑥)) 𝑇
𝑡=1 }  𝐻𝑓𝑖𝑛𝑎𝑙 is a weighted vote, so the hypothesis class is:  
Theorem [Freund&Schapire’ 97]  
∀ 𝑔∈𝐺,𝑒𝑟𝑟𝑔≤𝑒𝑟𝑟𝑆𝑔+𝑂 𝑇𝑑
𝑚   T= # of rounds  
Key reason : VCd𝑖𝑚𝐺=𝑂 𝑑𝑇 plus typical VC bounds.  •H space of weak hypotheses; d=VCdim (H) Theorem  where 𝜖𝑡=1/2−𝛾𝑡 𝑒𝑟𝑟𝑆𝐻𝑓𝑖𝑛𝑎𝑙≤exp −2  𝛾𝑡2
𝑡 
How about generalization guarantees?  
  
Original analysis [Freund&Schapire’ 97]  
Generalization Guarantees  
Theorem [Freund&Schapire’ 97]  
∀ 𝑔∈𝐺,𝑒𝑟𝑟𝑔≤𝑒𝑟𝑟𝑆𝑔+𝑂 𝑇𝑑
𝑚   where d=VCdim (H) 
error  
complexity  train error  generalization  
error  
T= # of rounds  
Generalization Guarantees  
•Experiments showed that the test error of the generated 
classifier usually does not increase  as its size becomes 
very large.  
•Experiments showed that continuing to add new weak 
learners after correct classification of the training set had 
been achieved could further improve  test set performance !!! 
Generalization Guarantees  
•Experiments showed that continuing to add new weak 
learners after correct classification of the training set had 
been achieved could further improve  test set performance !!! 
•These results seem to contradict FS’97 bound and Occam’s 
razor (in order achieve good test error the classifier should be as 
simple  as possible) ! 
•Experiments showed that the test error of the generated 
classifier usually does not increase  as its size becomes 
very large.  
∀ 𝑔∈𝐺,𝑒𝑟𝑟𝑔≤𝑒𝑟𝑟𝑆𝑔+𝑂 𝑇𝑑
𝑚   
How can we explain the experiments?  
Key Idea:  R. Schapire , Y. Freund, P. Bartlett, W. S. Lee. present in 
“Boosting the margin: A new explanation for the effectiveness 
of voting methods ” a nice theoretical explanation . 
Training error does not tell the whole story.  
We need also to consider the classification confidence !! 
Boosting didn’t seem 
to overfit …(!) 
test 
error  train 
error  test error of base classifier 
(weak learner)  
Error Curve, Margin Distr. Graph - Plots from [SFBL 98] 
…because it turned out to be 
increasing the margin of the 
classifier  
Classification Margin  
•H space of weak hypotheses. The convex hull of H: 
•Let 𝑓∈𝑐𝑜𝐻,𝑓= 𝛼𝑡ℎ𝑡,𝑇
𝑡=1𝛼𝑡≥0,   𝛼𝑡=1𝑇
𝑡=1 .  
The majority vote rule 𝐻𝑓 given by  𝑓 (given by 𝐻𝑓=𝑠𝑖𝑔𝑛(𝑓𝑥))  
predicts wrongly on example (𝑥,𝑦) iff 𝑦𝑓𝑥≤0.  𝑐𝑜𝐻=𝑓= 𝛼𝑡ℎ𝑡𝑇
𝑡=1,𝛼𝑡≥0, 𝛼𝑡=1,ℎ𝑡∈𝐻𝑇
𝑡=1  
Definition:  margin  of 𝐻𝑓 (or of  𝑓) on example (𝑥,𝑦) to be 𝑦𝑓(𝑥).  
𝑦𝑓𝑥=𝑦 𝛼𝑡ℎ𝑡𝑥= 𝑦𝛼𝑡ℎ𝑡𝑥= 𝛼𝑡− 𝛼𝑡
𝑡:𝑦≠ℎ𝑡𝑥 𝑡:𝑦=ℎ𝑡𝑥𝑇
𝑡=1𝑇
𝑡=1 
The margin is positive iff 𝑦=𝐻𝑓𝑥.  
See  𝑦𝑓𝑥=|𝑓𝑥| as the strength or the confidence of the vote.  
1 
High confidence, 
correct  -1 
High confidence, 
incorrect  Low confidence  
Boosting and Margins  
Theorem: VCdim(𝐻)=𝑑, then with prob. ≥1−𝛿, ∀𝑓∈𝑐𝑜(𝐻), ∀𝜃>0, 
Pr
𝐷𝑦𝑓𝑥≤0≤Pr
𝑆𝑦𝑓𝑥≤𝜃+𝑂1
𝑚 d ln2𝑚
𝑑
𝜃2+ln1
𝛿  
Note :  bound does not depend on  T (the # of rounds of boosting), 
depends only on the complex. of the weak hyp space and the margin!  

Boosting and Margins  
•If all training examples have large margins , then we can 
approximate  the final classifier by a much smaller classifier.  
•Can use this to prove that better margin  smaller test error , 
regardless of the number of weak classifiers.  
•Can also prove that boosting tends to increase the margin  of 
training examples by concentrating on those of smallest margin.  
•Although final classifier is getting larger , 
margins  are likely to be increasing , so the 
final classifier is actually getting closer to a 
simpler  classifier, driving down  test error.  Theorem: VCdim(𝐻)=𝑑, then with prob. ≥1−𝛿, ∀𝑓∈𝑐𝑜(𝐻), ∀𝜃>0, 
Pr
𝐷𝑦𝑓𝑥≤0≤Pr
𝑆𝑦𝑓𝑥≤𝜃+𝑂1
𝑚 d ln2𝑚
𝑑
𝜃2+ln1
𝛿  
Boosting and Margins  
Theorem: VCdim(𝐻)=𝑑, then with prob. ≥1−𝛿, ∀𝑓∈𝑐𝑜(𝐻), ∀𝜃>0, 
Pr
𝐷𝑦𝑓𝑥≤0≤Pr
𝑆𝑦𝑓𝑥≤𝜃+𝑂1
𝑚 d ln2𝑚
𝑑
𝜃2+ln1
𝛿  
Note :  bound does not depend on  T (the # of rounds of boosting), 
depends only on the complex. of the weak hyp space and the margin!  

•Shift in mindset: goal is now just to find classifiers a 
bit better than random guessing . 
•Relevant for big data age: quickly focuses on “core difficulties”, so 
well-suited to distributed settings, where data must be 
communicated efficiently [Balcan -Blum-Fine-Mansour COLT’ 12]. •Backed up by solid foundations.  
•Adaboost  work and its variations well in practice with 
many kinds of data  (one of the top 10 ML algos ). 
•More about classic applications in Recitation.  Boosting, Adaboost  Summary  
Interestingly, the usefulness of margin  
recognized in Machine Learning since late 50’s. 
Perceptron  [Rosenblatt’ 57] analyzed via geometric  
(aka 𝐿2,𝐿2) margin.  
Original guarantee in the online learning scenario.  
The Perceptron Algorithm  
•Online Learning Model  
•Margin Analysis  
•Kernels  
Mistake bound model  •Example arrive sequentially . The Online Learning Model  
•We need to make a prediction.  
Afterwards observe the outcome.  
•Analysis wise, make no distributional assumptions . 
•Goal: Minimize  the number of mistakes . Online Algorithm  Example 𝑥𝑖 
Prediction ℎ(𝑥𝑖) Phase i:  
Observe c∗(𝑥𝑖) For i=1, 2, …, :  
The Online Learning Model. Motivation  
- Email classification (distribution of both spam and regular 
mail changes over time, but the target function stays fixed - 
last year's spam still looks like spam).  
- Add placement in a new market.  - Recommendation systems. Recommending movies, etc.  
- Predicting whether a user will be interested in a new news 
article or not. 
Linear Separators  
X 
X X X 
X 
X X X 
X X O 
O 
O O 
O O 
O O w •Instance space X=Rd 
•Hypothesis class of linear decision 
surfaces in Rd. 
• hx=w⋅ x +w0, if ℎ𝑥≥ 0, then 
label x as +, otherwise label it as - 
Claim : WLOG w0=0. 
Proof: Can simulate a non -zero threshold with a dummy input 
feature 𝑥0 that is always set up to 1. 
•𝑥=𝑥1,…,𝑥𝑑→𝑥 =𝑥1,…,𝑥𝑑,1 
•w⋅ x +w0≥0 iff 𝑤1,…,𝑤𝑑,w0⋅𝑥 ≥0 
where w=𝑤1,…,𝑤𝑑 
•Set t=1, start with the all zero vector  𝑤1. Linear Separators: Perceptron Algorithm  
•Given example 𝑥, predict positive iff 𝑤𝑡⋅𝑥≥0 
•On a mistake, update as follows:  
•Mistake on positive, then update 𝑤𝑡+1←𝑤𝑡+𝑥 
•Mistake on negative, then update 𝑤𝑡+1←𝑤𝑡−𝑥 
Note:   𝑤𝑡 is weighted sum of incorrectly classified examples  
𝑤𝑡=𝑎𝑖1𝑥𝑖1+⋯+𝑎𝑖𝑘𝑥𝑖𝑘 
𝑤𝑡⋅𝑥=𝑎𝑖1𝑥𝑖1⋅𝑥+⋯+𝑎𝑖𝑘𝑥𝑖𝑘⋅𝑥 
Important when we talk about kernels.  
Perceptron Algorithm: Example  
Example:  −1,2− 
- 
+ 
+ 
𝑤1=(0,0) 
𝑤2=𝑤1−−1,2=(1,−2) 
𝑤3=𝑤2+1,1=(2,−1) 
𝑤4=𝑤3−−1,−2=(3,1) + - 
- 
Algorithm:  
Set t= 1, start with all -zeroes weight vector 𝑤1. 
Given example 𝑥, predict positive iff 𝑤𝑡⋅𝑥≥0. 
On a mistake, update as follows:  
•Mistake on positive, update 𝑤𝑡+1←𝑤𝑡+𝑥 
•Mistake on negative, update 𝑤𝑡+1←𝑤𝑡−𝑥 1,0+ 
1,1+ 
−1,0− 
−1,−2− 
1,−1+ X 
 
X 
 
X 
 
Geometric Margin  
Definition:  The margin  of example 𝑥 w.r.t.  a linear sep. 𝑤 is 
the distance from 𝑥 to the plane 𝑤⋅𝑥=0  (or the negative  if on wrong side)  
𝑥1 
w Margin of positive example 𝑥1 
𝑥2 Margin of negative example 𝑥2 
Geometric Margin  
Definition:  The margin 𝛾𝑤 of a set of examples 𝑆 wrt a 
linear separator 𝑤 is the smallest margin over points 𝑥∈𝑆. 
+ 
+ + 
+ + + 
- 
- - 
- - 𝛾𝑤 𝛾𝑤 
+ 
- - - - + 
w Definition:  The margin  of example 𝑥 w.r.t.  a linear sep. 𝑤 is 
the distance from 𝑥 to the plane 𝑤⋅𝑥=0  (or the negative  if on wrong side)  
+ + 
+ + - 
- - 
- - 𝛾 𝛾 
+ 
- - - - w Definition:  The margin 𝛾 of a set of examples 𝑆 is the 
maximum  𝛾𝑤 over all linear separators 𝑤. Geometric Margin  
Definition:  The margin 𝛾𝑤 of a set of examples 𝑆 wrt a 
linear separator 𝑤 is the smallest margin over points 𝑥∈𝑆. Definition:  The margin  of example 𝑥 w.r.t.  a linear sep. 𝑤 is 
the distance from 𝑥 to the plane 𝑤⋅𝑥=0 (or the negative  if on wrong side)  
Perceptron: Mistake  Bound  
Theorem : If data has margin 𝛾 and all points inside a ball of 
radius 𝑅, then Perceptron makes ≤𝑅/𝛾2 mistakes.  
(Normalized margin: multiplying  all points by 100, or dividing all points by 100, 
doesn’t change the number of mistakes; algo is invariant to scaling.)  
+ 
w* + 
+ + 
+ + + 
- 
- - 
- -   
- - - 
- + 
w* 
R 
Perceptron Algorithm : Analysis  
Theorem: If data has margin 𝛾 and all 
points inside a ball of radius 𝑅, then 
Perceptron makes ≤𝑅/𝛾2 mistakes.  Update  rule:  
•Mistake  on positive: 𝑤𝑡+1←𝑤𝑡+𝑥 
•Mistake on negative: 𝑤𝑡+1←𝑤𝑡−𝑥 
Proof:  
Idea : analyze 𝑤𝑡⋅𝑤∗ and ‖𝑤𝑡‖, where 𝑤∗ is the max -margin sep, ‖𝑤∗‖=1.  
Claim 1: 𝑤𝑡+1⋅𝑤∗≥𝑤𝑡⋅𝑤∗+𝛾. 
Claim 2: 𝑤𝑡+12≤𝑤𝑡2+𝑅2. (because 𝑙𝑥𝑥⋅𝑤∗≥𝛾) 
(by Pythagorean Theorem)  
𝑤𝑡 𝑤𝑡+1 
𝑥 
After 𝑀 mistakes:  
𝑤𝑀+1⋅𝑤∗≥𝛾𝑀 (by Claim 1) 
𝑤𝑀+1≤𝑅𝑀 (by Claim 2) 
𝑤𝑀+1⋅𝑤∗≤‖𝑤𝑀+1‖  (since  𝑤∗ is unit  length)  
So, 𝛾𝑀≤𝑅𝑀, so 𝑀≤𝑅
𝛾2
. 
Perceptron Extensions  
•Can use it to find a consistent separator (by cycling 
through the data).  
•One can convert the mistake bound guarantee into a 
distributional guarantee too (for the case where the 𝑥𝑖s 
come from a fixed distribution).  
•Can be adapted to the case where there is no perfect 
separator as long as the so called hinge loss (i.e., the total 
distance needed to move the points to classify them correctly large 
margin)  is small.  
•Can be kernelized  to handle non -linear decision boundaries!  
Perceptron Discussion  
•Simple online algorithm for learning linear separators with 
a nice guarantee that depends only on the geometric 
(aka 𝐿2,𝐿2) margin.  
•Simple, but very useful in applications like Branch 
prediction; it also has interesting extensions to 
structured prediction.  •It can be kernelized  to handle non -linear decision 
boundaries --- see next class!  
Word Embeddings in Feedforward Networks;
Tagging and Dependency Parsing using
Feedforward Networks
Michael Collins, Columbia University
Overview
IIntroduction
IMulti-layer feedforward networks
IRepresenting words as vectors (\word embeddings")
IThe dependency parsing problem
IDependency parsing using a shift-reduce neural-network
model
Multi-Layer Feedforward Networks
IAn integerdspecifying the input dimension. A set Yof
output labels with jYj=K.
IAn integerJspecifying the number of hidden layers in the
network.
IAn integermjforj2f1:::Jgspecifying the number of
hidden units in the j'th layer.
IA matrixW12Rm1dand a vector b12Rm1associated with
the rst layer.
IFor eachj2f2:::Jg, a matrixWj2Rmjmj 1and a
vectorbj2Rmjassociated with the j'th layer.
IFor eachj2f1:::Jg, a transfer function gj:Rmj!Rmj
associated with the j'th layer.
IA matrixV2RKmJand a vector 
2RKspecifying the
parameters in the output layer.
Multi-Layer Feedforward Networks (continued)
ICalculate output of rst layer:
z12Rm1=W1xi+b1
h12Rm1=g1(z1)
ICalculate outputs of layers 2:::J :
Forj= 2:::J :
zj2Rmj=Wjhj 1+bj
hj2Rmj=gj(zj)
ICalculate output value:
l2RK=VhJ+bJ
q2RK=LS(l)
o2R= logqyi
Overview
IIntroduction
IMulti-layer feedforward networks
IRepresenting words as vectors (\word embeddings")
IThe dependency parsing problem
IDependency parsing using a shift-reduce neural-network
model
An Example: Part-of-Speech Tagging
Hispaniola/NNP quickly/RB became/VB an/DT important/JJ
base/?? from which Spain expanded its empire into the rest of the
Western Hemisphere .
There are many possible tags in the position ??
fNN, NNS, Vt, Vi, IN, DT, . . . g
The task: model the distribution p(tjjt1;:::;t j 1;w1:::w n)
wheretjis thej'th tag in the sequence, wjis thej'th word
The input to the neural network will be ht1:::t j 1;w1:::w n;ji
One-Hot Encodings of Words, Tags etc.
IA dictionary Dwith sizes(D)maps each word win the
vocabulary to an integer Index (w;D)in the range 1:::s(D).
Index (the;D) = 1
Index (dog;D) = 2
Index (cat;D) = 3
Index (saw;D) = 4
:::
IFor any word w, dictionary D, Onehot (w;D)maps a word w
to a \one-hot vector" u=Onehot (w;D)2Rs(D). We have
uj= 1 forj=Index (w;D)
uj= 0 otherwise
One-Hot Encodings of Words, Tags etc. (continued)
IA dictionary Dwith sizes(D)maps each word win the
vocabulary to an integer in the range 1:::s(D).
Index (the;D) = 1
Index (dog;D) = 2
Index (cat;D) = 3
:::
Onehot (the;D) = [1;0;0;:::]
Onehot (dog;D) = [0;1;0;:::]
Onehot (cat;D) = [0;0;1;:::]
:::
The Concatenation Operation
IGiven column vectors vi2Rdifori= 1:::n ,
z2Rd=Concat (v1;v2;:::vn)
whered=Pn
i=1di
Izis a vector formed by concatenating the vectors v1:::vn
Izis a column vector of dimensionP
idi
The Concatenation Operation (continued)
IGiven vectors vi2Rdifori= 1:::n ,
z2Rd=Concat (v1;v2;:::vn)
whered=Pn
i=1di
IThe Jacobians:@z
@vi2Rddi
have entries @z
@vi
j;k= 1
ifj=k+P
i0<idi0,
@z
@vi
j;k= 0
otherwise
A Single-Layer Computational Network for Tagging
Inputs: A training example xi=ht1:::tj 1;w1:::w n;ji,yi2Y. A
word dictionary Dwith sizes(D), a tag dictionary Twith sizes(T).
Parameters of a single-layer feedforward network.
Computational Graph:
t0
 22Rs(T)=Onehot (tj 2;T)
t0
 12Rs(T)=Onehot (tj 1;T)
w0
 12Rs(D)=Onehot (wj 1;D)
w0
02Rs(D)=Onehot (wj;D)
w0
+12Rs(D)=Onehot (wj+1;D)
u2R2s(T)+3s(D)=Concat (t0
 2;t0
 1;w0
 1;w0
0;w0
+1)
z=Wu+b; h =g(z); l=Vh+
; q =LS(l)
o=qyi
The Number of Parameters
t0
 22Rs(T)=Onehot (tj 2;T)
:::
w0
+12Rs(D)=Onehot (wj+1;D)
u=Concat (t0
 2;t0
 1;w0
 1;w0
0;w0
+1)
z2Rm=Wu+b
:::
IAn example: s(T) = 50 (50 tags),s(D) = 10;000 (10,000
words),m= 1000 (1000 neurons in the single layer)
IThen
W2Rm(2s(T)+3s(D))
andm= 1000 ,2s(T) + 3s(D) = 30;100, so there are
m(2s(T) + 3s(D)) = 30;100;000 parameters in the matrix W
An Example
Hispaniola/NNP quickly/RB became/VB an/DT important/JJ
base/?? from which Spain expanded its empire into the rest of the
Western Hemisphere .
t0
 22Rs(T)= Onehot (tj 2; T)
t0
 12Rs(T)= Onehot (tj 1; T)
w0
 12Rs(D)= Onehot (wj 1; D)
w0
02Rs(D)= Onehot (wj; D)
w0
+12Rs(D)= Onehot (wj+1; D)
u = Concat (t0
 2; t0
 1; w0
 1; w0
0; w0
+1)
: : :
Embedding Matrices
IGiven a word w, a word dictionary Dwe can map wto a
one-hot representation
w02Rs(D)1=Onehot (w;D)
INow assume we have an embedding dictionary E2Res(D)
whereeis some integer. Typical values of earee= 100 or
e= 200
IWe can now map the one-hot representation w0to
w00|{z}
e1=E|{z}
es(D)w0|{z}
s(D)1=EOnehot (w;D)
IEquivalently, a word wis mapped to a vector E(:j)2Re
wherej=Index (w;D)is the integer that word wis mapped
to, andE(:j)is thej'th column in the matrix.
Embedding Matrices vs. One-hot Vectors
IOne-hot representation:
w02Rs(D)1=Onehot (w;D)
This representation is high-dimensional, sparse
IEmbedding representation:
w00|{z}
e1=E|{z}
es(D)w0|{z}
s(D)1=EOnehot (w;D)
This representation is low-dimensional, dense
IThe embedding matrices can be learned using stochastic
gradient descent and backpropagation (each entry of Eis a
new parameter in the model)
ICritically, embeddings allow shared information between
words: e.g., words with similar meaning or syntax get
mapped to \similar" embeddings
A Single-Layer Computational Network for Tagging
Inputs: A training example xi=ht1:::tj 1;w1:::w n;ji,yi2Y. A
word dictionary Dwith sizes(D), a tag dictionary Twith sizes(T). A
word embedding matrix E2Res(D). A tag embedding matrix
A2Ras(D). Parameters of a single-layer feedforward network.
Computational Graph:
t0
 22Ra=AOnehot (tj 2;T)
t0
 12Ra=AOnehot (tj 1;T)
w0
 12Re=EOnehot (wj 1;D)
w0
02Re=EOnehot (wj;D)
w0
+12Re=EOnehot (wj+1;D)
u2R2a+3e=Concat (t0
 2;t0
 1;w0
 1;w0
0;w0
+1)
z=Wu+b; h =g(z); l=Vh+
; q =LS(l)
o=qyi
An Example
Hispaniola/NNP quickly/RB became/VB an/DT important/JJ
base/?? from which Spain expanded its empire into the rest of the
Western Hemisphere .
t0
 22Ra=AOnehot (tj 2; T)
t0
 12Ra=AOnehot (tj 1; T)
w0
 12Re=EOnehot (wj 1; D)
w0
02Re=EOnehot (wj; D)
w0
+12Re=EOnehot (wj+1; D)
u2R2a+3e= Concat (t0
 2; t0
 1; w0
 1; w0
0; w0
+1)
Calculating Jacobians
w0
02Re=EOnehot (w;D)
Equivalently:
(w0
0)j=X
kEj;kOnehot k(w;D)
INeed to calculate the Jacobian
@w0
0
E
This has entries
@w0
0
E
j;(j0;k)= 1 ifj=j0and Onehot k(w;E) = 1 ,0otherwise
An Additional Perspective
t0
 22Ra=Onehot (tj 2;T)
:::
w0
+12Re=Onehot (wj+1;D)
u=Concat (t0
 2:::w0
+1)
z2Rm=Wu+bt0
 22Ra=AOnehot (tj 2;T)
:::
w0
+12Re=EOnehot (wj+1;D)
u=Concat (t0
 2:::w0
+1)
z2Rm=Wu+b
IIf we set
W|{z}
m(2s(T)+3s(E))= W|{z}
m(2a+3e)Diag(A;A;E;E;E )|{z}
(2a+3e)(2s(T)+3s(D))
thenWu+b=Wu+bhencez= z
An Additional Perspective (continued)
IIf we set
W|{z}
m(2s(T)+3s(E))= W|{z}
m(2a+3e)Diag(A;A;E;E;E )|{z}
(2a+3e)(2s(T)+3s(D))
thenWu+b=Wu+bhencez= z
IAn example: s(T) = 50 (50 tags),s(D) = 10;000 (10,000
words),a=e= 100 (recalla,eare size of embeddings for
tags and words respectively), m= 1000 (1000 neurons)
IThen we have parameters
W|{z}
100030;100vs. W|{z}
1000500A|{z}
10050E|{z}
10010;000
Overview
IIntroduction
IMulti-layer feedforward networks
IRepresenting words as vectors (\word embeddings")
IThe dependency parsing problem
IDependency parsing using a shift-reduce neural-network
model
Unlabeled Dependency Parses
root John saw a movie
Iroot is a special root symbol
IEach dependency is a pair (h;m)wherehis the index of a head
word,mis the index of a modier word. In the gures, we
represent a dependency (h;m)by a directed edge from htom.
IDependencies in the above example are (0;2),(2;1),(2;4), and
(4;3). (We take 0to be the root symbol.)
The (Unlabeled) Dependency Parsing Problem
John saw a movie
+
root John saw a movie
Conditions on Dependency Structures
saw a movie John root he liked today that
IThe dependency arcs form a directed tree , with the root
symbol at the root of the tree.
(Denition: A directed tree rooted at root is a tree, where for
every wordwother than the root, there is a directed path
from root tow.)
IThere are no \crossing dependencies".
Dependency structures with no crossing dependencies are
sometimes referred to as projective structures.
All Dependency Parses for John saw Mary
root John saw Maryroot John saw Mary
root John saw Maryroot John saw Mary
root John saw Mary
The Labeled Dependency Parsing Problem
I live in New York city .
+

Overview
IIntroduction
IMulti-layer feedforward networks
IRepresenting words as vectors (\word embeddings")
IThe dependency parsing problem
IDependency parsing using a shift-reduce neural-network
model
Shift-Reduce Dependency Parsing: Congurations
IA conguration consists of:
1. A stackconsisting of a sequence of words, e.g.,
= [root 0;I1;live 2]
2. A buerconsisting of a sequence of words, e.g.,
= [in3;New 4;York 5;city6;.7]
3. A setoflabeled dependencies , e.g.,
=ff1!nsubj2g;f6!nn5g
The Initial Conguration
= [root 0];  = [I1;live 2;in3;New 4;York 5;city6;.7];  =fg
Shift-Reduce Actions: The Shift Action
The shift action takes the rst word in the buer, and adds it to
the end of the stack.
= [root 0];  = [I1;live 2;in3;New 4;York 5;city6;.7];  =fg
SHIFT
+
= [root 0;I1];  = [live 2;in3;New 4;York 5;city6;.7];  =fg
Shift-Reduce Actions: The Shift Action
The shift action takes the rst word in the buer, and adds it to
the end of the stack.
= [root 0;I1];  = [live 2;in3;New 4;York 5;city6;.7];  =fg
SHIFT
+
= [root 0;I1;live 2];  = [in3;New 4;York 5;city6;.7];  =fg
Shift-Reduce Actions: The Left-Arc Action
The LEFT-ARCnsubjaction takes the top two words on the stack,
adds a dependency between them in the left direction with label
nsubj , and removes the modier word from the stack. There is a
LEFT-ARClaction for each possible dependency label l.
= [root 0;I1;live 2];  = [in3;New 4;York 5;city6;.7];  =fg
LEFT-ARCnsubj
+
= [root 0;live 2];  = [in3;New 4;York 5;city6;.7];  =ff2!nsubj1gg
Shift-Reduce Actions: The Right-Arc Action
The RIGHT-ARCprepaction takes the top two words on the stack,
adds a dependency between them in the right direction with label
prep , and removes the modier word from the stack. There is a
RIGHT-ARClaction for each possible dependency label l.
= [root 0;live 2;in3];  = [.7];  =ff2!nsubj1g;g
RIGHT-ARCprep
+
= [root 0;live 2];  = [.7];  =ff2!nsubj1g;f2!prep3gg
Each Dependency Parse is Mapped to a Sequence of
Actions
Action   hl  !d
Shift [root 0] [I 1,live 2, in 3, New 4, York 5, city 6, .7]
Shift [root 0, I1] [live 2, in 3, New 4, York 5, city 6, .7]
Left-Arcnsubj[root 0, I1, live 2] [in 3, New 4, York 5, city 6, .7] 2nsubj   ! 1
Shift [root 0, live 2] [in 3, New 4, York 5, city 6, .7]
Shift [root 0, live 2, in 3] [New 4, York 5, city 6, .7]
Shift [root 0, live 2, in 3, New 4] [York 5, city 6, .7]
Shift [root 0, live 2, in 3, New 4, York 5] [city 6, .7]
Left-Arcnn[root 0, live 2, in 3, New 4, York 5, city 6] [. 7] 6nn !5
Left-Arcnn[root 0, live 2, in 3, New 4, city 6] [. 7] 6nn !4
Right-Arcpobj[root 0, live 2, in 3, city 6] [. 7] 3pobj  ! 6
Right-Arcprep[root 0, live 2, in 3] [. 7] 2prep   ! 3
Shift [root 0, live 2] [. 7]
Right-Arcpunct[root 0, live 2, .7] [] 2punct   ! 7
Right-Arcroot[root 0, live 2] [] 0root  ! 2
Terminal [root 0] []
Each Dependency Parse is Mapped to a Sequence of
Actions
IInputw1:::w n=I live in New York city .
IDependency parse requires actions a1:::a m, e.g.,
a1:::a m=hShift;Shift;LEFT-ARCnsubj;Shift;Shift;Shift;Shift;
LEFT-ARCnn;LEFT-ARCnn;RIGHT-ARCpobj;RIGHT-ARCprep;
Shift;RIGHT-ARCpunc;RIGHT-ARCrooti
IWe use a feedforward neural network to model
p(a1:::a mjw1:::w n) =mY
i=1p(aija1:::a i 1;w1:::w n)
Feature Extractors
IWe use a feedforward neural network to model
p(a1:::a mjw1:::w n) =mY
i=1p(aija1:::a i 1;w1:::w n)
INote that the action sequence a1:::a i 1maps to a
conguration ci=hi;i;ii
IAfeature extractor maps a (ci;w1:::w n)pair to either a
word, part-of-speech tag, or dependency label
IWeiss et al. 2015 (see also Chen and Manning 2014) have 20
word-based feature extractors, 20 tag-based feature
extractors, 12 dependency label feature extractors
IThis gives 20 + 20 + 12 = 52 one-hot vectors as input to a
neural network that estimates p(ajc;w 1:::w n)
Word-Based Feature Extractors
IAfeature extractor maps a (ci;w1:::w n)pair to either a
word, part-of-speech tag, or dependency label
Isifori= 1:::4is the index of the i'th element on the stack.
bifori= 1:::4is the index of the i'th element on the
buer.lc1(si)is the rst left-child of word si,lc2(si)is the
second left-child. rc1(si)andrc2(si)are the rst and second
right-children of si.
IWe then have features:
word(s1) word(s2) word(s3) word(s4) word(b1) word(b2) word(b3)
word(b4) word(lc1(s1)) word(lc1(s2)) word(lc2(s1)) word(lc2(s2))
word(rc1(s1)) word(rc1(s2)) word(rc2(s1)) word(rc2(s2))
word(lc1(lc1(s1)) word(lc1(lc1(s2)) word(rc1(rc1(s1)) word(rc1(rc1(s2))
Some Results
Method Unlabeled Dep. Accuracy
Global linear model192.9%
Neural network, greedy293.0%
Neural network, beam393.6%
Neural network, beam, global training494.6%
1. Hand-constructed features very similar to features in log-linear
models. Uses beam search in conjunction with a global linear model.
Transition-based Dependency Parsing with Rich Non-local Features,
Zhang and Nivre 2011.
2, 3: feedforward neural network with greedy search, or beam search.
Globally normalized transition-based neural networks. Andor et al.,
ACL 2016. See also A Fast and Accurate Dependency Parser using
Neural Network Chen and Manning, ACL 2014.
4: Neural network with global training, related to training of global
linear models (but with word embeddings, and non-linearities from a
neural network). See Andor et al. 2016.
CS224d:	Deep	NLPLecture	12:Midterm	ReviewRichard	Socherrichard@metamind.io
Overview	Today	–Mostly	open	for	questions!•Linguistic	Background:	Levels	and	tasks•Word	Vectors•Backprop•RNNs
5/5/16Richard	SocherLecture	1,	Slide	2
Overview	of	linguistic	levels
5/5/16Richard	SocherLecture	1,	Slide	3

Tasks:	NER
5/5/16Richard	SocherLecture	1,	Slide	4

Tasks:	POS
5/5/16Richard	SocherLecture	1,	Slide	5

Tasks:	Sentiment	analysis
5/5/16Richard	SocherLecture	1,	Slide	6
Machine	Translation
5/5/16Richard	SocherLecture	1,	Slide	7
Skip-gram
ITask: given a center word ,p r e d i c ti t s
context words
IFor each word, we have an “ input vector ”
vwand an “ output vector ”v0
w
w(t)          INPUT         PROJECTION      OUTPUTw(t-2)w(t-1)w(t+1)w(t+2)
Figure 1: New model architectures. The CBOW architecture predicts the current word based on thecontext, and the Skip-gram predicts surrounding words given the current word.Rwords from the future of the current word as correct labels. This will require us to doR 2word classiﬁcations, with the current word as input, and each of theR+Rwords as output. In thefollowing experiments, we useC= 10.4 ResultsTo compare the quality of different versions of word vectors, previous papers typically use a tableshowing example words and their most similar words, and understand them intuitively. Althoughit is easy to show that wordFranceis similar toItalyand perhaps some other countries, it is muchmore challenging when subjecting those vectors in a more complex similarity task, as follows. Wefollow previous observation that there can be many different types of similarities between words, forexample, wordbigis similar tobiggerin the same sense thatsmallis similar tosmaller. Exampleof another type of relationship can be word pairsbig - biggestandsmall - smallest[20]. We furtherdenote two pairs of words with the same relationship as a question, as we can ask: ”What is theword that is similar tosmallin the same sense asbiggestis similar tobig?”Somewhat surprisingly, these questions can be answered by performing simple algebraic operationswith the vector representation of words. To ﬁnd a word that is similar tosmallin the same sense asbiggestis similar tobig, we can simply compute vectorX=vector(”biggest”) vector(”big”) +vector(”small”). Then, we search in the vector space for the word closest toXmeasured by cosinedistance, and use it as the answer to the question (we discard the input question words during thissearch). When the word vectors are well trained, it is possible to ﬁnd the correct answer (wordsmallest) using this method.Finally, we found that when we train high dimensional word vectors on a large amount of data, theresulting vectors can be used to answer very subtle semantic relationships between words, such asa city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectorswith such semantic relationships could be used to improve many existing NLP applications, suchas machine translation, information retrieval and question answering systems, and may enable otherfuture applications yet to be invented.5
Skip-gram v.s. CBOW
Allword2vec ﬁgures are from http://arxiv.org/pdf/1301.3781.pdfSkip-gram CBOW
w(t)          INPUT         PROJECTION      OUTPUTw(t-2)w(t-1)w(t+1)w(t+2)
Figure 1: New model architectures. The CBOW architecture predicts the current word based on thecontext, and the Skip-gram predicts surrounding words given the current word.Rwords from the future of the current word as correct labels. This will require us to doR 2word classiﬁcations, with the current word as input, and each of theR+Rwords as output. In thefollowing experiments, we useC= 10.4 ResultsTo compare the quality of different versions of word vectors, previous papers typically use a tableshowing example words and their most similar words, and understand them intuitively. Althoughit is easy to show that wordFranceis similar toItalyand perhaps some other countries, it is muchmore challenging when subjecting those vectors in a more complex similarity task, as follows. Wefollow previous observation that there can be many different types of similarities between words, forexample, wordbigis similar tobiggerin the same sense thatsmallis similar tosmaller. Exampleof another type of relationship can be word pairsbig - biggestandsmall - smallest[20]. We furtherdenote two pairs of words with the same relationship as a question, as we can ask: ”What is theword that is similar tosmallin the same sense asbiggestis similar tobig?”Somewhat surprisingly, these questions can be answered by performing simple algebraic operationswith the vector representation of words. To ﬁnd a word that is similar tosmallin the same sense asbiggestis similar tobig, we can simply compute vectorX=vector(”biggest”) vector(”big”) +vector(”small”). Then, we search in the vector space for the word closest toXmeasured by cosinedistance, and use it as the answer to the question (we discard the input question words during thissearch). When the word vectors are well trained, it is possible to ﬁnd the correct answer (wordsmallest) using this method.Finally, we found that when we train high dimensional word vectors on a large amount of data, theresulting vectors can be used to answer very subtle semantic relationships between words, such asa city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectorswith such semantic relationships could be used to improve many existing NLP applications, suchas machine translation, information retrieval and question answering systems, and may enable otherfuture applications yet to be invented.5
w(t-2)
w(t+1)w(t-1)
w(t+2)w(t)SUM       INPUT         PROJECTION         OUTPUTFigure 1: New model architectures. The CBOW architecture predicts the current word based on thecontext, and the Skip-gram predicts surrounding words given the current word.Rwords from the future of the current word as correct labels. This will require us to doR 2word classiﬁcations, with the current word as input, and each of theR+Rwords as output. In thefollowing experiments, we useC= 10.4 ResultsTo compare the quality of different versions of word vectors, previous papers typically use a tableshowing example words and their most similar words, and understand them intuitively. Althoughit is easy to show that wordFranceis similar toItalyand perhaps some other countries, it is muchmore challenging when subjecting those vectors in a more complex similarity task, as follows. Wefollow previous observation that there can be many different types of similarities between words, forexample, wordbigis similar tobiggerin the same sense thatsmallis similar tosmaller. Exampleof another type of relationship can be word pairsbig - biggestandsmall - smallest[20]. We furtherdenote two pairs of words with the same relationship as a question, as we can ask: ”What is theword that is similar tosmallin the same sense asbiggestis similar tobig?”Somewhat surprisingly, these questions can be answered by performing simple algebraic operationswith the vector representation of words. To ﬁnd a word that is similar tosmallin the same sense asbiggestis similar tobig, we can simply compute vectorX=vector(”biggest”) vector(”big”) +vector(”small”). Then, we search in the vector space for the word closest toXmeasured by cosinedistance, and use it as the answer to the question (we discard the input question words during thissearch). When the word vectors are well trained, it is possible to ﬁnd the correct answer (wordsmallest) using this method.Finally, we found that when we train high dimensional word vectors on a large amount of data, theresulting vectors can be used to answer very subtle semantic relationships between words, such asa city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectorswith such semantic relationships could be used to improve many existing NLP applications, suchas machine translation, information retrieval and question answering systems, and may enable otherfuture applications yet to be invented.5Task Center word !Context Context !Center word
r vwi f(vwi C,··· ,vwi 1,vwi+1,··· ,vwi+C)
word2vec as matrix factorization (conceptually)
IMatrix factorization
2
4M3
5
n⇥n⇡2
4.
A>
.3
5
n⇥k⇥
. B.⇤
k⇥n
Mij⇡a>
ibj
IImagine Mis a matrix of counts for events co-occurring, but
we only get to observe the co-occurrences one at a time. E.g.
M=2
4104
002
1303
5
but we only see
(1,1), (2,3), (3,2), (2,3), (1,3), . . .
word2vec as matrix factorization (conceptually)
Mij⇡a>
ibj
IWhenever we see a pair ( i,j) co-occur, we try to increasing
a>
ibj
IWe also try to make all the other inner-products smaller to
account for pairs never observed (or unobserved yet), by
decreasing a>
¬ibjand a>
ib¬j
IRemember from the lecture that the word co-occurrence
matrix usually captures the semantic meaning of a word?
Forword2vec models, roughly speaking, Mis the windowed
word co-occurrence matrix, Ais the output vector matrix, and
Bis the input vector matrix.
IWhy not just use one set of vectors? It’s equivalent to A=B
in our formulation here, but less constraints is usually easier
for optimization.
GloVe v.s. word2vec
*S k i p - g r a ma n dC B O Wa r eq u a l i t a t i v e l yd i ↵erent when it comes to smaller corporaFast
trainingE cient
usage of
statisticsQuality
a↵ected
by size of
corporaCaptures
complex
patterns
Direct
prediction
(word2vec )Scales
with size
of corpusNo No* Yes
GloVeYes Yes No Yes
Overview • Neural Network Example • Terminology  • Example 1:  • Forward Pass • Backpropagation Using Chain Rule • What is delta? From Chain Rule to Modular Error Flow • Example 2: • Forward Pass • Backpropagation  CS224D: Deep Learning for NLP 2 

Neural Networks • One of many different types of non-linear classifiers (i.e. leads to non-linear decision boundaries)  • Most common design involves the stacking of affine transformations followed by point-wise (element-wise) non-linearity 
CS224D: Deep Learning for NLP 3 
An example of a neural network 
• This is a 4 layer neural network.  • 2 hidden-layer neural network. • 2-10-10-3 neural network (complete architecture defn.)  CS224D: Deep Learning for NLP 4 
Our first example 
• This is a 3 layer neural network  • 1 hidden-layer neural network 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s CS224D: Deep Learning for NLP 5 Layer 1 Layer 2 Layer 3 
Our first example:  Terminology 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3)  s CS224D: Deep Learning for NLP 6 
Model Input Model Output Layer 1 Layer 2 Layer 3 
Our first example:  Terminology 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3)  s CS224D: Deep Learning for NLP 7 
Model Input Model Output Activation Units Layer 1 Layer 2 Layer 3 
Our first example:  Activation Unit Terminology σz1(2) a1(2) CS224D: Deep Learning for NLP 8 z1(2) a1(2) σ+ 
We draw this This is actually what’s going on z1(2) = W11(1)a1(1) + W12(1)a2(1) + W13(1)a3(1) + W14(1)a4(1) a1(2) is the 1st activation unit of layer 2 a1(2) = σ(z1(2)) 
Our first example:  Forward Pass CS224D: Deep Learning for NLP 9 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s z1(1) = x1 z2(1) = x2 z3(1) = x3 z4(1) = x4  
Our first example:  Forward Pass CS224D: Deep Learning for NLP 10 
a1(1) = z1(1) a2(1) = z2(1) a3(1) = z3(1) a4(1) = z4(1) 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s 
Our first example:  Forward Pass CS224D: Deep Learning for NLP 11 
z1(2) = W11(1)a1(1) + W12(1)a2(1) + W13(1)a3(1) + W14(1)a4(1) z2(2) = W21(1)a1(1) + W22(1)a2(1) + W23(1)a3(1) + W24(1)a4(1) 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s 
Our first example:  Forward Pass CS224D: Deep Learning for NLP 12 
W11(1) W12(1) W13(1) W14(1) W21(1) W22(1) W23(1) W24(1) 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s a1(1) a2(1) a3(1) a4(1)  z1(2)  z2(2) = W(1) z(2) a(1) 
Our first example:  Forward Pass CS224D: Deep Learning for NLP 13 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s z(2) =W(1)a(1)  Affine transformation 
Our first example:  Forward Pass CS224D: Deep Learning for NLP 14 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s a(2) = σ(z(2)) Point-wise/Element-wise non-linearity 
Our first example:  Forward Pass CS224D: Deep Learning for NLP 15 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s z(3) = W(2)a(2) Affine transformation 
Our first example:  Forward Pass CS224D: Deep Learning for NLP 16 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s a(3) = z(3) s = a(3)   
Our first example:  Backpropagation using chain rule CS224D: Deep Learning for NLP 17 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s Let us try to calculate the error gradient wrt W14(1) Thus we want to find: 

Our first example:  Backpropagation using chain rule CS224D: Deep Learning for NLP 18 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s   Let us try to calculate the error gradient wrt W14(1) Thus we want to find: 

Our first example:  Backpropagation using chain rule CS224D: Deep Learning for NLP 19 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s   
This is simply 1 

Our first example:  Backpropagation using chain rule CS224D: Deep Learning for NLP 20 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s   !(!!!(!)!!!+!!!"(!)!!(!))!!!(!)!!!(!)!!!(!)!!!(!)!!!"(!)!!!!(!)!!!(!)!!!(!)!!!(!)!!!(!)!!!"(!)!
Our first example:  Backpropagation using chain rule CS224D: Deep Learning for NLP 21 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s   

Our first example:  Backpropagation using chain rule CS224D: Deep Learning for NLP 22 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s   !!!!!′!!!!!!(!)!!!"(!)!
Our first example:  Backpropagation using chain rule CS224D: Deep Learning for NLP 23 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s   

Our first example:  Backpropagation using chain rule CS224D: Deep Learning for NLP 24 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s   !!!!!′!!!!!(!)!δ1(2)  
Our first example:  Backpropagation Observations CS224D: Deep Learning for NLP 25 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s   We got error  gradient wrt W14(1)  Required:  • the signal forwarded by W14(1) =  a4(1)  • the error propagating backwards W11(2) • the local gradient σ’(z1(2))  
Our first example:  Backpropagation Observations CS224D: Deep Learning for NLP 26 1 1 1 1 1 x1 x2 x3 x4 z1(1) z2(1) z3(1) z4(1) z1(2) z2(2) a1(1) a4(1) a1(2) a2(2) z1(3) a1(3) s   We tried to get error  gradient wrt W14(1)  Required:  • the signal forwarded by W14(1) =  a4(1)  • the error propagating backwards W11(2) • the local gradient σ’(z1(2))  We can do this for all of W(1):  (as outer product)  δ1(2)a1(1)  δ1(2)a2(1)  δ1(2)a3(1)  δ1(2)a4(1)  δ2(2)a1(1)  δ2(2)a2(1)  δ2(2)a3(1)  δ2(2)a4(1)  δ1(2) δ2(2) a1(1) a2(1 a3(1) a4(1)  
Our first example:  Let us define δ CS224D: Deep Learning for NLP 27   z1(2) a1(2) σ+ 
Recall that this is forward pass   δ1(2) σ+ 
This is the backpropagation δ1(2) is the error flowing backwards at the same  point where z1(2) passed forwards. Thus it is simply the gradient of the error wrt z1(2). 
Our first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 28 The chain rule of differentiation just boils down very simple patterns in error backpropagation:  1. An error x flowing backwards passes a neuron by getting amplified by the local gradient.  2. An error δ that needs to go through an affine transformation distributes itself in the way signal combined in forward pass.   σx δ = σ’(z)x + δ z a1w1 a2w2 a3w3 δw1 δw2 δw3 Orange = Backprop.Green = Fwd. Pass  
Our first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 29 
1 σ 1 
z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s 
Our first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 30 
1 σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s δ(3) This is             for softmax 

Our first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 31 
1 σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s δ(3) Gradient w.r.t W(2) = δ(3)a(2)T 
Our first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 32 
1 σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s δ(3) W(2)T δ(3) --Reusing the δ(3) for downstream updates. --Moving error vector across affine transformation simply requires multiplication with the transpose of forward matrix --Notice that the dimensions will line up perfectly too! 
Our first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 33 
1 σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s W(2)T δ(3) σ’(z(2))!W(2)T δ(3)  = δ(2)  --Moving error vector across point-wise non-linearity requires point-wise multiplication with local gradient of the non-linearity 
Our first example:  Backpropagation using error vectors CS224D: Deep Learning for NLP 34 
1 σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) z(3) s δ(2) Gradient w.r.t W(1) = δ(2)a(1)T W(1)T δ(2) 
Our second example (4-layer network):  Backpropagation using error vectors CS224D: Deep Learning for NLP 35 
σ σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) softmax a(3)  W(3) z(3) z(4) yp 
Our second example (4-layer network):  Backpropagation using error vectors CS224D: Deep Learning for NLP 36 
σ σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) softmax a(3)  W(3) z(3) z(4) yp yp– y = δ(4)   
Our second example (4-layer network):  Backpropagation using error vectors CS224D: Deep Learning for NLP 37 
σ σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) softmax a(3)  W(3) z(3) z(4) yp Grad W(3) = δ(4)a(3)T   
W(3)Tδ(4)   δ(4)   
Our second example (4-layer network):  Backpropagation using error vectors CS224D: Deep Learning for NLP 38 
σ σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) softmax a(3)  W(3) z(3) z(4) yp W(3)Tδ(4) δ(3)= σ’(z(3))!W(3)Tδ(4)  
Our second example (4-layer network):  Backpropagation using error vectors CS224D: Deep Learning for NLP 39 
σ σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) softmax a(3)  W(3) z(3) z(4) yp Grad W(2) = δ(3)a(2)T   
δ(3) W(2)Tδ(3)  
Our second example (4-layer network):  Backpropagation using error vectors CS224D: Deep Learning for NLP 40 
σ σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) softmax a(3)  W(3) z(3) z(4) yp W(2)Tδ(3) δ(2)= σ’(z(2))!W(2)Tδ(3)  
Our second example (4-layer network):  Backpropagation using error vectors CS224D: Deep Learning for NLP 41 
σ σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) softmax a(3)  W(3) z(3) z(4) yp δ(2) W(1)Tδ(2)  Grad W(1) = δ(2)a(1)T   
Our second example (4-layer network):  Backpropagation using error vectors CS224D: Deep Learning for NLP 42 
σ σ 1 z(1) a(1)  W(1) z(2) a(2)  W(2) softmax a(3)  W(3) z(3) z(4) yp W(1)Tδ(2)  Grad wrt input vector = W(1)Tδ(2)     
W(1)Tδ(2)  
CS224D Midterm Review
Ian Tenney
May 4, 2015
Outline
Backpropagation (continued)
RNN Structure
RNN Backpropagation
Backprop on a DAG
Example: Gated Recurrent Units (GRUs)
GRU Backpropagation
Outline
Backpropagation (continued)
RNN Structure
RNN Backpropagation
Backprop on a DAG
Example: Gated Recurrent Units (GRUs)
GRU Backpropagation
Basic RNN Structure
x(t)h(t)h(t 1)ˆy(t)
...
IBasic RNN ("Elman network")
IY ou’ve seen this on Assignment #2 (and also in Lecture #5)
Basic RNN Structure
x(t)h(t)h(t 1)ˆy(t)
...
ITwo layers between input and prediction, plus hidden state
h(t)=sigmoid⇣
Hh(t 1)+Wx(t)+b1⌘
ˆy(t)=softmax⇣
Uh(t)+b2⌘
Unrolled RNN
x(t)h(t)ˆy(t)
...h(t 1)
x(t 1)ˆy(t 1)
h(t 2)
x(t 2)ˆy(t 2)
h(t 3)
IHelps to think about as “unrolled” network: distinct nodes
for each timestep
IJust do backprop on this! Then combine shared gradients.
Backprop on RNN
IUsual cross-entropy loss ( k-class):
¯P(y(t)=j|x(t),...,x(1))=ˆ y(t)
j
J(t)(✓)=  kX
j=1y(t)
jlog ˆy(t)
j
IJust do backprop on this! First timestep ( ⌧=1):
@J(t)
@U@J(t)
@b2
@J(t)
@H    
(t)@J(t)
@h(t)@J(t)
@W    
(t)@J(t)
@x(t)
Backprop on RNN
IFirst timestep ( s=0):
@J(t)
@U@J(t)
@b2
@J(t)
@H    
(t)@J(t)
@h(t)@J(t)
@W    
(t)@J(t)
@x(t)
IBack in time ( s=1,2,..., ⌧ 1)
@J(t)
@H    
(t s)@J(t)
@h(t s)@J(t)
@W    
(t s)@J(t)
@x(t s)
Backprop on RNN
Yuck, that’s a lot of math!
IActually, it’s not so bad.
ISolution: error vectors (  )
Making sense of the madness
IChain rule to the rescue!
Ia(t)=Uh(t)+b2
Iˆy(t)=softmax (a(t))
IGradient is transpose of Jacobian:
raJ= 
@J(t)
@a(t)!T
=ˆy(t) y(t)= (2)( t)2Rk⇥1
INow dimensions work out:
@J(t)
@a(t)·@a(t)
@b2=( (2)( t))TI2R(1⇥k)·(k⇥k)=R1⇥k
Making sense of the madness
IChain rule to the rescue!
Ia(t)=Uh(t)+b2
Iˆy(t)=softmax (a(t))
IGradient is transpose of Jacobian:
raJ= 
@J(t)
@a(t)!T
=ˆy(t) y(t)= (2)( t)2Rk⇥1
INow dimensions work out:
@J(t)
@a(t)·@a(t)
@b2=( (2)( t))TI2R(1⇥k)·(k⇥k)=R1⇥k
Making sense of the madness
IChain rule to the rescue!
Ia(t)=Uh(t)+b2
Iˆy(t)=softmax (a(t))
IGradient is transpose of Jacobian:
raJ= 
@J(t)
@a(t)!T
=ˆy(t) y(t)= (2)( t)2Rk⇥1
INow dimensions work out:
@J(t)
@a(t)·@a(t)
@b2=( (2)( t))TI2R(1⇥k)·(k⇥k)=R1⇥k
Making sense of the madness
IChain rule to the rescue!
Ia(t)=Uh(t)+b2
Iˆy(t)=softmax (a(t))
IMatrix dimensions get weird:
@a(t)
@U2Rk⇥(k⇥Dh)
IBut we don’t need fancy tensors:
rUJ(t)= 
@J(t)
@a(t)·@a(t)
@U!T
= (2)( t)(h(t))T2Rk⇥Dh
INumPy: self.grads.U += outer(d2, hs[t])
Making sense of the madness
IChain rule to the rescue!
Ia(t)=Uh(t)+b2
Iˆy(t)=softmax (a(t))
IMatrix dimensions get weird:
@a(t)
@U2Rk⇥(k⇥Dh)
IBut we don’t need fancy tensors:
rUJ(t)= 
@J(t)
@a(t)·@a(t)
@U!T
= (2)( t)(h(t))T2Rk⇥Dh
INumPy: self.grads.U += outer(d2, hs[t])
Going deeper
IReally just need one simple pattern:
Iz(t)=Hh(t 1)+Wx(t)+b1
Ih(t)=f(z(t))
ICompute error delta ( s=0,1,2,...):
IFrom top:  (t)=⇥
h(t) (1 h(t))⇤
 UT (2)(t)
IDeeper:  (t s)=⇥
h(t s) (1 h(t s))⇤
 HT (t s+1)
IThese are just chain-rule expansions!
@J(t)
@z(t)=@J(t)
@a(t)·@a(t)
@h(t)·@h(t)
@z(t)=( (t))T
Going deeper
IReally just need one simple pattern:
Iz(t)=Hh(t 1)+Wx(t)+b1
Ih(t)=f(z(t))
ICompute error delta ( s=0,1,2,...):
IFrom top:  (t)=⇥
h(t) (1 h(t))⇤
 UT (2)(t)
IDeeper:  (t s)=⇥
h(t s) (1 h(t s))⇤
 HT (t s+1)
IThese are just chain-rule expansions!
@J(t)
@z(t)=@J(t)
@a(t)·@a(t)
@h(t)·@h(t)
@z(t)=( (t))T
Going deeper
IThese are just chain-rule expansions!
@J(t)
@b1    
(t)= 
@J(t)
@a(t)·@a(t)
@h(t)·@h(t)
@z(t)!
·@z(t)
@b1=( (t))T@z(t)
@b1
@J(t)
@H    
(t)= 
@J(t)
@a(t)·@a(t)
@h(t)·@h(t)
@z(t)!
·@z(t)
@H=( (t))T@z(t)
@H
@J(t)
@z(t 1)= 
@J(t)
@a(t)·@a(t)
@h(t)·@h(t)
@z(t)!
·@z(t)
@h(t 1)=( (t))T@z(t)
@z(t 1)
Going deeper
IAnd there’s shortcuts for them too:
 
@J(t)
@b1    
(t)!T
= (t)
 
@J(t)
@H    
(t)!T
= (t)·(h(t 1))T
 
@J(t)
@z(t 1)!T
=h
h(t 1) (1 h(t 1))i
 HT (t)= (t 1)
Outline
Backpropagation (continued)
RNN Structure
RNN Backpropagation
Backprop on a DAG
Example: Gated Recurrent Units (GRUs)
GRU Backpropagation
Motivation
IGated units with “reset” and “output” gates
IReduce problems with vanishing gradients
Figure : Y ou are likely to be eaten by a GRU. (Figure from Chung, et
al. 2014)
Intuition
IGates ziand riforeach hidden layer neuron
Izi,ri2[0,1]
I˜has “candidate” hidden layer
I˜h,z,rall depend on on x(t),h(t 1)
Ih(t)depends on h(t 1)mixed with ˜h(t)
Figure : Y ou are likely to be eaten by a GRU. (Figure from Chung, et
al. 2014)
Equations
Iz(t)=  
Wzx(t)+Uzh(t 1) 
Ir(t)=  
Wrx(t)+Urh(t 1) 
I˜h(t)=tanh 
Wx(t)+r(t) Uh(t 1) 
Ih(t)=z(t) h(t 1)+( 1  z(t)) ˜h(t)
IOptionally can have biases; omitted for clarity.
Figure : Y ou are likely to be eaten by a GRU. (Figure from Chung, et
al. 2014)
Same eqs. as Lecture 8, subscripts/superscripts as in Assignment #2.
Backpropagation
Multi-path to compute@J
@x(t)
IStart with  (t)=⇣
@J
@h(t)⌘T
2Rd
Ih(t)=z(t) h(t 1)+( 1  z(t)) ˜h(t)
IExpand chain rule into sum ( a.k.a. product rule ):
@J
@x(t)=@J
@h(t)·"
z(t) @h(t 1)
@x(t)+@z(t)
@x(t) h(t 1)#
+@J
@h(t)·"
(1 z(t)) @˜h(t)
@x(t)+@(1 z(t))
@x(t) ˜h(t)#
It gets (a little) better
Multi-path to compute@J
@x(t)
IDrop terms that don’t depend on x(t):
@J
@x(t)=@J
@h(t)·"
z(t) @h(t 1)
@x(t)+@z(t)
@x(t) h(t 1)#
+@J
@h(t)·"
(1 z(t)) @˜h(t)
@x(t)+@(1 z(t))
@x(t) ˜h(t)#
=@J
@h(t)·"
@z(t)
@x(t) h(t 1)+( 1  z(t)) @˜h(t)
@x(t)#
 @J
@h(t)@z(t)
@x(t) ˜h(t)
Almost there!
Multi-path to compute@J
@x(t)
INow we really just need to compute two things:
IOutput gate:
@z(t)
@x(t)=z(t) (1 z(t)) Wz
ICandidate ˜h:
@˜h(t)
@x(t)=( 1  (˜h(t))2) W
+( 1  (˜h(t))2) @r(t)
@x(t) Uh(t 1)
IOk, I lied - there’s a third.
IDon’t forget to check all paths!
Almost there!
Multi-path to compute@J
@x(t)
INow we really just need to compute two things:
IOutput gate:
@z(t)
@x(t)=z(t) (1 z(t)) Wz
ICandidate ˜h:
@˜h(t)
@x(t)=( 1  (˜h(t))2) W
+( 1  (˜h(t))2) @r(t)
@x(t) Uh(t 1)
IOk, I lied - there’s a third.
IDon’t forget to check all paths!
Almost there!
Multi-path to compute@J
@x(t)
INow we really just need to compute two things:
IOutput gate:
@z(t)
@x(t)=z(t) (1 z(t)) Wz
ICandidate ˜h:
@˜h(t)
@x(t)=( 1  (˜h(t))2) W
+( 1  (˜h(t))2) @r(t)
@x(t) Uh(t 1)
IOk, I lied - there’s a third.
IDon’t forget to check all paths!
Almost there!
Multi-path to compute@J
@x(t)
ILast one:
@r(t)
@x(t)=r(t) (1 r(t)) Wr
INow we can just add things up!
I(I’ll spare you the pain...)
Whew.
IWhy three derivatives?
IThree arrows from x(t)to distinct nodes
IFour paths total (@z(t)
@x(t)appears twice)

Whew.
IGRUs are complicated
IAll the pieces are simple
ISame matrix gradients that you’ve seen before

Summary
ICheck your dimensions!
IWrite error vectors  ; just parentheses around chain rule
ICombine simple operations to make complex network
IMatrix-vector product
IActivation functions (tanh, sigmoid, softmax)
Phrase -Based
Machine Translation
CMSC 723 / LING 723 / INST 725
MARINE CARPUAT
marine@cs.umd.edu
Noisy Channel Model
for Machine Translation
•The noisy channel model decomposes machine 
translation into two independent subproblems
–Language modeling
–Translation modeling / Alignment

Word Alignment with 
IBM Models 1, 2
•Probabilistic models with strong independence 
assumptions
•Alignments are hidden variables 
–unlike words which are observed
–require unsupervised learning (EM algorithm)
•Word alignments often used as building blocks 
for more complex translation models
–E.g., phrase -based machine translation
PHRASE -BASED MODELS
Phrase -based models
•Most common way to model P(F|E) nowadays 
(instead of IBM models)
Start position of 
f_i
End position of 
f_(i-1)
Probability of 
two consecutive 
English phrases 
being separated 
by a particular 
span in French
Phrase alignments are derived 
from word alignments
Get high confidence 
alignment links by
intersecting IBM 
word alignments 
from both directionsThis means that the 
IBM model represents 
P(Spanish|English )
Phrase alignments are derived 
from word alignments
Improve recall by adding 
some links from the 
union of alignments
Phrase alignments are derived 
from word alignments
Extract phrases that are consistent 
with word alignment
Phrase Translation Probabilities
•Given such phrases we can get the 
required statistics for the model from 

Phrase -based Machine Translation

DECODING
Decoding for phrase -based MT
•Basic idea
–search the space of possible English translations in an 
efficient manner.  
–According to our model


Decoding as Search
•Starting point: null state.  No French content 
covered, no English included.
•We’ll drive the search by 
–Choosing French word/phrases to “cover”, 
–Choosing a way to cover them
•Subsequent choices are pasted left -to-right to 
previous choices. 
•Stop: when all input words are covered.
Decoding
Maria no dio una bofetada a la bruja verde
Decoding
Maria no dio una bofetada a la bruja verde
Mary
Decoding
Maria no dio una bofetada a la bruja verde
Mary did not
Decoding
Maria no dio una bofetada a la bruja verde
Mary Did not slap
Decoding
Maria no dio una bofetada a la bruja verde
Mary Did not slap the
Decoding
Maria no dio una bofetada a la bruja verde
Mary Did not slap the green
Decoding
Maria no dio una bofetada a la bruja verde
Mary Did not slap the green witch
Decoding
Maria no dio una bofetada a la bruja verde
Mary did not slap the green witch
Decoding
•In practice: we need to incrementally 
pursue a large number of paths .
•Solution: heuristic search algorithm called 
“multi -stack beam search”
Space of possible English translations 
given phrase -based model

Stack decoding: a simplified view
Note: here “stack”  = priority queue
Three stages of stack decoding

“multi -stack beam search”
One stack per number of French 
words covered: so that we make 
apples -to-apples comparisons 
when pruning   
Beam -search pruning for each stack : prune 
high cost states (those “outside the beam”)
“multi -stack beam search”

Cost = current cost + future cost 
•Future cost = cost of translating remaining words in the 
French sentence
•Exact future cost = minimum probability of all remaining 
translations
–Too expensive to compute !
•Approximation
–Find sequence of English phrases that has the minimum product 
of language model and translation model costs
Recombination
•Two distinct hypothesis paths might lead to the 
same translation hypotheses
–Same number of source words translated
–Same output words
–Different scores
•Recombination
–Drop worse hypothesis
Recombination
•Two distinct hypothesis paths might lead to 
hypotheses that are indistinguishable in 
subsequent search
–Same number of source words translated
–Same last 2 output words (assuming 3 -gram LM)
–Different scores
•Recombination
–Drop worse hypothesis
Complexity Analysis
•Time complexity of decoding as described so far
O(max stack size x sentence length^2 )
–O( max stack size x number of ways to expand hyps . x sentence 
length)
Reordering Constraints
Idea: limit reordering to maximum reordering distance
Typically: 5 to 8 words
-Depending on language pair
-Empirically: larger limit hurts translation quality
Resulting complexity: O(max stack size x sentence length)
–because we limit reordering distance, so that only a constant 
number of hypothesis expansions are considered
RECAP
Noisy Channel Model
for Machine Translation
•The noisy channel model decomposes machine 
translation into two independent subproblems
–Language modeling
–Translation modeling / Alignment

Phrase -Based Machine Translation
•Phrase -translation dictionary
Phrase -Based Machine Translation
•A simple model of translation
–Phrase translation dictionary (“phrase -table”)
•Extract all phrase pairs consistent with given 
alignment
•Use relative frequency estimates for translation 
probabilities
–Distortion model
•Allows for reorderings
Decoding in Phrase -Based 
Machine Translation
•Approach: Heuristic search 
•With several strategies to reduce the search 
space
–Pruning
–Recombination
–Reordering constraints
What are the pros and cons of
phrase -based vs. neural MT?
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  February 25, 2015 
Today: • Graphical models • Bayes Nets:  • Inference • Learning • EM Readings: • Bishop chapter 8 • Mitchell chapter 6  
Midterm • In class on Monday, March 2 • Closed book • You may bring a 8.5x11 “cheat sheet” of notes • Covers all material through today • Be sure to come on time.  We’ll start precisely at 12 noon  
Bayesian Networks Definition A Bayes network represents the joint probability distribution over a collection of random variables  A Bayes network is a directed acyclic graph and a set of conditional probability distributions (CPD’s) • Each node denotes a random variable • Edges denote dependencies • For each node Xi its CPD defines P(Xi | Pa(Xi))• The joint distribution over all variables is defined to be 
Pa(X) = immediate parents of X in the graph 
What You Should Know • Bayes nets are convenient representation for encoding dependencies / conditional independence • BN = Graph plus parameters of CPD’s – Defines joint distribution over variables – Can calculate everything else from that – Though inference may be intractable • Reading conditional independence relations from the graph – Each node is cond indep of non-descendents, given only its parents – X and Y are conditionally independent given Z if Z D-separates every path connecting X to Y – Marginal independence : special case where Z={}  
Inference in Bayes Nets • In general, intractable (NP-complete) • For certain cases, tractable – Assigning probability to fully observed set of variables – Or if just one variable unobserved – Or for singly connected graphs (ie., no undirected loops) • Belief propagation • Sometimes use Monte Carlo methods – Generate many samples according to the Bayes Net distribution, then count up the results • Variational methods for tractable approximate solutions 
Example • Bird flu and Allegies both cause Sinus problems • Sinus problems cause Headaches and runny Nose 

Prob. of joint assignment: easy  • Suppose we are interested in joint  assignment <F=f,A=a,S=s,H=h,N=n>  What is P(f,a,s,h,n)? 
let’s use p(a,b) as shorthand for p(A=a, B=b) 
Prob. of marginals: not so easy  • How do we calculate P(N=n) ?  
let’s use p(a,b) as shorthand for p(A=a, B=b) 
Generating a sample from  joint distribution: easy  How can we generate random samples drawn according to P(F,A,S,H,N)?  Hint: random sample of F according to P(F=1) = θF=1 : • draw a value of r uniformly from [0,1] • if r<θ  then output F=1, else F=0   let’s use p(a,b) as shorthand for p(A=a, B=b) 
Generating a sample from  joint distribution: easy  How can we generate random samples drawn according to P(F,A,S,H,N)?  Hint: random sample of F according to P(F=1) = θF=1 : • draw a value of r uniformly from [0,1] • if r<θ  then output F=1, else F=0  Solution: • draw a random value f for F, using its CPD • then draw values for A, for S|A,F, for H|S, for N|S  
Generating a sample from  joint distribution: easy   Note we can estimate marginals like P(N=n) by generating many samples from joint distribution, then count the fraction of samples for which N=n  Similarly, for anything else we care about   P(F=1|H=1, N=0)  à weak but general method for estimating any probability term… 

Inference in Bayes Nets • In general, intractable (NP-complete) • For certain cases, tractable – Assigning probability to fully observed set of variables – Or if just one variable unobserved – Or for singly connected graphs (ie., no undirected loops) • Variable elimination • Belief propagation • Often use Monte Carlo methods – e.g., Generate many samples according to the Bayes Net distribution, then count up the results – Gibbs sampling • Variational methods for tractable approximate solutions  see Graphical Models course 10-708 
Learning of Bayes Nets • Four categories of learning problems – Graph structure may be known/unknown – Variable values may be fully observed / partly unobserved • Easy case: learn parameters for graph structure is known, and data is fully observed  • Interesting case: graph known, data partly known • Gruesome case: graph structure unknown, data partly unobserved 
Learning CPTs from Fully Observed Data  Flu Allergy Sinus Headache Nose 
kth training example 
δ(x) = 1 if x=true,         =  0 if x=false • Example: Consider learning  the parameter • Max Likelihood Estimate is • Remember why? let’s use p(a,b) as shorthand for p(A=a, B=b) 
MLE estimate of         from fully observed data  • Maximum likelihood estimate • Our case: Flu Allergy Sinus Headache Nose 

Estimate     from partly observed data • What if FAHN observed, but not S? • Can’t calculate MLE • Let X be all observed variable values (over all examples) • Let Z be all unobserved variable values   • Can’t calculate MLE:  Flu Allergy Sinus Headache Nose 
•   WHAT TO DO? 
Estimate     from partly observed data • What if FAHN observed, but not S? • Can’t calculate MLE • Let X be all observed variable values (over all examples) • Let Z be all unobserved variable values   • Can’t calculate MLE:  Flu Allergy Sinus Headache Nose 
•   EM seeks* to estimate: * EM guaranteed to find local maximum 
Flu Allergy Sinus Headache Nose 
•   EM seeks estimate: 
•   here, observed X={F,A,H,N}, unobserved Z={S} 

EM Algorithm - Informally EM is a general procedure for learning from partly observed data Given  observed variables X, unobserved Z  (X={F,A,H,N}, Z={S})  Begin with arbitrary choice for parameters θ Iterate until convergence: •  E Step: estimate the values of unobserved Z, using θ   •  M Step: use observed values plus E-step estimates to                  derive a better θGuaranteed to find local maximum. Each iteration increases   

EM Algorithm - Precisely EM is a general procedure for learning from partly observed data Given  observed variables X, unobserved Z  (X={F,A,H,N}, Z={S}) Define Iterate until convergence: •  E Step: Use X and current θ to calculate P(Z|X,θ) •  M Step: Replace current θ by  
Guaranteed to find local maximum. Each iteration increases   

E Step: Use X, θ, to Calculate P(Z|X,θ) • How?  Bayes net inference problem. Flu Allergy Sinus Headache Nose 
observed X={F,A,H,N}, unobserved Z={S} 
let’s use p(a,b) as shorthand for p(A=a, B=b) 
E Step: Use X, θ, to Calculate P(Z|X,θ) • How?  Bayes net inference problem. Flu Allergy Sinus Headache Nose 
observed X={F,A,H,N}, unobserved Z={S} 
let’s use p(a,b) as shorthand for p(A=a, B=b) 
EM and estimating   Flu Allergy Sinus Headache Nose 
observed X = {F,A,H,N}, unobserved Z={S} E step:  Calculate P(Zk|Xk; θ) for each training example, k  
M step: update all relevant parameters.  For example: 
Recall MLE was: 

EM and estimating   Flu Allergy Sinus Headache Nose More generally,  Given observed set X, unobserved set Z of boolean values E step:  Calculate for each training example, k   the expected value of each unobserved variable   M step: 
Calculate estimates similar to MLE, but replacing each count by its expected count 

Using Unlabeled Data to Help Train  Naïve Bayes Classifier YX1 X4 X3 X2 Y X1 X2 X3 X4 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 ? 0 1 1 0 ? 0 1 0 1 Learn P(Y|X) 
E step:  Calculate for each training example, k   the expected value of each unobserved variable   
EM and estimating   Given observed set X, unobserved set Y of boolean values E step:  Calculate for each training example, k   the expected value of each unobserved variable Y M step: 
Calculate estimates similar to MLE, but replacing each count by its expected count 
let’s use y(k) to indicate value of Y on kth example 
EM and estimating   Given observed set X, unobserved set Y of boolean values E step:  Calculate for each training example, k   the expected value of each unobserved variable Y M step: 
Calculate estimates similar to MLE, but replacing each count by its expected count 
MLE would be: 
From [Nigam et al., 2000] 

Experimental Evaluation  • Newsgroup postings  – 20 newsgroups, 1000/group • Web page classification  – student, faculty, course, project – 4199 web pages • Reuters newswire articles  – 12,902 articles – 90 topics categories 
20 Newsgroups  

Using one labeled example per class word w ranked by P(w|Y=course) /P(w|Y ≠ course) 
20 Newsgroups  

Bayes Nets – What You Should Know  • Representation – Bayes nets represent joint distribution as a DAG + Conditional Distributions – D-separation lets us decode conditional independence assumptions • Inference – NP-hard in general – For some graphs, some queries, exact inference is tractable – Approximate methods too, e.g., Monte Carlo methods, … • Learning – Easy for known graph, fully observed data (MLE’s, MAP est.) – EM for partly observed data, known graph 
Tom Mitchell, William Cohen, and Many Collaborators Carnegie Mellon University          
Never-Ending Language Learning 
 We will never really understand learning until we build machines that  • learn many different things,  • from years of diverse experience, • in a staged, curricular fashion,  • and become better learners over time. 
Tenet 2:  Natural language understanding requires a belief system  A natural language understanding system should react to text by saying either: • I understand, and already knew that • I understand, and didn’t know, but accept it • I understand, and disagree because …   
NELL: Never-Ending Language Learner Inputs: • initial ontology (categories and relations) • dozen examples of each ontology predicate • the web • occasional interaction with human trainers  The task: • run 24x7, forever • each day: 1. extract more facts from the web to populate the ontology 2. learn to read (perform #1) better than yesterday 
NELL today Running 24x7, since January, 12, 2010  Result: •  knowledge base with 90 million candidate beliefs •  learning to read  •  learning to reason •  extending ontology  
                Globe and Mail Stanley Cup hockey 
NHL Toronto CFRB  Wilson 
play 
hired 
won 
Maple Leafs home town 
city paper 
league 
Sundin Milson writer 
radio 
Air Canada Centre 
team stadium 
Canada 
city stadium politician country Miller airport 
member Toskala 
Pearson 
Skydome Connaught Sunnybrook 
hospital 
city company skates helmet 
uses equipment 
won 
Red Wings Detroit 
hometown GM city company 
competes with Toyota plays in 
league 
Prius Corrola created Hino 
acquired 
automobile economic sector 
city stadium NELL knowledge fragment climbing football 
uses equipment * including only correct beliefs 
NELL Is Improving Over Time (Jan 2010 to Nov 2014) 
number of NELL beliefs vs. time all beliefs high conf. beliefs 10’s of millions 
millions reading accuracy vs. time (average over 31 predicates)  
precision@10 mean avg. precision top 1000 
human feedback vs. time (average 2.4 feedbacks per predicate per month) 
NELL Today • eg. “diabetes”, “Avandia”, “tea”, “IBM”, “love” “baseball”  “San Juan” “BacteriaCausesCondition” “kitchenItem” “ClothingGoesWithClothing”      …  

Portuguese NELL  
[Estevam Hruschka, 2014] 

How does NELL work? 
Semi-Supervised Bootstrap Learning Paris Pittsburgh Seattle Montpelier mayor of  arg1 live in  arg1 San Francisco Berlin denial 
arg1 is home of traits such as arg1  it’s underconstrained!!  anxiety selfishness London Learn which noun phrases are cities: 
 hard (underconstrained) semi-supervised learning problem Key Idea 1: Coupled semi-supervised training of many functions 
much easier (more constrained) semi-supervised learning problem 
person 
noun phrase 

NP: person 
 Type 1 Coupling: Co-Training, Multi-View Learning  Supervised training of 1 function:  Minimize:   
NP: person 
 
Type 1 Coupling: Co-Training, Multi-View Learning  Coupled training of 2 functions:  Minimize:   

NP: person 
 
Type 1 Coupling: Co-Training, Multi-View Learning [Blum & Mitchell; 98] [Dasgupta et al; 01 ] [Ganchev et al., 08] [Sridharan & Kakade, 08] [Wang & Zhou, ICML10] 
NELL: Learned reading strategies Mountain:        "volcanic crater of _"  "volcanic eruptions like _"  "volcanic peak of _"  "volcanic region of _"  "volcano , called _"  "volcano called _"  "volcano is called _"  "volcano known as _"  "volcano Mt _"  "volcano named _"  "volcanoes , including _"  "volcanoes , like _"  "volcanoes , such as _"  "volcanoes include _"  "volcanoes including _"  "volcanoes such as _"  "We 've climbed _"  "weather atop _"  "weather station atop _"  "week hiking in _"  "weekend trip through _"  "West face of _"  "West ridge of _"  "west to beyond _"  "white ledge in _"  "white summit of _"  "whole earth , is _"  "wilderness area surrounding _"  "wilderness areas around _"  "wind rent _"  "winter ascent of _"  "winter ascents in _"  "winter ascents of _"  "winter expedition to _"  "wooded foothills of _"  "world famous view of _"  "world famous views of _"  "you 're popping by _"  "you 've just climbed _"  "you just climbed _"  "you’ve climbed _"   "_ ' crater"  "_ ' eruption"  "_ ' foothills"  "_ ' glaciers"  "_ ' new dome"  "_ 's Base  Camp"  "_ 's drug guide"  "_ 's east rift zone"  "_ 's main summit"  "_ 's North Face"  "_ 's North Peak"  "_ 's North Ridge"  "_ 's northern slopes"  "_ 's southeast ridge"  "_ 's summit caldera"  "_ 's West Face"  "_ 's West Ridge"  "_ 's west ridge"  "_ (D,DDD ft"  ” "_ climbing permits"  "_ climbing safari"  "_ consult el diablo"  "_ cooking planks"  "_ dominates the sky line"  "_ dominates the western skyline"  "_ dominating the scenery” 

NP: person 
 
Type 1 Coupling: Co-Training, Multi-View Learning [Blum & Mitchell; 98] [Dasgupta et al; 01 ] [Ganchev et al., 08] [Sridharan & Kakade, 08] [Wang & Zhou, ICML10] 
team 
person 
NP: athlete coach sport 
NP text context distribution NP morphology NP HTML contexts 
Multi-view, Multi-Task Coupling [Blum & Mitchell; 98] [Dasgupta et al; 01 ] [Ganchev et al., 08] [Sridharan & Kakade, 08] [Wang & Zhou, ICML10] 
athlete(NP) à person(NP) 
athlete(NP) à NOT sport(NP) NOT athlete(NP) ß sport(NP) [Taskar et al., 2009] [Carlson et al., 2009] 
coachesTeam(c,t) playsForTeam(a,t) teamPlaysSport(t,s) playsSport(a,s) 
NP1 
NP2 
Type 3 Coupling: Relation Argument Types 
team coachesTeam(c,t) playsForTeam(a,t) teamPlaysSport(t,s) playsSport(a,s) 
person 
NP1 
athlete coach sport 
team 
person 
NP2 
athlete coach sport 
playsSport(NP1,NP2) à athlete(NP1), sport(NP2) Type 3 Coupling: Relation Argument Types 
over 2500 coupled functions in NELL 
Pure EM Approach to Coupled Training 
E: estimate labels for each function of each unlabeled example  M: retrain all functions, using these probabilistic labels  Scaling problem: • E step: 25M NP’s,  1014 NP pairs to label • M step: 50M text contexts to consider for each function à 1010 parameters to retrain • even more URL-HTML contexts…  
NELL’s Approximation to EM E’ step: • Re-estimate the knowledge base:   – but consider only a growing subset of the latent variable assignments  – category variables: up to 250 new NP’s per category per iteration – relation variables: add only if confident and args of correct type – this set of explicit latent assignments *IS* the knowledge base M’ step: • Each view-based learner retrains itself from the updated KB • “context” methods create growing subsets of contexts 
                           Continually Learning Reading Components Initial NELL Architecture Knowledge Base (latent variables)       Text Context patterns (CPL) HTML-URL context patterns (SEAL) Morphology classifier  (CML) 
 Beliefs Candidate Beliefs 
Knowledge Integrator  
Human advice   

If coupled learning is the key, how can we get new coupling constraints? 
Key Idea 2:   Discover New Coupling Constraints  • learn horn clause rules/constraints: – learned by data mining the knowledge base – connect previously uncoupled relation predicates – infer new unread beliefs – modified version of FOIL [Quinlan]  
 0.93  athletePlaysSport(?x,?y) ß athletePlaysForTeam(?x,?z)                                                        teamPlaysSport(?z,?y) 
team coachesTeam(c,t) playsForTeam(a,t) teamPlaysSport(t,s) playsSport(a,s) 
person 
NP1 
athlete coach sport 
team 
person 
NP2 
athlete coach sport 
Learned Probabilistic Horn Clause Rules  0.93  playsSport(?x,?y) ß playsForTeam(?x,?z), teamPlaysSport(?z,?y) 
If:  x1 
competes with (x1,x2) x2 
economic sector (x2, x3) x3 Then:  economic sector (x1, x3) economic sector  
Infer New Beliefs [Lao, Mitchell, Cohen, EMNLP 2011] 
   
If:  x1 
competes with (x1,x2) x2 
economic sector (x2, x3) x3 Then:  economic sector (x1, x3) economic sector  
Inference by Random Walks PRA:  [Lao, Mitchell, Cohen, EMNLP 2011] 
PRA:  1. restrict precondition  to a  chain.  2. inference by random walks 

Inference by KB Random Walks [Lao, Mitchell, Cohen, EMNLP 2011] KB:  Random walk path type:  logistic function for R(x,y)  where ith feature = probability of arriving at node y starting at node x, and taking a random walk along path of type i Pr( R(x,y) ): x 
competes with ? 
economic sector y 

      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry                              0.32 Pittsburgh  
Feature Value Logistic  Regresssion Weight CityLocatedInCountry(Pittsburgh) = ?  [Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry                                               0.32    Pittsburgh  Pennsylvania 
Feature Value Logistic  Regresssion Weight CityLocatedInCountry(Pittsburgh) = ?  [Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry                                               0.32      Pittsburgh  Pennsylvania Philadelphia Harisburg …(14) 
Feature Value Logistic  Regresssion Weight CityLocatedInCountry(Pittsburgh) = ?  [Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry                                              0.32      Pittsburgh  Pennsylvania Philadelphia Harisburg …(14) U.S. 
Feature Value Logistic  Regresssion Weight CityLocatedInCountry(Pittsburgh) = ?  [Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry               0.8                          0.32      Pittsburgh  Pennsylvania Philadelphia Harisburg …(14) U.S. 
Feature Value Logistic  Regresssion Weight CityLocatedInCountry(Pittsburgh) = ?  
 Pr(U.S. | Pittsburgh, TypedPath)  
[Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry               0.8                          0.32      AtLocation-1, AtLocation, CityLocatedInCountry                                               0.20        Pittsburgh  Pennsylvania Philadelphia Harisburg …(14) U.S. 
Feature Value Logistic  Regresssion Weight CityLocatedInCountry(Pittsburgh) = ?  [Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry               0.8                          0.32      AtLocation-1, AtLocation, CityLocatedInCountry                                               0.20        Pittsburgh  Pennsylvania Philadelphia Harisburg …(14) U.S. 
Feature Value Logistic  Regresssion Weight Delta PPG CityLocatedInCountry(Pittsburgh) = ?  [Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry               0.8                          0.32      AtLocation-1, AtLocation, CityLocatedInCountry                                               0.20        Pittsburgh  Pennsylvania Philadelphia Harisburg …(14) U.S. 
Feature Value Logistic  Regresssion Weight Delta PPG AtLocation Atlanta Dallas Tokyo CityLocatedInCountry(Pittsburgh) = ?  [Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry               0.8                          0.32      AtLocation-1, AtLocation, CityLocatedInCountry                0.6                          0.20        Pittsburgh  Pennsylvania Philadelphia Harisburg …(14) U.S. 
Feature Value Logistic  Regresssion Weight Delta PPG AtLocation Atlanta Dallas Tokyo Japan CityLocatedInCountry(Pittsburgh) = ?  
CityLocatedInCountry [Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry               0.8                          0.32      AtLocation-1, AtLocation, CityLocatedInCountry                0.6                          0.20       …                                                                                       …                             … Pittsburgh  Pennsylvania Philadelphia Harisburg …(14) U.S. 
Feature Value Logistic  Regresssion Weight CityLocatedInCountry(Pittsburgh) = U.S.    p=0.58 
Delta PPG AtLocation Atlanta Dallas Tokyo Japan CityLocatedInCountry(Pittsburgh) = ?  
CityLocatedInCountry [Lao, Mitchell, Cohen, EMNLP 2011] 
      Feature = Typed Path      CityInState, CityInstate-1, CityLocatedInCountry               0.8                          0.32      AtLocation-1, AtLocation, CityLocatedInCountry                0.6                          0.20       …                                                                                       …                             … Pittsburgh  Pennsylvania Philadelphia Harisburg …(14) U.S. 
Feature Value Logistic  Regresssion Weight CityLocatedInCountry(Pittsburgh) = U.S.    p=0.58 
Delta PPG AtLocation Atlanta Dallas Tokyo Japan CityLocatedInCountry(Pittsburgh) = ?  
CityLocatedInCountry 1. Tractable   (bounded length) 2. Anytime 3. Accuracy increases as KB grows 4. combines probabilities from different horn clauses [Lao, Mitchell, Cohen, EMNLP 2011] 
Random walk inference: learned rules CityLocatedInCountry(city, country):  8.04 cityliesonriver, cityliesonriver-1, citylocatedincountry  5.42 hasofficeincity-1, hasofficeincity, citylocatedincountry 4.98 cityalsoknownas, cityalsoknownas, citylocatedincountry 2.85 citycapitalofcountry,citylocatedincountry-1,citylocatedincountry  2.29 agentactsinlocation-1, agentactsinlocation, citylocatedincountry 1.22 statehascapital-1, statelocatedincountry  0.66 citycapitalofcountry  .  .  .  7 of the 2985 learned rules for CityLocatedInCountry 
Opportunity:     Can infer more if we start with more     densely connected knowledge graph à as NELL learns, it will become more dense à augment knowledge graph with a second graph of corpus statistics:      <subject, verb, object> triples [Gardner et al, 2014] 
can refer to hometown c:penguins 
c:pittsburgh river flows   through 
c:monongahela 
 “Pgh”  “Pittsburgh”  “Monongahela”  “Mon river”  “Penguins”  “Pens” 
can refer to 
can refer to 
NELL:  concepts  and “noun phrases” [Gardner et al, 2014] 
                    
can refer to hometown team:penguins 
city:pittsburgh river flows   through 
river:monongahela  “sits astride” 
 “overlooks”  “enters”  “runs through” 
 “Pgh”  “Pittsburgh”  “Monongahela”  “Mon river”  “Penguins”  “Pens” 
 “remain in” 
 “began in”  “supports”  “reminded” 
can refer to 
can refer to 
NELL:  concepts  and “noun phrases” 
SVO triples from 500 M dependency parsed web pages (thank you Chris Re!) [Gardner et al, 2014] 
                    
can refer to hometown c:penguins 
c:pittsburgh river flows   through 
c:monongahela  “sits astride” 
 “overlooks”  “enters”  “runs through” 
 “Pgh”  “Pittsburgh”  “Monongahela”  “Mon river”  “Penguins”  “Pens” 
 “remain in” 
 “began in”  “supports”  “reminded” 
can refer to 
can refer to 
NELL:  concepts  and “noun phrases” 
SVO triples from 500 M dependency parsed web pages (thank you Chris Re!) - Circumvents NELL’s fixed vocabulary of relations! - Sadly, adding these does not help: too sparse - But clustering verb phrases based on latent embedding (NNMF), produces significant improvement - {“lies on”, “runs through”, “flows through”, …} - Precision/recall over 15 NELL relations:  KB only:            0.80 / 0.33  KB + SVOlatent:  0.87 / 0.42  [Gardner et al., 2014] [Gardner et al, 2014] 
Key Idea 3:     Automatically extend ontology 
Ontology Extension (1) Goal: • Add new relations to ontology  Approach: • For each pair of categories C1, C2,  • cluster pairs of known instances, in terms of text contexts that connect them    [Mohamed et al., EMNLP 2011] 
Example Discovered Relations Category Pair Frequent Instance Pairs Text Contexts Suggested Name MusicInstrument Musician sitar, George Harrison tenor sax, Stan Getz trombone, Tommy Dorsey vibes, Lionel Hampton ARG1 master ARG2 ARG1 virtuoso ARG2 ARG1 legend ARG2 ARG2 plays ARG1 Master Disease Disease pinched nerve, herniated disk tennis elbow, tendonitis blepharospasm, dystonia ARG1 is due to ARG2 ARG1 is caused by ARG2 IsDueTo CellType Chemical  epithelial cells, surfactant neurons, serotonin mast cells, histomine ARG1 that release ARG2 ARG2 releasing ARG1 ThatRelease Mammals Plant koala bears, eucalyptus sheep, grasses goats, saplings ARG1 eat ARG2 ARG2 eating ARG1 Eat River City Seine, Paris Nile, Cairo Tiber river, Rome ARG1 in heart of ARG2 ARG1 which flows through ARG2 InHeartOf [Mohamed et al. EMNLP 2011] 
NELL: sample of self-added relations • athleteWonAward • animalEatsFood • languageTaughtInCity • clothingMadeFromPlant • beverageServedWithFood • fishServedWithFood • athleteBeatAthlete • athleteInjuredBodyPart • arthropodFeedsOnInsect • animalEatsVegetable • plantRepresentsEmotion • foodDecreasesRiskOfDisease • clothingGoesWithClothing • bacteriaCausesPhysCondition • buildingMadeOfMaterial • emotionAssociatedWithDisease • foodCanCauseDisease • agriculturalProductAttractsInsect • arteryArisesFromArtery • countryHasSportsFans • bakedGoodServedWithBeverage • beverageContainsProtein • animalCanDevelopDisease • beverageMadeFromBeverage 
Ontology Extension (2) Goal: • Add new subcategories  Approach: • For each category C,  • train NELL to read the relation     SubsetOfC: C à C    [Burr Settles] 
*no new software here,  just add this relation to  ontology 
NELL: subcategories discovered by reading Animal: • Pets – Hamsters, Ferrets, Birds, Dog, Cats, Rabbits, Snakes, Parrots, Kittens, … • Predators – Bears, Foxes, Wolves, Coyotes, Snakes, Racoons, Eagles, Lions, Leopards, Hawks, Humans, … Learned reading patterns for      "arg1 and other medium sized arg2"  "arg1 and other jungle arg2”  "arg1 and other magnificent arg2" "arg1 and other pesky arg2" "arg1 and other mammals and arg2"  "arg1 and other Ice Age arg2" "arg1 or other biting arg2" "arg1 and other marsh arg2"  "arg1 and other migrant arg2”  "arg1 and other monogastric arg2"  "arg1 and other mythical arg2"  "arg1 and other nesting arg2"  "arg1 and other night arg2"  "arg1  and other nocturnal arg2"  "arg1 and other nonhuman arg2"  "arg1 and other nuisance arg2"  "arg1 and other Old World arg2"  "arg1 and other pack arg2"  "arg1 and other parasites and arg2"    "arg1 and other pest arg2"  "arg1 and other pet arg2"  "arg1 and other plains arg2"  "arg1 and other plants and arg2"  "arg1 and other plush arg2"  "arg1 and other pocket arg2"  "arg1 and other pollinating arg2"  "arg1 and other predatory arg2"  "arg1 and other prey arg2"  "arg1 and other ranch arg2"  "arg1 and other rare species of arg2"  "arg1 and other ruminant arg2"  "arg1 and other sea arg2"  "arg1 and other shore arg2"  "arg1 and other similar sized arg2"  "arg1 and other small aquatic arg2"  "arg1 and other small arg2"  "arg1 and other small domestic arg2"  "arg1 and other small flying arg2"  "arg1 and oth AnimalSubset(arg1,arg2) 
NELL: subcategories discovered by reading Animal: • Pets – Hamsters, Ferrets, Birds, Dog, Cats, Rabbits, Snakes, Parrots, Kittens, … • Predators – Bears, Foxes, Wolves, Coyotes, Snakes, Racoons, Eagles, Lions, Leopards, Hawks, Humans, … Chemical: • Fossil fuels – Carbon, Natural gas, Coal, Diesel, Monoxide, Gases, … • Gases – Helium, Carbon dioxide, Methane, Oxygen, Propane, Ozone, Radon…  Learned reading patterns: "arg1 and other medium sized arg2"  "arg1 and other jungle arg2”  "arg1 and other magnificent arg2" "arg1 and other pesky arg2" "arg1 and other mammals and arg2"  "arg1 and other Ice Age arg2" "arg1 or other biting arg2" "arg1 and other marsh arg2"  "arg1 and other migrant arg2”  "arg1 and other monogastric arg2"  "arg1 and other mythical arg2"  "arg1 and other nesting arg2"  "arg1 and other night arg2"  "arg1  and other nocturnal arg2"  "arg1 and other nonhuman arg2"  "arg1 and other nuisance arg2"  "arg1 and other Old World arg2"  "arg1 and other pack arg2"  "arg1 and other parasites and arg2"    "arg1 and other pest arg2"  "arg1 and other pet arg2"  "arg1 and other plains arg2"  "arg1 and other plants and arg2"  "arg1 and other plush arg2"  "arg1 and other pocket arg2"  "arg1 and other pollinating arg2"  "arg1 and other predatory arg2"  "arg1 and other prey arg2"  "arg1 and other ranch arg2"  "arg1 and other rare species of arg2"  "arg1 and other ruminant arg2"  "arg1 and other sea arg2"  "arg1 and other shore arg2"  "arg1 and other similar sized arg2"  "arg1 and other small aquatic arg2"  "arg1 and other small arg2"  "arg1 and other small domestic arg2"  "arg1 and other small flying arg2"  "arg1 and oth Learned reading patterns: "arg1 and other hydrocarbon arg2”  "arg1 and other aqueous arg2”  "arg1 and other hazardous air arg2"  "arg1 and oxygen are arg2”   "arg1 and such synthetic arg2”   "arg1 as a lifting arg2"  "arg1 as a tracer arg2"  "arg1 as the carrier arg2”  "arg1 as the inert arg2"  "arg1 as the primary cleaning arg2”  "arg1 and other noxious arg2"  "arg1 and other trace arg2"   "arg1 as the reagent arg2"  "arg1 as the tracer arg2”  
          NELL Architecture Knowledge Base (latent variables)       Text Context patterns (CPL) Orthographicclassifier  (CML) 
 Beliefs Candidate Beliefs 
Evidence Integrator  
Human advice   
Actively search for web text (OpenEval) Infer new beliefs from old (PRA) Image classifier  (NEIL) 
Ontology extender  (OntExt) 
URL specific HTML patterns (SEAL) 
Key Idea 4:  Cumulative, Staged Learning 1. Classify noun phrases (NP’s) by category 2. Classify NP pairs by relation 3. Discover rules to predict new relation instances 4. Learn which NP’s (co)refer to which latent concepts 5. Discover new relations to extend ontology 6. Learn to infer relation instances via targeted random walks 7. Vision: connect NELL and NEIL  8. Learn to microread single sentences 9. Learn to assign temporal scope to beliefs 10. Goal-driven reading: predict, then read to corroborate/correct  11. Make NELL a conversational agent on Twitter 12. Add a robot body to NELL  Learning X improves ability to learn Y 
NELL is here  
Consistency  Correctness  Self reflection  
The core problem: • Agents can measure internal consistency,  but not correctness  Challenge: • Under what conditions does consistency à correctness? 

The core problem: • Agents can measure internal consistency,  but not correctness  Challenge: • Under what conditions does consistency à correctness? • Can an autonomous agent determine its accuracy from observed consistency?  
Problem setting:  • have N different estimates               of target function • agreement between fi, fj  : 
[Platanios, Blum, Mitchell, UAI 2014] 

Problem setting:  • have N different estimates               of target function • agreement between fi, fj  :  Key insight: errors and agreement rates are related 
[Platanios, Blum, Mitchell, UAI 2014] 
Pr[neither makes error] + Pr[both make error] prob. fi and fi  agree prob. fi  error prob. fj error prob.  fi and fj both make error 

Estimating Error from Unlabeled Data 1. IF f1 , f2 ,  f3 make indep. errors, and accuracies > 0.5      THEN          à   Measure errors from unlabeled data:  - use unlabeled data to estimate a12, a13, a23  - solve three equations for three unknowns e1, e2, e3     

Estimating Error from Unlabeled Data 1. IF f1 , f2 ,  f3 make indep. errors, accuracies > 0.5      THEN          à   2. but if errors not independent      

Estimating Error from Unlabeled Data 1. IF f1 , f2 ,  f3 make indep. errors, accuracies > 0.5      THEN          à   2. but if errors not independent  
        
True error (red), estimated error (blue) 
NELL classifiers: [Platanios, Blum, Mitchell, UAI 2014] 
True error (red), estimated error (blue) 
NELL classifiers: 
Brain image fMRI classifiers: 
[Platanios, Blum, Mitchell, UAI 2014] 
Summary 1. Use coupled training for semi-supervised learning 2. Datamine the KB to learn probabilistic inference rules 3. Automatically extend ontology 4. Use staged learning curriculum  New directions: • Self-reflection, self-estimates of accuracy (A. Platanios) • Incorporate vision with NEIL (Abhinav Gupta) • Microreading (Jayant Krishnamurthy, Ndapa Nakashole) • Aggressive ontology expansion (Derry Wijaya) • Portuguese NELL (Estevam Hrushka) • never-ending learning phones?  robots?  traffic lights? 
thank you   and thanks to:        Darpa, Google, NSF, Yahoo!, Microsoft, Fulbright, Intel  follow NELL on Twitter:  @CMUNELL browse/download NELL’s KB at http://rtw.ml.cmu.edu   

COMS 4721: Machine Learning for Data Science
Lecture 10, 2/21/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
FEATURE EXPANSIONS
FEATURE EXPANSIONS
Feature expansions (also called basis expansions ) are names given to a
technique we’ve already discussed and made use of.
Problem: A linear model on the original feature space x2Rddoesn’t work.
Solution: Map the features to a higher dimensional space (x)2RD, where
D>d, and do linear modeling there.
Examples
IFor polynomial regression on R, we let(x) = ( x;x2;:::; xp):
IFor jump discontinuities, (x) = ( x;1fx<ag).
MAPPING EXAMPLE FOR REGRESSION
xy
(a) Data for linear regression
y
x cos(x) (b) Same data mapped to higher dimension
High-dimensional maps can transform the data so output is linear in inputs.
Left: Original x2Rand response y.
Right: xmapped to R2using(x) = ( x;cosx)T.
MAPPING EXAMPLE FOR REGRESSION
Using the mapping (x) = ( x;cosx)T, learn the linear regression model
yw0+(x)Tw
w0+w1x+w2cosx:
y
x cos(x)
xy
Left: Learn (w0;w1;w2)to approximate data on the left with a plane.
Right: For each point x, map to(x)and predict y. Plot as a function of x.
MAPPING EXAMPLE FOR CLASSIFICATION
x1x2
(e) Data for binary classiﬁcation
x1x2x12
x22 (f) Same data mapped to higher dimension
High-dimensional maps can transform data so it becomes linearly separable.
Left: Original data in R2.
Right: Data mapped to R3using(x) = ( x2
1;x1x2;x2
2)T.
MAPPING EXAMPLE FOR CLASSIFICATION
Using the mapping (x) = ( x2
1;x1x2;x2
2)T, learn a linear classiﬁer
y=sign(w0+(x)Tw)
=sign(w0+w1x2
1+w2x1x2+w3x2
2):
x1x2x12
x22
x1x2
Left: Learn (w0;w1;w2;w3)to linearly separate classes with hyperplane.
Right: For each point x, map to(x)and classify. Color decision regions in R2.
FEATURE EXPANSIONS AND DOT PRODUCTS
What expansion should I use?
This is not obvious. The illustrations required knowledge about the data that
we likely won’t have (especially if it’s in high dimensions).
One approach is to use the “kitchen sink”: If you can think of it, then use it.
Select the useful features with an `1penalty
w`1=arg min
wnX
i=1f(yi;(xi);w) +kwk1:
We know that this will ﬁnd a sparse subset of the dimensions of (x)to use.
Often we only need to work with dot products (xi)T(xj)K(xi;xj). This
is called a kernel and can produce some interesting results.
KERNELS
PERCEPTRON (SOME MOTIVATION )
Perceptron classiﬁer
Letxi2Rd+1andyi2f  1;+1gfori=1;:::; nobservations. We saw that
the Perceptron constructs the hyperplane from data,
w=P
i2Myixi; (assume =1 andMhas no duplicates)
whereMis the sequentially constructed set of misclassiﬁed examples.
Predicting new data
We also discussed how we can predict the label y0for a new observation x0:
y0=sign(xT
0w) =sign P
i2MyixT
0xi
We’ve taken feature expansions for granted, but we can explicitly write it as
y0=sign((x0)Tw) =sign P
i2Myi(x0)T(xi)
We can represent the decision using dot products between data points.
KERNELS
Kernel deﬁnition
A kernel K(;) :RdRd!Ris a symmetric function deﬁned as follows:
Deﬁnition: If for any npoints x1;:::; xn2Rd, the nnmatrix K, where
Kij=K(xi;xj), ispositive semideﬁnite , then K(;)is a “kernel.”
Intuitively, this means Ksatisﬁes the properties of a covariance matrix.
Mercer’s theorem
If the function K(;)satisﬁes the above properties, then there exists a
mapping:Rd!RD(Dcan equal1) such that
K(xi;xj) =(xi)T(xj):
If we ﬁrst deﬁne ()and then K, this is obvious. However, sometimes we
ﬁrst deﬁne K(;)and avoid ever using ().
GAUSSIAN KERNEL (RADIAL BASIS FUNCTION )
The most popular kernel is the Gaussian kernel, also called the radial basis
function (RBF),
K(x;x0) =aexp
 1
bkx x0k2
:
IThis is a good, general-purpose kernel that usually works well.
IIt takes into account proximity in Rd. Things close together in space
have larger value (as deﬁned by kernel width b).
In this case, the the mapping (x)that produces the RBF kernel is inﬁnite
dimensional (it’s a continuous function instead of a vector). Therefore
K(x;x0) =Z
t(x)t(x0)dt:
It(x)can be thought of as a function of twith parameter xthat also has
a Gaussian form.
KERNELS
Another kernel
Map :(x) = ( 1;p
2x1;:::;p
2xd;x2
1;:::; x2
d;:::;p
2xixj;:::)
Kernel :(x)T(x0) =K(x;x0) = ( 1+xTx0)2
In fact, we can show K(x;x0) = ( 1+xTx0)b;forb>0 is a kernel as well.
Kernel arithmetic
Certain functions of kernels can produce new kernels.
LetK1andK2be any two kernels, then constructing Kin the following ways
produces a new kernel (among many other ways):
K(x;x0) = K1(x;x0)K2(x;x0)
K(x;x0) = K1(x;x0) +K2(x;x0)
K(x;x0) = expfK1(x;x0)g
KERNELIZED PERCEPTRON
Returning to the Perceptron
We write the feature-expanded decision as
y0=sign P
i2Myi(x0)T(xi)
=sign P
i2MyiK(x0;xi)
We can pick the kernel we want to use. Let’s pick the RBF (set a=1). Then
y0=signP
i2Myie 1
bkx0 xik2
Notice that we never actually need to calculate (x).
What is this doing?
INotice 0<K(x0;xi)1, with bigger values when x0is closer to xi.
IThis is like a “soft voting” among the data picked by Perceptron.
KERNELIZED PERCEPTRON
Learning the kernelized Perceptron
Recall: Given a current vector w(t)=P
i2M tyixi, we update it as follows,
1. Find a new x0such that y06=sign(x0Tw(t))
2. Add the index of x0toMand set w(t+1)=P
i2M t+1yixi
Again we only need dot products, meaning these steps are equivalent to
1. Find a new x0such that y06=sign(P
i2M tyiK(x0;xi))
2. Add the index of x0toMbut don’t bother calculating w(t+1)
The trick is to realize that we never need to work with (x).
IWe don’t need (x)to do Step 1 above.
IWe don’t need (x)to classify new data (previous slide).
IWe only ever need to calculate K(x;x0)between two points.
KERNEL k-NN
An extension
We can generalize kernelized Perceptron to soft k -NN with a simple change.
Instead of summing over misclassiﬁed data M, sum over allthe data:
y0=signPn
i=1yie 1
bkx0 xik2
:
Next, notice the decision doesn’t change if we divide by a positive constant.
Let : Z=Pn
j=1e 1
bkx0 xjk2
Construct : Vector p(x0), where pi(x0) =1
Ze 1
bkx0 xik2
Declare : y0=signPn
i=1yipi(x0)
IWe let all data vote for the label based on a “conﬁdence score” p(x0).
ISetbso that most pi(x0)0 to only focus on neighborhood around x0.
KERNEL REGRESSION
Nadaraya-Watson model
The developments are almost limitless.
Here’s a regression example almost identical to the kernelized k-NN:
Before: y2f  1;+1g
Now: y2R
Using the RBF kernel, for a new (x0;y0)predict
y0=nX
i=1yiK(x0;xi)Pn
j=1K(x0;xj):
What is this doing?
We’re taking a locally weighted average of all yifor which xiis close to x0
(as decided by the kernel width). Gaussian processes are another option :::
GAUSSIAN PROCESSES
KERNELIZED BAYESIAN LINEAR REGRESSION
Regression setup : For nobservations, with response vector y2Rnand their
feature matrix X, we deﬁne the likelihood and prior
yN(Xw;2I);wN(0; 1I):
Marginalizing : What if we integrate out w? We can solve this,
p(yjX) =Z
p(yjX;w)p(w)dw=N(0;2I+ 1XXT):
Kernelization : Notice that (XXT)ij=xT
ixj. Replace each xwith(x)after
which we can say [(X)(X)T]ij=K(xi;xj). We can deﬁne Kdirectly, so
p(yjX) =Z
p(yjX;w)p(w)dw=N(0;2I+ 1K):
This is called a Gaussian process . We never use wor(x), but just K(xi;xj).
GAUSSIAN PROCESSES
Deﬁnition
Letf(x)2Randx2Rd.
Deﬁne the kernel K (x;x0)between two points xandx0.
Then f(x)is aGaussian process andy(x)the noise-added process if for
nobserved pairs (x1;y1);:::; (xn;yn), where x2X andy2R,
yjfN(f;2I);fN(0;K)() yN(0;2I+K)
where y= (y1;:::; yn)TandKisnnwith Kij=K(xi;xj).
Comments:
IWe assume=1 to reduce notation.
ITypical breakdown: f(x)is the GP and y(x)equals f(x)plus i.i.d. noise.
IThe kernel is what keeps this from being “just a Gaussian.”
GAUSSIAN PROCESSES
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1−2−10123
xf(x)
Above: A Gaussian process f(x)generated using
K(xi;xj) =exp
 kxi xjk2
b
:
Right: The covariance of f(x)deﬁned by K.
0
01
1
GAUSSIAN PROCESSES
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1−2−10123
xf(x)
Top: Unobserved underlying function,
Bottom: Noisy observed data sampled from this function
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1−2−10123xy(x)oo
o o
oooo
o
o
o
oooo
ooo
ooooooo
PREDICTIONS WITH GAUSSIAN VECTORS
Bayesian linear regression
Imagine we have nobservation pairsD=f(xi;yi)gN
i=1and want to predict
y0given x0. Integrating out wand setting=1, the joint distribution is
y0
y
Normal
0; 2I+xT
0x0(Xx0)T
Xx0 XXT
We want to predict y0givenDandx0. Calculations can show that
y0jD;x0Normal (0;2
0)
0= ( Xx0)T(2I+XXT) 1y
2
0=2+xT
0x0 (Xx0)T(2I+XXT) 1(Xx0)
The since the inﬁnite Gaussian process is only evaluated at a ﬁnite set of
points, we can use this fact.
PREDICTIONS WITH GAUSSIAN PROCESSES
Predictive distribution of y(x)
Given measured data Dn=f(x1;y1);:::; (xn;yn)g, the distribution of y(x)
can be calculated at any new x to make predictions.
LetK(x;Dn) = [ K(x;x1);:::; K(x;xn)]andKnbe the nnkernel matrix
restricted points in Dn. Then we can show
y(x)jDnN((x);(x));
(x) = K(x;Dn)(2I+Kn) 1y;
(x) =2+K(x;x) K(x;Dn)(2I+Kn) 1K(x;Dn)T
For the posterior of f(x)instead of y(x), just remove 2.
GAUSSIAN PROCESSES POSTERIOR
Mean
Standard Dev
Observed values
Truth
xf(x)
What does the posterior distribution of f(x)look like?
IWe have data marked by an .
IThese values pin down the function f(x)nearby
IWe get a mean and variance for every possible xfrom a previous slide.
IThe distribution on y(x)adds variance 2(very small above) point-wise.
CMSC 422 Introduction to Machine LearningLecture 16A Probabilistic View of Machine Learning IIFurong Huang / furongh@cs.umd.eduSlides adapted from Prof Carpuatand Duraiswami
Today’s topicsBayes rule reviewA probabilistic view of machine learningJoint DistributionsBayes optimal classifierStatistical EstimationMaximum likelihood estimatesDerive relative frequency as the solution to a constrained optimization problem
The Bayes Optimal ClassifierAssume we know the data generating distribution !We define the Bayes Optimal classifierasTheorem:Of all possible classifiers, the Bayes Optimal classifier achieves the smallest zero/one lossBayes error rateDefined as the error rate of the Bayes optimal classifierBest error rate we can ever hope to achieve under zero/one loss

The Bayes Optimal ClassifierAssume we know the data generating distribution !We define the Bayes Optimal classifierasTheorem:Of all possible classifiers, the Bayes Optimal classifier achieves the smallest zero/one lossBayes error rateDefined as the error rate of the Bayes optimal classifierBest error rate we can ever hope to achieve under zero/one loss
If we had access to !, Finding an optimal classifier would be trivial!we don’t have access to !So let’s try to estimate it instead!
What does “training” mean in probabilistic settings?•Training = estimating !from a finite training set•We typically assume that !comes from a specific family of probability distributions•e.g., Bernouilli, Gaussian, etc•Learning means inferring parameters of that distributions•e.g., mean and covariance of the Gaussian
Training assumption: training examples are iid•Independently and Identically distributed•i.e. as we draw a sequence of examples from !, the n-thdraw is independent from the previous n-1 sample•This assumption is usually false!•But sufficiently close to true to be useful
How can we estimate the joint probability distribution from data?What are the challenges?
What we know so far…•Bayes rule•A probabilistic view of machine learning•If we know the data generating distribution, we can define the Bayes optimal classifier•Under iidassumption•How to estimate a probability distribution from data?•Maximum likelihood estimation
Maximum Likelihood Estimation•Find the parameters that maximize the probability of the data•Example: how to model a biased coin?(on board)
Maximum Likelihood EstimatesEach coin flip yields a Boolean value for XX ~ Bernouilli: !"=$!(1−$)"#!Given a data set D of iidflips, which contains )"ones and )$zeros!%(*)=$&'(1−$)&(+$)*+=,-./,0%!%*=)")"+)$

Maximum Likelihood Estimation•Exercise: how to model a k-sided die?(on board)
Let’s learn a classifierby learning P(Y|X)Goal: learn a classifier P(Y|X) Prediction: Given an example xPredict !"=$%&'$(!)*="+=()
Parameters for P(X,Y) vs. P(Y|X) 
Y = WealthX = <Gender, Hours_worked>Joint probability distribution P(X,Y)Conditional probability distribution P(Y|X)
How many parametersdo we need to learn?Suppose !=<!!,!",…!#>where !$and 'are Boolean random variablesQ:  How many parameters do we need to estimate (('|!!,!",…!#)?A: Too many to estimate P(Y|X) directly from data!
Naïve Bayes AssumptionNaïve Bayes assumes!"#,"%,…"'(=∏+,#'!("+|()i.e., that "+and "0are conditionally independent given Y, for all 1≠3
Conditional IndependenceDefinition:X is conditionally independent of Y given Zif P(X|Y,Z) = P(X|Z)Recall that X is independent of Y if P(X|Y)=P(X)
Naïve Bayes classifier!"=$%&'$(!)*="+=()=$%&'$(!)(*="))+=(*=")=$%&'$(!)(*=")."#$%)+"=("*=")Bayes rule+ Conditional independence assumption 
How many parameters do we need to learn? To describe P(Y)?To describe !"=<"%,"',…")>+)Without conditional independence assumption?With conditional independence assumption?(Suppose all random variables are Boolean)
Training a Naïve Bayes classifier

Naïve Bayes Wrap-upAn easy to implement classifier, that performs well in practice SubtletiesOften the Xi are not really conditionally independentWhat if the Maximum Likelihood estimate for P(Xi|Y) is zero?
What is the decision boundary of a Naïve Bayes classifier?
Naïve Bayes PropertiesNaïve Bayes is a linear classifierSee CIML for example of computation of Log Likelihood RatioChoice of probability distribution is a form of inductive bias

Generative StoriesProbabilistic models tell a fictional story explaining how our training data was createdExample of a generative story for a multiclass classification task with continuous features

From the Generative Story to the Likelihood Function

What you should knowThe Naïve Bayes classifierConditional independence assumptionHow to train it?How to make predictions?How does it relate to other classifiers we know?Fundamental Machine Learning conceptsiidassumptionBayes optimal classifierMaximum Likelihood estimationGenerative story
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
Semantics:	Roles	and	Relations	Prof.	Sameer	SinghCS	295:	STATISTICAL	NLPWINTER	2017February	14,	2017Based	on	slides	from	Jan	Jurafsky,	Noah	Smith,	Nathan	Schneider,	and	everyone	else	they	copied	from.
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)2Structured	PerceptronWord	SensesSemantic	Roles
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)3Structured	PerceptronWord	SensesSemantic	Roles
Structured	Prediction
CS	295:	STATISTICAL	NLP	(WINTER	2017)4
Likelihood	Learning
CS	295:	STATISTICAL	NLP	(WINTER	2017)5
Perceptron	Algorithm
CS	295:	STATISTICAL	NLP	(WINTER	2017)6
Structured	Perceptron
CS	295:	STATISTICAL	NLP	(WINTER	2017)7
Structured	Hinge	Loss
CS	295:	STATISTICAL	NLP	(WINTER	2017)8
Weight	Averaging
CS	295:	STATISTICAL	NLP	(WINTER	2017)9
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)10Structured	PerceptronWord	SensesSemantic	Roles
Words	and	Senses
CS	295:	STATISTICAL	NLP	(WINTER	2017)11Instead,	a	bankcan	hold	the	investments	in	a	custodial	account	in	the	client’s	name.But	as	agriculture	burgeons	on	the	east	bank,	the	river	will	shrink	even	more.Senses•bank1:	financial	institution•bank2:	sloping	moundEach	word	can	have	many	senses..Most	non-rare	words	in	English	do.
Homonymy
CS	295:	STATISTICAL	NLP	(WINTER	2017)12bank1bank2bat1bat2HomographsHomophoneswrite1right2peace1piece2Same	form,	completely	different	meanings…
ApplicationsInformation	Retrieval•“bat	care”Machine	Translation•Bat:	murcielagoor	bate?Text	to	Speech•“bass”	(fish)	or	“bass”	(guitar)Speech	to	Text•“piece”	or	“peace”
Polysemy
CS	295:	STATISTICAL	NLP	(WINTER	2017)13bank2bank3Same	form,	but	very	related	meanings…The	bankwas	constructed	in	1875	out	of	local	brick.I	withdrew	the	money	from	the	bank.MetronymySystemic	relationship	between	senses.BuildingOrganizationschool,	university,	hospitalAuthorWorks	of	the	AuthorJane	Austen	wrote	EmmaI	love	Jane	Austen!TreeFruitPlums	have	beautiful	blossomsI	ate	a	preserved	plum
Multiple	senses	or	not?
CS	295:	STATISTICAL	NLP	(WINTER	2017)14Does	Lufthansa	servebreakfast	and	San	Jose?Which	flights	servebreakfast?Does	Lufthansa	servePhiladelphia?“Zeugma”	TestSounds	weird,	so	there	are	multiple	senses	of	“serve”.You are free to execute your laws,and your citizens, as you see fit.Riker, Star Trek: The Next Generation

How	do	we	define	the	sense?
CS	295:	STATISTICAL	NLP	(WINTER	2017)15
Dictionary
Define	senses	in	relation	to	other	senses!
Synonyms
CS	295:	STATISTICAL	NLP	(WINTER	2017)16couch	/	sofabig	/	largeautomobile	/	carvomit	/	throw	upwater	/	H20Substitute	one	for	the	other	in	anysentence.Perfect	synonymy,	doesn’t	existMany	things	define	acceptability:	politeness,	slang,	register,	genreSubstitute	one	for	the	other	in	mostsentence.Synonymy	is	between	sense,	not	words
Antonyms
CS	295:	STATISTICAL	NLP	(WINTER	2017)17Sense	that	are	opposite	with	respect	to	one	feature	of	meaning..otherwise	very	similar!dark/light			short/longfast/slowrise/fallhot/coldup/downin/outbig/littleBinary	OppositionOr	at	opposite	ends	of	a	scaledark/light			short/longfast/slowhot/coldbig/littleReversivesOpposite	directions	or	changerise/fallup/downin/out
Hyponymy	and	Hypernymy
CS	295:	STATISTICAL	NLP	(WINTER	2017)18One	sense	is	a	hyponymof	another	if	the	first	sense	is	more	specific,	denoting	a	subclass	of	the	othercaris	a	hyponym	of	vehiclemangois	a	hyponym	of	fruitHyponyms	/	Subordinate
Conversely	hypernymdenotes	one	is	a	superclass	of	the	othervehicleis	a	hypernymof	carfruitis	a	hypernym	of	mangoHypernyms	/	Superordinate
WordNet
CS	295:	STATISTICAL	NLP	(WINTER	2017)19CategoryUnique	StringsNoun117,798Verb11,529Adjective22,479Adverb4,481

WordNet	Hierarchy
CS	295:	STATISTICAL	NLP	(WINTER	2017)20

Noun	Relations
CS	295:	STATISTICAL	NLP	(WINTER	2017)21

Verb	Relations
CS	295:	STATISTICAL	NLP	(WINTER	2017)22

Word	Sense	Disambiguation
CS	295:	STATISTICAL	NLP	(WINTER	2017)23The bassline of the song is too weak.

Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)24Structured	PerceptronWord	SensesSemantic	Roles
Meaning	is	Subtle
CS	295:	STATISTICAL	NLP	(WINTER	2017)25I’m	thrilled	to	visit	sunny	California.I’m	thrilled	to	visit	California,	where	the	weather	is	sunny.I’m	thrilled	to	visit	California,	where	it’s	sunny.I’m	excitedto	visit	California,	where	it’s	sunny.I’m	excited	to	visit	California,	where	it’s	sunny	out.I’m	excited	to	spend	time	in	California,	where	it’s	sunny	out.I’m	notexcited	to	visit	sunny	California.I’m	thrilled	to	visit	sunny	Florida.I’m	thrilled	to	visit	sunny	Mountain	View.I’m	thrilled	to	visit	California	because	it’s	sunny.I’m	sort	of	happy	about	the	California	visit.
Verbs	are	key!
CS	295:	STATISTICAL	NLP	(WINTER	2017)26

Syntax	≠	Semantics
CS	295:	STATISTICAL	NLP	(WINTER	2017)27

Need	for	“Roles”
CS	295:	STATISTICAL	NLP	(WINTER	2017)28The	police	officer	detainedthe	subject	at	the	scene	of	the	crime.Who?The	police	officerDid	what?detainedTo	whom?The	subjectWhere?at	the	scene	of	the	crimeWhen?-
Thematic	Roles
CS	295:	STATISTICAL	NLP	(WINTER	2017)29AgentExperiencerForceThemeResultThe	waiter	spilled	the	soup.ContentInstrumentBeneficiaryJohnhas	a	headache.The	wind	blows	debris	into	our	yard.Jesse	broke	the	window.The	city	built	a	regulation-sized	baseball	diamond.Mona	asked,	“You	met	Mary	Ann	at	the	supermarket?”He	poached	catfish,	stunning	them	with	a	shocking	device.Ann	Callahan	makes	hotel	reservations	for	her	boss.SourceGoalI	flew	in	from	Boston.I	drove	to	Portland.
Problem	with	Thematic	Roles
CS	295:	STATISTICAL	NLP	(WINTER	2017)30Difficult	to	have	a	good	set	of	roles	that	works	all	the	time,where	each	role	can	have	a	small,	concrete	definition47	high-level	classes,	divided	into	193	more	specific	classes-Levin	(1993),	VerbNetFewer	RolesPropBank“Proto”-arguments,	shared	across	verbsExact	definition	depends	on	verb	senseMore	RolesFrameNetEach	verb	sense	is	part	of	a	“frame”Each	frame	has	its	own	arguments
Prop	Bank
CS	295:	STATISTICAL	NLP	(WINTER	2017)31•“Frames”	are	verb	senses•Arguments	of	each	verb	are	mapped	onto	Arg0,	Arg1,	Arg2•Arguments	are	always	constituents	(annotated	over	syntax)
fall.01	(move	downward)
fall.08	(fall	back	on)
fall.10	(fall	for	a	trick)
FrameNet
CS	295:	STATISTICAL	NLP	(WINTER	2017)32•“Frames”	can	be	any	content	word	(~1000	frames)•Each	frame	has	its	own	argument	roles,	everything	is	hierarchical•Annotated	without	syntax,	arguments	can	be	anythingVerbSensesRoles	/ArgumentsFrameRelations	betweenFrames
“Change	position	on	a	scale”
CS	295:	STATISTICAL	NLP	(WINTER	2017)338CHAPTER22•SEMANTICROLELABELINGCore RolesATTRIBUTEThe ATTRIBUTEis a scalar property that the ITEMpossesses.DIFFERENCEThe distance by which an ITEMchanges its position on the scale.FINALSTATEA description that presents the ITEM’s state after the change in the ATTRIBUTE’svalue as an independent predication.FINALVA L U EThe position on the scale where the ITEMends up.INITIALSTATEA description that presents the ITEM’s state before the change in the AT-TRIBUTE’s value as an independent predication.INITIALVA L U EThe initial position on the scale from which the ITEMmoves away.ITEMThe entity that has a position on the scale.VALUERANGEA portion of the scale, typically identiﬁed by its end points, along which thevalues of the ATTRIBUTEﬂuctuate.Some Non-Core RolesDURATIONThe length of time over which the change takes place.SPEEDThe rate of change of the VALUE.GROUPThe GROUPin which an ITEMchanges the value of anATTRIBUTEin a speciﬁed way.Figure 22.3The frame elements in thechangepositiononascaleframe from the FrameNet LabelersGuide(Ruppenhofer et al., 2006).VERBS:dwindlemovesoarescalationshiftadvanceedgemushroomswellexplosiontumbleclimbexplodeplummetswingfalldeclinefallreachtripleﬂuctuationADVERBS:decreaseﬂuctuaterisetumblegainincreasinglydiminishgainrocketgrowthdipgrowshiftNOUNS:hikedoubleincreaseskyrocketdeclineincreasedropjumpslidedecreaseriseFrameNet also codes relationships between frames, allowing frames to inheritfrom each other, or representing relations between frames like causation (and gen-eralizations among frame elements in different frames can be representing by inher-itance as well). Thus, there is aCausechangeofpositiononascaleframe that islinked to theChangeofpositiononascaleframe by thecauserelation, but thatadds an AGENTrole and is used for causative examples such as the following:(22.26)[AGENTThey]raised[ITEMthe price of their soda] [DIFFERENCEby 2%].Together, these two frames would allow an understanding system to extract thecommon event semantics of all the verbal and nominal causative and non-causativeusages.FrameNets have also been developed for many other languages including Span-ish, German, Japanese, Portuguese, Italian, and Chinese.22.6 Semantic Role LabelingSemantic role labeling(sometimes shortened as SRL) is the task of automaticallysemantic rolelabelingﬁnding thesemantic rolesof each argument of each predicate in a sentence. Cur-rent approaches to semantic role labeling are based on supervised machine learning,often using the FrameNet and PropBank resources to specify what counts as a pred-icate, deﬁne the set of roles used in the task, and provide training and test sets.
“Change	position	on	a	scale”
CS	295:	STATISTICAL	NLP	(WINTER	2017)348CHAPTER22•SEMANTICROLELABELINGCore RolesATTRIBUTEThe ATTRIBUTEis a scalar property that the ITEMpossesses.DIFFERENCEThe distance by which an ITEMchanges its position on the scale.FINALSTATEA description that presents the ITEM’s state after the change in the ATTRIBUTE’svalue as an independent predication.FINALVA L U EThe position on the scale where the ITEMends up.INITIALSTATEA description that presents the ITEM’s state before the change in the AT-TRIBUTE’s value as an independent predication.INITIALVA L U EThe initial position on the scale from which the ITEMmoves away.ITEMThe entity that has a position on the scale.VALUERANGEA portion of the scale, typically identiﬁed by its end points, along which thevalues of the ATTRIBUTEﬂuctuate.Some Non-Core RolesDURATIONThe length of time over which the change takes place.SPEEDThe rate of change of the VALUE.GROUPThe GROUPin which an ITEMchanges the value of anATTRIBUTEin a speciﬁed way.Figure 22.3The frame elements in thechangepositiononascaleframe from the FrameNet LabelersGuide(Ruppenhofer et al., 2006).VERBS:dwindlemovesoarescalationshiftadvanceedgemushroomswellexplosiontumbleclimbexplodeplummetswingfalldeclinefallreachtripleﬂuctuationADVERBS:decreaseﬂuctuaterisetumblegainincreasinglydiminishgainrocketgrowthdipgrowshiftNOUNS:hikedoubleincreaseskyrocketdeclineincreasedropjumpslidedecreaseriseFrameNet also codes relationships between frames, allowing frames to inheritfrom each other, or representing relations between frames like causation (and gen-eralizations among frame elements in different frames can be representing by inher-itance as well). Thus, there is aCausechangeofpositiononascaleframe that islinked to theChangeofpositiononascaleframe by thecauserelation, but thatadds an AGENTrole and is used for causative examples such as the following:(22.26)[AGENTThey]raised[ITEMthe price of their soda] [DIFFERENCEby 2%].Together, these two frames would allow an understanding system to extract thecommon event semantics of all the verbal and nominal causative and non-causativeusages.FrameNets have also been developed for many other languages including Span-ish, German, Japanese, Portuguese, Italian, and Chinese.22.6 Semantic Role LabelingSemantic role labeling(sometimes shortened as SRL) is the task of automaticallysemantic rolelabelingﬁnding thesemantic rolesof each argument of each predicate in a sentence. Cur-rent approaches to semantic role labeling are based on supervised machine learning,often using the FrameNet and PropBank resources to specify what counts as a pred-icate, deﬁne the set of roles used in the task, and provide training and test sets.
“Change	position	on	a	scale”
CS	295:	STATISTICAL	NLP	(WINTER	2017)3522.5•FRAMENET7price of bananasis what went up, and that5%is the amount it went up, no matterwhether the5%appears as the object of the verbincreasedor as a nominal modiﬁerof the nounrise.TheFrameNetproject is another semantic-role-labeling project that attemptsFrameNetto address just these kinds of problems (Baker et al. 1998,Fillmore et al. 2003,Fillmore and Baker 2009,Ruppenhofer et al. 2006). Whereas roles in the PropBankproject are speciﬁc to an individual verb, roles in the FrameNet project are speciﬁcto aframe.What is a frame? Consider the following set of words:reservation, ﬂight, travel, buy, price, cost, fare, rates, meal, planeThere are many individual lexical relations of hyponymy, synonymy, and so onbetween many of the words in this list. The resulting set of relations does not,however, add up to a complete account of how these words are related. They areclearly all deﬁned with respect to a coherent chunk of common-sense backgroundinformation concerning air travel.We call the holistic background knowledge that unites these words aframe(Fill-framemore, 1985). The idea that groups of words are deﬁned with respect to some back-ground information is widespread in artiﬁcial intelligence and cognitive science,where besidesframewe see related works like amodel(Johnson-Laird, 1983), ormodelevenscript(Schank and Abelson, 1977).scriptA frame in FrameNet is a background knowledge structure that deﬁnes a set offrame-speciﬁc semantic roles, calledframe elements, and includes a set of predi-frame elementscates that use these roles. Each word evokes a frame and proﬁles some aspect of theframe and its elements. The FrameNet dataset includes a set of frames and frameelements, the lexical units associated with each frame, and a set of labeled examplesentences.For example, thechangepositiononascaleframe is deﬁned as follows:This frame consists of words that indicate the change of an Item’s posi-tion on a scale (the Attribute) from a starting point (Initialvalue) to anend point (Finalvalue).Some of the semantic roles (frame elements) in the frame are deﬁned as inFig.22.3. Note that these are separated intocore roles, which are frame speciﬁc, andCore rolesnon-core roles, which are more like the Arg-M arguments in PropBank, expressedNon-core rolesmore general properties of time, location, and so on.Here are some example sentences:(22.20)[ITEMOil]rose[ATTRIBUTEin price] [DIFFERENCEby 2%].(22.21)[ITEMIt] hasincreased[FINALSTATEto having them 1 day a month].(22.22)[ITEMMicrosoft shares]fell[FINALVA L U Eto 7 5/8].(22.23)[ITEMColon cancer incidence]fell[DIFFERENCEby 50%] [GROUPamongmen].(22.24)a steadyincrease[INITIALVA L U Efrom 9.5] [FINALVA L U Eto 14.3] [ITEMin dividends](22.25)a[DIFFERENCE5%] [ITEMdividend]increase...Note from these example sentences that the frame includes target words likerise,fall, andincrease. In fact, the complete frame consists of the following words:
Relations	between	Frames
CS	295:	STATISTICAL	NLP	(WINTER	2017)36eventchange_position_on_scalechange_of_temperatureproliferating_in_numberInherits	from:	Is	Inherited	by:Perspective	on:	Is	Perspectivizedin:	Uses:	Is	Used	by:	Subframeof:	Has	Subframe(s):	Precedes:	Is	Preceded	by:	Is	Inchoative	of:	Is	Causative	of:
Semantic	Role	Labeling
CS	295:	STATISTICAL	NLP	(WINTER	2017)37You	can’t	blamethe	program	for	being	unable	to	identify	it.The	San	Francisco	Examiner	issueda	special	edition	yesterday.CognizerEvalueeReasonFrameNet
Arg0Arg1ArgM-TmpPropBank
Approach	to	SRL	Predictions
CS	295:	STATISTICAL	NLP	(WINTER	2017)38

Features	for	SRL
CS	295:	STATISTICAL	NLP	(WINTER	2017)3910CHAPTER22•SEMANTICROLELABELINGSNP-SBJ=A R G 0VPDT NNP NNP NNPThe San Francisco ExaminerVBD= TARGETNP=A R G 1PP-TMP=A R G M - T M Pissued DT JJ NN IN NPas p e c i a l e d i t i o n a r o u n d N N N P - T M Pnoon yesterdayFigure 22.5Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted lineshows thepathfeature NP"S#VP#VBD for ARG0, the NP-SBJ constituentThe San Francisco Examiner.•Theheadwordof the constituent,Examiner. The headword of a constituentcan be computed with standard head rules, such as those given in Chapter 11in Fig.??. Certain headwords (e.g., pronouns) place strong constraints on thepossible semantic roles they are likely to ﬁll.•Theheadword part of speechof the constituent,NNP.•Thepathin the parse tree from the constituent to the predicate. This path ismarked by the dotted line in Fig.22.5. FollowingGildea and Jurafsky (2000),we can use a simple linear representation of the path, NP"S#VP#VBD."and#represent upward and downward movement in the tree, respectively. Thepath is very useful as a compact representation of many kinds of grammaticalfunction relationships between the constituent and the predicate.•Thevoiceof the clause in which the constituent appears, in this case,active(as contrasted withpassive). Passive sentences tend to have strongly differentlinkings of semantic roles to surface form than do active ones.•The binarylinear positionof the constituent with respect to the predicate,eitherbeforeorafter.•Thesubcategorizationof the predicate, the set of expected arguments thatappear in the verb phrase. We can extract this information by using the phrase-structure rule that expands the immediate parent of the predicate; VP!VBDNP PP for the predicate in Fig.22.5.•The named entity type of the constituent.•The ﬁrst words and the last word of the constituent.The following feature vector thus represents the ﬁrst NP in our example (recallthat most observations will have the value NONE rather than, for example,ARG0,since most constituents in the parse tree will not bear a semantic role):ARG0: [issued, NP, Examiner, NNP, NP"S#VP#VBD, active, before, VP!NP PP,ORG, The, Examiner]Other features are often used in addition, such as sets of n-grams inside theconstituent, or more complex versions of the path features (the upward or downwardhalves, or whether particular nodes occur in the path).It’s also possible to use dependency parses instead of constituency parses as thebasis of features, for example using dependency parse paths instead of constituencypaths.Headword	of	constituent:	ExaminerHeadword	POS:	NNPVoice	of	the	clause:	ActiveSubcategorization	of	pred:VP	->	VBD	NP	PPNamed	Entity	type	of	constituent:ORGANIZATIONFirst	and	last	words	of	constituent:The,	ExaminerLinear	position,clausere:	predicate:beforePath	features:NP↑S↓VP↓VBD
Typical	SRL	Pipeline
CS	295:	STATISTICAL	NLP	(WINTER	2017)40PruningUse	rules	to	filter	out	unlikely	constituents.IdentificationUse	a	classifier	to	further	filter	constituents.ClassificationUse	a	classifier	predict	multiple	roles	for	each	constituent.Joint	InferenceJointly	predict	a	consistent	set	of	roles.
SelectionalRestrictions
CS	295:	STATISTICAL	NLP	(WINTER	2017)41I	want	to	eat	someplace	nearby.Interpretation	1someplace	nearbyis	a	location	adjunct	(intransitive)Interpretation	2someplace	nearbyis	a	direct	object	(transitive	verb)Why	is	Interpretation	2	unlikely?Theme	of	“eat”	is	usually	edible.Introduce	constraints	based	on	WordNetIn	this	case,	it	should	be	“food,	nutrient”
SelectionalPreferences!
CS	295:	STATISTICAL	NLP	(WINTER	2017)42Instead	of	restrictions,	measure	association	scoresfor	each	role.how	often	a	class/noun	appears	as	an	argument.eatfood#n#1, aliment#n#1, entity#n#1, solid#n#1, food#n#2drinkﬂuid#n#1, liquid#n#1, entity#n#1, alcohol#n#1, beverage#n#1appointindividual#n#1, entity#n#1, chief#n#1, being#n#2, expert#n#1publishabstractentity#n#1, pieceofwriting#n#1, communication#n#2, publication#n#1Table 2: Most probable cuts learned by WN-CUTfor the object argument of selected verbsVerb-objectNoun-nounAdjective-nounSeenUnseenSeenUnseenSeenUnseenr r r r r r WN-CUT.593.582.514 .571.550 .584.564 .590.561 .618.453 .439WN-CUT-100.500 .529.575 .630.619.639.662 .706.537 .510.464.431WN-CUT-200.538 .546.557 .608.595 .632.639 .669.585.587.435 .431LDAWN-100.497 .538.558 .594.605 .619.635 .633.549 .545.459 .462LDAWN-200.546 .562.508 .548.610.654.526 .568.578 .583.453 .450Resnik.384 .473.469 .470.242 .187.152 .037.309 .388.311 .280Clark/Weir.489 .546.312 .365.441 .521.543 .576.440 .476.271 .242BNC (MLE).620 .614.196 .222.544 .604.114 .125.543.622.135 .102LDA.504 .541.558 .603.615 .641.636 .666.594.558.468 .459Table 3: Results (Pearsonrand Spearman correlations) on Keller and Lapata’s (2003) plausibility data; underliningdenotes the best-performing WordNet-based model, boldface denotes the overall best performance4.2 ResultsTable 2 demonstrates the top cuts learned by theWN-CUTmodel from the verb-object training datafor a selection of verbs. Table 3 gives quanti-tative results for the WordNet-based models un-der consideration, as well as results reported by´OS´eaghdha (2010) for a purely distributional LDAmodel with 100 topics and a Maximum LikelihoodEstimate model learned from the BNC. In general,the Bayesian WordNet-based models outperform themodels of Resnik and Clark and Weir, and are com-petitive with the state-of-the-art LDA results. Totest the statistical signiﬁcance of performance differ-ences we use the test proposed by Meng et al. (1992)for comparing correlated correlations, i.e., correla-tion scores with a shared gold standard. The dif-ferences between Bayesian WordNet models are notsigniﬁcant (p>0.05, two-tailed) for any dataset orevaluation measure. However, all Bayesian mod-els improve signiﬁcantly over Resnik’s and Clarkand Weir’s models for multiple conditions. Perhapssurprisingly, the relatively simple WN-CUTmodelscores the greatest number of signiﬁcant improve-ments over both Resnik (7 out of 12 conditions)and Clark and Weir (8 out of 12), though the otherBayesian models do follow close behind. This maysuggest that the incorporation of WordNet structureinto the model in itself provides much of the cluster-ing beneﬁt provided by an additional layer of “topic”latent variables.4In order to test the ability of the WordNet-basedmodels to make predictions about arguments thatare absent from the training vocabulary, we createdan artiﬁcial out-of-vocabulary dataset by removingeach of the Keller and Lapata argument words fromthe input corpus and retraining. An LDA selectionalpreference model will completely fail here, but wehope that the WordNet models can still make rela-tively accurate predictions by leveraging the addi-tional lexical knowledge provided by the hierarchy.For example, if one knows that a tomatillo is classedas a vegetable in WordNet, one can predict a rel-atively high probability that it can be eaten, eventhough the wordtomatillodoes not appear in theBNC.As a baseline we use a BNC-trained model that4An alternative hypothesis is that samplers for the morecomplex models take longer to “mix”. We have run some exper-iments with 5,000 iterations but did not observe an improvementin performance.176ClassesVerb Plaus./Implaus.ResnikDagan et al.ErkMIDSPsee friend/method5.79/-0.010.20/1.40*0.46/-0.071.11/-0.570.98/0.02read article/fashion6.80/-0.203.00/0.113.80/1.904.00/—2.12/-0.65ﬁnd label/fever1.10/0.221.50/2.20*0.59/0.010.42/0.071.61/0.81hear story/issue1.89/1.89*0.66/1.50*2.00/2.60*2.99/-1.031.66/0.67write letter/market7.26/0.002.50/-0.433.60/-0.245.06/-4.123.08/-1.31urge daughter/contrast1.14/1.86*0.14/1.60*1.10/3.60*-0.95/—-0.34/-0.62warn driver/engine4.73/3.611.20/0.052.30/0.622.87/—2.00/-0.99judge contest/climate1.30/0.281.50/1.90*1.70/1.70*3.90/—1.00/0.51teach language/distance1.87/1.862.50/1.303.60/2.703.53/—1.86/0.19show sample/travel1.44/0.411.60/0.140.40/-0.820.53/-0.491.00/-0.83expect visit/mouth0.59/5.93*1.40/1.50*1.40/0.371.05/-0.651.44/-0.15answer request/tragedy4.49/3.882.70/1.503.10/-0.642.93/—1.00/0.01recognize author/pocket0.50/0.50*0.03/0.37*0.77/1.30*0.48/—1.00/0.00repeat comment/journal1.23/1.23*2.30/1.402.90/—2.59/—1.00/-0.48understand concept/session1.52/1.512.70/0.252.00/-0.283.96/—2.23/-0.46remember reply/smoke1.31/0.202.10/1.200.54/2.60*1.13/-0.061.00/-0.42Table 2: Selectional ratings for plausible/implausible direct objects (Holmes et al., 1989). Mistakes are marked withan asterisk (*), undeﬁned scores are marked with a dash (—). Only DSPis completely deﬁned and completely correct.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
 0 0.2 0.4 0.6 0.8 1Interpolated PrecisionRecallDSP>TMI>TDSP>0MI>0
Figure 2: Pronoun resolution precision-recall on MUC.ther evidence, if we build a model of MI on the SJMcorpus and use it in our pseudodisambiguation ex-periment (Section 4.3), MI>0gets aMacroAvgpre-cision of 86% but aMacroAvgrecall of only 12%.94.6 Pronoun ResolutionFinally, we evaluate DSPon a common applicationof selectional preferences: choosing the correct an-tecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004). We study the cases where a9Recall that even the Keller and Lapata (2003) system, builton the world’s largest corpus, achieves only 34% recall (Table 1)(with only 48% of positives and 27% of all pairs previouslyobserved, but see Footnote 5).pronoun is the direct object of a verb predicate,v.Apronoun’s antecedent must obeyv’s selectional pref-erences. If we have a better model of SP, we shouldbe able to better select pronoun antecedents.We parsed the MUC-7 (1997) coreference corpusand extracted all pronouns in a direct object rela-tion. For each pronoun,p, modiﬁed by a verb,v,w eextracted all preceding nouns within the current orprevious sentence. Thirty-nine anaphoric pronounshad an antecedent in this window and are used inthe evaluation. For eachp, letN(p)+by the set ofpreceding nouns coreferent withp, and letN(p) be the remaining non-coreferent nouns. We takeall(v,n+)wheren+ N(p)+as positive, and allother pairs(v,n ),n  N(p) as negative.We compare MI and DSPon this set, classifyingevery(v,n)with MI>T(or DSP>T) as positive.By varyingT, we get a precision-recall curve (Fig-ure 2). Precision is low because, of course, thereare many nouns that satisfy the predicate’s SPs thatare not coreferent. DSP>0has both a higher recalland higher precision than accepting every pair pre-viously seen in text (the right-most point on MI>T).The DSP>Tsystem achieves higher precision thanMI>Tfor points where recall is greater than 60%(where MI<0). Interestingly, the recall of MI>0is66Nouns
Ó	Séaghdhaand	Korhonen(2012)Resnik(1996)
Upcoming…
CS	295:	STATISTICAL	NLP	(WINTER	2017)43•Homework	3	is	due	on	February	27•Write-up	and	data	will	be	released	very	soon.Homework•Status	report	due	in	~2	weeks:	March	2,	2017•Instructions	coming	soon•Only	5	pagesProject•Paper	summaries:	February	17,	February	28,	March	14•Only	1page	eachSummaries
Text	Classification	Contd+Document	RepresentationsProf.	Sameer	SinghCS	295:	STATISTICAL	NLPWINTER	2017January	17,	2017Based	on	slides	from	Nathan	Schneider,	Noah	Smith,	Dan	Klein	and	everyone	else	they	copied	from.
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)2Logistic	RegressionBrief	Intro	to	Neural	NetworksDocument	Representations
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)3Logistic	RegressionBrief	Intro	to	Neural	NetworksDocument	Representations
Text	Classification
CS	295:	STATISTICAL	NLP	(WINTER	2017)4Human	machine	interface	for	ABC	computer	applications•Human	Computer	Interaction•Theory•Artificial	Intelligence•SystemsPaper	TitleCS	Area
Linear	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)5Human	machine	interface	for	ABC	computer	applications
Matrix/Neural	View
CS	295:	STATISTICAL	NLP	(WINTER	2017)6
Naïve	Bayes	as	a	Linear	Model
CS	295:	STATISTICAL	NLP	(WINTER	2017)7
Joint	vs	Conditional	Likelihood
CS	295:	STATISTICAL	NLP	(WINTER	2017)8
Logistic	Regression	Model
CS	295:	STATISTICAL	NLP	(WINTER	2017)9
Logistic	Regression:	2	classes
CS	295:	STATISTICAL	NLP	(WINTER	2017)10
Estimating	the	parameters
CS	295:	STATISTICAL	NLP	(WINTER	2017)11
Gradient	Descent
CS	295:	STATISTICAL	NLP	(WINTER	2017)12
Tips	and	Tricks:	TF-IDF
CS	295:	STATISTICAL	NLP	(WINTER	2017)13Sparsity	of	Words•Remember	Zipf’sLaw?	Lots	of	rare	words•For	classification,	they	can	be	more	informative!
Tips	and	Tricks:	TF-IDF
CS	295:	STATISTICAL	NLP	(WINTER	2017)14Why	use	log(proportion)•It	works…•Importance	is	not	a	linear	function•IDF	is	an	additive	function
Tips	and	Tricks:	Regularization
CS	295:	STATISTICAL	NLP	(WINTER	2017)15Overfitting•Training	data	is	finite:	thus	has	spurious	correlations•Rare	words	that	occur	with	one	label!•Or	don’t	occur	often	enough•Curse	of	the	Zipf’sLaw	continues…
For	a	word	that	occurs	10	times…There	are	many	that	occur	~10	times!
Tips	and	Tricks:	Regularization
CS	295:	STATISTICAL	NLP	(WINTER	2017)16Fixing	Overfitting•Ignore	rare	words	(opposite	of	TF-IDF)•Penalize	really	high	weights…
Regularization	StrengthAccuracy
Tips	and	Tricks:	Featurizing
CS	295:	STATISTICAL	NLP	(WINTER	2017)17
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)18Logistic	RegressionBrief	Intro	to	Neural	NetworksDocument	Representations
Neural	View	of	Log.	Regression
CS	295:	STATISTICAL	NLP	(WINTER	2017)19
Linear	vs	Non-linear	Model
CS	295:	STATISTICAL	NLP	(WINTER	2017)20
Introducing	a	Hidden	Layer
CS	295:	STATISTICAL	NLP	(WINTER	2017)21
What	is	Deep	Learning?
CS	295:	STATISTICAL	NLP	(WINTER	2017)22Many	hidden	layers
In	NLP ,	utilize	unlabeled	data	to	learn	representations…(next	lecture)
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)23Logistic	RegressionBrief	Intro	to	Neural	NetworksDocument	Representations
Document	Similarity
CS	295:	STATISTICAL	NLP	(WINTER	2017)24Relation	of	user	perceived	response	time	to	error	measurementA	survey	of	user	opinion	of	computer	system	response	time
The	generation	of	random,	binary,	ordered	trees
Cosine	Distance
CS	295:	STATISTICAL	NLP	(WINTER	2017)25Advantages•Between	-1	and	1	(0	means	no	overlap)•If	all	>0,	it	is	between	0	and	1•Size	of	vectors	don’t	matter
Term	Document	Matrix
CS	295:	STATISTICAL	NLP	(WINTER	2017)26
Local	and	Global	Weighting
CS	295:	STATISTICAL	NLP	(WINTER	2017)27Local	Weighting•Binary:•Term	Freq:•Log:Global	Weighting•Binary:•Normal:•IDF:
Example:	Documents
CS	295:	STATISTICAL	NLP	(WINTER	2017)28c1:	Human	machine	interface	for	ABC	computer	applicationsc2:	A	survey	of	user	opinion	of	computer	system	response	timec3:	The	EPS	user	interface	management	systemc4:	System	and	human	system	engineering	testing	of	EPSc5:	Relation	of	user	perceived	response	time	to	error	measurementm1:	The	generation	of	random,	binary,	ordered	treesm2:	The	intersection	graph	of	paths	in	treesm3:	Graph	minors	IV:	Widths	of	trees	and	well-quasi-orderingm4:	Graph	minors:	A	surveyFrom	http://lsa.colorado.edu/papers/dp1.LSAintro.pdf
Example:	Term-Doc	Matrix
CS	295:	STATISTICAL	NLP	(WINTER	2017)29
c1 c2 c3 c4 c5 m1 m2 m3 m4humaninterfacecomputerusersystemresponsetimeEPSsurveytreesgraphminors
Example:	Distance	Matrix
CS	295:	STATISTICAL	NLP	(WINTER	2017)30
c1 c2 c3 c4 c5 m1 m2 m3 m4c1c2c3c4c5m1m2m3m4
Problems	with	Sparse	Vectors
CS	295:	STATISTICAL	NLP	(WINTER	2017)31c1:	Human	machine	interface	for	ABC	computer	applicationsc2:	A	survey	of	user	opinion	of	computer	system	response	timem4:	Graph	minors:	A	survey
Example:	Distance	Matrix
CS	295:	STATISTICAL	NLP	(WINTER	2017)32
c1 c2 c3 c4 c5 m1 m2 m3 m4c1c2c3c4c5m1m2m3m4
Option	1:	Clustering
CS	295:	STATISTICAL	NLP	(WINTER	2017)33
Example:	Clustering
CS	295:	STATISTICAL	NLP	(WINTER	2017)34
c1c2c3c4c5m1m2m3m4c1c2c3c4c5m1m2m3m4
Upcoming…
CS	295:	STATISTICAL	NLP	(WINTER	2017)35•Homework	1	is	up!•No	more	material	will	be	covered•Due:	January	26,	2017Homework•Project	pitch	is	due	January	23,	2017!•Start	assembling	teams	now•Tons	of	datasets	on	the	“projects”	page	on	websiteProject
CMSC 422 Introduction to Machine LearningLecture 18 Neural Networks IFurong Huang / furongh@cs.umd.eduSlides adapted from Prof Carpuatand Duraiswami
Neural NetworksTodayWhat are Neural Networks?How to make a prediction given an input?Why are neural networks powerful? Next Tuesdayhow to train them?
A warm-up examplesentiment analysis for movie reviewthe movie was horrible +1the actors are excellent -1the movie was not horrible -1he is usually an excellent actor, but not in this movie +1(on board)
Binary classificationvia hyperplanesAt test time, we check on what side of the hyperplane examples fall!"=$%&'()*++-)

Function Approximationwith PerceptronProblem settingSet of possible instances !Each instance "∈!is a feature vector "=["&,…,")]Unknown target function +:!→..is binary valued {-1; +1}Set of function hypotheses /=ℎℎ:!→.}Each hypothesis ℎis a hyperplane in D-dimensional spaceInputTraining examples {"&,3&,…"4,34}of unknown target function +OutputHypothesis ℎ∈/that best approximates target function +
Neural NetworksWe can think of neural networks as combination of multiple perceptronsMultilayer perceptronWhy would we want to do that?Discover more complex decision boundariesLearn combinations of features
What does a 2-layerperceptron look like?(illustration on board) Key concepts:Input dimensionalityHidden unitsHidden layerOutput layerActivation functions 
Activation functionsActivation functions are non-linearfunctionssign function as in the perceptronhyperbolic tangent and other sigmoid functions that approximate sign but are differentiableWhat happens if the hidden units use the identify function as an activation function? 
Matrix of hidden layer parametersVector of output layer parameters
What functions can we approximate with a 2 layer perceptron?Problem settingSet of possible instances !Each instance "∈!is a feature vector "=["&,…,")]Unknown target function +:!→..is binary valued {-1; +1}Set of function hypotheses /=ℎℎ:!→.}InputTraining examples {"&,3&,…"4,34}of unknown target function +OutputHypothesis ℎ∈/that best approximates target function +
Two-Layer Networks are Universal Function ApproximatorsTheorem (Th9 in CIML):Let F be a continuous function on a bounded subset of D-dimensional space. Then there exists a two-layer neural network !"with a finite number of hidden units that approximates F arbitrarily well. Namely, for all x in the domain of F, 

Example: a neural network to solve the XOR problemXOOXφ0(x2) = {1, 1}φ0(x1) = {-1, 1}φ0(x4) = {1, -1}φ0(x3) = {-1, -1}11-1-1-1-1φ1φ2
φ1[1]φ1[0]φ1[0]φ1[1]φ1(x1) = {-1, -1}XOφ1(x2) = {1, -1}Oφ1(x3) = {-1, 1}
φ1(x4) = {-1, -1}
Example●In new space, the examples are linearly separable!XOOXφ0(x2) = {1, 1}φ0(x1) = {-1, 1}φ0(x4) = {1, -1}φ0(x3) = {-1, -1}11-1-1-1-1φ0[0]φ0[1]
φ1[1]φ1[0]φ1[0]φ1[1]φ1(x1) = {-1, -1}XOφ1(x2) = {1, -1}Oφ1(x3) = {-1, 1}φ1(x4) = {-1, -1}-1-1-1φ2[0]= y
Example●The final nettanhtanhφ0[0]φ0[1]1φ0[0]φ0[1]111-1-1-1-11-1-1-1tanhφ1[0]φ1[1]φ2[0]
Discussion2-layer perceptron lets usDiscover more complex decision boundaries than perceptronLearn combinations of features that are useful for classificationKey design questionHow many hidden units?More hidden units yield more complex functionsFewer hidden units requires fewer examples to train
Neural NetworksTodayWhat are Neural Networks?Multilayer perceptronHow to make a prediction given an input?Simple matrix operations + non-linearitiesWhy are neural networks powerful? Universal function approximators!NextHow to train them?
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
COMS 4721: Machine Learning for Data Science
Lecture 8, 2/14/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
LINEAR CLASSIFICATION
BINARY CLASSIFICATION
We focus on binary classiﬁcation, with input xi2Rdand output yi2f 1g.
IWe deﬁne a classiﬁer f , which makes prediction yi=f(xi;)based on
a function of xiand parameters . In other words f:Rd!f  1;+1g.
Last lecture, we discussed the Bayes classiﬁcation framework.
IHere, contains: (1) class prior probabilities on y,
(2) parameters for class-dependent distribution on x.
This lecture we’ll introduce the linear classiﬁcation framework.
IIn this approach the prediction is linear in the parameters .
IIn fact, there is an intersection between the two that we discuss next.
A B AYES CLASSIFIER
Bayes decisions
With the Bayes classiﬁer we predict the class of a new xto be the most
probable label given the model and training data (x1;y1);:::; (xn;yn).
In the binary case, we declare class y=1 if
p(xjy=1)P(y=1)|{z}
1>p(xjy=0)P(y=0)|{z}
0
m
lnp(xjy=1)P(y=1)
p(xjy=0)P(y=0)>0
This second line is referred to as the log odds .
A B AYES CLASSIFIER
Gaussian with shared covariance
Let’s look at the log odds for the special case where
p(xjy) =N(xjy;)
(i.e., a single Gaussian with a shared covariance matrix)
lnp(xjy=1)P(y=1)
p(xjy=0)P(y=0)=ln1
0 1
2(1+0)T 1(1 0)
| {z }
a constant, call it w0
+xT 1(1 0)|{z}
a vector, call it w
This is also called “linear discriminant analysis” (used to be called LDA).
A B AYES CLASSIFIER
So we can write the decision rule for this Bayes classiﬁer as a linear one:
f(x) =sign(xTw+w0):
IThis is what we saw last lecture
(but now class 0 is called  1)
IThe Bayes classiﬁer produced a
linear decision boundary in the
data space when 1=  0.
Iwandw0are obtained through a
speciﬁc equation.
2.6. DISCRIMINANT FUNCTIONS FOR THE NORMAL DENSITY21
-2240.10.20.30.4
P(ω1)=.5 P(ω2)=.5p(x|ωi)
xω1ω2
R1R2-2024-2024
00.050.10.15
-2024-2024
R1R2P(ω2)=.5P(ω1)=.5-2-1012012
-2-1012
-2-1012012
P(ω1)=.5P(ω2)=.5R2R1Figure 2.10: If the covariances of two distributions are equal and proportional to theidentity matrix, then the distributions are spherical inddimensions, and the boundaryis a generalized hyperplane ofd−1 dimensions, perpendicular to the line separatingthe means. In these 1-, 2-, and 3-dimensional examples, we indicatep(x|ωi) and theboundaries for the caseP(ω1)=P(ω2). In the 3-dimensional case, the grid planeseparatesR1fromR2.wi=1σ2µi(52)andwi0=−12σ2µtiµi+ lnP(ωi).(53)We callwi0thethresholdorbiasin theith direction.thresholdbiasA classiﬁer that uses linear discriminant functions is called alinear machine. Thislinearmachinekind of classiﬁer has many interesting theoretical properties, some of which will bediscussed in detail in Chap.??. At this point we merely note that the decisionsurfaces for a linear machine are pieces of hyperplanes deﬁned by the linear equationsgi(x)=gj(x) for the two categories with the highest posterior probabilities. For ourparticular case, this equation can be written aswt(x−x0)=0,(54)wherew=µi−µj(55)andx0=12(µi+µj)−σ2∥µi−µj∥2lnP(ωi)P(ωj)(µi−µj).(56)This equation deﬁnes a hyperplane through the pointx0and orthogonal to thevectorw. Sincew=µi−µj, the hyperplane separatingRiandRjis orthogonal tothe line linking the means. IfP(ωi)=P(ωj), the second term on the right of Eq. 56vanishes, and thus the pointx0is halfway between the means, and the hyperplane isthe perpendicular bisector of the line between the means (Fig. 2.11). IfP(ωi)̸=P(ωj),the pointx0shifts away from the more likely mean. Note, however, that if the variance
LINEAR CLASSIFIERS
This Bayes classiﬁer is one instance of a linear classiﬁer
f(x) = sign(xTw+w0)
where
w0=ln1
0 1
2(1+0)T 1(1 0)
w=  1(1 0)
With MLE used to ﬁnd values for y;yand.
Setting w0andwthis way may be too restrictive:
IThis Bayes classiﬁer assumes single Gaussian with shared covariance.
IMaybe if we relax what values w0andwcan take we can do better.
LINEAR CLASSIFIERS (BINARY CASE )
Deﬁnition: Binary linear classiﬁer
Abinary linear classiﬁer is a function of the form
f(x) =sign(xTw+w0);
where w2Rdandw02R. Since the goal is to learn w;w0from data, we are
assuming that linear separability inxis an accurate property of the classes.
Deﬁnition: Linear separability
Two sets A;BRdare called linearly separable if
xTw+w0(
>0 if x2A(e.g, class +1)
<0 if x2B(e.g, class 1)
The pair (w;w0)deﬁnes an afﬁne hyperplane . It is important to develop the
right geometric understanding about what this is doing.
HYPERPLANES
Geometric interpretation of linear classiﬁers:
x1x2
H
wAhyperplane inRdis a linear subspace of
dimension (d 1).
IAR2-hyperplane is a line.
IAR3-hyperplane is a plane.
IAs a linear subspace, a hyperplane
always contains the origin.
A hyperplane Hcan be represented by a
vector was follows:
H=n
x2RdjxTw=0o
:
WHICH SIDE OF THE PLANE ARE WE ON ?
H
wx
kxk2cosDistance from the plane
IHow close is a point xtoH?
ICosine rule: xTw=kxk2kwk2cos
IThe distance of xto the hyperplane is
kxk2jcosj=jxTwj=kwk2:
SojxTwjgives a sense of distance.
Which side of the hyperplane?
IThe cosine satisﬁes cos >0 if2( 
2;
2).
ISo the sign of cos ()tells us the side of H, and by the cosine rule
sign(cos) =sign(xTw):
AFFINE HYPERPLANES
x1x2H
w
 w0=kwk2Afﬁne Hyperplanes
IAnafﬁne hyperplane H is a hyperplane
translated (shifted) using a scalar w0.
IThink of: H=xTw+w0=0.
ISetting w0>0 moves the hyperplane in the
opposite direction of w. (w0<0 in ﬁgure)
Which side of the hyperplane now?
IThe plane has been shifted by distance w0
kwk2in the direction w.
IFor a given w,w0and input xthe inequality xTw+w0>0 says that xis
on the far side of an afﬁne hyperplane Hin the direction wpoints.
CLASSIFICATION WITH AFFINE HYPERPLANES
Hw
sign(xTw+w0)<0sign(xTw+w0)>0 w0
kwk2
POLYNOMIAL GENERALIZATIONS
The same generalizations from regression also hold for classiﬁcation:
I(left) A linear classiﬁer using x= (x1;x2).
I(right) A linear classiﬁer using x= (x1;x2;x2
1;x2
2).
The decision boundary is linear in R4, but isn’t when plotted in R2.
ANOTHER BAYES CLASSIFIER
Gaussian with different covariance
Let’s look at the log odds for the general case where p(xjy) =N(xjy;y)
(i.e., now each class has its own covariance)
lnp(xjy=1)P(y=1)
p(xjy=0)P(y=0)=something complicated not involving x| {z }
a constant
+xT( 1
11  1
00)| {z }
a part that’s linear in x
+xT( 1
0=2  1
1=2)x| {z }
a part that’s quadratic in x
Also called “quadratic discriminant analysis,” but it’s linear in the weights.
ANOTHER BAYES CLASSIFIER
IWe also saw this last lecture.
INotice that
f(x) =sign(xTAx+xTb+c)
is linear in A;b;c.
IWhen x2R2, rewrite as
x (x1;x2;2x1x2;x2
1;x2
2)
and do linear classiﬁcation in R5.
26CHAPTER 2. BAYESIAN DECISION THEORY
-1001020-10010200.01.02.03.04p-1001020-20-1001020-100102000.0050.01p
-20-1001020
-1001020-1001000.010.020.03p
-1001020-1001020-1001000.010.020.030.040.05p
-1001020
-1001020-100102000.0050.01P
-100102000.0050.01p
-505-50500.050.10.15p
-505Figure 2.14: Arbitrary Gaussian distributions lead to Bayes decision boundaries thatare general hyperquadrics. Conversely, given any hyperquadratic, one can ﬁnd twoGaussian distributions whose Bayes decision boundary is that hyperquadric.
Whereas the Bayes classiﬁer with shared covariance is a version of linear
classiﬁcation, using different covariances is like polynomial classiﬁcation.
LEAST SQUARES ON f 1;+1g
How do we deﬁne more general classiﬁers of the form
f(x) =sign(xTw+w0) ?
IOne simple idea is to treat classiﬁcation as a regression problem:
1. Let y= (y1;:::; yn)T, where yi2f  1;+1gis the class of xi.
2. Add dimension equal to 1 to xiand construct the matrix X= [x1;:::; xn]T.
3. Learn the least squares weight vector w= (XTX) 1XTy.
4. For a new point x0declare y0=sign(xT
0w)   w0is included in w.
IAnother option: Instead of LS, use `pregularization.
IThese are “baseline” options. We can use them, along with k-NN, to get
a quick sense what performance we’re aiming to beat.
SENSITIVITY TO OUTLIERS
−4−202468−8−6−4−2024
−4−202468−8−6−4−2024
Least squares can do well, but it is sensitive to outliers. In general we can
ﬁnd better classiﬁers that focus more on the decision boundary.
I(left) Least squares (purple) does well compared with another method
I(right) Least squares does poorly because of outliers
THEPERCEPTRON ALGORITHM
EASY CASE : LINEARLY SEPARABLE DATA
(Assume data xihas a 1 attached.)
Suppose there is a linear classiﬁer
with zero training error:
yi=sign(xT
iw);for all i:
Then the data is “linearly separable”
Left: Can separate classes with a line.
(Can ﬁnd an inﬁnite number of lines.)
PERCEPTRON (ROSENBLATT , 1958)
Using the linear classiﬁer
y=f(x) =sign(xTw);
the Perceptron seeks to minimize
L= nX
i=1(yixT
iw)1fyi6=sign(xT
iw)g:
Because y2f  1;+1g,
yixT
iwis(
>0 if yi=sign(xT
iw)
<0 if yi6=sign(xT
iw)
By minimizingLwe’re trying to
always predict the correct label.
LEARNING THE PERCEPTRON
IUnlike other techniques we’ve talked about, we can’t ﬁnd the minimum
ofLby taking a derivative and setting to zero:
rwL=0 cannot be solved for wanalytically.
HoweverrwLdoes tell us the direction in which Lisincreasing inw.
ITherefore, for a sufﬁciently small , if we update
w0 w rwL;
thenL(w0)<L(w)— i.e., we have a better value for w.
IThis is a very general method for optimizing an objective functions
called gradient descent . Perceptron uses a “stochastic” version of this.
LEARNING THE PERCEPTRON
Input : Training data (x1;y1);:::; (xn;yn)and a positive step size 
1.Setw(1)=~0
2.For step t=1;2;::: do
a)Search for all examples (xi;yi)2D such that yi6=sign(xT
iw(t))
b)Ifsuch a (xi;yi)exists, randomly pick one and update
w(t+1)=w(t)+yixi;
Else: Return w(t)as the solution since everything is classiﬁed correctly.
IfMtindexes the misclassiﬁed observations at step t, then we have
L= nX
i=1(yixT
iw)1fyi6=sign(xT
iw)g;rwL= X
i2M tyixi:
The full gradient step is w(t+1)=w(t) rwL.Stochastic optimization just
picks out one element in rwL—we could have also used the full summation.
LEARNING THE PERCEPTRON
−1 −0.5 0 0.5 1−1−0.500.51
red= +1, blue = 1,=1
1. Pick a misclassiﬁed (xi;yi)
2. Set w w+yixi
LEARNING THE PERCEPTRON
−1 −0.5 0 0.5 1−1−0.500.51
red= +1, blue = 1,=1
The update to wdeﬁnes a new
decision boundary (hyperplane)
LEARNING THE PERCEPTRON
−1 −0.5 0 0.5 1−1−0.500.51
red= +1, blue = 1,=1
1. Pick another misclassiﬁed (xj;yj)
2. Set w w+yjxj
LEARNING THE PERCEPTRON
−1 −0.5 0 0.5 1−1−0.500.51
red= +1, blue = 1,=1
Again update w, i.e., the hyperplane
This time we’re done.
DRAWBACKS OF PERCEPTRON
The perceptron represents a ﬁrst attempt at linear classiﬁcation by directly
learning the hyperplane deﬁned by w. It has some drawbacks:
1. When the data is separable, there are an inﬁnite #of hyperplanes.
IWe may think some are better than others, but this algorithm doesn’t take
“quality” into consideration. It converges to the ﬁrst one it ﬁnds.
2. When the data isn’t separable, the algorithm doesn’t converge. The
hyperplane of wis always moving around.
IIt’s hard to detect this since it can take a long time for the algorithm to
converge when the data is separable.
Later, we will discuss algorithms that use the same idea of directly learning
the hyperplane w, but alters the objective function to ﬁx these problems.
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  January 26, 2015 
Today: • Bayes Classifiers • Conditional Independence • Naïve Bayes Readings: Mitchell:     “Naïve Bayes and Logistic Regression”      (available on class website) 
Two Principles for Estimating Parameters • Maximum Likelihood Estimate (MLE): choose θ that maximizes probability of observed data 
• Maximum a Posteriori (MAP) estimate: choose θ that is most probable given prior probability and the data 

Maximum Likelihood Estimate 
X=1 X=0 
P(X=1) = θ P(X=0) = 1-θ (Bernoulli)  

Maximum A Posteriori (MAP) Estimate 
X=1 X=0 

Let’s learn classifiers by learning P(Y|X) Consider Y=Wealth,  X=<Gender, HoursWorked>          Gender HrsWorked P(rich | G,HW) P(poor | G,HW) F <40.5 .09 .91 F >40.5 .21 .79 M <40.5 .23 .77 M >40.5 .38 .62 

How many parameters must we estimate?  Suppose X =<X1,… Xn>  where Xi and Y are boolean RV’s  To estimate P(Y| X1, X2, … Xn)     If we have 30 boolean Xi’s:  P(Y | X1, X2, … X30)             

How many parameters must we estimate?  Suppose X =<X1,… Xn>  where Xi and Y are boolean RV’s  To estimate P(Y| X1, X2, … Xn)    If we have 30 Xi’s instead of 2?           

Bayes Rule  
Which is shorthand for: 
Equivalently: 

Can we reduce params using Bayes Rule? Suppose X =<X1,… Xn>  where Xi and Y are boolean RV’s  How many parameters to define P(X1,… Xn | Y)?     How many parameters to define P(Y)? 

Can we reduce params using Bayes Rule? Suppose X =<X1,… Xn>  where Xi and Y are boolean RV’s 

Naïve Bayes Naïve Bayes assumes    i.e., that Xi and Xj are conditionally independent given Y, for all i≠j 

Conditional Independence  Definition: X is conditionally independent of Y  given Z, if the probability distribution governing X is independent of the value of Y, given the value of Z    Which we often write    E.g.,   

Naïve Bayes uses assumption that the Xi are conditionally independent, given Y.   E.g.,  Given this assumption, then:    

Naïve Bayes uses assumption that the Xi are conditionally independent, given Y.   E.g.,  Given this assumption, then:     in general:   

Naïve Bayes uses assumption that the Xi are conditionally independent, given Y.   E.g.,  Given this assumption, then:     in general:  How many parameters to describe P(X1…Xn|Y)?  P(Y)? • Without conditional indep assumption? • With conditional indep assumption? 

Naïve Bayes uses assumption that the Xi are conditionally independent, given Y  Given this assumption, then:    in general:  How many parameters to describe P(X1…Xn|Y)?  P(Y)? • Without conditional indep assumption? • With conditional indep assumption? 

Bayes rule: Assuming conditional independence among Xi’s:   So, to pick most probable Y for Xnew = < X1, …, Xn >   
Naïve Bayes in a Nutshell 

Naïve Bayes Algorithm – discrete Xi  • Train Naïve Bayes (examples)    for each* value yk  estimate   for each* value xij of each attribute Xi   estimate  • Classify (Xnew)    
 * probabilities must sum to 1, so need estimate only n-1 of these... 

Estimating Parameters: Y, Xi discrete-valued  Maximum likelihood estimates (MLE’s): 
Number of items in dataset D for which Y=yk 
Example: Live in Sq Hill?  P(S|G,D,M) • S=1 iff live in Squirrel Hill • G=1 iff shop at SH Giant Eagle • D=1 iff Drive to CMU • M=1 iff Rachel Maddow fan  What probability parameters must we estimate? 
Example: Live in Sq Hill?  P(S|G,D,M) • S=1 iff live in Squirrel Hill • G=1 iff shop at SH Giant Eagle • D=1 iff Drive to CMU • M=1 iff Rachel Maddow fan   P(S=1) : P(D=1 | S=1) : P(D=1 | S=0) : P(G=1 | S=1) : P(G=1 | S=0) : P(M=1 | S=1) : P(M=1 | S=0) : P(S=0) : P(D=0 | S=1) : P(D=0 | S=0) : P(G=0 | S=1) : P(G=0 | S=0) : P(M=0 | S=1) : P(M=0 | S=0) : 
Example: Live in Sq Hill?  P(S|G,D,B) • S=1 iff live in Squirrel Hill • G=1 iff shop at SH Giant Eagle • D=1 iff Drive or carpool to CMU • B=1 iff Birthday is before July 1  What probability parameters must we estimate? 
Example: Live in Sq Hill?  P(S|G,D,E) • S=1 iff live in Squirrel Hill • G=1 iff shop at SH Giant Eagle • D=1 iff Drive or Carpool to CMU • B=1 iff Birthday is before July 1   P(S=1) : P(D=1 | S=1) : P(D=1 | S=0) : P(G=1 | S=1) : P(G=1 | S=0) : P(B=1 | S=1) : P(B=1 | S=0) : P(S=0) : P(D=0 | S=1) : P(D=0 | S=0) : P(G=0 | S=1) : P(G=0 | S=0) : P(B=0 | S=1) : P(B=0 | S=0) : 
Naïve Bayes: Subtlety #1 Often the Xi are not really conditionally independent  • We use Naïve Bayes in many cases anyway, and it often works pretty well – often the right classification, even when not the right probability (see [Domingos&Pazzani, 1996])  • What is effect on estimated P(Y|X)? – Extreme case: what if we add two copies: Xi  = Xk   
Extreme case: what if we add two copies: Xi  = Xk  
Extreme case: what if we add two copies: Xi  = Xk  
Naïve Bayes: Subtlety #2 If unlucky, our MLE estimate for P(Xi | Y) might be zero.  (for example, Xi = birthdate.  Xi = Jan_25_1992)  • Why worry about just one parameter out of many? • What can be done to address this? 
Naïve Bayes: Subtlety #2 If unlucky, our MLE estimate for P(Xi | Y) might be zero.  (e.g., Xi = Birthday_Is_January_30_1992)  • Why worry about just one parameter out of many? • What can be done to address this? 
Estimating Parameters • Maximum Likelihood Estimate (MLE): choose θ that maximizes probability of observed data 
• Maximum a Posteriori (MAP) estimate: choose θ that is most probable given prior probability and the data 

Maximum likelihood estimates: 
Estimating Parameters: Y, Xi discrete-valued  
MAP estimates (Beta, Dirichlet priors):  Only difference: “imaginary” examples 

Learning to classify text documents • Classify which emails are spam? • Classify which emails promise an attachment? • Classify which web pages are student home pages? How shall we represent text documents for Naïve Bayes? 
Baseline: Bag of Words Approach 
aardvark 0 about 2 all 2 Africa 1 apple 0 anxious 0 ... gas 1 ... oil 1 … Zaire 0 
Learning to classify document: P(Y|X) the “Bag of Words” model • Y discrete valued.  e.g., Spam or not • X = <X1, X2, … Xn> = document  • Xi is a random variable describing the word at position i in the document • possible values for Xi : any word wk in English  • Document = bag of words: the vector of counts for all wk’s – like #heads, #tails, but we have many more than 2 values – assume word probabilities are position independent  (i.i.d. rolls of a 50,000-sided die) 
Naïve Bayes Algorithm – discrete Xi  • Train Naïve Bayes (examples)    for each value yk  estimate   for each value xj of each attribute Xi   estimate   • Classify (Xnew)    
prob that word xj appears in position i, given Y=yk  * Additional assumption:  word probabilities are position independent 
MAP estimates for bag of words  Map estimate for multinomial     What β’s should we choose?  


For code and data, see www.cs.cmu.edu/~tom/mlbook.html  click on “Software and Data” 
What you should know: • Training and using classifiers based on Bayes rule • Conditional independence – What it is – Why it’s important • Naïve Bayes – What it is – Why we use it so much – Training using MLE, MAP estimates – Discrete variables and continuous (Gaussian) 
Questions:  • How can we extend Naïve Bayes if just 2 of the Xi‘s are dependent? • What does the decision surface of a Naïve Bayes classifier look like? • What error will the classifier achieve if Naïve Bayes assumption is satisfied and we have infinite training data? • Can you use Naïve Bayes for a combination of discrete and real-valued Xi?  
What if we have continuous Xi ? Eg., image classification: Xi is ith pixel  

What if we have continuous Xi ? image classification: Xi is ith pixel, Y = mental state  
Still have:   Just need to decide how to represent P(Xi | Y)  

What if we have continuous Xi ? Eg., image classification: Xi is ith pixel  Gaussian Naïve Bayes (GNB): assume     Sometimes assume σik • is independent of Y (i.e., σi),  • or independent of Xi (i.e., σk) • or both (i.e., σ) 

Gaussian Naïve Bayes Algorithm – continuous Xi   (but still discrete Y) • Train Naïve Bayes (examples)    for each value yk  estimate*   for each attribute Xi estimate    class conditional mean        , variance         • Classify (Xnew)    
 * probabilities must sum to 1, so need estimate only n-1 parameters... 

Estimating Parameters: Y discrete, Xi continuous  Maximum likelihood estimates: 
jth training example δ(z)=1 if z true, else 0 ith feature kth class 
GNB Example: Classify a person’s cognitive activity, based on brain image •  are they reading a sentence or viewing a picture? •  reading the word “Hammer” or “Apartment” •  viewing a vertical or horizontal line? •  answering the question, or getting confused? 
Stimuli for our study: 
ant or 60 distinct exemplars, presented 6 times each 
fMRI voxel means for “bottle”: means defining P(Xi | Y=“bottle) Mean fMRI activation over all stimuli: “bottle” minus mean activation: 
fMRI activation  high 
below average average 
Rank Accuracy Distinguishing among 60 words  

Tools vs Buildings: where does brain encode their word meanings? 
Accuracies of  cubical 27-voxel  Naïve Bayes classifiers centered at each voxel  [0.7-0.8] 

Expected values Given discrete random variable X, the expected value of  X, written E[X] is     We also can talk about the expected value of functions of X 

Covariance Given two random vars X and Y, we define the covariance of X and Y as   e.g., X=gender, Y=playsFootball or     X=gender, Y=leftHanded   Remember: 

10-601 Machine Learning
Maria-Florina Balcan Spring 2015
Plan: Perceptron algorithm for learning linear separators.
1 Learning Linear Separators
Here we can think of examples as being from f0;1gnor fromRn. Given a training set
of labeled examples (that is consistent with a linear separator),we can nd a hyperplane
wx=w0such that all positive examples are on one side and all negative examples are
on other. I.e., wx > w 0for positive x's andwx < w 0for negative x's. We can solve
this using linear programming. The sample complexity results for classes of nite VC-
dimension together with known results about linear programming imply that the class of
linear separators is eciently learnable in the PAC (distributional) model. Today we will
talk about the Perceptron algorithm.
1.1 The Perceptron Algorithm
One of the oldest algorithms used in machine learning (from early 60s) is an online algorithm
for learning a linear threshold function called the Perceptron Algorithm.
We present the Perceptron algorithm in the online learning model. In this model, the
following scenario is repeats:
1. The algorithm receives an unlabeled example.
2. The algorithm predicts a classication of this example.
3. The algorithm is then told the correct answer.
We will call whatever is used to perform step (2), the algorithm's \current hypothesis."
As mentioned, the Perceptron algorithm is an online algorithm for learning linear separators.
For simplicity, we'll use a threshold of 0, so we're looking at learning functions like:
w1x1+w2x2+:::+wnxn>0:
We can simulate a nonzero threshold with a \dummy" input x0that is always 1, so this can
be done without loss of generality.
1
The Perceptron Algorithm:
1. Start with the all-zeroes weight vector w1=0, and initialize tto 1.
2. Given example x, predict positive i wtx>0.
3. On a mistake, update as follows:
Mistake on positive: wt+1 wt+x.
Mistake on negative: wt+1 wt x.
t t+ 1.
So, this seems reasonable. If we make a mistake on a positive xwe get wt+1x= (wt+x)x=
wtx+jjxjj2, and similarly if we make a mistake on a negative xwe have wt+1x=
(wt x)x=wtx jjxjj2. So, in both cases we move closer (by jjxjj2) to the value we
wanted.
We will show the following guarantee for the Perceptron Algorithm :
Theorem 1 LetSbe a sequence of labeled examples consistent with a linear threshold func-
tionwx>0, where wis a unit-length vector. Then the number of mistakes MonS
made by the online Perceptron algorithm is at most (R=
)2, where
R= max
x2Sjjxjj;and
= min
x2Sjwxj:
Note that since wis a unit-length vector, the quantity jwxjis equal to the distance of x
to the separating hyperplane wx= 0. The parameter \ 
" is often called the \margin" of
w, or more formally, the L2margin because we are measuring Euclidean distance.
Proof of Theorem 1 . We are going to look at the following two quantities wtwandjjwtjj.
Claim 1: wt+1wwtw+
. That is, every time we make a mistake, the dot-product
of our weight vector with the target increases by at least 
.
Proof: if xwas a positive example, then we get wt+1w= (wt+x)w=
wtw+xwwtw+
(by denition of 
). Similarly, if xwas a negative
example, we get ( wt x)w=wtw xwwtw+
.
Claim 2:jjwt+1jj2jjwtjj2+R2. That is, every time we make a mistake, the length squared
of our weight vector increases by at most R2.
Proof: if xwas a positive example, we get jjwt+xjj2=jjwtjj2+ 2wtx+jjxjj2.
This is less thanjjwtjj2+jjxjj2because wtxis negative (remember, we made
a mistake on x), and this in turn is at most jjwtjj2+R2. Same thing (
ipping
signs) if xwas negative but we predicted positive.
2
Claim 1 implies that after Mmistakes, wM+1w
M. On the other hand, Claim 2
implies that after Mmistakes,jjwM+1jj2R2M. Now, all we need to do is use the fact
thatwM+1wjjwM+1jj, since wis a unit-length vector. So, this means we must have

MRp
M, and thusM(R=
)2.
Discussion: In order to use the Perceptron algorithm to nd a consistent linear separator
given a setSof labeled examples that is linearly separable by margin 
, we do the following.
We repeatedly feed the whole set Sof labeled examples into the Perceptron algorithm up to
(R=
)2+ 1 rounds, until we get to a point where the current hypothesis is consistent with
the whole set S. Note that by theorem 1, we are guaranteed to reach such a point. The
runnning time is then polynomial in jSjand (R=
)2.
In the worst case, 
can be exponentially small in n. On the other hand, if we're lucky
and the data is well-separated, 
might even be large compared to 1 =n. This is called the
\large margin" case. (In fact, the latter is the more modern spin on things: namely, that in
many natural cases, we would hope that there exists a large-margin separator.) In fact, one
nice thing here is that the mistake-bound depends on just a purely geometric quantity: the
amount of \wiggle-room" available for a solution and doesn't depend in any direct way on
the number of features in the space.
So, if data is separable by a large margin, then the Perceptron is a good algorithm to use.
1.2 Additional More Advanced Notes
Guarantee in a distributional setting: In order to get a distributional guarantee we can
do the following.1LetM= (R=
)2. For any,, we draw a sample of size ( M=)log(M=).
We then run Perceptron on the data set and look at the sequence of hypotheses produced:
h1,h2, ... . For each i, ifhiis consistent with following 1 =log(M=) examples, then we
stop and output hi. We can argue that with probability at least 1  , the hypothesis we
output has error at most . This can be shown as follows. If hiwas a bad hypothsis with
true error greater than , then the chance we stopped and output hiwas at most =M. So,
by union bound, there's at most a chance we are fooled by anyof the hypotheses.
Note that this implies that if the margin over the whole distribution is 1 =poly (n), the Per-
ceptron algorithm can be used to PAC learn the class of linear separators.
What if there is no perfect separator? What if only most of the data is separable by
a large margin, or what if wis not perfect? We can see that the thing we need to look at
is Claim 1. Claim 1 said that we make \ 
amount of progress" on every mistake. Now it's
possible there will be mistakes where we make very little progress, or even negative progress.
One thing we can do is bound the total number of mistakes we make in terms of the total
distance we would have to move the points to make them actually separable by margin 
.
Let's call that TD 
. Then, we get that after Mmistakes, wM+1w
M TD
. So,
1This is not the most sample ecient online to PAC reduction, but it is the simplest to think about.
3
combining with Claim 2, we get that Rp
M
M TD
. We could solve the quadratic,
but this implies, for instance, that M(R=
)2+ (2=
)TD
. The quantity1

TD
is called
the total hinge-loss ofw.
So, this is not too bad: we can't necessarily say that we're making only a small multiple of
the number of mistakes that wis (in fact, the problem of nding an approximately-optimal
separator is NP-hard), but we can say we're doing well in terms of the \total distance"
parameter.
Perceptron for approximately maximizing margins. We saw that the perceptron
algorithm makes at most ( R=
)2mistakes on any sequence of examples that is linearly-
separable by margin 
, i.e., any sequence for which there exists a unit-length vector wsuch
that all examples xsatisfy`(x)(wx)
, where`(x)2f  1;1gis the label of x.
Suppose we are handed a set of examples Sand we want to actually nda large-margin
separator for them. One approach is to directly solve for the maximum-margin separator
using convex programming (which is what is done in the SVM algorithm). However, if we
only need to approximately maximize the margin, then another approach is to use Perceptron.
In particular, suppose we cycle through the data using the Perceptron algorithm, updating
not only on mistakes, but also on examples xthat our current hypothesis gets correct by
margin less than 
=2. Assuming our data is separable by margin 
, then we can show that
this is guaranteed to halt in a number of rounds that is polynomial in R=
. (In fact, we can
replace
=2 with (1 )
and have bounds that are polynomial in R=(
).)
The Margin Perceptron Algorithm (
):
1. Initialize w1=`(x)x, where xis the rst example seen and initialize tto 1.
2. Predict positive ifwtx
jjwtjj
=2, predict negative ifwtx
jjwtjj  
=2, and consider an
example to be a margin mistake whenwtx
jjwtjj2( 
=2;
=2).
3. On a mistake (incorrect prediction or margin mistake), update as in the standard
Perceptron algorithm: wt+1 wt+`(x)x;t t+ 1.
Theorem 2 LetSbe a sequence of labeled examples consistent with a linear threshold func-
tionwx>0, where wis a unit-length vector, and let
R= max
x2Sjjxjj;and
= min
x2Sjwxj:
Then the number of mistakes (including margin mistakes) made by Margin Perceptron (
)on
Sis at most 8(R=
)2+ 4(R=
).
Proof: The argument for this new algorithm follows the same lines as the argument for the
original Perceptron algorithm.
4
As before, each update increases wtwby at least
. What is now a little more complicated
is to bound the increase in jjwtjj. For the original algorithm, we had: jjwt+1jj2jjwtjj2+R2,
which impliesjjwt+1jjjj wtjj+R2
2jjwtjj.
For the new algorithm, we can show instead:
jjwt+1jjjj wtjj+R2
2jjwtjj+
2: (1)
To see this note that:
jjwt+1jj2=jjwtjj2+ 2l(x)wtx+jjxjj2=jjwtjj2 
1 +2l(x)
jjwtjjwtx
jjwtjj+jjxjj2
jjwtjj2!
Taking the square-root of both sides and using the inequalityp1 +1+
2andjjxjj2R2
we get:
jjwt+1jjjj wtjj 
1 +l(x)
jjwtjjwtx
jjwtjj+R2
2jjwtjj2!
:
Combining this with the factl(x)wtx
jjwtjj
2(sincewtmade a mistake or margin mistake on x)
we get the desired upper bound on jjwt+1jj, namely:jjwt+1jjjj wtjj+
2+R2
2jjwtjj:
Note that (1) implies that if jjwtjj 2R2=
thenjjwt+1jjjj wtjj+ 3
=4. Note also that
jjwt+1jjjj wtjj+jjxjj(by triangle inequality), so jjwt+1jjjj wtjj+R. These two facts
imply that after Mupdates we have:
jjwM+1jj(2R2=
+R) + 3M
= 4:
SolvingM
(2R2=
+R) + 3M
= 4 we getM8R2=
2+ 4R=
, as desired.
5
Word	Meaning:Distributional	Representations	&	Word	Sense	DisambiguationCMSC	723	/	LING	723	/	INST	725Marine	CarpuatSlides	credit:	Dan	Jurafsky
Reminders•Read	the	syllabus•Make	sure	you	have	access	to	piazza•Get	started	on	homework	1	–due	Thursday	Sep	7	by	12pm.
Today:	Word	Meaning2	core	issues	from	an	NLP	perspective•Semantic	similarity:	given	two	words,	how	similar	are	they	in	meaning?•Word	sense	disambiguation:	given	a	word	that	has	more	than	one	meaning,		which	one	is	used	in	a	specific	context?
Word	similarity	for	question	answering“fast”	is	similar	to	“rapid”“tall”	is	similar	to	“height”Question	answering:Q:	“How	tallis	Mt.	Everest?”Candidate	A:	“The	official	heightof	Mount	Everest	is	29029	feet”
Word	similarityfor	plagiarism	detection

Word	similarity	for	historical	linguistics:semantic	change	over	time
Kulkarni,	Al-Rfou,	Perozzi,	Skiena2015
tesgüinoA bottle of tesgüinois on the tableEverybody likes tesgüinoTesgüinomakes you drunkWe make tesgüinoout of corn.Intuition:	two	words	are	similar	if	they	have	similar	word	contexts.
Embedding	word	meaning	in	vector	spaceVector	Semantics
Distributional	models	of	meaning=	vector-space	models	of	meaning	=	vector	semanticsIntuitionsZelligHarris	(1954):•“oculist	and	eye-doctor	…	occur	in	almost	the	same	environments”•“If	A	and	B	have	almost	identical	environments	we	say	that	they	are	synonyms.”Firth	(1957):	•“You	shall	know	a	word	by	the	company	it	keeps!”
Vector	Semantics•Model	the	meaning	of	a	word	by	“embedding”	in	a	vector	space.•The	meaning	of	a	word	is	a	vector	of	numbers•Vector	models	are	also	called	“embeddings”.•Contrast:	word	meaning	is	represented	in	many	NLP	applications	by	a	vocabulary	index	(“word	number	545”)
Many	varieties	of	vector	modelsSparse	vector	representations1.Mutual-information	weighted	word	co-occurrence	matricesDense	vector	representations:2.Singular	value	decomposition	(and	Latent	Semantic	Analysis)3.Neural-network-inspired	models	(skip-grams,	CBOW)
As#You#Like#ItTwelfth#NightJulius#CaesarHenry#Vbattle11815soldier221236fool375815clown611700Term-document	matrix•Each	cell:	count	of	term	tin	a	document	d:		tft,d•Each	document	is	a	count	vector	in	ℕv:	a	column	below	
Term-document	matrix•Two	documents	are	similar	if	their	vectors	are	similarAs#You#Like#ItTwelfth#NightJulius#CaesarHenry#Vbattle11815soldier221236fool375815clown611700
The	wordsin	a	term-document	matrix•Each	word	is	a	count	vector	in	ℕD:	a	row	below	As#You#Like#ItTwelfth#NightJulius#CaesarHenry#Vbattle11815soldier221236fool375815clown611700
The	words	in	a	term-document	matrix•Two	wordsare	similar	if	their	vectors	are	similarAs#You#Like#ItTwelfth#NightJulius#CaesarHenry#Vbattle11815soldier221236fool375815clown611700
The	word-word	or	word-context	matrix•Instead	of	entire	documents,	use	smaller	contexts•Paragraph•Window	of	±4	words•A	word	is	now	defined	by	a	vector	over	counts	of	context	words•Instead	of	each	vector	being	of	length	D•Each	vector	is	now	of	length	|V|•The	word-word	matrix	is	|V|x|V|
Word-Word	matrixSample	contexts	±7	wordsaardvarkcomputerdatapinchresultsugar…apricot000101pineapple000101digital021010information01604019.1•WORDS ANDVECTORS3tors of numbers representing the terms (words) that occur within the collection(Salton, 1971). In information retrieval these numbers are called theterm weight,aterm weightfunction of the term’s frequency in the document.More generally, the term-document matrixXhasVrows (one for each wordtype in the vocabulary) andDcolumns (one for each document in the collection).Each column represents a document. A query is also represented by a vectorqoflength|V|. We go about ﬁnding the most relevant document to query by ﬁndingthe document whose vector is most similar to the query; later in the chapter we’llintroduce some of the components of this process: the tf-idf term weighting, and thecosine similarity metric.But now let’s turn to the insight of vector semantics for representing the meaningofwords. The idea is that we can also represent each word by a vector, now a rowvector representing the counts of the word’s occurrence in each document. Thusthe vectors forfool[37,58,1,5] andclown[5,117,0,0] are more similar to each other(occurring more in the comedies) whilebattle[1,1,8,15] andsoldier[2,2,12,36] aremore similar to each other (occurring less in the comedies).More commonly used for vector semantics than this term-document matrix is analternative formulation, theterm-term matrix, more commonly called theword-term-termmatrixword matrixoro theterm-context matrix, in which the columns are labeled bywords rather than documents. This matrix is thus of dimensionality|V|⇥|V|andeach cell records the number of times the row (target) word and the column (context)word co-occur in some context in some training corpus. The context could be thedocument, in which case the cell represents the number of times the two wordsappear in the same document. It is most common, however, to use smaller contexts,such as a window around the word, for example of 4 words to the left and 4 wordsto the right, in which case the cell represents the number of times (in some trainingcorpus) the column word occurs in such a±4 word window around the row word.For example here are 7-word windows surrounding four sample words from theBrown corpus (just one example of each word):sugar, a sliced lemon, a tablespoonful ofapricotpreserve or jam, a pinch each of,their enjoyment. Cautiously she sampled her ﬁrstpineappleand another fruit whose taste she likenedwell suited to programming on the digitalcomputer. In ﬁnding the optimal R-stage policy fromfor the purpose of gathering data andinformationnecessary for the study authorized in theFor each word we collect the counts (from the windows around each occurrence)of the occurrences of context words. Fig.17.2shows a selection from the word-wordco-occurrence matrix computed from the Brown corpus for these four words.aardvark...computer data pinch result sugar...apricot0 ... 0 0101pineapple0 ... 0 0101digital0 ...21010information0 ...16040Figure 19.2Co-occurrence vectors for four words, computed from the Brown corpus,showing only six of the dimensions (hand-picked for pedagogical purposes). Note that areal vector would be vastly more sparse.The shading in Fig.17.2makes clear the intuition that the two wordsapricotandpineappleare more similar (bothpinchandsugartend to occur in their window)whiledigitalandinformationare more similar.Note that|V|, the length of the vector, is generally the size of the vocabulary,usually between 10,000 and 50,000 words (using the most frequent words in the……
Word-word	matrix•The	|V|x|V|	matrix	is	very	sparse	(most	values	are	0)•The	size	of	windows	depends	on	representation	goals•The	shorter	the	windows	,	the	more	syntacticthe	representation±1-3	very	syntacticy•The	longer	the	windows,	the	more	semanticthe	representation±4-10	more	semanticy
Positive	Pointwise	Mutual	Information	(PPMI)Vector	Semantics
Problem	with	raw	counts•Raw	word	frequency	is	not	a	great	measure	of	association	between	words•We’d	rather	have	a	measure	that	asks	whether	a	context	word	is	particularly	informative	about	the	target	word.•Positive	Pointwise	Mutual	Information	(PPMI)
PointwiseMutual	InformationPointwise	mutual	information:	Do	events	x	and	y	co-occur	more	than	if	they	were	independent?PMI	between	two	words:		(Church	&	Hanks	1989)Do	words	x	and	y	co-occur	more	than	if	they	were	independent?	PMI𝑤𝑜𝑟𝑑),𝑤𝑜𝑟𝑑+=log+0(23456,23457)0234560(23457)PMI(X,Y)=log2P(x,y)P(x)P(y)
Positive	PointwiseMutual	Information•PMI	ranges	from	−∞		to	+∞•But	the	negative	values	are	problematic•Things	are	co-occurring	less	than	we	expect	by	chance•Unreliable	without	enormous	corpora•So	we	just	replace	negative	PMI	values	by	0•Positive	PMI	(PPMI)	between	word1	and	word2:PPMI𝑤𝑜𝑟𝑑),𝑤𝑜𝑟𝑑+=maxlog+𝑃(𝑤𝑜𝑟𝑑),𝑤𝑜𝑟𝑑+)𝑃𝑤𝑜𝑟𝑑)𝑃(𝑤𝑜𝑟𝑑+),0
Computing	PPMI	on	a	term-context	matrix•Matrix	Fwith	Wrows	(words)	and	Ccolumns	(contexts)•fijis	#	of	times	wioccurs	in	context	cj
pij=fijfijj=1C∑i=1W∑pi*=fijj=1C∑fijj=1C∑i=1W∑p*j=fiji=1W∑fijj=1C∑i=1W∑pmiij=log2pijpi*p*jppmiij=pmiijif  pmiij>00otherwise!"#$#
p(w=information,c=data)	=	p(w=information)	=p(c=data)	=
p(w,context)p(w)computerdatapinchresultsugarapricot0.000.000.050.000.050.11pineapple0.000.000.050.000.050.11digital0.110.050.000.050.000.21information0.050.320.000.210.000.58p(context)0.160.370.110.260.11=	.326/1911/19=	.587/19=	.37pij=fijfijj=1C∑i=1W∑p(wi)=fijj=1C∑Np(cj)=fiji=1W∑N
pmiij=log2pijpi*p*jp(w,context)p(w)computerdatapinchresultsugarapricot0.000.000.050.000.050.11pineapple0.000.000.050.000.050.11digital0.110.050.000.050.000.21information0.050.320.000.210.000.58p(context)0.160.370.110.260.11PPMI(w,context)computerdatapinchresultsugarapricot112.2512.25pineapple112.2512.25digital1.660.0010.001information0.000.5710.471
Weighting	PMI•PMI	is	biased	toward	infrequent	events•Very	rare	words	have	very	high	PMI	values•Two	solutions:•Give	rare	words	slightly	higher	probabilities•Use	add-ksmoothing	(which	has	a	similar	effect)
Weighting	PMI:	Giving	rare	context	words	slightly	higher	probability•Raise	the	context	probabilities	to	𝛼=0.75:•Consider	two	events,	P(a)	=	.99	and	P(b)=.01𝑃F𝑎=.HH.IJ.HH.IJK.L).IJ=.97𝑃F𝑏=.L).IJ.L).IJK.L).IJ=.036CHAPTER19•VECTORSEMANTICSp(w,context) p(w)computer data pinch result sugar p(w)apricot0 0 0.5 0 0.5 0.11pineapple0 0 0.5 0 0.5 0.11digital0.11 0.5 0 0.5 0 0.21information0.5 .32 0 0.21 0 0.58p(context)0.16 0.37 0.11 0.26 0.11Figure 19.3Replacing the counts in Fig.17.2with joint probabilities, showing themarginals around the outside.computer data pinch result sugarapricot002.25 02.25pineapple002.25 02.25digital1.66 0 0 0 0information00.57 00.47 0Figure 19.4The PPMI matrix showing the association between words and context words,computed from the counts in Fig.17.2again showing six dimensions.PMI has the problem of being biased toward infrequent events; very rare wordstend to have very high PMI values. One way to reduce this bias toward low frequencyevents is to slightly change the computation forP(c), using a different functionPa(c)that raises contexts to the power ofa(Levy et al., 2015):PPMIa(w,c)=max(log2P(w,c)P(w)Pa(c),0)(19.8)Pa(c)=count(c)aPccount(c)a(19.9)Levy et al. (2015)found that a setting ofa=0.75 improved performance ofembeddings on a wide range of tasks (drawing on a similar weighting used for skip-grams(Mikolov et al., 2013a)and GloVe(Pennington et al., 2014)). This worksbecause raising the probability toa=0.75 increases the probability assigned to rarecontexts, and hence lowers their PMI (Pa(c)>P(c)whencis rare).Another possible solution is Laplace smoothing: Before computing PMI, a smallconstantk(values of 0.1-3 are common) is added to each of the counts, shrinking(discounting) all the non-zero values. The larger thek, the more the non-zero countsare discounted.computer data pinch result sugarapricot22 3 2 3pineapple22 3 2 3digital43 2 3 2information38 2 6 2Figure 19.5Laplace (add-2) smoothing of the counts in Fig.17.2.19.2.1 Measuring similarity: the cosineTo deﬁne similarity between two target wordsvandw, we need a measure for takingtwo such vectors and giving a measure of vector similarity. By far the most commonsimilarity metric is thecosineof the angle between the vectors. In this section we’llmotivate and introduce this important measure.
Add-2	smoothingAdd#2%Smoothed%Count(w,context)computerdatapinchresultsugarapricot22323pineapple22323digital43232information38262
PPMI	vs	add-2	smoothed	PPMI
PPMI(w,context).[add22]computerdatapinchresultsugarapricot0.000.000.560.000.56pineapple0.000.000.560.000.56digital0.620.000.000.000.00information0.000.580.000.370.00PPMI(w,context)computerdatapinchresultsugarapricot112.2512.25pineapple112.2512.25digital1.660.0010.001information0.000.5710.471
tf.idf:	an	alternative	to	PPMI	for	measuring	association•The	combination	of	two	factors•TF:	Term	frequency	(Luhn1957):	frequency	of	the	word•IDF:	Inverse	document	frequency(SparckJones	1972)•N	is	the	total	number	of	documents•dfi=	“document	frequency	of	word	i”=	#	of	documents	with	word	i•wij=	word	iin	document	jwij=tfijidfiidfi=logNdfi!"##$%&&
Measuring	similarity:	the	cosineVector	Semantics
Cosine	for	computing	similaritycos(v,w)=v•wvw=vv•ww=viwii=1N∑vi2i=1N∑wi2i=1N∑Dot	productUnit	vectorsviis	the	PPMI	value	for	word	vin	context	iwiis	the	PPMI	value	for	word	win	context	i.Cos(v,w)	is	the	cosine	similarity	of	vand	wSec.	6.3
Other	possible	similarity	measures

Evaluating	similarityVector	Semantics
Evaluating	similarity•Extrinsic	(task-based,	end-to-end)	Evaluation:•Question	Answering•Spell	Checking•Essay	grading•Intrinsic	Evaluation:•Correlation	between	algorithm	and	human	word	similarity	ratings•Wordsim353:	353	noun	pairs	rated	0-10.			sim(plane,car)=5.77•Taking	TOEFL	multiple-choice	vocabulary	tests•Leviedis closest in meaning to:imposed, believed, requested, correlated
Today:	Word	Meaning2	core	issues	from	an	NLP	perspective•Semantic	similarity:	given	two	words,	how	similar	are	they	in	meaning?•Word	sense	disambiguation:	given	a	word	that	has	more	than	one	meaning,		which	one	is	used	in	a	specific	context?
“Big	rig	carrying	fruit	crashes	on	210	Freeway,	creates	jam”http://articles.latimes.com/2013/may/20/local/la-me-ln-big-rig-crash-20130520
How	do	we	know	that	a	word	(lemma)	has	distinct	senses?•Linguists	often	design	tests	for	this	purpose•e.g.,	zeugmacombines	distinct	senses	in	an	uncomfortable	wayWhich ﬂight serves breakfast?Which ﬂights serve BWI?*Which ﬂights serve breakfast and BWI? 
Word	Senses•“Word	sense”	=	distinct	meaning	of	a	word•Same	word,	different	senses•Homonyms(homonymy):	unrelated	senses;	identical	orthographic	form	is	coincidental•E.g.,	financial	bank	vs.	river	bank•Polysemes(polysemy):	related,	but	distinct	senses•E.g.,	Financial	bank	vs.	blood	bank	vs.	tree	bank•Metonyms(metonymy):	“stand	in”,	technically,	a	sub-case	of	polysemy•E.g.,	use	“Washington”	in	place	of	“the	US	government”•Different	word,	same	sense•Synonyms(synonymy)
•Homophones:	same	pronunciation,	different	orthography,	different	meaning•Examples:	would/wood,	to/too/two•Homographs:	distinct	senses,	same	orthographic	form,	different	pronunciation•Examples:	bass	(fish)	vs.	bass	(instrument)
Relationship	Between	Senses•IS-A	relationships•From	specific	to	general	(up):	hypernym(hypernymy)•From	general	to	specific	(down):	hyponym	(hyponymy)•Part-Whole	relationships•wheel	is	a	meronymof	car	(meronymy)•car	is	a	holonymof	wheel	(holonymy)
WordNet:	a	lexical	database	for	Englishhttps://wordnet.princeton.edu/•Includes	most	English	nouns,	verbs,	adjectives,	adverbs•Electronic	format	makes	it	amenable	to	automatic	manipulation:	used	in	many	NLP	applications•“WordNets”	generically	refers	to	similar	resources	in	other	languages
Synonymy	in	WordNet•WordNet	is	organized	in	terms	of	“synsets”•Unordered	set	of	(roughly)	synonymous	“words”	(or	multi-word	phrases)•Each	synsetexpresses	a	distinct	meaning/concept	
WordNet:	ExampleNoun{pipe,	tobacco	pipe}	(a	tube	with	a	small	bowl	at	one	end;	used	for	smoking	tobacco)	{pipe,	pipage,	piping}	(a	long	tube	made	of	metal	or	plastic	that	is	used	to	carry	water	or	oil	or	gas	etc.)	{pipe,	tube}	(a	hollow	cylindrical	shape)	{pipe}	(a	tubular	wind	instrument)	{organ	pipe,	pipe,	pipework}	(the	flues	and	stops	on	a	pipe	organ)	Verb{shriek,	shrill,	pipe	up,	pipe}	(utter	a	shrill	cry)	{pipe}	(transport	by	pipeline)	“pipe	oil,	water,	and	gas	into	the	desert”{pipe}	(play	on	a	pipe)	“pipe	a	tune”{pipe}	(trim	with	piping)	“pipe	the	skirt”
WordNet	3.0:	SizePart	of	speechWord	formSynsetsNoun117,79882,115Verb11,52913,767Adjective21,47918,156Adverb4,4813,621Total155,287117,659http://wordnet.princeton.edu/
Word	Sense	Disambiguation•Task:	automatically	select	the	correct	sense	of	a	word•Input:	a	word	in	context•Output:	sense	of	the	word•Motivated	by	many	applications:•Information	retrieval•Machine	translation•…
How	big	is	the	problem?•Most	words	in	English	have	only	one	sense•62%	in	Longman’s	Dictionary	of	Contemporary	English•79%	in	WordNet•But	the	others	tend	to	have	several	senses•Average	of	3.83	in	LDOCE•Average	of	2.96	in	WordNet•Ambiguous	words	are	more	frequently	used•In	the	British	National	Corpus,	84%	of	instances	have	more	than	one	sense•Some	senses	are	more	frequent	than	others
Baseline	Performance•Baseline:	most	frequent	sense•Equivalent	to	“take	first	sense”	in	WordNet•Does	surprisingly	well!
62%	accuracy	in	this	case!
Upper	Bound	Performance•Upper	bound•Fine-grained	WordNet	sense:	75-80%	human	agreement•Coarser-grained	inventories:	90%	human	agreement	possible
Simplest	WSD	algorithm:Lesk’sAlgorithm•Intuition:	note	word	overlap	between	context	and	dictionary	entries•Unsupervised,but	knowledge	rich
The	bank	can	guarantee	deposits	will	eventually	cover	future	tuition	costs	because	it	invests	in	adjustable-rate	mortgage	securities.		WordNet
Lesk’sAlgorithm•Simplest	implementation:•Count	overlapping	content	words	between	glosses	and	context•Lots	of	variants:•Include	the	examples	in	dictionary	definitions•Include	hypernymsand	hyponyms•Give	more	weight	to	larger	overlaps	(e.g.,	bigrams)•Give	extra	weight	to	infrequent	words•…
Alternative:	WSD	as	SupervisedClassification
label1label2label3label4
Classifiersupervised machine learning algorithm?unlabeled document
label1?label2?label3?label4?TestingTrainingtraining data
Feature Functions
Existing	Corpora•Lexical	sample•line-hard-servecorpus	(4k	sense-tagged	examples)•interest	corpus(2,369	sense-tagged	examples)•…	•All-words•SemCor(234k	words,	subset	of	Brown	Corpus)•Senseval/SemEval(2081	tagged	content	words	from	5k	total	words)•…
Word	Meaning2	core	issues	from	an	NLP	perspective•Semantic	similarity:	given	two	words,	how	similar	are	they	in	meaning?•Key	concepts:	vector	semantics,	PPMI	and	its	variants,	cosine	similarity•Word	sense	disambiguation:	given	a	word	that	has	more	than	one	meaning,		which	one	is	used	in	a	specific	context?•Key	concepts:	word	sense,	WordNet	and	sense	inventories,	unsupervised	disambiguation	(Lesk),	supervised	disambiguation

Dynamic	Memory	Networks	for	Question	Answering	over	Text	and	ImagesRichard	SocherJoint	work	with	the	MetaMind	teamCaimingXiong,	Stephen	Merity,	James	Bradbury,	AnkitKumar,	OzanIrsoyand	others

Current	ResearchAll	NLP/AI	tasks	canbe	reduced	to	question	answering
QA	Examples054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099100101102103104105106107I: Mary walked to the bathroom.I: Sandra went to the garden.I: Daniel went back to the garden.I: Sandra took the milk there.Q: Where is the milk?A: gardenI: Everybody is happy.Q: What’s the sentiment?A: positiveI: Jane has a baby in Dresden.Q: What are the named entities?A: Jane - person, Dresden - locationI: Jane has a baby in Dresden.Q: What are the POS tags?A: NNP VBZ DT NN IN NNP .I: I think this model is incredibleQ: In French?A: Je pense que ce mod`ele est incroyable.Figure 1: Example inputs and questions together with answers all of which are generated by thesame dynamic memory network.2 The Dynamic Memory NetworkThe Dynamic memory network (DMNs) is a general model for asking questions over inputs. Thedefault for the DMN is to represent inputs and their parts in vector form. In this paper, we will focuson asking questions over natural language inputs.Seman&c(Memory((• Word(vectors(• Knowledge(Basis((Input(Text(Sequence(Ques&on(Episodic(Memory(Answer(
Figure 2: Overview of DMN modules. Communication between them is indicated by arrows anduses only semantic vector representations. Questions trigger gates which allow some input words orsentences to be given to the episodic memory module. The ﬁnal state of the episodic memory is theinput to the answer module.We will ﬁrst give an overview of the model as illustrated in Fig.2and then describe our speciﬁcinstantiation of each module. The DMN consist of the following modules which are listed in theorder of which they are used:Input Module:This module processes raw inputs and maps them into a representation that is usefulfor asking questions about this input. Generally, the input can be visual, speech or text. Inour case the inputs are sequences of words and the representations that are being computedare semantic vectors at every time step. For instance, this may be a long story, a moviereview, news article or all of Wikipedia.Semantic Memory:Semantic memory stores general world knowledge about concepts and facts.For example, it might contain information about what a hang glider is. This module isinspired by cognitive neuroscience. While the exact location of semantic memory in thehuman brain is still being explored, its general existence is well established [1]. Distributedword vectors like Glove [2] or Word2Vec [3] form the basis of semantic memory. Morecomplex information can be stored in the form of knowledge bases which capture relation-ships between various words in the form of triplets [4].Question Module:The question module simply computes a representation of a question such asWhere did the author ﬁrst ﬂy?. This representation, in our case a vector, then triggers agated attention and retrieval process over facts from the input sequence.Episodic Memory:In humans this memory stores speciﬁc experiences, times and places in theirtemporal and autonoetic context. For instance, it might contain the ﬁrst memory somebodyﬂew a hang glider. It is also a central part of the DMN. Each question draws attention to2054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099100101102103104105106107I: Mary walked to the bathroom.I: Sandra went to the garden.I: Daniel went back to the garden.I: Sandra took the milk there.Q: Where is the milk?A: gardenI: Everybody is happy.Q: What’s the sentiment?A: positiveI: Jane has a baby in Dresden.Q: What are the named entities?A: Jane - person, Dresden - locationI: Jane has a baby in Dresden.Q: What are the POS tags?A: NNP VBZ DT NN IN NNP .I: I think this model is incredibleQ: In French?A: Je pense que ce mod`ele est incroyable.Figure 1: Example inputs and questions together with answers all of which are generated by thesame dynamic memory network.2 The Dynamic Memory NetworkThe Dynamic memory network (DMNs) is a general model for asking questions over inputs. Thedefault for the DMN is to represent inputs and their parts in vector form. In this paper, we will focuson asking questions over natural language inputs.Seman&c(Memory((• Word(vectors(• Knowledge(Basis((Input(Text(Sequence(Ques&on(Episodic(Memory(Answer(
Figure 2: Overview of DMN modules. Communication between them is indicated by arrows anduses only semantic vector representations. Questions trigger gates which allow some input words orsentences to be given to the episodic memory module. The ﬁnal state of the episodic memory is theinput to the answer module.We will ﬁrst give an overview of the model as illustrated in Fig.2and then describe our speciﬁcinstantiation of each module. The DMN consist of the following modules which are listed in theorder of which they are used:Input Module:This module processes raw inputs and maps them into a representation that is usefulfor asking questions about this input. Generally, the input can be visual, speech or text. Inour case the inputs are sequences of words and the representations that are being computedare semantic vectors at every time step. For instance, this may be a long story, a moviereview, news article or all of Wikipedia.Semantic Memory:Semantic memory stores general world knowledge about concepts and facts.For example, it might contain information about what a hang glider is. This module isinspired by cognitive neuroscience. While the exact location of semantic memory in thehuman brain is still being explored, its general existence is well established [1]. Distributedword vectors like Glove [2] or Word2Vec [3] form the basis of semantic memory. Morecomplex information can be stored in the form of knowledge bases which capture relation-ships between various words in the form of triplets [4].Question Module:The question module simply computes a representation of a question such asWhere did the author ﬁrst ﬂy?. This representation, in our case a vector, then triggers agated attention and retrieval process over facts from the input sequence.Episodic Memory:In humans this memory stores speciﬁc experiences, times and places in theirtemporal and autonoetic context. For instance, it might contain the ﬁrst memory somebodyﬂew a hang glider. It is also a central part of the DMN. Each question draws attention to2
GoalA	joint	model	for	general	QA
First	Major	Obstacle•For	NLP	no	single	model	architecturewith	consistent	state	of	the	art	results	across	tasks
TaskState	of	the	art	modelQuestion	answering	(babI)Strongly	Supervised	MemNN(Weston	et	al	2015)Sentiment	Analysis(SST)Tree-LSTMs	(Tai	et	al.	2015)Part	of	speech	tagging(PTB-WSJ)Bi-directional	LSTM-CRF	(Huang	et	al.	2015)	
Second	Major	Obstacle•Fully	joint	multitask	learning*	is	hard:–Usually	restricted	to	lower	layers–Usually	helps	only	if	tasks	are	related–Often	hurts	performance	if	tasks	are	not	related*	meaning:	same	decoder/classifier	and	not	only	transfer	learning
Dynamic	Memory	Networks	An	architecture	for	any	QA	taskTackling	First	Obstacle
High	level	idea	for	harder	questions•Imagine	having	to	read	an	article,	memorizeit,	then	get	asked	various	questions	àHard!•You	can't	store	everything	in	working	memory•Optimal:give	you	the	input	data,	give	you	the	question,	allow	as	many	glances	as	possible

Basic	Lego	Block:	RNNs•Gated	Recurrent	Unit	(GRU),	Cho	et	al.	2014•A	type	of	recurrent	neural	network	(RNN),	similar	to	the	LSTM	•Consumes	and/or	generates	sequences	(chars,	words,	...)•The	GRU	updates	an	internal	state	haccording	to	the	existing	state	hand	the	current	input	x:	
108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161speciﬁc facts from the input sequence which are henceforth given as input to this module. Itthen takes several passes over input sequence and incorporates new facts in its hidden state.This can then trigger the retrieval of new facts which were previously thought to not berelevant. In our case it is a hierarchical, recurrent, neural sequence model which collects,stores and reasons over facts.Answer Module:Depending on the question, the answer module can be triggered in two ways: (i)It can produce an output at every time step of the episodic memory sequence, correspondingto word or sentence level labels like named entity tags. (ii) For tasks that are not sequencelabeling the output is produced based on the ﬁnal hidden representation of the episodicmemory.The remainder of this section describes the general mathematical framework and our speciﬁc instan-tiation of the DMN for natural language processing tasks.2.1 Input ModuleGeneral. The input module is responsible for computing representations of audio, visual or textualinputs such that they can be retrieved when needed later. For most cognitive inputs we can assume atemporal sequence indexable by a time stamp. For instance, in the case of video input, this would bean image at each time step. For written language we have a sequence ofTwwordsv1,...,vTw. Inorder to compute the most useful input representations it is often beneﬁcial to do both unsupervisedand supervised learning as well as computing context-independent and context-dependent hiddenstates.NLP. In order to compute context-independent, unsupervised representations at each time step, theentirety of a language corpus is used to compute semantic word vectors using the Glove model [2].The glove objective is the ﬁrst element of the full DMN objective function. Henceforth, we describeeach word sequence in terms of a list of corresponding word vectorsvtat each time stept. Theseword vectors are stored in the semantic memory described in the next subsection.Furthermore, word vectors are given as inputs to a recurrent neural network (RNN) sequence model[5] to compute context-dependent representations at each time step:wt=SEQMODEL(vt,wt 1)resulting in the full sequenceW. Other modules can access both the original word vectors as well asthe hidden stateswt. In particular, we use a gated recurrent network (GRU) [6,7]. We also exploredthe more complex LSTM [8] but it performed similarly and is more complex. Both work muchbetter than the standard single layertanhRNN and we postulate that the main strength comes fromhaving gates that allow the model to suffer less from the vanishing gradient problem.GRU Deﬁnition: We deﬁne the GRU subcomponent in general since it is used for various sequencesinside DMN modules. Assume each time step has an inputxtand a hidden nodeht. We willabbreviate the below computation withht=GRU(xt,ht 1):zt= ⇣W(z)xt+U(z)ht 1+b(z)⌘(1)rt= ⇣W(r)xt+U(r)ht 1+b(r)⌘(2)˜ht= tanh⇣Wxt+rt Uht 1+b(h)⌘(3)ht=zt ht 1+( 1 zt) ˜ht,(4)where is an element-wise product. In some cases, there is a direct output in terms of a word(equivalent to a standard supervised class label) which is computed viayt= softmax(W(S)ht).Depending on which module is being described, the GRU notation changes but the internal statesand equations are the same. For the input module, we havewt=GRU(vt,wt).Different subsequences such as sentences can also be retrieved depending on the task. For somedatasets, the sentence separation is useful and we assume we can access each sentence’s last hiddenvector in order to obtain a sequenceSof sentence vectorsS=s1,...,sTs.3
DMN	OverviewAnswer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4
The	Modules:	InputAnswer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4
Answer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4
Further	Improvement:	BiGRUDynamic Memory Networks for Visual and Textual Question Answering
Input fusionlayerSentencereaderFactsGRUf1f1
w1w2w3w4GRU
Positional EncoderGRUf2f2
w1w2w3w4GRU
Positional EncoderGRUf3f3
w1w2w3w4GRU
Positional Encoder 1 1 1 1 2 2 2 2 3 3 3 3Textual Input Module
Figure 2.The input module with a “fusion layer”, where the sen-tence reader encodes the sentence and the bi-directional GRU al-lows information to ﬂow between sentences.The sentence reader could be based on any variety ofencoding schemes. We selected positional encoding de-scribed in Sukhbaatar et al. (2015) to allow for a compari-son to their work. GRUs and LSTMs were also consideredbut required more computational resources and were proneto overﬁtting if auxiliary tasks, such as reconstructing theoriginal sentence, were not used.For the positional encoding scheme, the sentence repre-sentation is produced byfi=Pj=1Mlj wij, where iselement-wise multiplication andljis a column vector withstructureljd=( 1 j/M) (d/D)(1 2j/M), wheredis the embedding index andDis the dimension of theembedding.The input fusion layer takes these input facts and enablesan information exchange between them by applying a bi-directional GRU. !fi=GRUfwd(fi,  !fi 1)(5)  fi=GRUbwd(fi,   fi+1)(6) !fi=  fi+ !fi(7)wherefiis the input fact at timestepi, !fiis the hidden stateof the forward GRU at timestepi, and  fiis the hidden stateof the backward GRU at timestepi. This allows contextualinformation from both future and past facts to impact !fi.We explored a variety of encoding schemes for the sen-tence reader, including GRUs, LSTMs, and the positionalencoding scheme described in Sukhbaatar et al. (2015).For simplicity and speed, we selected the positional en-coding scheme. GRUs and LSTMs were also consideredbut required more computational resources and were proneto overﬁtting if auxiliary tasks, such as reconstructing the5121414
W
CNNVisual feature extractionFeature embeddingInput fusion layerVisual Input ModuleGRUf1GRUGRUf2GRUGRUf196GRUWW. . .. . .Facts
Figure 3.VQA input module to represent images for the DMN.original sentence, were not used.3.2. Input Module for VQATo apply the DMN to visual question answering, we intro-duce a new input module for images. The module splitsan image into small local regions and considers each re-gion equivalent to a sentence in the input module for text.The input module for VQA is composed of three parts, il-lustrated in Fig. 3: local region feature extraction, visualfeature embedding, and the input fusion layer introducedin Sec. 3.1.Local region feature extraction:To extract featuresfrom the image, we use a convolutional neural network(Krizhevsky et al., 2012) based upon the VGG-19 model(Simonyan & Zisserman, 2014). We ﬁrst rescale the inputimage to448⇥448and take the output from the last pool-ing layer which has dimensionalityd= 512⇥14⇥14.The pooling layer divides the image into a grid of14⇥14,resulting in 196 local regional vectors ofd= 512.Visual feature embedding:As the VQA task involvesboth image features and text features, we add a linear layerwith tanh activation to project the local regional vectors tothe textual feature space used by the question vectorq.Input fusion layer:The local regional vectors extractedfrom above do not yet have global information availableto them. Without global information, their representationalpower is quite limited, with simple issues like object scal-ing or locational variance causing accuracy problems.To solve this, we add an input fusion layer similar to thatof the textual input module described in Sec. 3.1. First,to produce the input factsF, we traverse the image in asnake like fashion, as seen in Figure 3. We then apply abi-directional GRU over these input factsFto produce the
The	Modules:	QuestionAnswer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4
Answer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4Standard	GRU.	Output:	last	hidden	 state	àq
The	Modules:	Episodic	MemoryAnswer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4
Answer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4Dynamic Memory Networks for Visual and Textual Question Answering
c1c1m0m0AttnGRUGate AttentionAttnGRUAttnGRU. . .Attention MechanismMemory Updatem1m1 !F !Fm2m2AttnGRUGate AttentionAttnGRUAttnGRU. . .Attention MechanismMemory Update. . .. . .c2c2
Episodic Memory Pass 1Episodic Memory Pass 2
. . .. . .Figure 4.The episodic memory module of the DMN+ when usingtwo passes. The !Fis the output of the input module.globally aware input facts !F. The bi-directional GRU al-lows for information propagation from neighboring imagepatches, capturing spatial information.3.3. The Episodic Memory ModuleThe episodic memory module, as depicted in Fig. 4, re-trieves information from the input facts !F=[ !f1,..., !fN]provided to it by focusing attention on a subset of thesefacts. We implement this attention by associating a sin-gle scalar value, the attention gategti, with each fact !fiduring passt. This is computed by allowing interactionsbetween the fact and both the question representation andthe episode memory state.zti=[ !fi q; !fi mt 1;| !fi q|;| !fi mt 1|](8)Zti=W(2)tanh⇣W(1)zti+b(1)⌘+b(2)(9)gti=exp(Zti)PMik=1exp(Ztk)(10)where !fiis theithfact,mt 1is the previous episodememory,qis the original question, is the element-wiseproduct,|·|is the element-wise absolute value, and;rep-resents concatenation of the vectors.The DMN implemented in Kumar et al. (2015) involveda more complex set of interactions withinz, containingthe additional terms[f;mt 1;q;fTW(b)q;fTW(b)mt 1].After an initial analysis, we found these additional termswere not required.Attention MechanismOnce we have the attention gategtiwe use an attentionmechanism to extract a contextual vectorctbased upon thecurrent focus. We focus on two types of attention: soft at-tention and a new attention based GRU. The latter improvesperformance and is hence the ﬁnal modeling choice for theDMN+.hi˜hiriuiINOUThi˜hiriINOUTgti(a)(b) Figure 5.(a) The traditional GRU model, and (b) the proposedattention-based GRU modelSoft attention:Soft attention produces a contextual vec-torctthrough a weighted summation of the sorted list ofvectors !Fand corresponding attention gatesgti:ct=PNi=1gti !fiThis method has two advantages. First, it iseasy to compute. Second, if the softmax activation is spikyit can approximate a hard attention function by selectingonly a single fact for the contextual vector whilst still beingdifferentiable. However the main disadvantage to soft at-tention is that the summation process loses both positionaland ordering information. Whilst multiple attention passescan retrieve some of this information, this is inefﬁcient.Attention based GRU:For more complex queries, wewould like for the attention mechanism to be sensitive toboth the position and ordering of the input facts !F. AnRNN would be advantageous in this situation except theycannot make use of the attention gate from Equation 10.We propose a modiﬁcation to the GRU architecture by em-bedding information from the attention mechanism. Theupdate gateuiin Equation 1 decides how much of each di-mension of the hidden state to retain and how much shouldbe updated with the transformed inputxifrom the currenttimestep. Asuiis computed using only the current inputand the hidden state from previous timesteps, it lacks anyknowledge from the question or previous episode memory.By replacing the update gateuiin the GRU (Equation 1)with the output of the attention gategti(Equation 10) inEquation 4, the GRU can now use the attention gate forupdating its internal state. This change is depicted in Fig 5.hi=gti ˜hi+( 1 gti) hi 1(11)An important consideration is thatgtiis a scalar, generatedusing a softmax activation, as opposed to the vectorui2RnH, generated using a sigmoid activation. This allowsus to easily visualize how the attention gates activate overthe input, later shown for visual QA in Fig. 6. Thoughnot explored, replacing the softmax activation in Equation10 with a sigmoid activation would result ingti2RnH.To produce the contextual vectorctused for updating theepisodic memory statemt, we use the ﬁnal hidden state ofthe attention based GRU.Episode Memory UpdatesAfter each pass through the attention mechanism, we wishDynamic Memory Networks for Visual and Textual Question Answering
c1c1m0m0AttnGRUGate AttentionAttnGRUAttnGRU. . .Attention MechanismMemory Updatem1m1 !F !Fm2m2AttnGRUGate AttentionAttnGRUAttnGRU. . .Attention MechanismMemory Update. . .. . .c2c2
Episodic Memory Pass 1Episodic Memory Pass 2
. . .. . .Figure 4.The episodic memory module of the DMN+ when usingtwo passes. The !Fis the output of the input module.globally aware input facts !F. The bi-directional GRU al-lows for information propagation from neighboring imagepatches, capturing spatial information.3.3. The Episodic Memory ModuleThe episodic memory module, as depicted in Fig. 4, re-trieves information from the input facts !F=[ !f1,..., !fN]provided to it by focusing attention on a subset of thesefacts. We implement this attention by associating a sin-gle scalar value, the attention gategti, with each fact !fiduring passt. This is computed by allowing interactionsbetween the fact and both the question representation andthe episode memory state.zti=[ !fi q; !fi mt 1;| !fi q|;| !fi mt 1|](8)Zti=W(2)tanh⇣W(1)zti+b(1)⌘+b(2)(9)gti=exp(Zti)PMik=1exp(Ztk)(10)where !fiis theithfact,mt 1is the previous episodememory,qis the original question, is the element-wiseproduct,|·|is the element-wise absolute value, and;rep-resents concatenation of the vectors.The DMN implemented in Kumar et al. (2015) involveda more complex set of interactions withinz, containingthe additional terms[f;mt 1;q;fTW(b)q;fTW(b)mt 1].After an initial analysis, we found these additional termswere not required.Attention MechanismOnce we have the attention gategtiwe use an attentionmechanism to extract a contextual vectorctbased upon thecurrent focus. We focus on two types of attention: soft at-tention and a new attention based GRU. The latter improvesperformance and is hence the ﬁnal modeling choice for theDMN+.hi˜hiriuiINOUThi˜hiriINOUTgti(a)(b) Figure 5.(a) The traditional GRU model, and (b) the proposedattention-based GRU modelSoft attention:Soft attention produces a contextual vec-torctthrough a weighted summation of the sorted list ofvectors !Fand corresponding attention gatesgti:ct=PNi=1gti !fiThis method has two advantages. First, it iseasy to compute. Second, if the softmax activation is spikyit can approximate a hard attention function by selectingonly a single fact for the contextual vector whilst still beingdifferentiable. However the main disadvantage to soft at-tention is that the summation process loses both positionaland ordering information. Whilst multiple attention passescan retrieve some of this information, this is inefﬁcient.Attention based GRU:For more complex queries, wewould like for the attention mechanism to be sensitive toboth the position and ordering of the input facts !F. AnRNN would be advantageous in this situation except theycannot make use of the attention gate from Equation 10.We propose a modiﬁcation to the GRU architecture by em-bedding information from the attention mechanism. Theupdate gateuiin Equation 1 decides how much of each di-mension of the hidden state to retain and how much shouldbe updated with the transformed inputxifrom the currenttimestep. Asuiis computed using only the current inputand the hidden state from previous timesteps, it lacks anyknowledge from the question or previous episode memory.By replacing the update gateuiin the GRU (Equation 1)with the output of the attention gategti(Equation 10) inEquation 4, the GRU can now use the attention gate forupdating its internal state. This change is depicted in Fig 5.hi=gti ˜hi+( 1 gti) hi 1(11)An important consideration is thatgtiis a scalar, generatedusing a softmax activation, as opposed to the vectorui2RnH, generated using a sigmoid activation. This allowsus to easily visualize how the attention gates activate overthe input, later shown for visual QA in Fig. 6. Thoughnot explored, replacing the softmax activation in Equation10 with a sigmoid activation would result ingti2RnH.To produce the contextual vectorctused for updating theepisodic memory statemt, we use the ﬁnal hidden state ofthe attention based GRU.Episode Memory UpdatesAfter each pass through the attention mechanism, we wish
The	Modules:	Episodic	Memory•Gates	are	activated	if	relevant	to	the	question•When	the	end	of	the	input	is	reached,	the	relevant	facts	are	summarized	in	another	GRU	or	simple	NNet1621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152.2 Semantic MemoryThe semantic memory stores general facts about concepts based on information it receives from theinput module. For NLP, the semantic memory consists of (i) stored word concepts and (ii) facts aboutthem. The former is in the form of word vectors which are learned while input is being processed asdescribed in the section above. The latter is stored in terms of an embedded knowledge base (KB)[9,10,4]. The KB can include tables with single facts like lists of city names or people names aswell as relationship triplets like (dog,has-part,tail). If a KB is being used the semantic memory willtrain a max-margin objective similar to Socher et al. [4] to distinguish true facts from false ones.This is the second part of the full DMN objective function.2.3 QuestionThis module maps an input into a representation that can then be used for querying speciﬁc factsfrom the input module. Assume each question consists of a sequence ofTqword vectorsvt.W ecompute a hidden state for each viaqt=GRU(vt,qt 1), where the GRU weights are shared withthe input module. The ﬁnal question vector is deﬁned asq=qTq.2.4 Episodic MemoryThis section introduces our novel episodic memory module. It combines the previous three modules’outputs in order to reason over them and give the resulting knowledge to the answer module. Givena question vectorqit dynamically retrieves the necessary information over the sequence of wordsWor sentencesS. In many cases, the ﬁrst such retrieval process brings to light the necessity to retrieveadditional facts. Hence this process potentially iterates over the inputs multiple times, each iterationis deﬁned as anepisode. In other words, some questions require the model to do transitive inference(TI). TI has been studied extensively in psychology and neuroscience. Interestingly, it appears thatthe hippocampus, the seat of episodic memory in humans, is active during this kind of inference[11], and disruption of the hippocampus impairs TI [12].Generally, this memory module is a deep function that returns a memory representation from inputs:m=EM(W, S, q)that is relevant for the questionq. There are two options that are triggered based ona simple linear classiﬁer on the question vector: We can have a memory sequence over (i) sentencesor (ii) words. For words, the representation that is output is simply a sequenceM=m1,...,mTw,where eachmtis computed either via a simple neural network (mt=f(W(m)wt)) or an additionalGRU. In this case, the answer module will output a label for each element of this sequence. Thisis the case of part of speech tagging and named entity recognition and any other sequence labelingtasks.The more interesting scenario is when the model has to reason over complex semantic questionsinvolving multiple facts written in a series of natural language sentences. This case is describedin detail now. The ﬁnal output will be a memory vectorm, which is the last of a sequence ofincreasingly complete memory vectors. At the beginning of the retrieval process, we set the initialepisode’s memory to simply be the questionm0=q. Next we compute a series of gates, one foreach sentence in the input. The gate basically captures how relevant that sentence is for the currentquestion and takes into account what else the model has already stored in its memory.For instance, in the ﬁrst set of inputs of Fig.1, we may askWhere is Mary?and would hope thatthe gate for the ﬁrst sentence is close to 1, whereas all other gates of sentences that do not mentionMary would be close to 0.The gating functionGtakes as input a sentence vector at time stept, the current memory vectorand the question vector:g1t=G(st,m0,q)and returns a single scalarg. We deﬁne the functionG(s, m, q)as follows:z(s, m, q)=[s q,s m,|s q|,|s m|, s, m, q, sTW(b)q,sTW(b)m](5)G(s, m, q)= ⇣W(2)tanh⇣W(1)z(s, m, q)+b(1)⌘+b(2)⌘(6)4
Dynamic Memory Networks for Visual and Textual Question Answeringto update the episode memorymt 1with the newly con-structed contextual vectorct, producingmt. In the DMN,a GRU with the initial hidden state set to the question vec-torqis used for this purpose. The episodic memory forpasstis computed bymt=GRU(ct,mt 1)(12)The work of Sukhbaatar et al. (2015) suggests that usingdifferent weights for each pass through the episodic mem-ory may be advantageous. When the model contains onlyone set of weights for all episodic passes over the input, itis referred to as atied model, as in the “Mem Weights” rowin Table 1.Following the memory update component used inSukhbaatar et al. (2015) and Peng et al. (2015) we experi-ment with using a ReLU layer for the memory update, cal-culating the new episode memory state bymt=ReLU Wt[mt 1;ct;q]+b (13)where;is the concatenation operator,Wt2RnH⇥nH,b2RnH, andnHis the hidden size. The untying of weightsand using this ReLU formulation for the memory updateimproves accuracy by another 0.5% as shown in Table 1 inthe last column. The ﬁnal output of the memory network ispassed to the answer module as in the original DMN.4. Related WorkThe DMN is related to two major lines of recent work:memory and attention mechanisms. We work on both vi-sual and textual question answering which have, until now,been developed in separate communities.Neural Memory ModelsThe earliest recent work with amemory component that is applied to language processingis that of memory networks (Weston et al., 2015b) whichadds a memory component for question answering oversimple facts. They are similar to DMNs in that they alsohave input, scoring, attention and response mechanisms.However, unlike the DMN their input module computessentence representations independently and hence cannoteasily be used for other tasks such as sequence labeling.Like the original DMN, this memory network requires thatsupporting facts are labeled during QA training. End-to-end memory networks (Sukhbaatar et al., 2015) do not havethis limitation. In contrast to previous memory modelswith a variety of different functions for memory attentionretrieval and representations, DMNs (Kumar et al., 2015)have shown that neural sequence models can be used forinput representation, attention and response mechanisms.Sequence models naturally capture position and temporal-ity of both the inputs and transitive reasoning steps.Neural Attention MechanismsAttention mechanisms al-low neural network models to use a question to selectivelypay attention to speciﬁc inputs. They can beneﬁt imageclassiﬁcation (Stollenga et al., 2014), generating captionsfor images (Xu et al., 2015), among others mentioned be-low, and machine translation (Cho et al., 2014; Bahdanauet al., 2015; Luong et al., 2015). Other recent neural ar-chitectures with memory or attention which have proposedinclude neural Turing machines (Graves et al., 2014), neu-ral GPUs (Kaiser & Sutskever, 2015) and stack-augmentedRNNs (Joulin & Mikolov, 2015).Question Answering in NLPQuestion answering involv-ing natural language can be solved in a variety of ways towhich we cannot all do justice. If the potential input is alarge text corpus, QA becomes a combination of informa-tion retrieval and extraction (Yates et al., 2007). Neuralapproaches can include reasoning over knowledge bases,(Bordes et al., 2012; Socher et al., 2013a) or directly viasentences for trivia competitions (Iyyer et al., 2014).Visual Question Answering (VQA)In comparison to QAin NLP, VQA is still a relatively young task that is feasibleonly now that objects can be identiﬁed with high accuracy.The ﬁrst large scale database with unconstrained questionsabout images was introduced by Antol et al. (2015). WhileVQA datasets existed before they did not include open-ended, free-form questions about general images (Gemanet al., 2014). Others are were too small to be viable for adeep learning approach (Malinowski & Fritz, 2014). Theonly VQA model which also has an attention componentis the stacked attention network (Yang et al., 2015). Theirwork also uses CNN based features. However, unlike ourinput fusion layer, they use a single layer neural networkto map the features of each patch to the dimensionality ofthe question vector. Hence, the model cannot easily incor-porate adjacency of local information in its hidden state.A model that also uses neural modules, albeit logically in-spired ones, is that by Andreas et al. (2016) who evaluateon knowledgebase reasoning and visual question answer-ing. We compare directly to their method on the latter taskand dataset.Related to visual question answering is the task of describ-ing images with sentences (Kulkarni et al., 2011). Socheret al. (2014) used deep learning methods to map images andsentences into the same space in order to describe imageswith sentences and to ﬁnd images that best visualize a sen-tence. This was the ﬁrst work to map both modalities intoa joint space with deep learning methods, but it could onlyselect an existing sentence to describe an image. Shortlythereafter, recurrent neural networks were used to generateoften novel sentences based on images (Karpathy & Fei-Fei, 2015; Chen & Zitnick, 2014; Fang et al., 2015; Xuet al., 2015).
The	Modules:	Episodic	Memory•If	summary	is	insufficient	to	answer	the	question,	repeat	sequence	over	inputAnswer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4
Answer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4
Inspiration	from	Neuroscience•Episodic	memoryis	the	memoryof	autobiographical	events	(times,	places,	etc).	A	collection	of	past	personal	experiences	that	occurred	at	a	particular	time	and	place.•The	hippocampus,	the	seat	of	episodic	memory	in	humans,	is	active	during	transitive	inference•In	the	DMN	repeated	passes	over	the	input	are	needed	for	transitive	inference
The	Modules:	Answer
Answer module
Question ModuleSemantic MemoryModule Episodic MemoryModuleInput Module
Mary got the milk there.John moved to the bedroom.Sandra went back to the kitchen.Mary travelled to the hallway.John got the football there.John went to the hallway.John put down the football.Mary went to the garden.s1s2s3s4s5s6s7s8
Where is the fooball?q0.00.30.00.00.00.90.00.00.30.00.00.00.00.01.00.0e1e2e3e4e5e6e7e811111111e1e2e3e4e5e6e7e822222222hallway<EOS>m1m2(Glove vectors)
w1wTFigure 3: Real example of an input sentence sequence and the attention gates that are triggered by aspeciﬁc question. Gate valuesgitare shown above the corresponding vectors. The gates change witheach search over inputs. We do not draw connections for gates that are close to zero. See Section4.1 for details on the dataset that this example comes from.to take multiple passes over the facts, focusing attention on different facts at each pass. Each passproduces anepisode, and these episodes are then summarized into the memory. Endowing ourmodule with this episodic component allows its attention mechanism to attend more selectively tospeciﬁc facts on each pass, as it can attend to other important facts at a later pass. It also allows fora type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional facts.For instance, in the example in Fig. 3, we are askedWhere is the football?In the ﬁrst iteration,the model ought attend to sentence 7 (John put down the football.), as the question asks about thefootball. Only once the model sees that John is relevant can it reason the second iteration shouldretrieve where John was. In this example, taken from a true test question on Facebook’s bAbI task,this behavior is indeed seen. Note that the second iteration has wrongly placed some weight insentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.In its general form, the episodic memory module is characterized by an attention mechanism, afunction which returns an episode given the output of the attention mechanism and the facts fromthe input module, and a function that summarizes the episodes into a memory.In our work, we use a gating function as our attention mechanism. It takes as input, for each passi,acandidate factct, a previous statemi 1, and the questionqto compute a gate:git=G(ct,mi 1,q).The state is updated by way of a GRU:mi=GRU(ei,mi 1), whereeiis the computed episode atpassi. The state may be initialized randomly, but in practice we have found that initializing it to thequestion vector itself helps; e.g,m0=q. The functionGreturns a single scalar and is deﬁned asfollows:z(c, m, q)=[c, m, q, c q,c m,|c q|,|c m|,cTW(b)q,cTW(b)m](3)G(c, m, q)= ⇣W(2)tanh⇣W(1)z(c, m, q)+b(1)⌘+b(2)⌘(4)To compute the episode for passi, we employ a modiﬁed GRU over the sequence ofTCfactsct,endowed with our gates. The episode is the ﬁnal state of the GRU:hit=gitGRU(ct,hit 1)+( 1 git)hit 1(5)ei=hiTC(6)Finally, to summarize theTPepisodeseiinto a memory, we use the same GRU that updates theattention mechanism’s state:mi=GRU(ei,mi 1), and we set the memorymasm=mTP. Thisis equivalent to setting the memory to simply the attention mechanism’s ﬁnal state, but we have de-scribed it here as its own computation to highlight the potential modularity of these subcomponents.For datasets that mark which facts are important for a given question, such as Facebook’s bAbIdataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classiﬁcation4Input	to	Answer	GRU:	a	=	[m	q]GRU	for	sequences,	 standard	softmaxfor	single	class
Academic	papers	and	related	work•For	full	details:	•Ask	Me	Anything:	Dynamic	Memory	Networks	for	Natural	Language	Processing(Kumar	et	al.,	2015)•Dynamic	Memory	Networks	for	Visual	and	Textual	Question	Answering(Xionget	al.,	2016)	•Sequence	to	Sequence	(Sutskeveret	al.	2014)•Neural	Turing	Machines	(Graves	et	al.	2014)•Teach in g 	 Mach in es	 to 	Read 	 an d 	Co mp r eh en d 	 ( H er man n 	 et	 al. 	2 0 1 5 )•Learning	to	Transduce	with	Unbounded	Memory	(Grefenstette2015)•Structured	Memory	for	Neural	Turing	Machines	(Wei	Zhang	2015)•Memory	Networks	(Weston	et	al.	2015)•End	to	end	memory	networks	(Sukhbaataret	al.	2015)à
Comparison	to	MemNetsSimilarities:•MemNetsand	DMNs	have	input,	scoring,	attention	and	response	mechanismsDifferences:•For	input	representations	MemNetsuse	bag	of	word,	nonlinear	or	linear	embeddings	that	explicitly	 encode	position	•MemNetsiteratively	 run	functions	for	attention	 and	response•DMNs	shows	that	neural	sequence	models	can	be	used	for	input	representation,	attention	and	response	mechanisms	ànaturally	captures	position	and	temporality•Enables	broader	range	of	applications
Experiments:	QA	on	babI(1k)4.1 Question AnsweringThe Facebook bAbI dataset is a synthetic dataset meant to test a model’s ability to retrieve factsand reason over them. Each task tests a different skill that a good question answering model oughtto have, such as coreference resolution, deduction, and induction. Training on the bAbI datasetTask MemNN DMNTask MemNN DMN1: Single Supporting Fact 100 10011: Basic Coreference 100 99.92: Two Supporting Facts 100 98.212: Conjunction 100 1003: Three Supporting facts 100 95.213: Compound Coreference 100 99.84: Two Argument Relations 100 10014: Time Reasoning 99 1005: Three Argument Relations 98 99.315: Basic Deduction 100 1006: Yes/No Questions 100 10016: Basic Induction 100 99.47: Counting 85 96.917: Positional Reasoning 65 59.68: Lists/Sets 91 96.518: Size Reasoning 95 95.39: Simple Negation 100 10019: Path Finding 36 34.510: Indeﬁnite Knowledge 98 97.520: Agent’s Motivations 100 100Mean Accuracy (%) 93.393.6Table 1: Test accuracies on the bAbI dataset. MemNN numbers taken from Weston et al. [18]. TheDMN passes (accuracy>95%) 18 tasks, whereas the MemNN passes 16.uses the following objective function:J=↵ECE(Gates)+ ECE(Answers), whereECEis thestandard cross-entropy cost and↵and are hyperparameters. In practice, we begin training with↵set to 1 and set to 0, and then later switch to 1 while keeping↵at 1. We subsample the factsfrom the input module by end-of-sentence tokens. The gate supervision aims to select one sentenceper pass; thus, we also experimented with modifying Eq. 6 to a simplesoftmaxinstead of a GRU.Here, we compute the ﬁnal episode vector via:ei=PTt=1softmax(git)ct, wheresoftmax(git)=exp(git)PTj=1exp(gij), andgithere is the value of the gate before the sigmoid. This setting achieves betterresults, likely because the softmax is better suited to picking one sentence at a time.We list results in table 1. The DMN does worse than the MemNN on tasks 2 and 3, both tasks withlong input sequences. We suspect this is due to the recurrent input sequence model having troublemodeling very long inputs. The MemNN does not suffer from this problem as it views each sentenceseperately. The power of the episodic memory module is evident in tasks 7 and 8, where the DMNsigniﬁcantly outperforms the MemNN. Both tasks require the model to iteratively retrieve facts andstore them in a representation that slowly incorporates more of the relevant information of the inputsequence. Both models do poorly on tasks 17 and 19, though the MemNN does better. We suspectthis is due to the MemNN using n-gram features as well as explicit sequence position features.4.2 Sequence Tagging: Part of Speech TaggingPart-of-speech tagging is traditionally modeled as a sequence tagging problem: every word in asentence is to be classiﬁed into its part-of-speech class (see Fig. 1). We evaluate on the standardWall Street Journal dataset included in Penn-III [26]. We use the standard splits of sections 0-18for training, 19-21 for development and 22-24 for test sets [27]. Since this is a word level taggingtask, DMN memories are produced at the word -rather than sentence- level. We compare the DMNModel SVMTool Sogaard Suzuki et al. Spoustova et al. SCNNDMNAcc (%) 97.15 97.27 97.40 97.44 97.5097.56Table 2: Test accuracies on WSJ-PTBwith the results in [27]. The DMN achieves state-of-the-art accuracy with a single model, reachinga development set accuracy of 97.5. Ensembling the top 4 development models, the DMN gets to97.58 dev and 97.56 test accuracies, achieving a new state-of-the-art (Table 2).7This	still	requires	that	relevant	facts	are	marked	during	training	to	train	the	gates.
Live	Demo

Experiments:	Sentiment	Analysis•Stanford	Sentiment	 Treebank•Test	accuracies:•MV-RNN	and	RNTN:	Socher	et	al.	(2013)•DCNN:	Kalchbrenneret	al.	(2014)•PVec:	Le	&	Mikolov.	(2014)•CNN-MC:	Kim	(2014)•DRNN:	Irsoy&	Cardie(2015)•CT-LSTM:	Tai	et	al.	(2015)	Ask Me Anything: Dynamic Memory Networks for Natural Language ProcessingTask MemNN DMN1: Single Supporting Fact 100 1002: Two Supporting Facts 100 98.23: Three Supporting Facts 100 95.24: Two Argument Relations 100 1005: Three Argument Relations 98 99.36: Yes/No Questions 100 1007: Counting 85 96.98: Lists/Sets 91 96.59: Simple Negation 100 10010: Indeﬁnite Knowledge 98 97.511: Basic Coreference 100 99.912: Conjunction 100 10013: Compound Coreference 100 99.814: Time Reasoning 99 10015: Basic Deduction 100 10016: Basic Induction 100 99.417: Positional Reasoning 65 59.618: Size Reasoning 95 95.319: Path Finding 36 34.520: Agent’s Motivations 100 100Mean Accuracy (%) 93.393.6Table 1.Test accuracies on the bAbI dataset. MemNN numberstaken from Weston et al. (Weston et al., 2015a). The DMN passes(accuracy>95%) 18 tasks, whereas the MemNN passes 16.4.1. Question AnsweringThe Facebook bAbI dataset is a synthetic dataset for test-ing a model’s ability to retrieve facts and reason over them.Each task tests a different skill that a question answeringmodel ought to have, such as coreference resolution, de-duction, and induction. Showing an ability exists here isnot sufﬁcient to conclude a model would also exhibit it onreal world text data. It is, however, a necessary condition.Training on the bAbI dataset uses the following objectivefunction:J=↵ECE(Gates)+ ECE(Answers), whereECEis the standard cross-entropy cost and↵and are hy-perparameters. In practice, we begin training with↵set to1 and set to 0, and then later switch to 1 while keep-ing↵at 1. As described in Section 2.1, the input moduleoutputs fact representations by taking the encoder hiddenstates at time steps corresponding to the end-of-sentence to-kens. The gate supervision aims to select one sentence perpass; thus, we also experimented with modifying Eq. 8 toa simplesoftmaxinstead of a GRU. Here, we compute theﬁnal episode vector via:ei=PTt=1softmax(git)ct, wheresoftmax(git)=exp(git)PTj=1exp(gij), andgithere is the value ofthe gate before the sigmoid. This setting achieves better re-sults, likely because the softmax encourages sparsity and isbetter suited to picking one sentence at a time.Task Binary Fine-grainedMV-RNN 82.9 44.4RNTN 85.4 45.7DCNN 86.8 48.5PVec 87.8 48.7CNN-MC 88.1 47.4DRNN 86.6 49.8CT-LSTM 88.0 51.0DMN88.6 52.1Table 2.Test accuracies for sentiment analysis on the StanfordSentiment Treebank. MV-RNN and RNTN: Socher et al. (2013).DCNN: Kalchbrenner et al. (2014). PVec: Le & Mikolov. (2014).CNN-MC: Kim (2014). DRNN: Irsoy & Cardie (2015), 2014.CT-LSTM: Tai et al. (2015)We list results in Table 1. The DMN does worse thanthe Memory Network, which we refer to from here on asMemNN, on tasks 2 and 3, both tasks with long input se-quences. We suspect that this is due to the recurrent inputsequence model having trouble modeling very long inputs.The MemNN does not suffer from this problem as it viewseach sentence separately. The power of the episodic mem-ory module is evident in tasks 7 and 8, where the DMNsigniﬁcantly outperforms the MemNN. Both tasks requirethe model to iteratively retrieve facts and store them in arepresentation that slowly incorporates more of the rele-vant information of the input sequence. Both models dopoorly on tasks 17 and 19, though the MemNN does better.We suspect this is due to the MemNN using n-gram vectorsand sequence position features.4.2. Text Classiﬁcation: Sentiment AnalysisThe Stanford Sentiment Treebank (SST) (Socher et al.,2013) is a popular dataset for sentiment classiﬁcation. Itprovides phrase-level ﬁne-grained labels, and comes with atrain/development/test split. We present results on two for-mats: ﬁne-grained root prediction, where all full sentences(root nodes) of the test set are to be classiﬁed as either verynegative, negative, neutral, positive, or very positive, andbinary root prediction, where all non-neutral full sentencesof the test set are to be classiﬁed as either positive or neg-ative. To train the model, we use all full sentences as wellas subsample 50% of phrase-level labels every epoch. Dur-ing evaluation, the model is only evaluated on the full sen-tences (root setup). In binary classiﬁcation, neutral phrasesare removed from the dataset. The DMN achieves state-of-the-art accuracy on the binary classiﬁcation task, as well ason the ﬁne-grained classiﬁcation task.In all experiments, the DMN was trained with GRU se-quence models. It is easy to replace the GRU sequencemodel with any of the models listed above, as well as in-
Analysis	of	Number	of	Episodes•How	many	attention	+	memory	 passes	are	needed	in	the	episodic	memory?Ask Me Anything: Dynamic Memory Networks for Natural Language ProcessingModel Acc (%)SVMTool 97.15Sogaard 97.27Suzuki et al. 97.40Spoustova et al. 97.44SCNN 97.50DMN97.56Table 3.Test accuracies on WSJ-PTBcorporate tree structure in the retrieval process.4.3. Sequence Tagging: Part-of-Speech TaggingPart-of-speech tagging is traditionally modeled as a se-quence tagging problem: every word in a sentence is tobe classiﬁed into its part-of-speech class (see Fig. 1). Weevaluate on the standard Wall Street Journal dataset (Mar-cus et al., 1993). We use the standard splits of sections0-18 for training, 19-21 for development and 22-24 for testsets (Søgaard, 2011). Since this is a word level taggingtask, DMN memories are classiﬁed at each time step corre-sponding to each word. This is described in detail in Sec-tion 2.4’s discussion of sequence modeling.We compare the DMN with the results in (Søgaard, 2011).The DMN achieves state-of-the-art accuracy with a singlemodel, reaching a development set accuracy of 97.5. En-sembling the top 4 development models, the DMN gets to97.58 dev and 97.56 test accuracies, achieving a slightlyhigher new state-of-the-art (Table 3).4.4. Quantitative Analysis of Episodic Memory ModuleThe main novelty of the DMN architecture is in its episodicmemory module. Hence, we analyze how important theepisodic memory module is for NLP tasks and in particularhow the number of passes over the input affect accuracy.Table 4 shows the accuracies on a subset of bAbI tasks aswell as on the Stanford Sentiment Treebank. We note thatfor several of the hard reasoning tasks, multiple passes overthe inputs are crucial to achieving high performance. Forsentiment the differences are smaller. However, two passesoutperform a single pass or zero passes. In the latter case,there is no episodic memory at all and outputs are passeddirectly from the input module to the answer module. Wenote that, especially complicated examples are more of-ten correctly classiﬁed with 2 passes but many examplesin sentiment contain only simple sentiment words and nonegation or misleading expressions. Hence the need to havea complicated architecture for them is small. The same istrue for POS tagging. Here, differences in accuracy are lessthan 0.1 between different numbers of passes.Next, we show that the additional correct classiﬁcations areMaxpassestask 3three-factstask 7counttask 8lists/setssentiment(ﬁne grain)0 pass 0 48.8 33.6 50.01 pass 0 48.8 54.0 51.52 pass 16.7 49.1 55.652.13 pass 64.7 83.4 83.4 50.15 pass95.2 96.9 96.5N/ATable 4.Effectiveness of episodic memory module across tasks.Each row shows the ﬁnal accuracy in term of percentages witha different maximum limit for the number of passes the episodicmemory module can take. Note that for the 0-pass DMN, thenetwork essential reduces to the output of the attention module.hard examples with mixed positive/negative vocabulary.4.5. Qualitative Analysis of Episodic Memory ModuleApart from a quantitative analysis, we also show qualita-tively what happens to the attention during multiple passes.We present speciﬁc examples from the experiments to illus-trate that the iterative nature of the episodic memory mod-ule enables the model to focus on relevant parts of the input.For instance, Table 5 shows an example of what the DMNfocuses on during each pass of a three-iteration scan on aquestion from the bAbI dataset.We also evaluate the episodic memory module for senti-ment analysis. Given that the DMN performs well withboth one iteration and two iterations, we study test exam-ples where the one-iteration DMN is incorrect and the two-episode DMN is correct. Looking at the sentences in Fig. 4and 5, we make the following observations:1.The attention of the two-iteration DMN is generallymuch more focused compared to that of the one-iteration DMN. We believe this is due to the fact thatwith fewer iterations over the input, the hidden statesof the input module encoder have to capture more ofthe content of adjacent time steps. Hence, the atten-tion mechanism cannot only focus on a few key timesteps. Instead, it needs to pass all necessary informa-tion to the answer module from a single pass.2.During the second iteration of the two-iteration DMN,the attention becomes signiﬁcantly more focused onrelevant key words and less attention is paid to strongsentiment words that lose their sentiment in context.This is exempliﬁed by the sentence in Fig. 5 that in-cludes the very positive word ”best.” In the ﬁrst iter-ation, the word ”best” dominates the attention scores(darker color means larger score). However, once itscontext, ”is best described”, is clear, its relevance isdiminished and ”lukewarm” becomes more important.We conclude that the ability of the episodic memory mod-
Analysis	of	Attention	for	SentimentAsk Me Anything: Dynamic Memory Networks for Natural Language ProcessingQuestion:Where was Mary before the Bedroom?Answer:Cinema.Facts Episode 1 Episode 2 Episode 3Yesterday Julie traveled to the school.Yesterday Marie went to the cinema.This morning Julie traveled to the kitchen.Bill went back to the cinema yesterday.Mary went to the bedroom this morning.Julie went back to the bedroom this afternoon.[done reading]Table 5.An example of what the DMN focuses on during each episode on a real query in the bAbI task. Darker colors mean that theattention weight is higher.
Figure 4.Attention weights for sentiment examples that wereonly labeled correctly by a DMN with two episodes. The y-axisshows the episode number. This sentence demonstrates a casewhere the ability to iterate allows the DMN to sharply focus onrelevant words.ule to perform multiple passes over the data is beneﬁcial. Itprovides signiﬁcant beneﬁts on harder bAbI tasks, whichrequire reasoning over several pieces of information ortransitive reasoning. Increasing the number of passes alsoslightly improves the performance on sentiment analysis,though the difference is not as signiﬁcant. We did not at-tempt more iterations for sentiment analysis as the modelstruggles with overﬁtting with three passes.Figure 5.These sentence demonstrate cases where initially posi-tive words lost their importance after the entire sentence contextbecame clear either through a contrastive conjunction (”but”) or amodiﬁed action ”best described.”5. ConclusionThe DMN model is a potentially general architecture for avariety of NLP applications, including classiﬁcation, ques-tion answering and sequence modeling. A single architec-ture is a ﬁrst step towards a single joint model for multi-ple NLP problems. The DMN is trained end-to-end withone, albeit complex, objective function. Future work willexplore additional tasks, larger multi-task models and mul-timodal inputs and questions.•Sharper	 attention	when	2	passes	are	allowed.	•Examples	that	are	wrong	with	just	one	pass
Analysis	of	Attention	for	SentimentAsk Me Anything: Dynamic Memory Networks for Natural Language ProcessingQuestion:Where was Mary before the Bedroom?Answer:Cinema.Facts Episode 1 Episode 2 Episode 3Yesterday Julie traveled to the school.Yesterday Marie went to the cinema.This morning Julie traveled to the kitchen.Bill went back to the cinema yesterday.Mary went to the bedroom this morning.Julie went back to the bedroom this afternoon.[done reading]Table 5.An example of what the DMN focuses on during each episode on a real query in the bAbI task. Darker colors mean that theattention weight is higher.
Figure 4.Attention weights for sentiment examples that wereonly labeled correctly by a DMN with two episodes. The y-axisshows the episode number. This sentence demonstrates a casewhere the ability to iterate allows the DMN to sharply focus onrelevant words.ule to perform multiple passes over the data is beneﬁcial. Itprovides signiﬁcant beneﬁts on harder bAbI tasks, whichrequire reasoning over several pieces of information ortransitive reasoning. Increasing the number of passes alsoslightly improves the performance on sentiment analysis,though the difference is not as signiﬁcant. We did not at-tempt more iterations for sentiment analysis as the modelstruggles with overﬁtting with three passes.Figure 5.These sentence demonstrate cases where initially posi-tive words lost their importance after the entire sentence contextbecame clear either through a contrastive conjunction (”but”) or amodiﬁed action ”best described.”5. ConclusionThe DMN model is a potentially general architecture for avariety of NLP applications, including classiﬁcation, ques-tion answering and sequence modeling. A single architec-ture is a ﬁrst step towards a single joint model for multi-ple NLP problems. The DMN is trained end-to-end withone, albeit complex, objective function. Future work willexplore additional tasks, larger multi-task models and mul-timodal inputs and questions.
Analysis	of	Attention	for	SentimentAsk Me Anything: Dynamic Memory Networks for Natural Language ProcessingQuestion:Where was Mary before the Bedroom?Answer:Cinema.Facts Episode 1 Episode 2 Episode 3Yesterday Julie traveled to the school.Yesterday Marie went to the cinema.This morning Julie traveled to the kitchen.Bill went back to the cinema yesterday.Mary went to the bedroom this morning.Julie went back to the bedroom this afternoon.[done reading]Table 5.An example of what the DMN focuses on during each episode on a real query in the bAbI task. Darker colors mean that theattention weight is higher.
Figure 4.Attention weights for sentiment examples that wereonly labeled correctly by a DMN with two episodes. The y-axisshows the episode number. This sentence demonstrates a casewhere the ability to iterate allows the DMN to sharply focus onrelevant words.ule to perform multiple passes over the data is beneﬁcial. Itprovides signiﬁcant beneﬁts on harder bAbI tasks, whichrequire reasoning over several pieces of information ortransitive reasoning. Increasing the number of passes alsoslightly improves the performance on sentiment analysis,though the difference is not as signiﬁcant. We did not at-tempt more iterations for sentiment analysis as the modelstruggles with overﬁtting with three passes.
Figure 5.These sentence demonstrate cases where initially posi-tive words lost their importance after the entire sentence contextbecame clear either through a contrastive conjunction (”but”) or amodiﬁed action ”best described.”5. ConclusionThe DMN model is a potentially general architecture for avariety of NLP applications, including classiﬁcation, ques-tion answering and sequence modeling. A single architec-ture is a ﬁrst step towards a single joint model for multi-ple NLP problems. The DMN is trained end-to-end withone, albeit complex, objective function. Future work willexplore additional tasks, larger multi-task models and mul-timodal inputs and questions.•Examples	where	full	sentence	context	from	first	pass	changes	attention	to	words	more	relevant	for	final	prediction
Analysis	of	Attention	for	Sentiment•Examples	where	full	sentence	context	from	first	pass	changes	attention	to	words	more	relevant	for	final	predictionAsk Me Anything: Dynamic Memory Networks for Natural Language ProcessingQuestion:Where was Mary before the Bedroom?Answer:Cinema.Facts Episode 1 Episode 2 Episode 3Yesterday Julie traveled to the school.Yesterday Marie went to the cinema.This morning Julie traveled to the kitchen.Bill went back to the cinema yesterday.Mary went to the bedroom this morning.Julie went back to the bedroom this afternoon.[done reading]Table 5.An example of what the DMN focuses on during each episode on a real query in the bAbI task. Darker colors mean that theattention weight is higher.
Figure 4.Attention weights for sentiment examples that wereonly labeled correctly by a DMN with two episodes. The y-axisshows the episode number. This sentence demonstrates a casewhere the ability to iterate allows the DMN to sharply focus onrelevant words.ule to perform multiple passes over the data is beneﬁcial. Itprovides signiﬁcant beneﬁts on harder bAbI tasks, whichrequire reasoning over several pieces of information ortransitive reasoning. Increasing the number of passes alsoslightly improves the performance on sentiment analysis,though the difference is not as signiﬁcant. We did not at-tempt more iterations for sentiment analysis as the modelstruggles with overﬁtting with three passes.
Figure 5.These sentence demonstrate cases where initially posi-tive words lost their importance after the entire sentence contextbecame clear either through a contrastive conjunction (”but”) or amodiﬁed action ”best described.”5. ConclusionThe DMN model is a potentially general architecture for avariety of NLP applications, including classiﬁcation, ques-tion answering and sequence modeling. A single architec-ture is a ﬁrst step towards a single joint model for multi-ple NLP problems. The DMN is trained end-to-end withone, albeit complex, objective function. Future work willexplore additional tasks, larger multi-task models and mul-timodal inputs and questions.
Live	Demo

4.1 Question AnsweringThe Facebook bAbI dataset is a synthetic dataset meant to test a model’s ability to retrieve factsand reason over them. Each task tests a different skill that a good question answering model oughtto have, such as coreference resolution, deduction, and induction. Training on the bAbI datasetTask MemNN DMNTask MemNN DMN1: Single Supporting Fact 100 10011: Basic Coreference 100 99.92: Two Supporting Facts 100 98.212: Conjunction 100 1003: Three Supporting facts 100 95.213: Compound Coreference 100 99.84: Two Argument Relations 100 10014: Time Reasoning 99 1005: Three Argument Relations 98 99.315: Basic Deduction 100 1006: Yes/No Questions 100 10016: Basic Induction 100 99.47: Counting 85 96.917: Positional Reasoning 65 59.68: Lists/Sets 91 96.518: Size Reasoning 95 95.39: Simple Negation 100 10019: Path Finding 36 34.510: Indeﬁnite Knowledge 98 97.520: Agent’s Motivations 100 100Mean Accuracy (%) 93.393.6Table 1: Test accuracies on the bAbI dataset. MemNN numbers taken from Weston et al. [18]. TheDMN passes (accuracy>95%) 18 tasks, whereas the MemNN passes 16.uses the following objective function:J=↵ECE(Gates)+ ECE(Answers), whereECEis thestandard cross-entropy cost and↵and are hyperparameters. In practice, we begin training with↵set to 1 and set to 0, and then later switch to 1 while keeping↵at 1. We subsample the factsfrom the input module by end-of-sentence tokens. The gate supervision aims to select one sentenceper pass; thus, we also experimented with modifying Eq. 6 to a simplesoftmaxinstead of a GRU.Here, we compute the ﬁnal episode vector via:ei=PTt=1softmax(git)ct, wheresoftmax(git)=exp(git)PTj=1exp(gij), andgithere is the value of the gate before the sigmoid. This setting achieves betterresults, likely because the softmax is better suited to picking one sentence at a time.We list results in table 1. The DMN does worse than the MemNN on tasks 2 and 3, both tasks withlong input sequences. We suspect this is due to the recurrent input sequence model having troublemodeling very long inputs. The MemNN does not suffer from this problem as it views each sentenceseperately. The power of the episodic memory module is evident in tasks 7 and 8, where the DMNsigniﬁcantly outperforms the MemNN. Both tasks require the model to iteratively retrieve facts andstore them in a representation that slowly incorporates more of the relevant information of the inputsequence. Both models do poorly on tasks 17 and 19, though the MemNN does better. We suspectthis is due to the MemNN using n-gram features as well as explicit sequence position features.4.2 Sequence Tagging: Part of Speech TaggingPart-of-speech tagging is traditionally modeled as a sequence tagging problem: every word in asentence is to be classiﬁed into its part-of-speech class (see Fig. 1). We evaluate on the standardWall Street Journal dataset included in Penn-III [26]. We use the standard splits of sections 0-18for training, 19-21 for development and 22-24 for test sets [27]. Since this is a word level taggingtask, DMN memories are produced at the word -rather than sentence- level. We compare the DMNModel SVMTool Sogaard Suzuki et al. Spoustova et al. SCNNDMNAcc (%) 97.15 97.27 97.40 97.44 97.5097.56Table 2: Test accuracies on WSJ-PTBwith the results in [27]. The DMN achieves state-of-the-art accuracy with a single model, reachinga development set accuracy of 97.5. Ensembling the top 4 development models, the DMN gets to97.58 dev and 97.56 test accuracies, achieving a new state-of-the-art (Table 2).7Experiments:	POS	Tagging•PTB	WSJ,	standard	splits•Episodic	memory	does	not	require	multiple	passes,	single	pass	enough
Live	Demo

Modularization	Allows	for	Different	InputsDynamic Memory Networks for Visual and Textual Question AnsweringCaiming Xiong*, Stephen Merity*, Richard Socher{CMXIONG,SMERITY,RICHARD}METAMIND.IOMetaMind, Palo Alto, CA USAAbstractNeural network architectures with memory andattention mechanisms exhibit certain reason-ing capabilities required for question answering.One such architecture, the dynamic memory net-work (DMN), obtained high accuracy on a vari-ety of language tasks. However, it was not shownthat the architecture achieves strong results forquestion answering when supporting facts are notmarked during training or whether the questionanswering capability could be applied to othermodalities such as images. We analyze the DMNon the question answering task without support-ing fact labels. Based on this analysis, we pro-pose several improvements to the memory andinput modules. Together with these changes weintroduce a novel input module for images inorder to be able to answer questions about im-ages. Our new DMN+ model improves the stateof the art on both the Visual Question Answering(VQA) dataset and the bAbI-10k text question-answering dataset.1. IntroductionNeural network based methods have made tremendousprogress in image and text classiﬁcation (Krizhevsky et al.,2012;Socher et al.,2013b). However, only recently hasprogress been made on more complex tasks that requirelogical reasoning. This success is based in part on theaddition of memory and attention components to complexneural networks. For instance, memory networks (Westonet al.,2015b) are able to reason over several facts written innatural language or (subject, relation, object) triplets. At-tention mechanisms have been successful components inboth machine translation (Bahdanau et al.,2015;Luonget al.,2015) and image captioning models (Xu et al.,2015).The dynamic memory network (Kumar et al.,2015)Proceedings of the33rdInternational Conference on MachineLearning, New York, NY, USA, 2016. JMLR: W&CP volume48. Copyright 2016 by the author(s).Episodic MemoryAnswerQuestionInput ModuleEpisodic MemoryAnswerQuestionInput Module
(a) Text Question-Answering(b) Visual Question-Answering John moved to the garden. John got the apple there. John moved to the kitchen. Sandra picked up the milk there. John dropped the apple. John moved to the ofﬁce.Where is the apple?KitchenWhat kind of tree is in the background?Palm
Figure 1.Question Answering over text and images using a Dy-namic Memory Network.(DMN) is one example of a neural network model that hasboth a memory component and an attention mechanism.The DMN yields state of the art results on question an-swering with supporting facts labeled during training, sen-timent analysis, and part-of-speech tagging. Its main ideais to use a question to selectively pay attention to textualinputs. These inputs are then given to an episodic memorymodule which collects the relevant inputs in order to givean answer. The memory module has two important steps:(1) computing attention scores to focus on particular factsgiven a question and (2) updating the memory by reasoningover the attended facts.We analyze the DMN components, speciﬁcally the inputmodule and memory module, to improve accuracy overquestion answering. We propose a new input module whichuses a two level encoder with a sentence reader and inputfusion layer to allow for information ﬂow between sen-tences. For the memory, we propose a modiﬁcation to gatedrecurrent units (GRU) (Chung et al.,2014). The gates inthe new GRU formulation are dependent on the attentionscores and global knowledge over the facts. Unlike be-fore, the new DMN+ model does not require that support-ing facts (i.e. the facts that are relevant for answering aparticular question) are labeled during training. The modellearns to pick the important facts from a larger set.In addition, we introduce a new input module to representimages. This module is compatible with the rest of theDMN architecture and its output is fed into the memorymodule. We show that the changes in the memory module
Input	Module	for	ImagesDynamic Memory Networks for Visual and Textual Question Answeringthe hidden state to retain and how much should be updatedwith the transformed inputxifrom the current timestep. Asuiis computed using only the current input and the hiddenstate from previous timesteps, it lacks any knowledge fromthe question or previous episode memory.We propose replacing the update gatesuiin the GRU withthe output of the attention gatesgti. As the input to the up-date gate can be more detailed, we speculate it allows bet-ter informed update decisions. Additionally, the attentionbased GRU can now take positional and ordering informa-tion of facts into account, which the soft attention modelcannot do. To produce the contextual vectorctused forupdating the episodic memory statemt, we use the ﬁnalhidden state of the attention based GRU.Episode Memory UpdatesAfter each pass through the attention mechanism, we wishto update the episode memorymt 1with the newly con-structed contextual vectorct, producingmt. In the DMN,a GRU with the initial hidden state set to the question vec-torqis used for this purpose. The episodic memory forpasstis computed bymt=GRU(ct,mt 1)(4)The work ofSukhbaatar et al.(2015) suggests that usingdifferent weights for each pass through the episodic mem-ory may be advantageous. When the model contains onlyone set of weights for multiple episodic passes, it is re-ferred to as atied model. For untied experiments whereeach pass through the episodic memory module has inde-pendent weights, the GRU makes less sense for memoryupdates. Following the memory update component used inSukhbaatar et al.(2015) andPeng et al.(2015) we experi-ment with using a ReLU layer for memory update, calcu-lating the new episode memory state bymt=ReLU W[mt 1;ct;q]+b (5)3. DMN Input Module for VQATo apply the DMN to visual question answering, we intro-duce a new input module for images. The module splitsan image into small local regions and considers each re-gion equivalent to a sentence in the input module for text.The input module for VQA is composed of three parts, il-lustrated in Fig.3: local region feature extraction, visualfeature embedding, and the input fusion layer introducedin Sec.2.2.Local region feature extraction:To extract featuresfrom the image, we use a convolutional neural network(Krizhevsky et al.,2012;Szegedy et al.,2015) based uponthe VGG-19 model (Simonyan & Zisserman,2014). We5121414
WWWGRUGRUGRUGRUGRUGRU
CNNVisual feature extractionFeature embeddingInput fusion layerInput Module
Figure 3.VQA input module to represent images for the DMN.ﬁrst rescale the input image to448⇥448and take the out-put from the last pooling layer which has dimensionalityd= 512⇥14⇥14. The pooling layer divides the imageinto a grid of14⇥14, resulting in 196 local regional vectorsofd= 512.Visual feature embedding:As the VQA task involvesboth image features and text features, we add a linear layerwith tanh activation to project thed= 512local regionalvectors to the textual feature space used by the questionvectorq.Input fusion layer:The local regional vectors extractedfrom above do not yet have global information availableto them. Without global information, their representationalpower is quite limited, with simple issues like object scal-ing or locational variance causing accuracy problems.To solve this, we add an input fusion layer similar to thatof the textual input module described in Sec.2.2. First,to produce the input factsF, we traverse the image in asnake like fashion, as seen in Figure3. We then apply abi-directional GRU over these input factsFto produce theglobally aware input facts !F. The bi-directional GRU al-lows for information propagation from neighboring imagepatches. As the bi-directional GRU is one dimensional andthe original image 2D, some spatial information may bedifﬁcult to capture.4. Related WorkThe DMN is related to two major lines of recent work:memory and attention mechanisms. We work on both vi-sual and textual question answering which have, until now,been developed in separate communities.Neural Memory ModelsThe earliest recent work with amemory component that is applied to language processingis that of memory networks (Weston et al.,2015b) whichadds a memory component for question answering over
Accuracy:	Visual	Question	Answering	Dynamic Memory Networks for Visual and Textual Question AnsweringTask DMN+ E2E NR2: 2 supporting facts 0.3 0.3 -3: 3 supporting facts 1.1 2.1 -5: 3 argument relations 0.5 0.8 -6: yes/no questions 0.0 0.1 -7: counting 2.4 2.0 -8: lists/sets 0.0 0.9 -9: simple negation 0.0 0.3 -11: basic coreference 0.0 0.1 -14: time reasoning 0.2 0.1 -16: basic induction 45.3 51.8 -17: positional reasoning 4.2 18.6 0.918: size reasoning 2.1 5.3 -19: path ﬁnding 0.0 2.3 1.6Mean error (%) 2.8 4.2 -Failed tasks (err>5%) 1 3 -Table 2.Test error rates of various model architectures on tasksfrom the the bAbI English 10k dataset. E2E = End-To-End Mem-ory Network results fromSukhbaatar et al.(2015). NR = Neu-ral Reasoner with original auxiliary task fromPeng et al.(2015).DMN+ and E2E achieve an error of 0 on bAbI question sets(1,4,10,12,13,15,20).state of the art question answering architectures: the end toend memory network (E2E) (Sukhbaatar et al.,2015) andthe neural reasoner framework (NR) (Peng et al.,2015).Neither approach use supporting facts for training.The end-to-end memory network is a form of memory net-work (Weston et al.,2015b) tested on both textual ques-tion answering and language modeling. The model featuresboth explicit memory and a recurrent attention mechanism.We select the model from the paper that achieves the low-est mean error over the bAbI-10k dataset. This model uti-lizes positional encoding for input, RNN-style tied weightsfor the episode module, and a ReLU non-linearity for thememory update component.The neural reasoner framework is an end-to-end trainablemodel which features a deep architecture for logical rea-soning and an interaction-pooling mechanism for allowinginteraction over multiple facts. While the neural reasonerframework was only tested on QA17 and QA19, these weretwo of the most challenging question types at the time.In Table2we compare the accuracy of these question an-swering architectures, both as mean error and error on in-dividual tasks. The DMN+ model reduces mean error by1.4% compared to the the end-to-end memory network,achieving a new state of the art for the bAbI-10k dataset.One notable deﬁciency in our model is that of QA16: Ba-sic Induction. InSukhbaatar et al.(2015), an untied modelusing only summation for memory updates was able toachieve a near perfect error rate of0.4. When the memorytest-dev test-stdMethod All Y/N Other Num AllVQAImage 28.1 64.0 3.8 0.4 -Question 48.1 75.7 27.1 36.7 -Q+I 52.6 75.6 37.4 33.7 -LSTM Q+I 53.7 78.9 36.4 35.2 54.1ACK 55.7 79.2 40.1 36.1 56.0iBOWIMG 55.7 76.5 42.6 35.0 55.9DPPnet 57.2 80.7 41.7 37.2 57.4D-NMN 57.9 80.5 43.1 37.4 58.0SAN 58.7 79.3 46.1 36.6 58.9DMN+60.380.5 48.3 36.860.4Table 3.Performance of various architectures and approaches onVQA test-dev and test-standard data. VQA numbers are fromAntol et al.(2015); ACKWu et al.(2015);iBOWIMG -Zhouet al.(2015);DPPnet -Noh et al.(2015); D-NMN -Andreas et al.(2016); SAN -Yang et al.(2015)update was replaced with a linear layer with ReLU activa-tion, the end-to-end memory network’s overall mean errordecreased but the error for QA16 rose sharply. Our modelexperiences the same difﬁculties, suggesting that the morecomplex memory update component may prevent conver-gence on certain simpler tasks.The neural reasoner model outperforms both the DMN andend-to-end memory network on QA17: Positional Reason-ing. This is likely as the positional reasoning task onlyinvolves minimal supervision - two sentences for input,yes/no answers for supervision, and only 5,812 unique ex-amples after removing duplicates from the initial 10,000training examples.Peng et al.(2015) add an auxiliary taskof reconstructing both the original sentences and questionfrom their representations. This auxiliary task likely im-proves performance by preventing overﬁtting.6.3. Comparison to state of the art using VQAFor the VQA dataset, each question is answered by mul-tiple people and the answers may not be the same, thegenerated answers are evaluated using human consensus.For each predicted answeraifor theithquestion withtarget answer setTi, the accuracy of VQA:AccVQ A=1NPNi=1min(Pt2Ti1(ai==t)3,1)where1(·)is the indica-tor function. Simply put, the answeraiis only 100%accu-rate if at least 3 people provide that exact answer.Training DetailsWe use the Adam optimizer (Kingma &Ba,2014) with a learning rate of 0.003 and batch size of100. Training runs for up to 256 epochs with early stop-ping if the validation loss has not improved in the last 10epochs. For weight initialization, we sampled from a ran-dom uniform distribution with range[ 0.08,0.08]. BothVQA	test-devand	test-standard:•Antolet	al.	(2015)•ACK	Wu	et	al.	(2015);•iBOWIMG-Zhou	et	al.	(2015);•DPPnet-Noh	et	al.	(2015);	D-NMN	-Andreas	et	al.	(2016);	•SAN	-Yang	et	al.	(2015)	
Attention	VisualizationDynamic Memory Networks for Visual and Textual Question Answering
Which man is dressed more ﬂamboyantly ?Answer: right
What time of day was this picture taken ?Answer: night
What is the boy holding ?Answer: surfboard
Who is on both photos ?Answer: girl
What is the main color on the bus ?Answer: blue
How many pink ﬂagsare there ?Answer: 2
What is this sculpture made out of ?Answer: metal
What is the pattern on the cat ' s fur on its tail ?Answer: stripes
What type of trees are in the background ?Answer: pine
Did the player hitthe ball ?Answer: yes
What color are the bananas ?Answer: green
Is this in the wild ?Answer: no
Figure 4.Examples of qualitative results of attention for VQA. Each image (left) is shown with the attention that the episodic memorymodule places on each region (right). Answers are given by the DMN+.Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg,A. C., and Berg, T. L. Baby talk: Understanding andgenerating image descriptions. InCVPR, 2011.Kumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury,J., Gulrajani, I., and Socher, R. Ask Me Anything: Dy-namic Memory Networks for Natural Language Process-ing.arXiv preprint arXiv:1506.07285, 2015.Li, J., Luong, M. T., and Jurafsky, D. A Hierarchical Neu-ral Autoencoder for Paragraphs and Documents.arXivpreprint arXiv:1506.01057, 2015.Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P.,Ramanan, D., Doll´ar, P., and Zitnick, C. L. MicrosoftCOCO: Common Objects in Context. InECCV 2014,2014.Luong, M. T., Pham, H., and Manning, C. D. Effective ap-proaches to attention-based neural machine translation.InEMNLP, 2015.Ma, L., Lu, Z., and Li, H. Learning to Answer Ques-tions From Image Using Convolutional Neural Network.arXiv preprint arXiv:1506.00333, 2015.Malinowski, M. and Fritz, M. A Multi-World Approach toQuestion Answering about Real-World Scenes based onUncertain Input. InNIPS, 2014.Malinowski, M., Rohrbach, M., and Fritz, M. Ask yourneurons: A neural-based approach to answering ques-tions about images. InICCV, 2015.Noh, H., Seo, P. H., and Han, B. Image question answer-ing using convolutional neural network with dynamicparameter prediction.arXiv preprint arXiv:1511.05756,2015.Peng, B., Lu, Z., Li, H., and Wong, K. To-wards neural network-based reasoning.arXiv preprintarXiv:1508.05508, 2015.
Attention	VisualizationDynamic Memory Networks for Visual and Textual Question Answering
Which man is dressed more ﬂamboyantly ?Answer: right
What time of day was this picture taken ?Answer: night
What is the boy holding ?Answer: surfboard
Who is on both photos ?Answer: girl
What is the main color on the bus ?Answer: blue
How many pink ﬂagsare there ?Answer: 2
What is this sculpture made out of ?Answer: metal
What is the pattern on the cat ' s fur on its tail ?Answer: stripes
What type of trees are in the background ?Answer: pine
Did the player hitthe ball ?Answer: yes
What color are the bananas ?Answer: green
Is this in the wild ?Answer: no
Figure 4.Examples of qualitative results of attention for VQA. Each image (left) is shown with the attention that the episodic memorymodule places on each region (right). Answers are given by the DMN+.Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg,A. C., and Berg, T. L. Baby talk: Understanding andgenerating image descriptions. InCVPR, 2011.Kumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury,J., Gulrajani, I., and Socher, R. Ask Me Anything: Dy-namic Memory Networks for Natural Language Process-ing.arXiv preprint arXiv:1506.07285, 2015.Li, J., Luong, M. T., and Jurafsky, D. A Hierarchical Neu-ral Autoencoder for Paragraphs and Documents.arXivpreprint arXiv:1506.01057, 2015.Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P.,Ramanan, D., Doll´ar, P., and Zitnick, C. L. MicrosoftCOCO: Common Objects in Context. InECCV 2014,2014.Luong, M. T., Pham, H., and Manning, C. D. Effective ap-proaches to attention-based neural machine translation.InEMNLP, 2015.Ma, L., Lu, Z., and Li, H. Learning to Answer Ques-tions From Image Using Convolutional Neural Network.arXiv preprint arXiv:1506.00333, 2015.Malinowski, M. and Fritz, M. A Multi-World Approach toQuestion Answering about Real-World Scenes based onUncertain Input. InNIPS, 2014.Malinowski, M., Rohrbach, M., and Fritz, M. Ask yourneurons: A neural-based approach to answering ques-tions about images. InICCV, 2015.Noh, H., Seo, P. H., and Han, B. Image question answer-ing using convolutional neural network with dynamicparameter prediction.arXiv preprint arXiv:1511.05756,2015.Peng, B., Lu, Z., Li, H., and Wong, K. To-wards neural network-based reasoning.arXiv preprintarXiv:1508.05508, 2015.Dynamic Memory Networks for Visual and Textual Question Answering
Which man is dressed more ﬂamboyantly ?Answer: right
What time of day was this picture taken ?Answer: night
What is the boy holding ?Answer: surfboard
Who is on both photos ?Answer: girl
What is the main color on the bus ?Answer: blue
How many pink ﬂagsare there ?Answer: 2
What is this sculpture made out of ?Answer: metal
What is the pattern on the cat ' s fur on its tail ?Answer: stripes
What type of trees are in the background ?Answer: pine
Did the player hitthe ball ?Answer: yes
What color are the bananas ?Answer: green
Is this in the wild ?Answer: no
Figure 4.Examples of qualitative results of attention for VQA. Each image (left) is shown with the attention that the episodic memorymodule places on each region (right). Answers are given by the DMN+.Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg,A. C., and Berg, T. L. Baby talk: Understanding andgenerating image descriptions. InCVPR, 2011.Kumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury,J., Gulrajani, I., and Socher, R. Ask Me Anything: Dy-namic Memory Networks for Natural Language Process-ing.arXiv preprint arXiv:1506.07285, 2015.Li, J., Luong, M. T., and Jurafsky, D. A Hierarchical Neu-ral Autoencoder for Paragraphs and Documents.arXivpreprint arXiv:1506.01057, 2015.Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P.,Ramanan, D., Doll´ar, P., and Zitnick, C. L. MicrosoftCOCO: Common Objects in Context. InECCV 2014,2014.Luong, M. T., Pham, H., and Manning, C. D. Effective ap-proaches to attention-based neural machine translation.InEMNLP, 2015.Ma, L., Lu, Z., and Li, H. Learning to Answer Ques-tions From Image Using Convolutional Neural Network.arXiv preprint arXiv:1506.00333, 2015.Malinowski, M. and Fritz, M. A Multi-World Approach toQuestion Answering about Real-World Scenes based onUncertain Input. InNIPS, 2014.Malinowski, M., Rohrbach, M., and Fritz, M. Ask yourneurons: A neural-based approach to answering ques-tions about images. InICCV, 2015.Noh, H., Seo, P. H., and Han, B. Image question answer-ing using convolutional neural network with dynamicparameter prediction.arXiv preprint arXiv:1511.05756,2015.Peng, B., Lu, Z., Li, H., and Wong, K. To-wards neural network-based reasoning.arXiv preprintarXiv:1508.05508, 2015.
Attention	VisualizationDynamic Memory Networks for Visual and Textual Question Answering
Which man is dressed more ﬂamboyantly ?Answer: right
What time of day was this picture taken ?Answer: night
What is the boy holding ?Answer: surfboard
Who is on both photos ?Answer: girl
What is the main color on the bus ?Answer: blue
How many pink ﬂagsare there ?Answer: 2
What is this sculpture made out of ?Answer: metal
What is the pattern on the cat ' s fur on its tail ?Answer: stripes
What type of trees are in the background ?Answer: pine
Did the player hitthe ball ?Answer: yes
What color are the bananas ?Answer: green
Is this in the wild ?Answer: no
Figure 4.Examples of qualitative results of attention for VQA. Each image (left) is shown with the attention that the episodic memorymodule places on each region (right). Answers are given by the DMN+.Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg,A. C., and Berg, T. L. Baby talk: Understanding andgenerating image descriptions. InCVPR, 2011.Kumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury,J., Gulrajani, I., and Socher, R. Ask Me Anything: Dy-namic Memory Networks for Natural Language Process-ing.arXiv preprint arXiv:1506.07285, 2015.Li, J., Luong, M. T., and Jurafsky, D. A Hierarchical Neu-ral Autoencoder for Paragraphs and Documents.arXivpreprint arXiv:1506.01057, 2015.Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P.,Ramanan, D., Doll´ar, P., and Zitnick, C. L. MicrosoftCOCO: Common Objects in Context. InECCV 2014,2014.Luong, M. T., Pham, H., and Manning, C. D. Effective ap-proaches to attention-based neural machine translation.InEMNLP, 2015.Ma, L., Lu, Z., and Li, H. Learning to Answer Ques-tions From Image Using Convolutional Neural Network.arXiv preprint arXiv:1506.00333, 2015.Malinowski, M. and Fritz, M. A Multi-World Approach toQuestion Answering about Real-World Scenes based onUncertain Input. InNIPS, 2014.Malinowski, M., Rohrbach, M., and Fritz, M. Ask yourneurons: A neural-based approach to answering ques-tions about images. InICCV, 2015.Noh, H., Seo, P. H., and Han, B. Image question answer-ing using convolutional neural network with dynamicparameter prediction.arXiv preprint arXiv:1511.05756,2015.Peng, B., Lu, Z., Li, H., and Wong, K. To-wards neural network-based reasoning.arXiv preprintarXiv:1508.05508, 2015.
Live	Demo

Summary•Most	NLP	tasks	can	be	reduced	to	QA•DMN	accurately	solves	variety	of	QA	tasks•Next	goals:	One	joint	multitask	DMN

The IBM Translation Models
Michael Collins, Columbia University
Recap: The Noisy Channel Model
IGoal: translation system from French to English
IHave a model p(ejf)which estimates conditional probability
of any English sentence egiven the French sentence f. Use
the training corpus to set the parameters.
IA Noisy Channel Model has two components:
p(e)the language model
p(fje)the translation model
IGiving:
p(ejf) =p(e;f)
p(f)=p(e)p(fje)P
ep(e)p(fje)
and
argmaxep(ejf) = argmaxep(e)p(fje)
Roadmap for the Next Few Lectures
IIBM Models 1 and 2
IPhrase-based models
Overview
IIBM Model 1
IIBM Model 2
IEM Training of Models 1 and 2
IBM Model 1: Alignments
IHow do we model p(fje)?
IEnglish sentence ehaslwordse1:::e l,
French sentence fhasmwordsf1:::f m.
IAn alignment aidenties which English word each French
word originated from
IFormally, an alignment aisfa1;:::a mg, where each
aj2f0:::lg.
IThere are (l+ 1)mpossible alignments.
IBM Model 1: Alignments
Ie.g.,l= 6,m= 7
e=And the program has been implemented
f=Le programme a ete mis en application
IOne alignment is
f2;3;4;5;6;6;6g
IAnother (bad!) alignment is
f1;1;1;1;1;1;1g
Alignments in the IBM Models
IWe'll dene models for p(aje;m)andp(fja;e;m ),
giving
p(f;aje;m) =p(aje;m)p(fja;e;m )
IAlso,
p(fje;m) =X
a2Ap(aje;m)p(fja;e;m )
whereAis the set of all possible alignments
A By-Product: Most Likely Alignments
IOnce we have a model p(f;aje;m) =p(aje)p(fja;e;m )
we can also calculate
p(ajf;e;m ) =p(f;aje;m)P
a2Ap(f;aje;m)
for any alignment a
IFor a given f;e pair, we can also compute the most likely
alignment,
a= arg max
ap(ajf;e;m )
INowadays, the original IBM models are rarely (if ever) used
for translation, but they are used for recovering alignments
An Example Alignment
French:
le conseil a rendu son avis , et nous devons  a pr esent adopter un
nouvel avis sur la base de la premi ere position .
English:
the council has stated its position , and now , on the basis of the
rst position , we again have to give our opinion .
Alignment:
the/le council/conseil has/ a stated/rendu its/son position/avis ,/,
and/et now/pr esent ,/NULL on/sur the/le basis/base of/de the/la
rst/premi ere position/position ,/NULL we/nous again/NULL
have/devons to/a give/adopter our/nouvel opinion/avis ./.
IBM Model 1: Alignments
IIn IBM model 1 all allignments aare equally likely:
p(aje;m) =1
(l+ 1)m
IThis is a major simplifying assumption, but it gets things
started...
IBM Model 1: Translation Probabilities
INext step: come up with an estimate for
p(fja;e;m )
IIn model 1, this is:
p(fja;e;m ) =mY
j=1t(fjjeaj)
Ie.g.,l= 6,m= 7
e=And the program has been implemented
f=Le programme a ete mis en application
Ia=f2;3;4;5;6;6;6g
p(fja;e) =t(Lejthe)
t(programmejprogram )
t(ajhas)
t(etejbeen )
t(misjimplemented )
t(enjimplemented )
t(applicationjimplemented )
IBM Model 1: The Generative Process
To generate a French string ffrom an English string e:
IStep 1: Pick an alignment awith probability1
(l+1)m
IStep 2: Pick the French words with probability
p(fja;e;m ) =mY
j=1t(fjjeaj)
The nal result:
p(f;aje;m) =p(aje;m)p(fja;e;m ) =1
(l+ 1)mmY
j=1t(fjjeaj)
An Example Lexical Entry
English French Probability
position position 0.756715
position situation 0.0547918
position mesure 0.0281663
position vue 0.0169303
position point 0.0124795
position attitude 0.0108907
:::de la situation au niveau des n egociations de l ' ompi :::
:::of the current position in the wipo negotiations :::
nous ne sommes pas en mesure de d ecider , :::
we are not in a position to decide , :::
:::le point de vue de la commission face  a ce probl eme complexe .
:::the commission 's position on this complex problem .
Overview
IIBM Model 1
IIBM Model 2
IEM Training of Models 1 and 2
IBM Model 2
IOnly dierence: we now introduce alignment ordistortion
parameters
q(ijj;l;m ) = Probability that j'th French word is connected
toi'th English word, given sentence lengths of
eandfarelandmrespectively
IDene
p(aje;m) =mY
j=1q(ajjj;l;m )
wherea=fa1;:::a mg
IGives
p(f;aje;m) =mY
j=1q(ajjj;l;m )t(fjjeaj)
INote: Model 1 is a special case of Model 2, where
q(ijj;l;m ) =1
l+1for alli;j.
An Example
l= 6
m = 7
e=And the program has been implemented
f=Le programme a ete mis en application
a=f2;3;4;5;6;6;6g
p(aje;7) = q(2j1;6;7)
q(3j2;6;7)
q(4j3;6;7)
q(5j4;6;7)
q(6j5;6;7)
q(6j6;6;7)
q(6j7;6;7)
p(fja;e; 7) = t(Lejthe)
t(programmejprogram )
t(ajhas)
t(etejbeen )
t(misjimplemented )
t(enjimplemented )
t(applicationjimplemented )
An Example
l= 6
m = 7
e=And the program has been implemented
f=Le programme a ete mis en application
a=f2;3;4;5;6;6;6g
p(fja;e; 7) = t(Lejthe)
t(programmejprogram )
t(ajhas)
t(etejbeen )
t(misjimplemented )
t(enjimplemented )
t(applicationjimplemented )
IBM Model 2: The Generative Process
To generate a French string ffrom an English string e:
IStep 1: Pick an alignment a=fa1;a2:::a mgwith
probability
mY
j=1q(ajjj;l;m )
IStep 3: Pick the French words with probability
p(fja;e;m ) =mY
j=1t(fjjeaj)
The nal result:
p(f;aje;m) =p(aje;m)p(fja;e;m ) =mY
j=1q(ajjj;l;m )t(fjjeaj)
Recovering Alignments
IIf we have parameters qandt, we can easily recover the most
likely alignment for any sentence pair
IGiven a sentence pair e1;e2;:::;e l,f1;f2;:::;f m, dene
aj= arg max
a2f0:::lgq(ajj;l;m )t(fjjea)
forj= 1:::m
e=And the program has been implemented
f=Le programme a ete mis en application
Overview
IIBM Model 1
IIBM Model 2
IEM Training of Models 1 and 2
The Parameter Estimation Problem
IInput to the parameter estimation algorithm: (e(k);f(k))for
k= 1:::n . Eache(k)is an English sentence, each f(k)is a
French sentence
IOutput: parameters t(fje)andq(ijj;l;m )
IA key challenge: we do not have alignments on our
training examples , e.g.,
e(100)=And the program has been implemented
f(100)=Le programme a ete mis en application
Parameter Estimation if the Alignments are Observed
IFirst: case where alignments are observed in training data.
E.g.,
e(100)=And the program has been implemented
f(100)=Le programme a ete mis en application
a(100)=h2;3;4;5;6;6;6i
ITraining data is (e(k);f(k);a(k))fork= 1:::n . Eache(k)is
an English sentence, each f(k)is a French sentence, each a(k)
is an alignment
IMaximum-likelihood parameter estimates in this case are
trivial:
tML(fje) =Count(e,f)
Count(e)qML(jji;l;m ) =Count (jji;l;m )
Count (i;l;m )
Input: A training corpus (f(k);e(k);a(k))fork= 1:::n , where
f(k)=f(k)
1:::f(k)
mk,e(k)=e(k)
1:::e(k)
lk,a(k)=a(k)
1:::a(k)
mk.
Algorithm:
ISet all counts c(:::) = 0
IFork= 1:::n
IFori= 1:::m k, Forj= 0:::lk,
c(e(k)
j;f(k)
i) c(e(k)
j;f(k)
i) +(k;i;j )
c(e(k)
j) c(e(k)
j) +(k;i;j )
c(jji;l;m ) c(jji;l;m ) +(k;i;j )
c(i;l;m ) c(i;l;m ) +(k;i;j )
where(k;i;j ) = 1 ifa(k)
i=j,0otherwise.
Output:tML(fje) =c(e;f)
c(e),qML(jji;l;m ) =c(jji;l;m )
c(i;l;m )
Parameter Estimation with the EM Algorithm
ITraining examples are (e(k);f(k))fork= 1:::n . Eache(k)is
an English sentence, each f(k)is a French sentence
IThe algorithm is related to algorithm when alignments are
observed, but two key dierences:
1. The algorithm is iterative . We start with some initial (e.g.,
random) choice for the qandtparameters. At each iteration
we compute some \counts" based on the data together with
our current parameter estimates. We then re-estimate our
parameters with these counts, and iterate.
2. We use the following denition for (k;i;j )at each iteration:
(k;i;j ) =q(jji;lk;mk)t(f(k)
ije(k)
j)
Plk
j=0q(jji;lk;mk)t(f(k)
ije(k)
j)
Input: A training corpus (f(k);e(k))fork= 1:::n , where
f(k)=f(k)
1:::f(k)
mk,e(k)=e(k)
1:::e(k)
lk.
Initialization: Initializet(fje)andq(jji;l;m )parameters (e.g.,
to random values).
Fors= 1:::S
ISet all counts c(:::) = 0
IFork= 1:::n
IFori= 1:::m k, Forj= 0:::lk
c(e(k)
j;f(k)
i) c(e(k)
j;f(k)
i) +(k;i;j )
c(e(k)
j) c(e(k)
j) +(k;i;j )
c(jji;l;m ) c(jji;l;m ) +(k;i;j )
c(i;l;m ) c(i;l;m ) +(k;i;j )
where
(k;i;j ) =q(jji;lk;mk)t(f(k)
ije(k)
j)
Plk
j=0q(jji;lk;mk)t(f(k)
ije(k)
j)
IRecalculate the parameters:
t(fje) =c(e;f)
c(e)q(jji;l;m ) =c(jji;l;m )
c(i;l;m )
The EM Algorithm for IBM Model 1
Fors= 1:::S
ISet all counts c(:::) = 0
IFork= 1:::n
IFori= 1:::m k, Forj= 0:::lk
c(e(k)
j;f(k)
i) c(e(k)
j;f(k)
i) +(k;i;j )
c(e(k)
j) c(e(k)
j) +(k;i;j )
c(jji;l;m ) c(jji;l;m ) +(k;i;j )
c(i;l;m ) c(i;l;m ) +(k;i;j )
where
(k;i;j ) =1
(1+lk)t(f(k)
ije(k)
j)
Plk
j=01
(1+lk)t(f(k)
ije(k)
j)=t(f(k)
ije(k)
j)
Plk
j=0t(f(k)
ije(k)
j)
IRecalculate the parameters: t(fje) =c(e;f)=c(e)
(k;i;j ) =q(jji;lk;mk)t(f(k)
ije(k)
j)
Plk
j=0q(jji;lk;mk)t(f(k)
ije(k)
j)
e(100)=And the program has been implemented
f(100)=Le programme a ete mis en application
Justication for the Algorithm
ITraining examples are (e(k);f(k))fork= 1:::n . Eache(k)is
an English sentence, each f(k)is a French sentence
IThe log-likelihood function:
L(t;q) =nX
k=1logp(f(k)je(k)) =nX
k=1logX
ap(f(k);aje(k))
IThe maximum-likelihood estimates are
arg max
t;qL(t;q)
IThe EM algorithm will converge to a local maximum of the
log-likelihood function
Summary
IKey ideas in the IBM translation models:
IAlignment variables
ITranslation parameters, e.g., t(chienjdog)
IDistortion parameters, e.g., q(2j1;6;7)
IThe EM algorithm: an iterative algorithm for training the q
andtparameters
IOnce the parameters are trained, we can recover the most
likely alignments on our training examples
e=And the program has been implemented
f=Le programme a ete mis en application
Maria -Florina  Balcan  
04/08/2015  PCA, Kernel PCA, ICA  
Learning Representations.  
Dimensionality Reduction.  
•High-Dimensions = Lot of Features  
  
 Document classification  
   Features per document =  
    thousands of words/unigrams  
    millions of bigrams,  contextual  
    information  
 
 Surveys - Netflix  
   480189 users x 17770 movies  
 
  Big & High -Dimensional Data  

•High-Dimensions = Lot of Features  
 
 MEG Brain Imaging  
   120 locations x 500 time points  
    x 20 objects  
 
Big & High -Dimensional Data 
 Or any high -dimensional image data  
 

•Useful to learn lower dimensional 
representations of the data.  •Big & High -Dimensional Data.  
PCA, Kernel PCA, ICA: Powerful unsupervised learning 
techniques for extracting hidden (potentially lower 
dimensional) structure from high dimensional datasets.  Learning Representations  
Useful for : 
•Visualization  
•Further processing by machine learning algorithms  •More efficient use of resources  
   (e.g., time, memory, communication ) 
•Statistical: fewer dimensions  better generalization  
•Noise removal (improving data quality)  
Principal Component Analysis (PCA)  
What is PCA : Unsupervised technique for extracting 
variance structure from high dimensional datasets.  
•PCA  is an orthogonal projection or transformation  of the data 
into a (possibly lower dimensional) subspace so that the variance 
of the projected data is maximized.  

Principal Component Analysis (PCA)  
Both features are relevant  Only one relevant feature  
Question: Can we transform the features so that we only need to 
preserve one latent feature?  Intrinsically lower dimensional than the 
dimension of the ambient space.  If we rotate data , again only one 
coordinate is more important.  
Principal Component Analysis (PCA)  
In case where data  lies on or near a low d -dimensional linear 
subspace, axes of this subspace are an effective representation 
of the data . 
Identifying the axes is known as Principal Components Analysis , and 
can be obtained by using classic matrix computation tools (Eigen or 
Singular Value Decomposition).  
Principal Component Analysis (PCA)  
Principal Components (PC) are orthogonal directions 
that capture most of the variance in the data.  
 
xi v 
v⋅  xi •Projection of data points along first PC 
discriminates data most along any one direction 
(pts are the most spread out when we project the data on 
that direction compared to any other directions ). 
||v||=1 , Point xi (D-dimensional vector)  
Projection of xi onto v is  v⋅  xi 
 •First PC – direction of greatest variability in data . 
Quick reminder:  
Principal Component Analysis (PCA)  
Principal Components (PC) are orthogonal directions 
that capture most of the variance in the data . 
xi−v⋅  xi xi 
v⋅  xi •1st PC – direction of greatest variability in data . 
•2nd PC – Next orthogonal (uncorrelated) direction 
of greatest variability  
(remove all variability in first direction, then find next direction of 
greatest variability)  
•And so on …  
Principal Component Analysis (PCA)  
Let v1, v2, …, vd denote the d principal components.  
Wrap constraints into the  
objective function  vi⋅ vj =0,i≠ j 
Find vector that maximizes sample variance of projected data  and vi⋅ vi =1, i= j 
Let X=[x1,x2,…,xn] (columns are the datapoints ) Assume data is centered (we extracted the sample mean).  
Principal Component Analysis (PCA)  
X XTv=λv , so v (the first PC) is the eigenvector 
of sample correlation/covariance matrix 𝑋 𝑋𝑇 
Sample variance of projection v𝑇𝑋 𝑋𝑇v=𝜆v𝑇v=𝜆 
Thus, the eigenvalue 𝜆 denotes the amount of variability 
captured along that dimension  (aka amount of energy along that 
dimension) . 
Eigenvalues 𝜆1≥𝜆2≥𝜆3≥⋯ 
•The 1st PC 𝑣1 is the the eigenvector of the sample covariance matrix 
𝑋 𝑋𝑇 associated with the largest eigenvalue  
•The 2nd PC 𝑣2 is the the eigenvector of the sample covariance 
matrix 𝑋 𝑋𝑇 associated with the second largest eigenvalue  
•And so on …  
x1 x2  
•Transformed features are uncorrelated.  •So, the new axes are the eigenvectors of the matrix of sample 
correlations 𝑋 𝑋𝑇 of the data . 
•Geometrically: centering followed by rotation.  Principal Component Analysis (PCA)  
–Linear transformation  
Key computation : eigendecomposition  of 𝑋𝑋𝑇 (closely related 
to SVD of 𝑋). 
Two Interpretations  
So far: Maximum Variance Subspace. PCA finds vectors v such that 
projections on to the  vectors capture maximum variance in the data  
Alternative viewpoint:   Minimum Reconstruction Error.  PCA 
finds vectors v such that projection on to the vectors yields 
minimum MSE reconstruction  
xi v 
v⋅  xi 
Two Interpretations  
Maximum Variance Direction:  1st PC a vector v such that projection 
on to this vector capture maximum variance in the data (out of all 
possible one dimensional projections)  
Minimum Reconstruction Error:  1st PC a vector v such that 
projection on to this vector yields minimum MSE reconstruction  
xi v 
v⋅  xi E.g., for the first component.  
Why? Pythagorean Theorem  
xi v 
v⋅  xi blue2 + green2 = black2 
black2 is fixed (it’s just the data)  
So, maximizing blue2 is 
equivalent to minimizing green2  Maximum Variance Direction:  1st PC a vector v such that projection 
on to this vector capture maximum variance in the data (out of all 
possible one dimensional projections)  
Minimum Reconstruction Error:  1st PC a vector v such that 
projection on to this vector yields minimum MSE reconstruction  
E.g., for the first component.  
Dimensionality Reduction using PCA  
xi v 
vTxi The eigenvalue 𝜆 denotes the amount of variability captured along 
that dimension  (aka amount of energy along that dimension) . 
Zero eigenvalues indicate no variability along those directions => 
data lies exactly on a linear subspace  
 
Only keep data projections onto principal components with 
non-zero eigenvalues, say v1,…,vk, where k=rank(𝑋 𝑋𝑇) 
 
Original representation   
Transformed representation  
 
Data point  
𝑥𝑖=(𝑥𝑖1,…,𝑥𝑖𝐷)  
projection  
(𝑣1⋅𝑥𝑖,…,𝑣𝑑⋅𝑥𝑖) 
 
D-dimensional vector   
d-dimensional vector  
Dimensionality Reduction using PCA  
In high -dimensional problems, data sometimes lies near a linear 
subspace, as noise introduces small variability  
 
Only keep data projections onto principal components with large  
eigenvalues   
0510152025
PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10Variance (%)
Can ignore the components of smaller significance .   
Might lose some info , but if eigenvalues are small, do not lose much  
  
 
Can represent a face image using just 15 numbers!  
•PCA provably useful before doing k -means clustering and also 
empirically useful. E.g.,  
PCA Discussion  
Strengths  
21 Eigenvector method  
No tuning of the parameters  
No local optima  
Weaknesses  
Limited to second order statistics  
Limited to linear projections  
Kernel PCA ( Kernel Principal 
Component Analysis)  
Useful when data  lies on or near a low d -
dimensional linear subspace of the 𝜙-
space associated with a kernel  
Properties of PCA  
•Given a set of 𝑛 centered observations 
𝑥𝑖∈𝑅𝐷,  1st PC is the direction that 
maximizes the variance  
–𝐶𝑣1=𝜆𝑣1(of maximum 𝜆) –𝑋=𝑥1,𝑥2,…,𝑥𝑛 
–𝑣1=𝑎𝑟𝑔𝑚𝑎𝑥𝑣=11
𝑛 𝑣⊤𝑥𝑖2
𝑖  
=𝑎𝑟𝑔𝑚𝑎𝑥𝑣=11
𝑛𝑣⊤𝑋𝑋⊤𝑣 
•Covariance matrix 𝐶=1
𝑛𝑋𝑋⊤ 
•𝑣1 can be found by solving the 
eigenvalue problem:  
Properties of PCA  
•Covariance matrix 𝐶=1
𝑛𝑋𝑋⊤is a DxD matrix  
the ( i,j) entry of 𝑋𝑋⊤ is the correlation of the i-th coordinate 
ofexamples  with jth coordinate of examples  
•To use kernels, need to use the inner -product matrix 𝑋𝑇𝑋. •Covariance matrix 𝐶=1
𝑛𝑋𝑋⊤ •Given a set of 𝑛 centered observations 
𝑥𝑖∈𝑅𝐷,  1st PC is the direction that 
maximizes the variance  
–𝑋=𝑥1,𝑥2,…,𝑥𝑛 
–𝑣1=𝑎𝑟𝑔𝑚𝑎𝑥𝑣=11
𝑛 𝑣⊤𝑥𝑖2
𝑖  
=𝑎𝑟𝑔𝑚𝑎𝑥𝑣=11
𝑛𝑣⊤𝑋𝑋⊤𝑣 
Alternative expression for PCA   
•The principal component lies in the span of the data  
𝑣1= 𝛼𝑘𝑥𝑖
𝑖=𝑋𝛼 
Why? 1st PC is direction of largest variance, and for any direction outside of 
the span of the data, only get more variance if we project that direction into 
the span.  
Only depends on 
the inner product     
matrix  •Plug this in we have  
𝐶𝑣1=1
𝑛𝑋𝑋⊤𝑋𝛼=𝜆 𝑋𝛼  
•Now, left -multiply the LHS and RHS by 𝑋𝑇. 
1
𝑛𝑋⊤𝑋𝑋⊤𝑋𝛼=𝜆𝑋⊤𝑋𝛼 
 
 
Kernel PCA  
•Key Idea: Replace inner product matrix by kernel matrix  
PCA: 1
𝑛𝑋⊤𝑋𝑋⊤𝑋𝛼=𝜆𝑋⊤𝑋𝛼 
Let 𝐾=𝐾𝑥𝑖,𝑥𝑗
𝑖𝑗 be the matrix of all dot -products 
in the 𝜙-space.  
Kernel PCA: replace “ 𝑋𝑇𝑋” with 𝐾. 
•Key computation: form an 𝑛 by 𝑛  kernel matrix 𝐾, and 
then perform eigen -decomposition on 𝐾.  1
𝑛𝐾𝐾𝛼=𝜆𝐾𝛼, or equivalently,  1
𝑛𝐾𝛼=𝜆 𝛼 
Kernel PCA Example  
27 
•Gaussian RBF kernel exp−𝑥−𝑥′2
2𝜎2 over 2 dimensional space  
•Eigenvector evaluated at a test point 𝑥 is a function 
𝑤⊤𝜙𝑥= 𝛼𝑖<𝜙𝑥𝑖,𝜙𝑥> =𝑖  𝛼𝑖𝑘(𝑥𝑖,𝑥)𝑖  
What You Should Know  
•Principal Component Analysis  (PCA)  
•Kernel PCA  •What PCA is, what is useful for.  
•Both the maximum variance subspace and the 
minimum reconstruction error viewpoint.  
Additional material on computing the principal 
components and ICA  
Power method for computing PCs  
Given matrix 𝑋∈𝑅𝐷×𝑛, compute the top eigenvector of 𝑋 𝑋𝑇  
Initialize with random   𝑣 ∈ 𝑅𝐷 
Repeat  
v ←X XTv  
v ←v /||v || 
Claim 
Then can subtract the 𝑣  component off of each example and 
repeat to get the next.  For any 𝜖>0, whp over choice of initial vector, after 𝑂1
𝜖log𝑑
𝜖 
iterations, we have 𝑣 𝑇𝑋𝑋𝑇𝑣 ≥1−𝜖𝜆1. 
Eigendecomposition  
Any symmetric matrix 𝐴=𝑋𝑋𝑇 is guaranteed to have an 
eigendecomposition  with real eigenvalues: 𝐴=𝑉 Λ 𝑉𝑇. 
A 
(DxD) = 
V 
(DxD) Λ 
(DxD) 𝜆1 
𝜆2 
𝜆3 … 0 
0 
𝑉𝑇 
(DxD) = 𝜆𝑖𝑣𝑖𝑣𝑖𝑇
𝑖   
Matrix Λ is diagonal with eigenvalues 𝜆1≥𝜆2≥⋯ on the 
diagonal.  Matrix V has the eigenvectors as the columns.  
Singular Value Decomposition (SVD)  
•𝑆 is a diagonal matrix with the singular values 𝜎1,…,𝜎𝑑 of 𝑋. 
•Columns of 𝑈, 𝑉 are orthogonal, unit length.  
So, 𝜆𝑖=𝜎𝑖2 and can read off the solution from the SVD.  Given a matrix 𝑋∈𝑅𝐷×𝑛, the SVD is a decomposition: 𝑋𝑇=𝑈𝑆𝑉𝑇 Eigendecomp of 𝑋𝑋𝑇 is closely related to SVD of 𝑋. 
𝑋𝑇 
(𝑛×𝐷) = 
𝑈 
(𝑛×𝑑) 𝑆 
(𝑑×𝑑) 𝜎1 
𝜎2 
… 0 
0 
𝑉𝑇 
(𝑑×𝐷) = 𝜎𝑖𝑢𝑖𝑣𝑖𝑇
𝑖   
•So, 𝑋𝑋𝑇=𝑉𝑆𝑈𝑇𝑈𝑆𝑉𝑇=𝑉𝑆2𝑉𝑇= eigendecomposition  of 𝑋𝑋𝑇. 
Singular Value Decomposition (SVD)  
So, 𝜆𝑖=𝜎𝑖2 and can read off the solution from the SVD.  Given a matrix 𝑋∈𝑅𝐷×𝑛, the SVD is a decomposition: 𝑋𝑇=𝑈𝑆𝑉𝑇 Eigendecomp of 𝑋𝑋𝑇 is closely related to SVD of 𝑋. 
𝑋𝑇 
(𝑛×𝐷) = 
𝑈 
(𝑛×𝑑) 𝑆 
(𝑑×𝑑) 𝜎1 
𝜎2 
… 0 
0 
𝑉𝑇 
(𝑑×𝐷) = 𝜎𝑖𝑢𝑖𝑣𝑖𝑇
𝑖   
•In fact, can view the rows of 𝑈𝑆 as the coordinates of 
each example along the axes given by the 𝑑 eigenvectors.  
Independent Component Analysis (ICA)  
𝑝𝑠1,𝑠2,…,𝑠𝐷=𝑝1𝑠1𝑝2𝑠2…𝑝𝑛𝑠𝐷 𝒙=𝑉∙𝒔 Find a linear transformation  
for which coefficients 𝒔=𝑠1,𝑠2,…,𝑠𝐷𝑇 are 
statistically independent  
Algorithmically, we need to identify matrix  V and coefficients s, 
s.t. under the condition 𝒙=𝑉𝑇∙𝒔 the mutual information 
between 𝑠1,𝑠2,…,𝑠𝐷 is minimized:  
𝐼𝑠1,𝑠2,…,𝑠𝐷= 𝐻𝑠𝑖 −𝐻𝑠1,𝑠2,…,𝑠𝐷𝐷
𝑖=1 
PCA finds directions of maximum variation,  
ICA would find directions most “aligned” with data. 
Bayesian  Networks(Part  I)
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  22April  10,  2017Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Graphical  Model  Readings:Murphy  10  –10.2.1Bishop  8.1,  8.2.2HTF  -­‐-­‐Mitchell  6.11
Reminders•Peer  Tutoring•Homework7:  Deep  Learning–Release:  Wed,  Apr.  05  –PartI  dueWed,  Apr.  12–PartII  dueMon,  Apr.  17
2
Start  Early
CONVOLUTIONAL  NEURAL  NETS
3
Deep  Learning  Outline•Background:  Computer  Vision–Image  Classification–ILSVRC  2010  -­‐2016–Traditional  Feature  Extraction  Methods–Convolution  as  Feature  Extraction•Convolutional  Neural  Networks  (CNNs)–Learning  Feature  Abstractions–Common  CNN  Layers:•Convolutional  Layer•Max-­‐Pooling  Layer•Fully-­‐connected  Layer  (w/tensor  input)•SoftmaxLayer•ReLULayer–Background:  Subgradient–Architecture:  LeNet–Architecture:  AlexNet•Training  a  CNN–SGD  for  CNNs–Backpropagation  for  CNNs4
Convolutional  Neural  Network  (CNN)•Typical  layers  include:–Convolutional  layer–Max-­‐pooling  layer–Fully-­‐connected  (Linear)  layer–ReLUlayer  (or  some  other  nonlinear  activation  function)–Softmax•These  can  be  arranged  into  arbitrarily  deep  topologies
5
Architecture  #1:  LeNet-­‐5
Convolutional  Layer
60000000011111001001000101000011000001000000000000.4.5.5.5.4.4.2.3.6.3.5.4.4.2.1.5.6.2.10.4.3.100θ11θ12θ13θ21θ22θ23θ31θ32θ33LearnedConvolutionInputImageConvolved  Image
CNN  key  idea:  Treat  convolution  matrix  as  parameters  and  learn  them!

Downsamplingby  Averaging•Downsamplingby  averaging  used  to  bea  common  approach•This  is  a  special  case  of  convolution  where  the  weights  are  fixed  to  a  uniform  distribution•The  example  below  uses  a  stride  of  2
7ConvolutionInputImageConvolved  Image1111101001001010001100001000000000003/43/41/43/41/401/4001/41/41/41/4
Max-­‐Pooling•Max-­‐pooling  is  another  (common)  form  of  downsampling•Instead  of  averaging,  we  take  the  max  value  within  the  same  range  as  the  equivalently-­‐sized  convolution•The  example  below  uses  a  stride  of  2
8Max-­‐poolingInputImageMax-­‐Pooled  Image111110100100101000110000100000000000111110100xi,jxi,j+1xi+1,jxi+1,j+1

Multi-­‐Class  Output
10
……
Output
InputHidden  Layer
…

Multi-­‐Class  Output
11
……
Output
InputHidden  Layer
…
yk=2tT(bk) Kl=12tT(bl)SoftmaxLayer:(F)LossJ= Kk=1y kHQ;(yk)(E)Output(softmax)yk=2tT(bk) Kl=12tT(bl)(D)Output(linear)bk= Dj=0 kjzj k(C)Hidden(nonlinear)zj= (aj), j(B)Hidden(linear)aj= Mi=0 jixi, j(A)InputGivenxi, i
Training  a  CNNWhiteboard–SGD  for  CNNs–Backpropagation  for  CNNs
12
Common  CNN  LayersWhiteboard–ReLULayer–Background:  Subgradient–Fully-­‐connected  Layer  (w/tensor  input)–SoftmaxLayer–Convolutional  Layer–Max-­‐Pooling  Layer
13
Convolutional  Layer
14
Convolutional  Layer
15
Max-­‐Pooling  Layer
16
Max-­‐Pooling  Layer
17
Convolutional  Neural  Network  (CNN)•Typical  layers  include:–Convolutional  layer–Max-­‐pooling  layer–Fully-­‐connected  (Linear)  layer–ReLUlayer  (or  some  other  nonlinear  activation  function)–Softmax•These  can  be  arranged  into  arbitrarily  deep  topologies
18
Architecture  #1:  LeNet-­‐5
Architecture  #2:  AlexNet
19Figure 2:An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilitiesbetween the two GPUs. One GPU runs the layer-parts at the top of the ﬁgure while the other runs the layer-partsat the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, andthe number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264–4096–4096–1000.neurons in a kernel map). The second convolutional layer takes as input the (response-normalizedand pooled) output of the ﬁrst convolutional layer and ﬁlters it with 256 kernels of size5⇥5⇥48.The third, fourth, and ﬁfth convolutional layers are connected to one another without any interveningpooling or normalization layers. The third convolutional layer has 384 kernels of size3⇥3⇥256connected to the (normalized, pooled) outputs of the second convolutional layer. The fourthconvolutional layer has 384 kernels of size3⇥3⇥192, and the ﬁfth convolutional layer has 256kernels of size3⇥3⇥192. The fully-connected layers have 4096 neurons each.4 Reducing OverﬁttingOur neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRCmake each training example impose 10 bits of constraint on the mapping from image to label, thisturns out to be insufﬁcient to learn so many parameters without considerable overﬁtting. Below, wedescribe the two primary ways in which we combat overﬁtting.4.1 Data AugmentationThe easiest and most common method to reduce overﬁtting on image data is to artiﬁcially enlargethe dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct formsof data augmentation, both of which allow transformed images to be produced from the originalimages with very little computation, so the transformed images do not need to be stored on disk.In our implementation, the transformed images are generated in Python code on the CPU while theGPU is training on the previous batch of images. So these data augmentation schemes are, in effect,computationally free.The ﬁrst form of data augmentation consists of generating image translations and horizontal reﬂec-tions. We do this by extracting random224⇥224patches (and their horizontal reﬂections) from the256⇥256images and training our network on these extracted patches4. This increases the size of ourtraining set by a factor of 2048, though the resulting training examples are, of course, highly inter-dependent. Without this scheme, our network suffers from substantial overﬁtting, which would haveforced us to use much smaller networks. At test time, the network makes a prediction by extractingﬁve224⇥224patches (the four corner patches and the center patch) as well as their horizontalreﬂections (hence ten patches in all), and averaging the predictions made by the network’s softmaxlayer on the ten patches.The second form of data augmentation consists of altering the intensities of the RGB channels intraining images. Speciﬁcally, we perform PCA on the set of RGB pixel values throughout theImageNet training set. To each training image, we add multiples of the found principal components,4This is the reason why the input images in Figure 2 are224⇥224⇥3-dimensional.5CNN  for  Image  Classification(Krizhevsky,  Sutskever&  Hinton,  2012)15.3%  error  on  ImageNetLSVRC-­‐2012  contestInput  image  (pixels)•Five  convolutional  layers  (w/max-­‐pooling)•Three  fully  connected  layers1000-­‐way  softmax
CNNs  for  Image  Recognition
20Lecture 7 -27 Jan 2016Fei-Fei Li & Andrej Karpathy & Justin JohnsonFei-Fei Li & Andrej Karpathy & Justin JohnsonLecture 7 -27 Jan 201678
(slide from Kaiming He’s recent presentation)Slide  from  KaimingHe
Mini-­‐Batch  SGD•Gradient  Descent:  Compute  true  gradient  exactly  from  all  N  examples•Mini-­‐Batch  SGD:  Approximate  true  gradient  by  the  average  gradient  of  K  randomly  chosen  examples•Stochastic  Gradient  Descent  (SGD):Approximate  true  gradient  by  the  gradient  of  one  randomly  chosen  example21
Mini-­‐Batch  SGD
22
Three  variants  of  first-­‐order  optimization:
CNN  VISUALIZATIONS
23
3D  Visualization  of  CNNhttp://scs.ryerson.ca/~aharley/vis/conv/

Convolution  of  a  Color  Image
25Lecture 7 -27 Jan 2016Fei-Fei Li & Andrej Karpathy & Justin JohnsonFei-Fei Li & Andrej Karpathy & Justin JohnsonLecture 7 -27 Jan 201623A closer look at spatial dimensions:
3232
332x32x3 image5x5x3 filterconvolve (slide) over all spatial locationsactivation map
12828
Figure  from  Fei-­‐FeiLi  &  Andrej  Karpathy&  Justin  Johnson  (CS231N)  •Color  images  consist  of  3  floats  per  pixel  for  RGB  (red,  green  blue)  color  values•Convolution  must  also  be  3-­‐dimensional
Animation  of  3D  Convolution
26Figure  from  Fei-­‐FeiLi  &  Andrej  Karpathy&  Justin  Johnson  (CS231N)  http://cs231n.github.io/convolutional-­‐networks/

MNIST  Digit  Recognition  with  CNNs  (in  your  browser)
27https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html
Figure  from  Andrej  Karpathy
CNN  SummaryCNNs–Are  used  for  all  aspects  of  computer  vision,  and  have  won  numerous  pattern  recognition  competitions–Able  learn  interpretable  features  at  different  levels  of  abstraction–Typically,  consist  of  convolutionlayers,  poolinglayers,  nonlinearities,  and  fully  connected  layersOther  Resources:–Readings  on  course  website–Andrej  Karpathy,  CS231n  Noteshttp://cs231n.github.io/convolutional-­‐networks/28
BAYESIAN  NETWORKS
29
Bayes  Nets  Outline•Motivation–Structured  Prediction•Background–Conditional  Independence–Chain  Rule  of  Probability•Directed  Graphical  Models–Writing  Joint  Distributions–Definition:  Bayesian  Network–Qualitative  Specification–Quantitative  Specification–Familiar  Models  as  Bayes  Nets•Conditional  Independence  in  Bayes  Nets–Three  case  studies–D-­‐separation–Markov  blanket•Learning–Fully  Observed  Bayes  Net–(Partially  Observed  Bayes  Net)•Inference–Sampling  directly  from  the  joint  distribution–Gibbs  Sampling31
MOTIVATION:  STRUCTURED  PREDICTION32
Structured  Prediction•Most  of  the  models  we’ve  seen  so  far  were  for  classification–Given  observations:x= (x1, x2, …, xK) –Predict  a  (binary)  label:y•Many  real-­‐world  problems  require  structured  prediction–Given  observations:  x= (x1, x2, …, xK) –Predict  a  structure:y= (y1, y2, …, yJ) •Some  classificationproblems  benefit  from  latentstructure33
Structured  Prediction  Examples•Examples  of  structured  prediction–Part-­‐of-­‐speech  (POS)  tagging–Handwriting  recognition–Speech  recognition–Word  alignment–Congressional  voting•Examples  of  latent  structure–Object  recognition34
nnvdnSample  2:timelikefliesanarrowDataset  for  Supervised  Part-­‐of-­‐Speech  (POS)  Tagging
35nvpdnSample  1:timelikefliesanarrow
pnnvvSample  4:withyoutimewillseenvpnnSample  3:flieswithflytheirwingsD={x(n),y(n)}Nn=1Data:
y(1)x(1)
y(2)x(2)
y(3)x(3)
y(4)x(4)
Dataset  for  Supervised  Handwriting  Recognition
36D={x(n),y(n)}Nn=1Data:values. The obtained results are depicted in Table 4; weprovide means, standard deviations, and the p-metric valueof the Student’s-ttest run on the pairs of performances ofthe models (CRF,CRF1), (moderate order CRF,CRF1),and (HMM,CRF1).As we observe, the proposed approach offers a sig-nificant improvement over first-order linear-chain CRFs, aswell as the rest of the considered alternatives. Therefore, weonce again notice the practical significance of coming upwith computationally efficient ways of relaxing the Marko-vian assumption in linear-chain CRF models applied tosequential data modeling. Note also that, in this experi-ment, the moderate order CRF models of [41] seem to yielda rather competitive result. This was expectable since theaverage modeled sequence in this experiment is less than10 time points long. Finally, regarding the HMM method,with the number of mixture componentsMselected so as tooptimize model performance, we observe that theCRF1model yields a clear improvement, irrespective of theemployed likelihood optimization approach.4.3 Part-of-Speech TaggingFinally, here we consider an experiment with the PennTreebank corpus [25], containing 74,029 sentences with atotal of 1,637,267 words. It is comprised of 49,115 uniquewords, and each word in the corpus is labeled according toits part of speech; there are a total of 43 different part-of-speech labels. We use four types of features:1.First-order word-presence features.2.Four-character prefix presence features.10 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 35, NO. 12, XXXXXXX 2013
TABLE 3Activity-Based Segmentation of Skateboard: Push and TurnVideos: Error Rates Obtained by the Evaluated Methods
Fig. 4. Skateboard: push and turn: A few example frames from a sequence considered in our experiments.
Fig. 5. Handwriting recognition: Example words from the dataset used.TABLE 4Handwriting Recognition: Error Rates Obtainedby the Evaluated Methods
values. The obtained results are depicted in Table 4; weprovide means, standard deviations, and the p-metric valueof the Student’s-ttest run on the pairs of performances ofthe models (CRF,CRF1), (moderate order CRF,CRF1),and (HMM,CRF1).As we observe, the proposed approach offers a sig-nificant improvement over first-order linear-chain CRFs, aswell as the rest of the considered alternatives. Therefore, weonce again notice the practical significance of coming upwith computationally efficient ways of relaxing the Marko-vian assumption in linear-chain CRF models applied tosequential data modeling. Note also that, in this experi-ment, the moderate order CRF models of [41] seem to yielda rather competitive result. This was expectable since theaverage modeled sequence in this experiment is less than10 time points long. Finally, regarding the HMM method,with the number of mixture componentsMselected so as tooptimize model performance, we observe that theCRF1model yields a clear improvement, irrespective of theemployed likelihood optimization approach.4.3 Part-of-Speech TaggingFinally, here we consider an experiment with the PennTreebank corpus [25], containing 74,029 sentences with atotal of 1,637,267 words. It is comprised of 49,115 uniquewords, and each word in the corpus is labeled according toits part of speech; there are a total of 43 different part-of-speech labels. We use four types of features:1.First-order word-presence features.2.Four-character prefix presence features.10 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 35, NO. 12, XXXXXXX 2013
TABLE 3Activity-Based Segmentation of Skateboard: Push and TurnVideos: Error Rates Obtained by the Evaluated Methods
Fig. 4. Skateboard: push and turn: A few example frames from a sequence considered in our experiments.
Fig. 5. Handwriting recognition: Example words from the dataset used.TABLE 4Handwriting Recognition: Error Rates Obtainedby the Evaluated Methods
values. The obtained results are depicted in Table 4; weprovide means, standard deviations, and the p-metric valueof the Student’s-ttest run on the pairs of performances ofthe models (CRF,CRF1), (moderate order CRF,CRF1),and (HMM,CRF1).As we observe, the proposed approach offers a sig-nificant improvement over first-order linear-chain CRFs, aswell as the rest of the considered alternatives. Therefore, weonce again notice the practical significance of coming upwith computationally efficient ways of relaxing the Marko-vian assumption in linear-chain CRF models applied tosequential data modeling. Note also that, in this experi-ment, the moderate order CRF models of [41] seem to yielda rather competitive result. This was expectable since theaverage modeled sequence in this experiment is less than10 time points long. Finally, regarding the HMM method,with the number of mixture componentsMselected so as tooptimize model performance, we observe that theCRF1model yields a clear improvement, irrespective of theemployed likelihood optimization approach.4.3 Part-of-Speech TaggingFinally, here we consider an experiment with the PennTreebank corpus [25], containing 74,029 sentences with atotal of 1,637,267 words. It is comprised of 49,115 uniquewords, and each word in the corpus is labeled according toits part of speech; there are a total of 43 different part-of-speech labels. We use four types of features:1.First-order word-presence features.2.Four-character prefix presence features.10 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 35, NO. 12, XXXXXXX 2013
TABLE 3Activity-Based Segmentation of Skateboard: Push and TurnVideos: Error Rates Obtained by the Evaluated Methods
Fig. 4. Skateboard: push and turn: A few example frames from a sequence considered in our experiments.
Fig. 5. Handwriting recognition: Example words from the dataset used.TABLE 4Handwriting Recognition: Error Rates Obtainedby the Evaluated Methods
Figures  from  (Chatzis&  Demiris,  2013)uepctSample  1:
y(1)x(1)nxedevlaicSample  2:ocnebaesSample  2:mrc
y(2)x(2)
y(3)x(3)
Dataset  for  Supervised  Phoneme  (Speech)  Recognition
37D={x(n),y(n)}Nn=1Data:
Figures  from  (Jansen  &  Niyogi,  2013)h#ihwziySample  1:
y(1)x(1)dhsuhiyz1704IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 61, NO. 7, APRIL 1, 2013
Fig. 5. Extrinsic (top) and intrinsic (bottom) spectral representations for the utterance “This was easy for us.” Note that a nonlinear mel-scale frequency warpingwas used.whereare the input unlabeled data andis thenew parametrization of the function we need to estimate. Toproceed, we plug the functional formo f( 9 )i n t ot h eo p t i m i z a t i o nproblem of (8). Taking the gradient with respect to the parametervectorand setting it to zero sets up the following generalizedeigenvalue problem:(10)Here,is theGram matrix deﬁned on the input unlabeleddata by.T h i se i g e n v a l u ed e c o m p o s i t i o nw i l lproduce a full spectrum of eigenvectors, each deﬁning its ownintrinsic projection mapdeﬁned by theth eigenvector.Unlike the unsupervised learning algorithm of [5], we are nowinterested in several of the,n o tj u s to n ef o rb i n a r yc l a s -siﬁcation or clustering. Recall thatt h ei n t r i n s i cb a s i sf u n c t i o n sproduced by the Laplacian eigenmaps algorithm were deﬁnedonly on the points used to construct the graph Laplacian. Ournew set of projection maps is now deﬁned out-of-sample, i.e.,may be computed for arbitrary points on the manifold andmay also be used more generally for any point in.B. Intrinsic Spectrogram AlgorithmGiven the nomenclature deﬁne above, the algorithm for com-puting the intrinsic spectrogram is comprised of three steps:1) Given a set of unlabeled datasampled fromthe manifold, construct anearest neighbor graph andcompute the graph Laplacian(either normalized or un-normalized).2) Given a kernel,s o l v et h eg e n e r a l i z e de i g e n v a l u eproblem of (10) for the weights.3) Project amplitude spectrum at each time point of the ex-trinsic spectrogram onto theﬁrstintrinsic basis functions(sorted by increasing eigenvalue) according to (9).Note that steps 1 and 2 are computed ofﬂine using the standardtraining set.T h u s ,c o n v e r t i n gt h ee x t r i n s i cs p e c t r o g r a mo fanovel utterance into this intrinsic representation requires onlythe computation of Equation (9) across the utterance.Fig. 5 shows an example extrinsicand intrinsicspectrograms for the TIMIT utterance “This was easyfor us” (TIMIT sentence sx3). Here, we constructed the datasetwith 200 examples of each of the4 8p h o n e t i cc a t e g o r i e ss p e c -iﬁed in [26].2Each example was extrinsically represented bya4 0 - d i m e n s i o n a l ,h o m o m o r p h i c a l l ys m o o t h e d ,a u d i t o r y( l o g )spectrum (40 mel scale bands, from 0–8 kHz) computed froma2 5m ss i g n a lw i n d o wc e n t e r e di ne a c hp h o n e t i cs e g m e n t .T h eadjacency graph was constructed usingnearest Euclideanneighbors and binary-valued edge weights. For the optimiza-tion problem of (8), we take as the intrinsic smoothness param-eter.F i n a l l y ,t oa c c o m m o d a t en o n l i n e a ri n t r i n s i cp r o j e c -tions maps, we employ the radialbasis function (RBF) kernel,,w h e r eis taken to be 1/3 of the meanEuclidean distance between the graph vertices. Note that op-timal settings of,a n ddepend on the intended applicationand manifold sampling density; wei n v e s t i g a t et h er o l et h i sp a -rameter in the experiments described below. Given the low-di-mensional curved manifold structure motivated in previous sec-tions, one might expect phonetic content to be more transpar-ently differentiated in the intrinsic basis than in a traditionalspectrogram. It is clear from Fig. 5 that the intrinsic represen-tation redistributes much of the spectral variation to the lowereigenvalued components. It is also clear that these initial com-ponents do not each covary with the presence of a single speechsound. In the next section, we examine whether this alternativeorganization may have a natural linguistic interpretation.V. INTRINSICSPECTRALANALYSISINTERPRETATIONThei n t r i n s i cr e p r e s e n t a t i o ni sap r o j e c t i o no fs p e c t r a li n f o r -mation onto a set of basis functions ordered by their smooth-2Note that while we use a class balanced sample here, balancing was notrequired to obtain good performance in the experiments in Section VII in whichwe randomly selected examples from the entire corpus (ignoring class).1704IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 61, NO. 7, APRIL 1, 2013
Fig. 5. Extrinsic (top) and intrinsic (bottom) spectral representations for the utterance “This was easy for us.” Note that a nonlinear mel-scale frequency warpingwas used.whereare the input unlabeled data andis thenew parametrization of the function we need to estimate. Toproceed, we plug the functional formo f( 9 )i n t ot h eo p t i m i z a t i o nproblem of (8). Taking the gradient with respect to the parametervectorand setting it to zero sets up the following generalizedeigenvalue problem:(10)Here,is theGram matrix deﬁned on the input unlabeleddata by.T h i se i g e n v a l u ed e c o m p o s i t i o nw i l lproduce a full spectrum of eigenvectors, each deﬁning its ownintrinsic projection mapdeﬁned by theth eigenvector.Unlike the unsupervised learning algorithm of [5], we are nowinterested in several of the,n o tj u s to n ef o rb i n a r yc l a s -siﬁcation or clustering. Recall thatt h ei n t r i n s i cb a s i sf u n c t i o n sproduced by the Laplacian eigenmaps algorithm were deﬁnedonly on the points used to construct the graph Laplacian. Ournew set of projection maps is now deﬁned out-of-sample, i.e.,may be computed for arbitrary points on the manifold andmay also be used more generally for any point in.B. Intrinsic Spectrogram AlgorithmGiven the nomenclature deﬁne above, the algorithm for com-puting the intrinsic spectrogram is comprised of three steps:1) Given a set of unlabeled datasampled fromthe manifold, construct anearest neighbor graph andcompute the graph Laplacian(either normalized or un-normalized).2) Given a kernel,s o l v et h eg e n e r a l i z e de i g e n v a l u eproblem of (10) for the weights.3) Project amplitude spectrum at each time point of the ex-trinsic spectrogram onto theﬁrstintrinsic basis functions(sorted by increasing eigenvalue) according to (9).Note that steps 1 and 2 are computed ofﬂine using the standardtraining set.T h u s ,c o n v e r t i n gt h ee x t r i n s i cs p e c t r o g r a mo fanovel utterance into this intrinsic representation requires onlythe computation of Equation (9) across the utterance.Fig. 5 shows an example extrinsicand intrinsicspectrograms for the TIMIT utterance “This was easyfor us” (TIMIT sentence sx3). Here, we constructed the datasetwith 200 examples of each of the4 8p h o n e t i cc a t e g o r i e ss p e c -iﬁed in [26].2Each example was extrinsically represented bya4 0 - d i m e n s i o n a l ,h o m o m o r p h i c a l l ys m o o t h e d ,a u d i t o r y( l o g )spectrum (40 mel scale bands, from 0–8 kHz) computed froma2 5m ss i g n a lw i n d o wc e n t e r e di ne a c hp h o n e t i cs e g m e n t .T h eadjacency graph was constructed usingnearest Euclideanneighbors and binary-valued edge weights. For the optimiza-tion problem of (8), we take as the intrinsic smoothness param-eter.F i n a l l y ,t oa c c o m m o d a t en o n l i n e a ri n t r i n s i cp r o j e c -tions maps, we employ the radialbasis function (RBF) kernel,,w h e r eis taken to be 1/3 of the meanEuclidean distance between the graph vertices. Note that op-timal settings of,a n ddepend on the intended applicationand manifold sampling density; wei n v e s t i g a t et h er o l et h i sp a -rameter in the experiments described below. Given the low-di-mensional curved manifold structure motivated in previous sec-tions, one might expect phonetic content to be more transpar-ently differentiated in the intrinsic basis than in a traditionalspectrogram. It is clear from Fig. 5 that the intrinsic represen-tation redistributes much of the spectral variation to the lowereigenvalued components. It is also clear that these initial com-ponents do not each covary with the presence of a single speechsound. In the next section, we examine whether this alternativeorganization may have a natural linguistic interpretation.V. INTRINSICSPECTRALANALYSISINTERPRETATIONThei n t r i n s i cr e p r e s e n t a t i o ni sap r o j e c t i o no fs p e c t r a li n f o r -mation onto a set of basis functions ordered by their smooth-2Note that while we use a class balanced sample here, balancing was notrequired to obtain good performance in the experiments in Section VII in whichwe randomly selected examples from the entire corpus (ignoring class).frsh#Sample  2:aoahs
y(2)x(2)
Word  Alignment  /  Phrase  Extraction•Variables  (boolean):–For  each  (Chinese  phrase,  English  phrase)  pair,  are  they  linked?•Interactions:–Word  fertilities–Few  “jumps”  (discontinuities)–Syntactic  reorderings–“ITG  contraint”  on  alignment–Phrases  are  disjoint  (?)38(Burkett  &  Klein,  2012)Application:

Figure 1: An example of a debate structure from the Con-V ote corpus. Each black square node represents a factorand is connected to the variables in that factor, shownas round nodes. Unshaded variables correspond to therepresentatives’ votes and depict the output variables thatwe learn to jointly predict. Shaded variables correspondto the observed input data— the text of all speeches of arepresentative (in dark gray) or all local contexts of refer-ences between two representatives (in light gray).and that ERMA further signiﬁcantly improves per-formance, particularly when it properly trains withthe same inference algorithm (max-product vs. sum-product) to be used at test time.Baseline.As an exact baseline, we compareagainst the results of Thomas et al. (2006). Theirtest-time Min-Cut algorithm is exact in this case: bi-nary variables and a two-way classiﬁcation.4.2 Information Extraction fromSemi-Structured TextWe utilize the CMU seminar announcement corpusof Freitag (2000) consisting of emails with seminarannouncements. The task is to extract four ﬁelds thatdescribe each seminar:speaker , location, start timeandend time. The corpus annotates the documentwith all mentions of these four ﬁelds.Sequential CRFs have been used successfully forsemi-structured information extraction (Sutton andMcCallum, 2005; Finkel et al., 2005). However,they cannot model non-local dependencies in thedata. For example, in the seminar announcementscorpus, if “Sutner” is mentioned once in an emailin a context that identiﬁes him as a speaker, it isFigure 2: Skip-chain CRF for semi-structured informa-tion extraction.likely that other occurrences of “Sutner” in the sameemail should be marked asspeaker. Hence Finkel etal. (2005) and Sutton and McCallum (2005) proposeadding non-local edges to a sequential CRF to repre-sent soft consistency constraints. The model, calleda “skip-chain CRF” and shown in Figure 2, containsa factor linking each pair of capitalized words withthe same lexical form. The skip-chain CRF modelexhibits better empirical performance than its se-quential counterpart (Sutton and McCallum, 2005;Finkel et al., 2005).The non-local skip links make exact inferenceintractable. To train the full model, Finkel et al.(2005) estimate the parameters of a sequential CRFand then manually select values for the weights ofthe non-local edges. At test time, they use Gibbssampling to perform inference. Sutton and McCal-lum (2005) use max-product loopy belief propaga-tion for test-time inference, and compare a train-ing procedure that uses a piecewise approximationof the partition function against using sum-productloopy belief propagation to compute output variablemarginals. They ﬁnd that the two training regimensperform similarly on the overall task. All of thesetraining procedures try to approximately maximizeconditional likelihood, whereas we will aim to mini-mize the empirical loss of the approximate inferenceand decoding procedures.Baseline.As an exact (non-loopy) baseline, wetrain a model without the skip chains. We give twobaseline numbers in Table 1—for training the exactCRF with MLE and with ERM. The ERM setting re-sulted in a statistically signiﬁcant improvement evenin the exact case, thanks to the use of the loss func-tion at training time.4.3 Multi-Label ClassiﬁcationMulti-label classiﬁcation is the problem of assign-ing multiple labels to a document. For example, anews article can be about both “Libya” and “civil125Congressional  Voting
39(Stoyanov&  Eisner,  2012)Application:
•Variables:–Text  of  all  speeches  of  a  representative  –Local  contexts  of  references  between  two  representatives•Interactions:–Words  used  by  representative  and  their  vote–Pairs  of  representatives  and  their  local  context
Structured  Prediction  Examples•Examples  of  structured  prediction–Part-­‐of-­‐speech  (POS)  tagging–Handwriting  recognition–Speech  recognition–Word  alignment–Congressional  voting•Examples  of  latent  structure–Object  recognition40
Case  Study:  Object  RecognitionData  consists  of  images  xand  labels  y.
41
pigeon
leopardllamarhinoceros
y(1)x(1)
y(2)x(2)
y(4)x(4)
y(3)x(3)
Case  Study:  Object  RecognitionData  consists  of  images  xand  labels  y.
42•Preprocess  data  into  “patches”•Posit  a  latent  labeling  zdescribing  the  object’s  parts  (e.g.  head,  leg,  tail,  torso,  grass)
leopard
•Define  graphical  model  with  these  latent  variables  in  mind•zis  not  observed  at  train  or  test  time
Case  Study:  Object  RecognitionData  consists  of  images  xand  labels  y.
43•Preprocess  data  into  “patches”•Posit  a  latent  labeling  zdescribing  the  object’s  parts  (e.g.  head,  leg,  tail,  torso,  grass)
leopard
•Define  graphical  model  with  these  latent  variables  in  mind•zis  not  observed  at  train  or  test  timeX2Z2X7Z7
Y
Case  Study:  Object  RecognitionData  consists  of  images  xand  labels  y.
44•Preprocess  data  into  “patches”•Posit  a  latent  labeling  zdescribing  the  object’s  parts  (e.g.  head,  leg,  tail,  torso,  grass)
leopard
•Define  graphical  model  with  these  latent  variables  in  mind•zis  not  observed  at  train  or  test  timeψ2ψ4X2Z2ψ3X7Z7ψ1ψ4ψ4
Y
Structured  Prediction
45
Preview  of  challenges  to  come…
•Consider  the  task  of  finding  the  most  probable  assignmentto  the  output  ClassificationStructured  Predictionˆy=`;Ktyp(y|t)wherey {+1, 1}ˆv=`;Ktvp(v|t)wherev Yand|Y|isverylarge
Machine  Learning
46Thedata  inspires  the  structures  we  want  to  predictIt  also  tells  us  what  to  optimizeOur  modeldefines  a  score  for  each  structure
Learningtunes  the  parameters  of  the  modelInferencefinds  {best  structure,  marginals,  partition  function}for  a  new  observationDomain  KnowledgeMathematical  ModelingOptimizationCombinatorial  OptimizationML(Inferenceis  usually  called  as  a  subroutine  in  learning)

Machine  Learning
47DataModel
LearningInference(Inferenceis  usually  called  as  a  subroutine  in  learning)
3 Alice saw Bob on a hill with a telescopeAlicesawBobonahillwithatelescope4 time ﬂies like an arrowtimeﬂieslikeanarrowtimeﬂieslikeanarrowtimeﬂieslikeanarrowtimeﬂieslikeanarrow
2
Objective
X1
X3
X2
X4
X5

BACKGROUND
48
BackgroundWhiteboard–Chain  Rule  of  Probability–Conditional  Independence
49
Background:  Chain  Ruleof  Probability
50ForrandomvariablesAandB:P(A, B)=P(A|B)P(B)P(X1,X2,X3,X4)=P(X1|X2,X3,X4)P(X2|X3,X4)P(X3|X4)P(X4)ForrandomvariablesX1,X2,X3,X4:
Background:Conditional  Independence
51RandomvariablesAandBareconditionallyindependentgivenCif:P(A, B|C)=P(A|C)P(B|C)(1)orequivalently:P(A|B,C)=P(A|C)(2)Wewritethisas:A|4B|C(3)Later  we  will  also  write:  I<A, {C}, B>
DIRECTED  GRAPHICAL  MODELSBayesian  Networks
52
Example:  Tornado  Alarms1.Imagine  that  you  work  at  the  911  call  center  in  Dallas2.You  receive  six  calls  informing  you  that  the  Emergency  Weather  Sirens  are  going  off3.What  do  you  conclude?53Figure  from  https://www.nytimes.com/2017/04/08/us/dallas-­‐emergency-­‐sirens-­‐hacking.html
Example:  Tornado  Alarms1.Imagine  that  you  work  at  the  911  call  center  in  Dallas2.You  receive  six  calls  informing  you  that  the  Emergency  Weather  Sirens  are  going  off3.What  do  you  conclude?54Figure  from  https://www.nytimes.com/2017/04/08/us/dallas-­‐emergency-­‐sirens-­‐hacking.html
Directed  Graphical  Models  (Bayes  Nets)Whiteboard–Example:  Tornado  Alarms–Writing  Joint  Distributions•Idea  #1:  Giant  Table•Idea  #2:  Rewrite  using  chain  rule•Idea  #3:  Assume  full  independence•Idea  #4:  Drop  variables  from  RHS  of  conditionals–Definition:  Bayesian  Network–Observed  Variables  in  Graphical  Models55
Bayesian  Network
56p(X1,X2,X3,X4,X5)=p(X5|X3)p(X4|X2,X3)p(X3)p(X2|X1)p(X1)
X1
X3
X2
X4
X5

Bayesian  Network
•A  Bayesian  Network  is  a  directed  graphical  model•It  consists  of  a  graph  Gand  the  conditional  probabilities  P•These  two  parts  full  specify  the  distribution:–Qualitative  Specification:  G–Quantitative  Specification:  P57
X1
X3
X2
X4
X5
Definition:P(X1…Xn)=P(Xi|parents(Xi))i=1n∏
Learning  Theory
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  13March  1,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Learning  Theory  Readings:Murphy  -­‐-­‐Bishop  -­‐-­‐HTF  -­‐-­‐Mitchell  7
Reminders•Homework4:  Perceptron/  Kernels/  SVM–Release:  Wed,  Feb.  22–Due:  Fri,  Mar.  03  at  11:59pm•MidtermExam(EveningExam)–Tue,  Mar.  07  at  7:00pm  –9:30pm–SeePiazza  fordetailsaboutlocation
29  daysfor  HW4
Outline•Generative  vs.  Discriminative  Modeling•Bayes  Optimal  Classifier–Loss  function,  Expected  Loss–Bayes  Error–Bayes  Optimal•Consistency  of  MLE–Consistency–Almost  Sure  Convergence  of  MLE3
DISCRIMINATIVE  AND  GENERATIVE  CLASSIFIERS4
Generative  vs.  Discriminative•Generative  Classifiers:–Example:  Naïve  Bayes–Define  a  joint  model  of  the  observations  xand  the  labels  y:–Learning  maximizes  (joint)  likelihood–Use  Bayes’  Rule  to  classify  based  on  the  posterior:•Discriminative  Classifiers:–Example:  Logistic  Regression–Directly  model  the  conditional:    –Learning  maximizes  conditional  likelihood5p(x,y)p(y|x)p(y|x)=p(x|y)p(y)/p(x)
Generative  vs.  DiscriminativeWhiteboard–Contrast:  To  model  p(x)  or  not  to  model  p(x)?
6
Generative  vs.  DiscriminativeFinite  Sample  Analysis  (Ng  &  Jordan,  2002)[Assume  that  we  are  learning  from  a  finite  training  dataset]
7If  model  assumptions  are  correct:  Naive  Bayes  is  a  more  efficient  learner  (requires  fewer  samples)  than  Logistic  RegressionIf  model  assumptions  are  incorrect:  Logistic  Regression  has  lower  asymtoticerror,  and  does  better  than  Naïve  Bayes
solid:  NB  dashed:  LR
8Slide  courtesy  of  William  Cohen
Naïve  Bayes  makes  stronger  assumptions  about  the  databut  needs  fewer  examples  to  estimate  the  parameters“On  Discriminative  vsGenerative  Classifiers:  ….”  Andrew  Ng  and  Michael  Jordan,  NIPS  2001.9
solid:  NB  dashed:  LR
Slide  courtesy  of  William  Cohen
Generative  vs.  DiscriminativeLearning  (Parameter  Estimation)
10Naïve  Bayes:  Parameters  are  decoupled  àClosed  form  solution  for  MLELogistic  Regression:  Parameters  are  coupled  àNo  closed  form  solution  –must  use  iterative  optimization  techniques  instead
Naïve  Bayes  vs.  Logistic  Reg.Learning  (MAP  Estimation  of  Parameters)
11Bernoulli  Naïve  Bayes:  Parameters  are  probabilities  àBeta  prior  (usually)  pushes  probabilities  away  from  zero  /  one  extremesLogistic  Regression:  Parameters  are  not  probabilities  àGaussian  prior  encourages  parameters  to  be  close  to  zero  (effectively  pushes  the  probabilities  away  from  zero  /  one  extremes)
Naïve  Bayes  vs.  Logistic  Reg.Features
12Naïve  Bayes:  Features  xare  assumed  to  be  conditionally  independent  given  y.  (i.e.  Naïve  Bayes  Assumption)Logistic  Regression:  No  assumptions  are  made  about  the  form  of  the  features  x.    They  can  be  dependent  and  correlated  in  any  fashion.  
LEARNING  THEORY
14
Outline•Generative  vs.  Discriminative  Modeling•Bayes  Optimal  Classifier–Loss  function,  Expected  Loss–Bayes  Error–Bayes  Optimal•Consistency  of  MLE–Consistency–Almost  Sure  Convergence  of  MLE15
Questions  For  Today1.Given  a  probability  distribution,  how  do  we  construct  the  optimal  classifier?(Bayes  Optimal  Classifier)2.Given  data,  what  guarantees  do  we  have  about  the  MLE  parameters?(Consistency  of  MLE)
16
Bayes  Optimal  ClassifierWhiteboard:–Loss  function,  Expected  Loss–Bayes  Error–Bayes  Optimal
17
Questions  For  Today1.Given  a  probability  distribution,  how  do  we  construct  the  optimal  classifier?(Bayes  Optimal  Classifier)2.Given  data,  what  guarantees  do  we  have  about  the  MLE  parameters?(Consistency  of  MLE)
18
Consistency  of  MLEWhiteboard:–Consistency–Almost  Sure  Convergence  of  MLE
19
MIDTERM  EXAM  LOGISTICS
20
Midterm  Exam•Logistics–Evening  ExamTue,  Mar.  07  at  7:00pm  –9:30pm–8-­‐9  Sections–Format  of  questions:•Multiple  choice•True  /  False  (with  justification)•Derivations•Short  answers•Interpreting  figures–No  electronic  devices–You  are  allowed  to  bringone  8½  x  11  sheet  of  notes  (front  and  back)21
Midterm  Exam•How  to  Prepare–Attend  the  midterm  review  session:  Thu,  March  2  at  6:30pm  (PH  100)–Attend  the  midterm  review  lectureMon,  March  6  (in-­‐class)–Review  prior  year’s  exam  and  solutions(we’ll  post  them)–Review  this  year’s  homework  problems
22
Midterm  Exam•Advice  (for  during  the  exam)–Solve  the  easy  problems  first  (e.g.  multiple  choice  before  derivations)•if  a  problem  seems  extremely  complicated  you’re  likely  missing  something–Don’t  leave  any  answer  blank!–If  you  make  an  assumption,  write  it  down–If  you  look  at  a  question  and  don’t  know  the  answer:•we  probably  haven’t  told  you  the  answer•but  we’ve  told  you  enough  to  work  it  out•imagine  arguing  for  some  answer  and  see  if  you  like  it23
Topics  for  Midterm•Foundations–Probability–MLE,  MAP–Optimization•Classifiers–KNN–Naïve  Bayes–Logistic  Regression–Perceptron–SVM•Regression–Linear  Regression•Important  Concepts–Kernels–Regularization  and  Overfitting–Experimental  Design
24
CSC321 Lecture 20: Reversible and
Autoregressive Models
Roger Grosse
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 1 / 23
Overview
Four modern approaches to generative modeling:
Generative adversarial networks (last lecture)
Reversible architectures (today)
Autoregressive models (today)
Variational autoencoders (CSC412)
All four approaches have dierent pros and cons.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 2 / 23
Overview
Remember that the GAN generator network represents a distribution
by sampling from a simple distribution pZ(z) over code vectors z.
I'll use pZhere to emphasize that it's a distribution on z.
A GAN was an implicit generative model, since we could only
generate samples, not evaluate the log-likelihood.
Can't tell if it's missing modes, memorizing the training data, etc.
Reversible architectures are an elegant kind of generator network with
tractable log-likelihood.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 3 / 23
Change of Variables Formula
Letfdenote a dierentiable, bijective mapping from space Zto
spaceX. (I.e., it must be 1-to-1 and cover all of X.)
Since f denes a one-to-one correspondence between values z2Z
andx2X , we can think of it as a change-of-variables transformation.
Change-of-Variables Formula from probability theory: if x=f(z),
then
pX(x) = pZ(z)det@x
@z 1
Intuition for the Jacobian term:
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 4 / 23
Change of Variables Formula
Suppose we have a generator network which computes the function f.
It's tempting to apply the change-of-variables formula in order to
compute the density pX(x).
I.e., compute z=f 1(x)
pX(x) = pZ(z)det@x
@z 1
Problems?
The mapping fneeds to be invertible, with an easy-to-compute inverse.
It needs to be dierentiable, so that the Jaobian @x=@zis dened.
We need to be able to compute the (log) determinant.
The GAN generator may be dierentiable, but it doesn't satisfy the
other two properties.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 5 / 23
Reversible Blocks
Now let's dene a reversible block which is invertible and has a
tractable determinant.
Such blocks can be composed.
Inversion: f 1=f 1
1 f 1
k
Determinants:@xk
@z=@xk
@xk 1@x2
@x1@x1
@z
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 6 / 23
Reversible Blocks
Recall the residual blocks:
y=x+F(x)
Reversible blocks are a variant of
residual blocks. Divide the units into
two groups, x1andx2.
y1=x1+F(x2)
y2=x2
Inverting a reversible block:
x2=y2
x1=y1 F (x2)
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 7 / 23
Reversible Blocks
Composition of two reversible blocks, but with x1andx2swapped:
Forward:
y1=x1+F(x2)
y2=x2+G(y1)
Backward:
x2=y2 G(y1)
x1=y1 F (x2)
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 8 / 23
Volume Preservation
It remains to compute the log determinant of the Jacobian.
The Jacobian of the reversible block:
y1=x1+F(x2)
y2=x2@y
@x=I@F
@x2
0 I
This is an upper triangular matrix. The determinant of an upper
triangular matrix is the product of the diagonal entries, or in this
case, 1.
Since the determinant is 1, the mapping is said to be volume
preserving.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 9 / 23
Nonlinear Independent Components Estimation
We've just dened the reversible block.
Easy to invert by subtracting rather than adding the residual function.
The determinant of the Jacobian is 1.
Nonlinear Independent Components Estimation (NICE) trains a
generator network which is a composition of lots of reversible blocks.
We can compute the likelihood function using the change-of-variables
formula:
pX(x) = pZ(z)det@x
@z 1
=pZ(z)
We can train this model using maximum likelihood. I.e., given a
datasetfx(1); : : : ; x(N)g, we maximize the likelihood
NY
i=1pX(x(i)) =NY
i=1pZ(f 1(x(i)))
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 10 / 23
Nonlinear Independent Components Estimation
Likelihood:
pX(x) = pZ(z) = pZ(f 1(x))
Remember, pZis a simple, xed distribution (e.g. independent
Gaussians)
Intuition: train the network such that f 1maps each data point to a
high-density region of the code vector space Z.
Without constraints on f, it could map everything to 0, and this
likelihood objective would make no sense.
But it can't do this because it's volume preserving.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 11 / 23
Nonlinear Independent Components Estimation
Dinh et al., 2016. Density estimation using RealNVP.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 12 / 23
Nonlinear Independent Components Estimation
Samples produced by RealNVP, a model based on NICE.
Dinh et al., 2016. Density estimation using RealNVP.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 13 / 23
RevNets (optional)
A side benet of reversible blocks: you don't need to store the
activations in memory to do backprop, since you can reverse the
computation.
I.e., compute the activations as you need them, moving backwards
through the computation graph.
Notice that reversible blocks look a lot like residual blocks.
We recently designed a reversible residual network (RevNet)
architecture which is like a ResNet, but with reversible blocks instead
of residual blocks.
Matches state-of-the-art performance on ImageNet, but without the
memory cost of activations!
Gomez et al., NIPS 2017. \The revesible residual network: backrpop
without storing activations".
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 14 / 23
Overview
Four modern approaches to generative modeling:
Generative adversarial networks (last lecture)
Reversible architectures (today)
Autoregressive models (today)
Variational autoencoders (CSC412)
All four approaches have dierent pros and cons.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 15 / 23
Autoregressive Models
We've already looked at autoregressive models in this course:
Neural language models
RNN language models (and decoders)
We can push this further, and generate very long sequences.
Problem: training an RNN to generate these sequences requires a for
loop over >10,000 time steps.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 16 / 23
Causal Convolution
Idea 1: causal convolution
For RNN language models, we used the training sequence as both the
inputs and the outputs to the RNN.
We made sure the model was causal: each prediction depended only on
inputs earlier in the sequence.
We can do the same thing using a convolutional architecture.
No for loops! Processing each input sequence just requires a series of
convolution operations.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 17 / 23
Causal Convolution
Causal convolution for images:
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 18 / 23
CNN vs. RNN
We can turn a causal CNN into an RNN by adding recurrent
connections. Is this a good idea?
The RNN has a memory, so it can use information from all past time
steps. The CNN has a limited context.
But training the RNN is very expensive since it requires a for loop over
time steps. The CNN only requires a series of convolutions.
Generating from both models is very expensive, since it requires a for
loop. (Whereas generating from a GAN or a reversible model is very
fast.)
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 19 / 23
PixelCNN and PixelRNN
Van den Oord et al., ICML 2016, \Pixel recurrent neural networks"
This paper introduced two autoregressive models of images: the
PixelRNN and the PixelCNN. Both generated amazingly good
high-resolution images.
The output is a softmax over 256 possible pixel intensities.
Completing an image using an PixelCNN:
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 20 / 23
PixelCNN and PixelRNN
Samples from a PixelRNN trained on ImageNet:
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 21 / 23
Dilated Convolution
Idea 2: dilated convolution
The advantage of RNNs over CNNs is that their memory lets them
learn arbitrary long-distance dependencies.
But we can dramatically increase a CNN's receptive eld using dilated
convolution.
You did this in Programming Assignment 2.
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 22 / 23
WaveNet
WaveNet is an autoregressive model for raw audio based on causal
dilated convolutions.
van den Oord et al., 2016. \WaveNet: a generative model for raw
audio".
Audio needs to be sampled at at least 16k frames per second for good
quality. So the sequences are very long.
WaveNet uses dilations of 1 ;2; : : : ; 512, so each unit at the end of
this block as a receptive eld of length 1024, or 64 milliseconds.
It stacks several of these blocks, so the total context length is about
300 milliseconds.
https://deepmind.com/blog/wavenet-generative-model-raw-audio/
Roger Grosse CSC321 Lecture 20: Reversible and Autoregressive Models 23 / 23
Midterm  Exam  Review
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  14March  6,  2017Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University

Reminders•MidtermExam(EveningExam)–Tue,  Mar.  07  at  7:00pm  –9:30pm–SeePiazza  fordetailsaboutlocation
2
Outline•Midterm  Exam  Logistics•Sample  Questions•Classification  and  Regression:  The  Big  Picture•Q&A
3
MIDTERM  EXAM  LOGISTICS
4
Midterm  Exam•Logistics–Evening  ExamTue,  Mar.  07  at  7:00pm  –9:30pm–8-­‐9  Sections–Format  of  questions:•Multiple  choice•True  /  False  (with  justification)•Derivations•Short  answers•Interpreting  figures–No  electronic  devices–You  are  allowed  to  bringone  8½  x  11  sheet  of  notes  (front  and  back)5
Midterm  Exam•How  to  Prepare–Attend  the  midterm  review  session:  Thu,  March  2  at  6:30pm  (PH  100)–Attend  the  midterm  review  lectureMon,  March  6  (in-­‐class)–Review  prior  year’s  exam  and  solutions(we’ll  post  them)–Review  this  year’s  homework  problems
6
Midterm  Exam•Advice  (for  during  the  exam)–Solve  the  easy  problems  first  (e.g.  multiple  choice  before  derivations)•if  a  problem  seems  extremely  complicated  you’re  likely  missing  something–Don’t  leave  any  answer  blank!–If  you  make  an  assumption,  write  it  down–If  you  look  at  a  question  and  don’t  know  the  answer:•we  probably  haven’t  told  you  the  answer•but  we’ve  told  you  enough  to  work  it  out•imagine  arguing  for  some  answer  and  see  if  you  like  it7
Topics  for  Midterm•Foundations–Probability–MLE,  MAP–Optimization•Classifiers–KNN–Naïve  Bayes–Logistic  Regression–Perceptron–SVM•Regression–Linear  Regression•Important  Concepts–Kernels–Regularization  and  Overfitting–Experimental  Design
8
SAMPLE  QUESTIONS
9
Sample  Questions
1010-601: Machine LearningPage 4 of162/29/20161.3MAP vs MLEAnswer each question withTorFandprovide a one sentence explanation of youranswer:(a)[2 pts.]To rF :In the limit, asn(the number of samples) increases, the MAP andMLE estimates become the same.
(b)[2 pts.]To rF :Naive Bayes can only be used with MAP estimates, and not MLEestimates.
1.4ProbabilityAssume we have a sample space⌦. Answer each question withTorF.No justiﬁcationis required.(a)[1 pts.]To rF :If eventsA,B,a n dCare disjoint then they are independent.(b)[1 pts.]To rF :P(A|B)/P(A)P(B|A)P(A|B). (The sign ‘/’ means ‘is proportional to’)(c)[1 pts.]To rF :P(A[B)P(A).(d)[1 pts.]To rF :P(A\B) P(A).10-601: Machine LearningPage 4 of162/29/20161.3MAP vs MLEAnswer each question withTorFandprovide a one sentence explanation of youranswer:(a)[2 pts.]To rF :In the limit, asn(the number of samples) increases, the MAP andMLE estimates become the same.
(b)[2 pts.]To rF :Naive Bayes can only be used with MAP estimates, and not MLEestimates.
1.4ProbabilityAssume we have a sample space⌦. Answer each question withTorF.No justiﬁcationis required.(a)[1 pts.]To rF :If eventsA,B,a n dCare disjoint then they are independent.(b)[1 pts.]To rF :P(A|B)/P(A)P(B|A)P(A|B). (The sign ‘/’ means ‘is proportional to’)(c)[1 pts.]To rF :P(A[B)P(A).(d)[1 pts.]To rF :P(A\B) P(A).10-601: Machine LearningPage 4 of162/29/20161.3MAP vs MLEAnswer each question withTorFandprovide a one sentence explanation of youranswer:(a)[2 pts.]To rF :In the limit, asn(the number of samples) increases, the MAP andMLE estimates become the same.
(b)[2 pts.]To rF :Naive Bayes can only be used with MAP estimates, and not MLEestimates.
1.4ProbabilityAssume we have a sample space⌦. Answer each question withTorF.No justiﬁcationis required.(a)[1 pts.]To rF :If eventsA,B,a n dCare disjoint then they are independent.(b)[1 pts.]To rF :P(A|B)/P(A)P(B|A)P(A|B). (The sign ‘/’ means ‘is proportional to’)(c)[1 pts.]To rF :P(A[B)P(A).(d)[1 pts.]To rF :P(A\B) P(A).
Sample  Questions
1110-701 Machine Learning Midterm Exam - Page 8 of 17 11/02/2016Now we will apply K-Nearest Neighbors using Euclidean distance to a binary classiﬁ-cation task. We assign the class of the test point to be the class of the majority of theknearest neighbors. A point can be its own neighbor.
Figure 53.[2 pts]What value ofkminimizes leave-one-out cross-validation error for the datasetshown in Figure5? What is the resulting error?
4.[2 pts] Sketchthe 1-nearest neighbor boundary over Figure5.5.[2 pts]What value ofkminimizes the training set error for the dataset shown inFigure5? What is the resulting training error?10-701 Machine Learning Midterm Exam - Page 7 of 17 11/02/20164 K-NN [12 pts]In this problem, you will be tested on your knowledge of K-Nearest Neighbors (K-NN), wherekindicates the number of nearest neighbors.1.[3 pts]For K-NN in general, are there any cons of using verylargekvalues?Selectone. Brieﬂy justify your answer.(a) Yes (b) No2.[3 pts]For K-NN in general, are there any cons of using verysmallkvalues?Selectone. Brieﬂy justify your answer.(a) Yes (b) No
Sample  Questions
1210-601: Machine LearningPage 3 of162/29/20161.2Maximum Likelihood Estimation (MLE)Assume we have a random sample that is Bernoulli distributedX1,...,Xn⇠Bernoulli(✓).We are going to derive the MLE for✓. Recall that a Bernoulli random variableXtakesvalues in{0,1}and has probability mass function given byP(X;✓)=✓X(1 ✓)1 X.(a)[2 pts.] Derive the likelihood,L(✓;X1,...,Xn).
(b)[2 pts.] Derive the following formula for the log likelihood:`(✓;X1,...,Xn)= nXi=1Xi!log(✓)+ n nXi=1Xi!log(1 ✓).
(c)Extra Credit:[2 pts.] Derive the following formula for the MLE:ˆ✓=1n(Pni=1Xi).10-601: Machine LearningPage 3 of162/29/20161.2Maximum Likelihood Estimation (MLE)Assume we have a random sample that is Bernoulli distributedX1,...,Xn⇠Bernoulli(✓).We are going to derive the MLE for✓. Recall that a Bernoulli random variableXtakesvalues in{0,1}and has probability mass function given byP(X;✓)=✓X(1 ✓)1 X.(a)[2 pts.] Derive the likelihood,L(✓;X1,...,Xn).
(b)[2 pts.] Derive the following formula for the log likelihood:`(✓;X1,...,Xn)= nXi=1Xi!log(✓)+ n nXi=1Xi!log(1 ✓).
(c)Extra Credit:[2 pts.] Derive the following formula for the MLE:ˆ✓=1n(Pni=1Xi).10-601: Machine LearningPage 3 of162/29/20161.2Maximum Likelihood Estimation (MLE)Assume we have a random sample that is Bernoulli distributedX1,...,Xn⇠Bernoulli(✓).We are going to derive the MLE for✓. Recall that a Bernoulli random variableXtakesvalues in{0,1}and has probability mass function given byP(X;✓)=✓X(1 ✓)1 X.(a)[2 pts.] Derive the likelihood,L(✓;X1,...,Xn).
(b)[2 pts.] Derive the following formula for the log likelihood:`(✓;X1,...,Xn)= nXi=1Xi!log(✓)+ n nXi=1Xi!log(1 ✓).
(c)Extra Credit:[2 pts.] Derive the following formula for the MLE:ˆ✓=1n(Pni=1Xi).
Sample  Questions
1310-601: Machine LearningPage 4 of162/29/20161.3MAP vs MLEAnswer each question withTorFandprovide a one sentence explanation of youranswer:(a)[2 pts.]To rF :In the limit, asn(the number of samples) increases, the MAP andMLE estimates become the same.
(b)[2 pts.]To rF :Naive Bayes can only be used with MAP estimates, and not MLEestimates.
1.4ProbabilityAssume we have a sample space⌦. Answer each question withTorF.No justiﬁcationis required.(a)[1 pts.]To rF :If eventsA,B,a n dCare disjoint then they are independent.(b)[1 pts.]To rF :P(A|B)/P(A)P(B|A)P(A|B). (The sign ‘/’ means ‘is proportional to’)(c)[1 pts.]To rF :P(A[B)P(A).(d)[1 pts.]To rF :P(A\B) P(A).
Sample  Questions
1410-601: Machine LearningPage 2 of162/29/20161Naive Bayes, Probability, and MLE [20 pts. + 2 Extra Credit]1.1Naive BayesYou are given a data set of 10,000 students with their sex, height, and hair color. You aretrying to build a classiﬁer to predict the sex of a student, so you randomly split the datainto a training set and a testing set. Here are the speciﬁcations of the data set:•sex2{male,female}•height2[0,300] centimeters•hair2{brown, black, blond, red, green}•3240 men in the data set•6760 women in the data setUnder the assumptions necessary for Naive Bayes (not the distributional assumptions youmight naturally or intuitively make about the dataset) answer each question withTorFandprovide a one sentence explanation of your answer:(a)[2 pts.]To rF :As height is a continuous valued variable, Naive Bayes is not appropriatesince it cannot handle continuous valued variables.(b)[2 pts.]To rF :Since there is not a similar number of men and women in the dataset,Naive Bayes will have high test error.(c)[2 pts.]To rF :P(height|sex,hair)=P(height|sex).(d)[2 pts.]To rF :P(height,hair|sex)=P(height|sex)P(hair|sex).10-601: Machine LearningPage 2 of162/29/20161Naive Bayes, Probability, and MLE [20 pts. + 2 Extra Credit]1.1Naive BayesYou are given a data set of 10,000 students with their sex, height, and hair color. You aretrying to build a classiﬁer to predict the sex of a student, so you randomly split the datainto a training set and a testing set. Here are the speciﬁcations of the data set:•sex2{male,female}•height2[0,300] centimeters•hair2{brown, black, blond, red, green}•3240 men in the data set•6760 women in the data setUnder the assumptions necessary for Naive Bayes (not the distributional assumptions youmight naturally or intuitively make about the dataset) answer each question withTorFandprovide a one sentence explanation of your answer:(a)[2 pts.]To rF :As height is a continuous valued variable, Naive Bayes is not appropriatesince it cannot handle continuous valued variables.(b)[2 pts.]To rF :Since there is not a similar number of men and women in the dataset,Naive Bayes will have high test error.(c)[2 pts.]To rF :P(height|sex,hair)=P(height|sex).(d)[2 pts.]To rF :P(height,hair|sex)=P(height|sex)P(hair|sex).10-601: Machine LearningPage 2 of162/29/20161Naive Bayes, Probability, and MLE [20 pts. + 2 Extra Credit]1.1Naive BayesYou are given a data set of 10,000 students with their sex, height, and hair color. You aretrying to build a classiﬁer to predict the sex of a student, so you randomly split the datainto a training set and a testing set. Here are the speciﬁcations of the data set:•sex2{male,female}•height2[0,300] centimeters•hair2{brown, black, blond, red, green}•3240 men in the data set•6760 women in the data setUnder the assumptions necessary for Naive Bayes (not the distributional assumptions youmight naturally or intuitively make about the dataset) answer each question withTorFandprovide a one sentence explanation of your answer:(a)[2 pts.]To rF :As height is a continuous valued variable, Naive Bayes is not appropriatesince it cannot handle continuous valued variables.(b)[2 pts.]To rF :Since there is not a similar number of men and women in the dataset,Naive Bayes will have high test error.(c)[2 pts.]To rF :P(height|sex,hair)=P(height|sex).(d)[2 pts.]To rF :P(height,hair|sex)=P(height|sex)P(hair|sex).
Sample  Questions
1510-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 8 of162/29/2016
(a) Adding one outlier to theoriginal data set.
(b) Adding two outliers to the original dataset.
(c) Adding three outliers to the original dataset. Two on one side and one on the otherside.
(d) Duplicating the original data set.
(e) Duplicating the original data set andadding four points that lie on the trajectoryof the original regression line.Figure 3: New data setSnew.Dataset
Sample  Questions
1610-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 8 of162/29/2016
(a) Adding one outlier to theoriginal data set.
(b) Adding two outliers to the original dataset.
(c) Adding three outliers to the original dataset. Two on one side and one on the otherside.
(d) Duplicating the original data set.
(e) Duplicating the original data set andadding four points that lie on the trajectoryof the original regression line.Figure 3: New data setSnew.Dataset
Sample  Questions
1710-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 8 of162/29/2016
(a) Adding one outlier to theoriginal data set.
(b) Adding two outliers to the original dataset.
(c) Adding three outliers to the original dataset. Two on one side and one on the otherside.
(d) Duplicating the original data set.
(e) Duplicating the original data set andadding four points that lie on the trajectoryof the original regression line.Figure 3: New data setSnew.Dataset
Sample  Questions
1810-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 7 of162/29/20163Linear and Logistic Regression [20 pts. + 2 Extra Credit]3.1Linear regressionGiven that we have an inputxand we want to estimate an outputy, in linear regressionwe assume the relationship between them is of the formy=wx+b+✏, wherewandbarereal-valued parameters we estimate and✏represents the noise in the data. When the noiseis Gaussian, maximizing the likelihood of a datasetS={(x1,y1),...,(xn,yn)}to estimatethe parameterswandbis equivalent to minimizing the squared error:arg minwnXi=1(yi (wxi+b))2.Consider the datasetSplotted in Fig.1along with its associated regression line. Foreach of the altered data setsSnewplotted in Fig.3, indicate which regression line (relativeto the original one) in Fig.2corresponds to the regression line for the new data set. Writeyour answers in the table below.Dataset(a)(b)(c)(d)(e)Regression line
Figure 1: An observed data set and its associated regression line.
Figure 2: New regression lines for altered data setsSnew.10-601: Machine LearningPage 8 of162/29/2016
(a) Adding one outlier to theoriginal data set.
(b) Adding two outliers to the original dataset.
(c) Adding three outliers to the original dataset. Two on one side and one on the otherside.
(d) Duplicating the original data set.
(e) Duplicating the original data set andadding four points that lie on the trajectoryof the original regression line.Figure 3: New data setSnew.Dataset
Sample  Questions
1910-601: Machine LearningPage 9 of162/29/20163.2Logistic regressionGiven a training set{(xi,yi),i=1,...,n}wherexi2Rdis a feature vector andyi2{0,1}is a binary label, we want to ﬁnd the parameters ˆwthat maximize the likelihood for thetraining set, assuming a parametric model of the formp(y=1|x;w)=11+e x p ( wTx).The conditional log likelihood of the training set is`(w)=nXi=1yilogp(yi,|xi;w)+( 1 yi)l o g ( 1 p(yi,|xi;w)),and the gradient isr`(w)=nXi=1(yi p(yi|xi;w))xi.(a)[5 pts.] Is it possible to get a closed form for the parameters ˆwthat maximize theconditional log likelihood? How would you compute ˆwin practice?(b)[5 pts.] What is the form of the classiﬁer output by logistic regression?
(c)[2 pts.]Extra Credit:Consider the case with binary features, i.e,x2{0,1}d⇢Rd,where featurex1is rare and happens to appear in the training set with only label 1.What is ˆw1?I s t h e g r a d i e n t e v e r z e r o f o r a n y ﬁ n i t ew? Why is it important to includea regularization term to control the norm of ˆw?10-601: Machine LearningPage 9 of162/29/20163.2Logistic regressionGiven a training set{(xi,yi),i=1,...,n}wherexi2Rdis a feature vector andyi2{0,1}is a binary label, we want to ﬁnd the parameters ˆwthat maximize the likelihood for thetraining set, assuming a parametric model of the formp(y=1|x;w)=11+e x p ( wTx).The conditional log likelihood of the training set is`(w)=nXi=1yilogp(yi,|xi;w)+( 1 yi)l o g ( 1 p(yi,|xi;w)),and the gradient isr`(w)=nXi=1(yi p(yi|xi;w))xi.(a)[5 pts.] Is it possible to get a closed form for the parameters ˆwthat maximize theconditional log likelihood? How would you compute ˆwin practice?(b)[5 pts.] What is the form of the classiﬁer output by logistic regression?
(c)[2 pts.]Extra Credit:Consider the case with binary features, i.e,x2{0,1}d⇢Rd,where featurex1is rare and happens to appear in the training set with only label 1.What is ˆw1?I s t h e g r a d i e n t e v e r z e r o f o r a n y ﬁ n i t ew? Why is it important to includea regularization term to control the norm of ˆw?10-601: Machine LearningPage 9 of162/29/20163.2Logistic regressionGiven a training set{(xi,yi),i=1,...,n}wherexi2Rdis a feature vector andyi2{0,1}is a binary label, we want to ﬁnd the parameters ˆwthat maximize the likelihood for thetraining set, assuming a parametric model of the formp(y=1|x;w)=11+e x p ( wTx).The conditional log likelihood of the training set is`(w)=nXi=1yilogp(yi,|xi;w)+( 1 yi)l o g ( 1 p(yi,|xi;w)),and the gradient isr`(w)=nXi=1(yi p(yi|xi;w))xi.(a)[5 pts.] Is it possible to get a closed form for the parameters ˆwthat maximize theconditional log likelihood? How would you compute ˆwin practice?(b)[5 pts.] What is the form of the classiﬁer output by logistic regression?
(c)[2 pts.]Extra Credit:Consider the case with binary features, i.e,x2{0,1}d⇢Rd,where featurex1is rare and happens to appear in the training set with only label 1.What is ˆw1?I s t h e g r a d i e n t e v e r z e r o f o r a n y ﬁ n i t ew? Why is it important to includea regularization term to control the norm of ˆw?10-601: Machine LearningPage 9 of162/29/20163.2Logistic regressionGiven a training set{(xi,yi),i=1,...,n}wherexi2Rdis a feature vector andyi2{0,1}is a binary label, we want to ﬁnd the parameters ˆwthat maximize the likelihood for thetraining set, assuming a parametric model of the formp(y=1|x;w)=11+e x p ( wTx).The conditional log likelihood of the training set is`(w)=nXi=1yilogp(yi,|xi;w)+( 1 yi)l o g ( 1 p(yi,|xi;w)),and the gradient isr`(w)=nXi=1(yi p(yi|xi;w))xi.(a)[5 pts.] Is it possible to get a closed form for the parameters ˆwthat maximize theconditional log likelihood? How would you compute ˆwin practice?(b)[5 pts.] What is the form of the classiﬁer output by logistic regression?
(c)[2 pts.]Extra Credit:Consider the case with binary features, i.e,x2{0,1}d⇢Rd,where featurex1is rare and happens to appear in the training set with only label 1.What is ˆw1?I s t h e g r a d i e n t e v e r z e r o f o r a n y ﬁ n i t ew? Why is it important to includea regularization term to control the norm of ˆw?
Samples  Questions
2010-601B: MACHINELEARNINGPage 5 of??10/10/20162To err is machine-like [20 pts]2.1Train and test errorsIn this problem, we will see how you can debug a classiﬁer by looking at its train and test errors.Consider a classiﬁer trained till convergence on some training dataDtrain, and tested on a separatetest setDtest. You look at the test error, and ﬁnd that it is very high. You then compute the trainingerror and ﬁnd that it is close to 0.1.[4 pts]Which of the following is expected to help? Select all that apply.(a)Increase the training data size.(b)Decrease the training data size.(c)Increase model complexity (For example, if your classiﬁer is an SVM, use a morecomplex kernel. Or if it is a decision tree, increase the depth).(d)Decrease model complexity.(e)Train on a combination ofDtrainandDtestand test onDtest(f)Conclude that Machine Learning does not work.2.[5 pts]Explain your choices.
3.[2 pts]What is this scenario called?4.[1 pts]Say you plot the train and test errors as a function of the model complexity. Whichof the following two plots is your plot expected to look like?10-601B: MACHINELEARNINGPage 5 of??10/10/20162To err is machine-like [20 pts]2.1Train and test errorsIn this problem, we will see how you can debug a classiﬁer by looking at its train and test errors.Consider a classiﬁer trained till convergence on some training dataDtrain, and tested on a separatetest setDtest. You look at the test error, and ﬁnd that it is very high. You then compute the trainingerror and ﬁnd that it is close to 0.1.[4 pts]Which of the following is expected to help? Select all that apply.(a)Increase the training data size.(b)Decrease the training data size.(c)Increase model complexity (For example, if your classiﬁer is an SVM, use a morecomplex kernel. Or if it is a decision tree, increase the depth).(d)Decrease model complexity.(e)Train on a combination ofDtrainandDtestand test onDtest(f)Conclude that Machine Learning does not work.2.[5 pts]Explain your choices.
3.[2 pts]What is this scenario called?4.[1 pts]Say you plot the train and test errors as a function of the model complexity. Whichof the following two plots is your plot expected to look like?
Samples  Questions
2110-601B: MACHINELEARNINGPage 5 of??10/10/20162To err is machine-like [20 pts]2.1Train and test errorsIn this problem, we will see how you can debug a classiﬁer by looking at its train and test errors.Consider a classiﬁer trained till convergence on some training dataDtrain, and tested on a separatetest setDtest. You look at the test error, and ﬁnd that it is very high. You then compute the trainingerror and ﬁnd that it is close to 0.1.[4 pts]Which of the following is expected to help? Select all that apply.(a)Increase the training data size.(b)Decrease the training data size.(c)Increase model complexity (For example, if your classiﬁer is an SVM, use a morecomplex kernel. Or if it is a decision tree, increase the depth).(d)Decrease model complexity.(e)Train on a combination ofDtrainandDtestand test onDtest(f)Conclude that Machine Learning does not work.2.[5 pts]Explain your choices.
3.[2 pts]What is this scenario called?4.[1 pts]Say you plot the train and test errors as a function of the model complexity. Whichof the following two plots is your plot expected to look like?10-601B: MACHINELEARNINGPage 5 of??10/10/20162To err is machine-like [20 pts]2.1Train and test errorsIn this problem, we will see how you can debug a classiﬁer by looking at its train and test errors.Consider a classiﬁer trained till convergence on some training dataDtrain, and tested on a separatetest setDtest. You look at the test error, and ﬁnd that it is very high. You then compute the trainingerror and ﬁnd that it is close to 0.1.[4 pts]Which of the following is expected to help? Select all that apply.(a)Increase the training data size.(b)Decrease the training data size.(c)Increase model complexity (For example, if your classiﬁer is an SVM, use a morecomplex kernel. Or if it is a decision tree, increase the depth).(d)Decrease model complexity.(e)Train on a combination ofDtrainandDtestand test onDtest(f)Conclude that Machine Learning does not work.2.[5 pts]Explain your choices.
3.[2 pts]What is this scenario called?4.[1 pts]Say you plot the train and test errors as a function of the model complexity. Whichof the following two plots is your plot expected to look like?10-601B: MACHINELEARNINGPage 6 of??10/10/2016
(a)
(b)2.2True and sample errorsConsider a classiﬁcation problem with distributionDand target functionc⇤:Rd7!±1. For anysampleSdrawn fromD, answer whether the following statements are true or false, along with abrief explanation.1.[4 pts]For a given hypothesis spaceH, it is possible to deﬁne a sufﬁcient size ofSsuch thatthe true error is bounded by the sample error by a margin✏, for all hypothesesh2Hwith agiven probability.2.[4 pts]The true error of any hypothesishis an upper bound on its training error on thesampleS.
Sample  Questions
2410-601: Machine LearningPage 10 of162/29/20164SVM, Perceptron and Kernels [20 pts. + 4 Extra Credit]4.1True or FalseAnswer each of the following questions withTorFandprovide a one line justiﬁcation.(a)[2 pts.] Consider two datasetsD(1)andD(2)whereD(1)={(x(1)1,y(1)1),. . . ,(x(1)n,y(1)n)}andD(2)={(x(2)1,y(2)1),. . . ,(x(2)m,y(2)m)}such thatx(1)i2Rd1,x(2)i2Rd2. Supposed1>d2andn>m. Then the maximum number of mistakes a perceptron algorithm will makeis higher on datasetD(1)than on datasetD(2).(b)[2 pts.] Suppose (x) is an arbitrary feature mapping from inputx2Xto (x)2RNand letK(x,z)= (x)· (z). ThenK(x,z)w i l la l w a y sb eav a l i dk e r n e lf u n c t i o n .(c)[2 pts.] Given the same training data, in which the points are linearly separable, themargin of the decision boundary produced by SVM will always be greater than or equalto the margin of the decision boundary produced by Perceptron.4.2Multiple Choice(a)[3 pt.] If the data is linearly separable, SVM minimizeskwk2subject to the constraints8i, yiw·xi 1. In the linearly separable case, which of the following may happen to thedecision boundary if one of the training samples is removed?Circle all that apply.•Shifts toward the point removed•Shifts away from the point removed•Does not change(b)[3 pt.] Recall that when the data are not linearly separable, SVM minimizeskwk2+CPi⇠isubject to the constraint that8i,yiw·xi 1 ⇠iand⇠i 0. Which of thefollowing may happen to the size of the margin if the tradeo↵parameterCis increased?Circle all that apply.•Increases•Decreases•Remains the same
Sample  Questions
2510-601: Machine LearningPage 11 of162/29/20164.3Analysis(a)[4 pts.] In one or two sentences, describe the beneﬁt of using the Kernel trick.(b)[4 pt.] The concept of margin is essential in both SVM and Perceptron. Describe why alarge margin separator is desirable for classiﬁcation.(c)[4 pts.]Extra Credit:Consider the dataset in Fig.4. Under the SVM formulation insection 4.2(a),(1)Draw the decision boundary on the graph.(2)What is the size of the margin?(3)Circle all the support vectors on the graph.
Figure 4: SVM toy dataset10-601: Machine LearningPage 11 of162/29/20164.3Analysis(a)[4 pts.] In one or two sentences, describe the beneﬁt of using the Kernel trick.(b)[4 pt.] The concept of margin is essential in both SVM and Perceptron. Describe why alarge margin separator is desirable for classiﬁcation.(c)[4 pts.]Extra Credit:Consider the dataset in Fig.4. Under the SVM formulation insection 4.2(a),(1)Draw the decision boundary on the graph.(2)What is the size of the margin?(3)Circle all the support vectors on the graph.
Figure 4: SVM toy dataset
Sample  Questions
2610-601: Machine LearningPage 11 of162/29/20164.3Analysis(a)[4 pts.] In one or two sentences, describe the beneﬁt of using the Kernel trick.(b)[4 pt.] The concept of margin is essential in both SVM and Perceptron. Describe why alarge margin separator is desirable for classiﬁcation.(c)[4 pts.]Extra Credit:Consider the dataset in Fig.4. Under the SVM formulation insection 4.2(a),(1)Draw the decision boundary on the graph.(2)What is the size of the margin?(3)Circle all the support vectors on the graph.
Figure 4: SVM toy dataset
Sample  Questions
2810-601B: MACHINELEARNINGPage 8 of??10/10/20163.[Extra Credit: 3 pts.]One formulation of soft-margin SVM optimization problem is:minw12kwk22+CNXi=1⇠is.t.yi(w>xi) 1 ⇠i8i=1,. . . ,N⇠i 08i=1,. . . ,NC 0where(xi,yi)are training samples andwdeﬁnes a linear decision boundary.Derive a formula for⇠iwhen the objective function achieves its minimum (No steps neces-sary). Note it is a function ofyiw>xi. Sketch a plot of⇠iwithyiw>xion the x-axis andvalue of⇠ion the y-axis. What is the name of this function?
Figure 2: Plot here10-601B: MACHINELEARNINGPage 8 of??10/10/20163.[Extra Credit: 3 pts.]One formulation of soft-margin SVM optimization problem is:minw12kwk22+CNXi=1⇠is.t.yi(w>xi) 1 ⇠i8i=1,. . . ,N⇠i 08i=1,. . . ,NC 0where(xi,yi)are training samples andwdeﬁnes a linear decision boundary.Derive a formula for⇠iwhen the objective function achieves its minimum (No steps neces-sary). Note it is a function ofyiw>xi. Sketch a plot of⇠iwithyiw>xion the x-axis andvalue of⇠ion the y-axis. What is the name of this function?
Figure 2: Plot here
CLASSIFICATION  AND  REGRESSIONThe  Big  Picture
30
Classification  and  Regression:  The  Big  PictureWhiteboard–Decision  Rules  /  Models  (probabilistic  generative,  probabilistic  discriminative,  perceptron,  SVM,  regression)–Objective  Functions  (likelihood,  conditional  likelihood,  hinge  loss,  mean  squared  error)–Regularization(L1,  L2,  priors  for  MAP)–Update  Rules  (SGD,  perceptron)–Nonlinear  Features  (preprocessing,  kernel  trick)31
Q&A
32
Kernels  +  Support  Vector  Machines  (SVMs)
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  12February  27,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
SVM  Readings:Murphy  14.5Bishop  7.1HTF  12  -­‐12.38Mitchell  -­‐-­‐
Reminders•Homework4:  Perceptron/  Kernels/  SVM–Release:  Wed,  Feb.  22–Due:  Fri,  Mar.  03  at  11:59pm•MidtermExam(EveningExam)–Tue,  Mar.  07  at  7:00pm  –9:30pm–SeePiazza  fordetailsaboutlocation•Grading
29  daysfor  HW4
Outline•Kernels–Kernel  Perceptron–Kernel  as  a  dot  product–Gram  matrix–Examples:  Polynomial,  RBF•Support  Vector  Machine  (SVM)–Background:  Constrained  Optimization,  Linearly  Separable,  Margin–SVM  Primal  (Linearly  Separable  Case)–SVM  Primal  (Non-­‐linearly  Separable  Case)–SVM  Dual3
This  Lecture
Last  Lecture
KERNELS
4
Kernels:  MotivationMost  real-­‐world  problems  exhibit  data  that  is  not  linearly  separable.
5Q:  When  your  data  is  not  linearly  separable,  how  can  you  still  use  a  linear  classifier?A:Preprocess  the  data  to  produce  nonlinearfeaturesExample:  pixel  representation  for  Facial  Recognition:

Kernels:  Motivation•Motivation  #1:  Inefficient  Features–Non-­‐linearly  separable  data  requires  high  dimensional  representation–Might  be  prohibitively  expensive  to  compute  or  store•Motivation  #2:  Memory-­‐based  Methods–k-­‐Nearest  Neighbors  (KNN)  for  facial  recognition  allows  a  distance  metricbetween  images  -­‐-­‐no  need  to  worry  about  linearity  restriction  at  all6
KernelsWhiteboard–Kernel  Perceptron–Kernel  as  a  dot  product–Gram  matrix–Examples:  RBF  kernel,  string  kernel
7
Kernel  Methods•Key  idea:  1.Rewritethe  algorithm  so  that  we  only  work  with  dot  productsxTzof  feature  vectors2.Replacethe  dot  products  xTzwith  a  kernel  function  k(x,  z)•The  kernel  k(x,z)  can  be  anylegal  definition  of  a  dot  product:  k(x,  z)  =  φ(x)Tφ(z)  for  any  function  φ:  X àRDSo  we  only  compute  the  φ  dot  product  implicitly•This  “kernel  trick”can  be  applied  to  many  algorithms:–classification:  perceptron,  SVM,  …–regression:  ridge  regression,  …–clustering:  k-­‐means,  …8

Kernel  Methods
9Q:  These  are  just  non-­‐linear  features,  right?A:Yes,  but…Q:  Can’t  we  just  compute  the  feature  transformation  φexplicitly?A:That  depends...Q:  So,  why  all  the  hype  about  the  kernel  trick?A:Because  the  explicit  features  might  either  be  prohibitively  expensive  to  compute  or  infinite  length  vectors
Example:  Polynomial  Kernel
10Slide  from  Nina  BalcanExample For n=2, d=2, the kernel Kx,z=x⋅zd corresponds to  𝑥1,𝑥2→Φ𝑥=(𝑥12,𝑥22,2𝑥1𝑥2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space Example For n=2, d=2, the kernel Kx,z=x⋅zd corresponds to  𝑥1,𝑥2→Φ𝑥=(𝑥12,𝑥22,2𝑥1𝑥2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space Example For n=2, d=2, the kernel Kx,z=x⋅zd corresponds to  𝑥1,𝑥2→Φ𝑥=(𝑥12,𝑥22,2𝑥1𝑥2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space Example ϕ:R2→R3, x1,x2→Φx=(x12,x22,2x1x2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space ϕx⋅ϕ𝑧=x12,x22,2x1x2⋅(𝑧12,𝑧22,2𝑧1𝑧2) =x1𝑧1+x2𝑧22=x⋅𝑧2=K(x,z) Example ϕ:R2→R3, x1,x2→Φx=(x12,x22,2x1x2) x2 x1 O O O O O O O O X X X X X X X X X 
X X X X X X X X X z1 z3 O O O O O O O O O X X X X X X X X X X X X X X X X X X Φ-space Original space ϕx⋅ϕ𝑧=x12,x22,2x1x2⋅(𝑧12,𝑧22,2𝑧1𝑧2) =x1𝑧1+x2𝑧22=x⋅𝑧2=K(x,z) 
Example:  Polynomial  Kernel
11Slide  from  Nina  BalcanAvoid explicitly expanding the features Feature space can grow really large and really quickly…. Crucial to think of ϕ as implicit, not explicit!!!! –𝑥1𝑑, 𝑥1𝑥2…𝑥𝑑, 𝑥12𝑥2…𝑥𝑑−1 –Total number of such feature is 𝑑+𝑛−1𝑑=𝑑+𝑛−1!𝑑!𝑛−1! –𝑑=6,𝑛=100, there are 1.6 billion terms 
•Polynomial kernel degreee 𝑑, 𝑘𝑥,𝑧=𝑥⊤𝑧𝑑=𝜙𝑥⋅𝜙𝑧 
𝑘𝑥,𝑧=𝑥⊤𝑧𝑑=𝜙𝑥⋅𝜙𝑧 
𝑂𝑛  𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛! 
Kernel  ExamplesSide  Note:  The  feature  space  might  not  be  unique!
12Example Note:  feature space might not be unique. 
ϕ:R2→R4, x1,x2→Φx=(x12,x22,x1x2,x2x1) ϕx⋅ϕ𝑧=(x12,x22,x1x2,x2x1)⋅(z12,z22,z1z2,z2z1) =x⋅𝑧2=K(x,z) ϕ:R2→R3, x1,x2→Φx=(x12,x22,2x1x2) ϕx⋅ϕ𝑧=x12,x22,2x1x2⋅(𝑧12,𝑧22,2𝑧1𝑧2) =x1𝑧1+x2𝑧22=x⋅𝑧2=K(x,z) 
Slide  from  Nina  BalcanExplicit  representation  #1:Explicit  representation  #2:These  two  different  feature  representations  correspond  to  the  same  kernel  function!
Kernel  Examples
13NameKernel  Function(implicitdot  product)Feature  Space(explicit  dot  product)LinearSame  as  original  input  spacePolynomial  (v1)All  polynomialsofdegree  dPolynomial(v2)All  polynomialsup  to  degree  dGaussianInfinite  dimensional  spaceHyperbolicTangent  (Sigmoid)  Kernel(WithSVM,  this  is  equivalent  to  a  2-­‐layer  neural  network)

RBF  Kernel  Example
14
RBF  Kernel:
RBF  Kernel  Example
15
RBF  Kernel:
RBF  Kernel  Example
16
RBF  Kernel:
RBF  Kernel  Example
17
RBF  Kernel:
RBF  Kernel  Example
18
RBF  Kernel:
RBF  Kernel  Example
19
RBF  Kernel:
RBF  Kernel  Example
20
RBF  Kernel:
RBF  Kernel  Example
21
RBF  Kernel:
RBF  Kernel  Example
22
RBF  Kernel:
RBF  Kernel  Example
23
RBF  Kernel:
RBF  Kernel  Example
24
RBF  Kernel:
RBF  Kernel  Example
25
RBF  Kernel:
RBF  Kernel  Example
26
RBF  Kernel:KNN  vs.  SVM
RBF  Kernel  Example
27
RBF  Kernel:KNN  vs.  SVM
RBF  Kernel  Example
28
RBF  Kernel:KNN  vs.  SVM
RBF  Kernel  Example
29
RBF  Kernel:KNN  vs.  SVM
Example:  String  KernelSetup:–Input  instances  xare  strings  of  characters  (e.g.  x(3)=  [‘s’,  ‘a’,  ‘t’],  x(7)=  [‘c’,  ‘a’,  ‘t’]  –Want  indicator  features  for  the  presence  /  absence  of  each  possible  substring  up  to  length  KQuestions:1.What  is  the  best  runtimeof  a  single  Standard  Perceptronupdate?2.What  is  the  best  runtimeof  a  single  Kernel  Perceptronupdate?30
Kernels, Discussion •If  all computations involving instances are in terms of inner products then: Conceptually, work in a very high diml space and the alg’s performance depends only on linear separability in that extended space.  Computationally, only need to modify the algo by replacing each x⋅z with a Kx,z. How to choose a kernel: •Use Cross-Validation to choose the parameters, e.g., 𝜎  for Gaussian Kernel  Kx,𝑧=exp−𝑥−𝑧22 𝜎2  •Learn a good kernel; e.g.,  [Lanckriet-Cristianini-Bartlett-El Ghaoui-Jordan’04] •Kernels often encode domain knowledge (e.g., string kernels) Kernels:  Discussion
31Slide  from  Nina  Balcan
SUPPORT  VECTOR  MACHINE  (SVM)32
SVM:  Optimization  BackgroundWhiteboard–Constrained  Optimization–Linear  programming–Quadratic  programming–Example:  2D  quadratic  function  with  linear  constraints
33
Quadratic  Program
34
Quadratic  Program
35
Quadratic  Program
36
Quadratic  Program
37
Quadratic  Program
38
SVMWhiteboard–SVM  Primal  (Linearly  Separable  Case)–SVM  Primal  (Non-­‐linearly  Separable  Case)
39
SVM  QP
40
SVM  QP
41
SVM  QP
42
SVM  QP
43
SVM  QP
44
SVM  QP
45
Support Vector Machines (SVMs) Input: S={(x1,𝑦1), …,(xm,𝑦m)};  argminw,𝜉1,…,𝜉𝑚 𝑤2+𝐶 𝜉𝑖𝑖 s.t.:   •For all i, 𝑦𝑖𝑤⋅𝑥𝑖≥1−𝜉𝑖 Which is equivalent to: Find 𝜉𝑖≥0 Primal form Input: S={(x1,y1), …,(xm,ym)};  argminα12  yiyj αiαjxi⋅xj− αiiji s.t.:   •For all i,  Find 0≤αi≤Ci Lagrangian Dual  yiαi=0i Can  be kernelized!!! 
46Slide  from  Nina  Balcan
SVMs (Lagrangian Dual) 
•Final classifier is: w= αiyixii  •The points xi for which αi≠0 are called the “support vectors” Input: S={(x1,y1), …,(xm,ym)};  argminα12  yiyj αiαjxi⋅xj− αiiji s.t.:   •For all i,  Find 0≤αi≤Ci  yiαi=0i + + + + - - - - - + - - - - w 𝑤⋅𝑥=−1 𝑤⋅𝑥=1 
47Slide  from  Nina  Balcan
SVM  Takeaways•Maximizing  the  margin  of  a  linear  separator  is  a  good  training  criteria•Support  Vector  Machines  (SVMs)  learn  a  max-­‐margin  linear  classifier•The  SVM  optimization  problem  can  be  solved  with  black-­‐box  Quadratic  Programming  (QP)  solvers•Learned  decision  boundary  is  defined  by  its  support  vectors48
Language Modeling
Michael Collins, Columbia University
Overview
IThe language modeling problem
ITrigram models
IEvaluating language models: perplexity
IEstimation techniques:
ILinear interpolation
IDiscounting methods
The Language Modeling Problem
IWe have some (nite) vocabulary,
sayV=fthe, a, man, telescope, Beckham, two ;:::g
IWe have an (innite) set of strings, Vy
the STOP
a STOP
the fan STOP
the fan saw Beckham STOP
the fan saw saw STOP
the fan saw Beckham play for Real Madrid STOP
The Language Modeling Problem (Continued)
IWe have a training sample of example sentences in
EnglishIWe need to \learn" a probability distribution p
i.e.,pis a function that satises
X
x2Vyp(x) = 1; p (x)0for allx2Vy
p(the STOP ) = 10 12
p(the fan STOP ) = 10 8
p(the fan saw Beckham STOP ) = 210 8
p(the fan saw saw STOP ) = 10 15
:::
p(the fan saw Beckham play for Real Madrid STOP ) = 210 9
:::
The Language Modeling Problem (Continued)
IWe have a training sample of example sentences in
English
IWe need to \learn" a probability distribution p
i.e.,pis a function that satises
X
x2Vyp(x) = 1; p (x)0for allx2Vyp(the STOP ) = 10 12
p(the fan STOP ) = 10 8
p(the fan saw Beckham STOP ) = 210 8
p(the fan saw saw STOP ) = 10 15
:::
p(the fan saw Beckham play for Real Madrid STOP ) = 210 9
:::
The Language Modeling Problem (Continued)
IWe have a training sample of example sentences in
English
IWe need to \learn" a probability distribution p
i.e.,pis a function that satises
X
x2Vyp(x) = 1; p (x)0for allx2Vy
p(the STOP ) = 10 12
p(the fan STOP ) = 10 8
p(the fan saw Beckham STOP ) = 210 8
p(the fan saw saw STOP ) = 10 15
:::
p(the fan saw Beckham play for Real Madrid STOP ) = 210 9
:::
Why on earth would we want to do this?!
ISpeech recognition was the original motivation.
(Related problems are optical character recognition,
handwriting recognition.)IThe estimation techniques developed for this problem will
beVERY useful for other problems in NLP
Why on earth would we want to do this?!
ISpeech recognition was the original motivation.
(Related problems are optical character recognition,
handwriting recognition.)
IThe estimation techniques developed for this problem will
beVERY useful for other problems in NLP
A Naive Method
IWe haveNtraining sentences
IFor any sentence x1:::xn,c(x1:::xn)is the number of
times the sentence is seen in our training data
IA naive estimate:
p(x1:::xn) =c(x1:::xn)
N
Overview
IThe language modeling problem
ITrigram models
IEvaluating language models: perplexity
IEstimation techniques:
ILinear interpolation
IDiscounting methods
Markov Processes
IConsider a sequence of random variables X1;X2;:::Xn.
Each random variable can take any value in a nite set V.
For now we assume the length nis xed (e.g., n= 100 ).
IOur goal: model
P(X1=x1;X2=x2;:::;Xn=xn)
First-Order Markov Processes
P(X1=x1;X2=x2;:::Xn=xn)=P(X1=x1)nY
i=2P(Xi=xijX1=x1;:::;Xi 1=xi 1)
=P(X1=x1)nY
i=2P(Xi=xijXi 1=xi 1)
The rst-order Markov assumption: For any i2f2:::ng, for
anyx1:::xi,
P(Xi=xijX1=x1:::Xi 1=xi 1) =P(Xi=xijXi 1=xi 1)
First-Order Markov Processes
P(X1=x1;X2=x2;:::Xn=xn)
=P(X1=x1)nY
i=2P(Xi=xijX1=x1;:::;Xi 1=xi 1)=P(X1=x1)nY
i=2P(Xi=xijXi 1=xi 1)
The rst-order Markov assumption: For any i2f2:::ng, for
anyx1:::xi,
P(Xi=xijX1=x1:::Xi 1=xi 1) =P(Xi=xijXi 1=xi 1)
First-Order Markov Processes
P(X1=x1;X2=x2;:::Xn=xn)
=P(X1=x1)nY
i=2P(Xi=xijX1=x1;:::;Xi 1=xi 1)
=P(X1=x1)nY
i=2P(Xi=xijXi 1=xi 1)The rst-order Markov assumption: For any i2f2:::ng, for
anyx1:::xi,
P(Xi=xijX1=x1:::Xi 1=xi 1) =P(Xi=xijXi 1=xi 1)
First-Order Markov Processes
P(X1=x1;X2=x2;:::Xn=xn)
=P(X1=x1)nY
i=2P(Xi=xijX1=x1;:::;Xi 1=xi 1)
=P(X1=x1)nY
i=2P(Xi=xijXi 1=xi 1)
The rst-order Markov assumption: For any i2f2:::ng, for
anyx1:::xi,
P(Xi=xijX1=x1:::Xi 1=xi 1) =P(Xi=xijXi 1=xi 1)
Second-Order Markov Processes
P(X1=x1;X2=x2;:::Xn=xn)=P(X1=x1)P(X2=x2jX1=x1)
nY
i=3P(Xi=xijXi 2=xi 2;Xi 1=xi 1)
=nY
i=1P(Xi=xijXi 2=xi 2;Xi 1=xi 1)
(For convenience we assume x0=x 1=*, where * is a
special \start" symbol.)
Second-Order Markov Processes
P(X1=x1;X2=x2;:::Xn=xn)
=P(X1=x1)P(X2=x2jX1=x1)
nY
i=3P(Xi=xijXi 2=xi 2;Xi 1=xi 1)=nY
i=1P(Xi=xijXi 2=xi 2;Xi 1=xi 1)
(For convenience we assume x0=x 1=*, where * is a
special \start" symbol.)
Second-Order Markov Processes
P(X1=x1;X2=x2;:::Xn=xn)
=P(X1=x1)P(X2=x2jX1=x1)
nY
i=3P(Xi=xijXi 2=xi 2;Xi 1=xi 1)
=nY
i=1P(Xi=xijXi 2=xi 2;Xi 1=xi 1)
(For convenience we assume x0=x 1=*, where * is a
special \start" symbol.)
Modeling Variable Length Sequences
IWe would like the length of the sequence, n, to also be a
random variable
IA simple solution: always dene Xn=STOP where
STOP is a special symbolIThen use a Markov process as before:
P(X1=x1;X2=x2;:::Xn=xn)
=nY
i=1P(Xi=xijXi 2=xi 2;Xi 1=xi 1)
(For convenience we assume x0=x 1=*, where * is a
special \start" symbol.)
Modeling Variable Length Sequences
IWe would like the length of the sequence, n, to also be a
random variable
IA simple solution: always dene Xn=STOP where
STOP is a special symbol
IThen use a Markov process as before:
P(X1=x1;X2=x2;:::Xn=xn)
=nY
i=1P(Xi=xijXi 2=xi 2;Xi 1=xi 1)
(For convenience we assume x0=x 1=*, where * is a
special \start" symbol.)
Trigram Language Models
IA trigram language model consists of:
1. A nite setV
2. A parameter q(wju;v)for each trigram u;v;w such that
w2V[f STOPg, andu;v2V[f *g.IFor any sentence x1:::xnwherexi2V for
i= 1:::(n 1), andxn=STOP, the probability of the
sentence under the trigram language model is
p(x1:::xn) =nY
i=1q(xijxi 2;xi 1)
where we dene x0=x 1=*.
Trigram Language Models
IA trigram language model consists of:
1. A nite setV
2. A parameter q(wju;v)for each trigram u;v;w such that
w2V[f STOPg, andu;v2V[f *g.
IFor any sentence x1:::xnwherexi2V for
i= 1:::(n 1), andxn=STOP, the probability of the
sentence under the trigram language model is
p(x1:::xn) =nY
i=1q(xijxi 2;xi 1)
where we dene x0=x 1=*.
An Example
For the sentence
the dog barks STOP
we would have
p(the dog barks STOP ) =q(thej*, *)
q(dogj*, the )
q(barksjthe, dog )
q(STOPjdog, barks )
The Trigram Estimation Problem
Remaining estimation problem:
q(wijwi 2;wi 1)
For example:
q(laughsjthe, dog )A natural estimate (the \maximum likelihood estimate"):
q(wijwi 2;wi 1) =Count (wi 2;wi 1;wi)
Count (wi 2;wi 1)
q(laughsjthe, dog ) =Count (the, dog, laughs )
Count (the, dog )
The Trigram Estimation Problem
Remaining estimation problem:
q(wijwi 2;wi 1)
For example:
q(laughsjthe, dog )
A natural estimate (the \maximum likelihood estimate"):
q(wijwi 2;wi 1) =Count (wi 2;wi 1;wi)
Count (wi 2;wi 1)
q(laughsjthe, dog ) =Count (the, dog, laughs )
Count (the, dog )
Sparse Data Problems
A natural estimate (the \maximum likelihood estimate"):
q(wijwi 2;wi 1) =Count (wi 2;wi 1;wi)
Count (wi 2;wi 1)
q(laughsjthe, dog ) =Count (the, dog, laughs )
Count (the, dog )
Say our vocabulary size is N=jVj, then there are N3
parameters in the model.
e.g.,N= 20;000) 20;0003= 81012parameters
Overview
IThe language modeling problem
ITrigram models
IEvaluating language models: perplexity
IEstimation techniques:
ILinear interpolation
IDiscounting methods
Evaluating a Language Model: Perplexity
IWe have some test data, msentences
s1;s2;s3;:::;smIWe could look at the probability under our modelQm
i=1p(si). Or more conveniently, the log probability
logmY
i=1p(si) =mX
i=1logp(si)
IIn fact the usual evaluation measure is perplexity
Perplexity = 2 lwherel=1
MmX
i=1logp(si)
andMis the total number of words in the test data.
Evaluating a Language Model: Perplexity
IWe have some test data, msentences
s1;s2;s3;:::;sm
IWe could look at the probability under our modelQm
i=1p(si). Or more conveniently, the log probability
logmY
i=1p(si) =mX
i=1logp(si)IIn fact the usual evaluation measure is perplexity
Perplexity = 2 lwherel=1
MmX
i=1logp(si)
andMis the total number of words in the test data.
Evaluating a Language Model: Perplexity
IWe have some test data, msentences
s1;s2;s3;:::;sm
IWe could look at the probability under our modelQm
i=1p(si). Or more conveniently, the log probability
logmY
i=1p(si) =mX
i=1logp(si)
IIn fact the usual evaluation measure is perplexity
Perplexity = 2 lwherel=1
MmX
i=1logp(si)
andMis the total number of words in the test data.
Some Intuition about Perplexity
ISay we have a vocabulary V, andN=jVj+ 1
and model that predicts
q(wju;v) =1
N
for allw2V[f STOPg, for allu;v2V[f *g.
IEasy to calculate the perplexity in this case:
Perplexity = 2 lwherel= log1
N
)
Perplexity =N
Perplexity is a measure of eective \branching factor"
Typical Values of Perplexity
IResults from Goodman (\A bit of progress in language
modeling"), where jVj= 50;000
IA trigram model: p(x1:::xn) =Qn
i=1q(xijxi 2;xi 1).
Perplexity = 74IA bigram model: p(x1:::xn) =Qn
i=1q(xijxi 1).
Perplexity = 137
IA unigram model: p(x1:::xn) =Qn
i=1q(xi).
Perplexity = 955
Typical Values of Perplexity
IResults from Goodman (\A bit of progress in language
modeling"), where jVj= 50;000
IA trigram model: p(x1:::xn) =Qn
i=1q(xijxi 2;xi 1).
Perplexity = 74
IA bigram model: p(x1:::xn) =Qn
i=1q(xijxi 1).
Perplexity = 137IA unigram model: p(x1:::xn) =Qn
i=1q(xi).
Perplexity = 955
Typical Values of Perplexity
IResults from Goodman (\A bit of progress in language
modeling"), where jVj= 50;000
IA trigram model: p(x1:::xn) =Qn
i=1q(xijxi 2;xi 1).
Perplexity = 74
IA bigram model: p(x1:::xn) =Qn
i=1q(xijxi 1).
Perplexity = 137
IA unigram model: p(x1:::xn) =Qn
i=1q(xi).
Perplexity = 955
Some History
IShannon conducted experiments on entropy of English
i.e., how good are people at the perplexity game?
C. Shannon. Prediction and entropy of printed
English. Bell Systems Technical Journal,
30:50{64, 1951.
Some History
Chomsky (in Syntactic Structures (1957)):
Second, the notion \grammatical" cannot be identied with
\meaningful" or \signicant" in any semantic sense.
Sentences (1) and (2) are equally nonsensical, but any speaker
of English will recognize that only the former is grammatical.
(1) Colorless green ideas sleep furiously.
(2) Furiously sleep ideas green colorless.
:::
:::Third, the notion \grammatical in English" cannot be
identied in any way with the notion \high order of statistical
approximation to English". It is fair to assume that neither
sentence (1) nor (2) (nor indeed any part of these sentences)
has ever occurred in an English discourse. Hence, in any
statistical model for grammaticalness, these sentences will be
ruled out on identical grounds as equally `remote' from
English. Yet (1), though nonsensical, is grammatical, while
(2) is not.:::
Overview
IThe language modeling problem
ITrigram models
IEvaluating language models: perplexity
IEstimation techniques:
ILinear interpolation
IDiscounting methods
Sparse Data Problems
A natural estimate (the \maximum likelihood estimate"):
q(wijwi 2;wi 1) =Count (wi 2;wi 1;wi)
Count (wi 2;wi 1)
q(laughsjthe, dog ) =Count (the, dog, laughs )
Count (the, dog )
Say our vocabulary size is N=jVj, then there are N3
parameters in the model.
e.g.,N= 20;000) 20;0003= 81012parameters
The Bias-Variance Trade-O
ITrigram maximum-likelihood estimate
qML(wijwi 2;wi 1) =Count (wi 2;wi 1;wi)
Count (wi 2;wi 1)
IBigram maximum-likelihood estimate
qML(wijwi 1) =Count (wi 1;wi)
Count (wi 1)
IUnigram maximum-likelihood estimate
qML(wi) =Count (wi)
Count ()
Linear Interpolation
ITake our estimate q(wijwi 2;wi 1)to be
q(wijwi 2;wi 1) =1qML(wijwi 2;wi 1)
+2qML(wijwi 1)
+3qML(wi)
where1+2+3= 1, andi0for alli.
Linear Interpolation (continued)
Our estimate correctly denes a distribution (dene
V0=V[f STOPg):
P
w2V0q(wju;v)=P
w2V0[1qML(wju;v) +2qML(wjv) +3qML(w)]
=1P
wqML(wju;v) +2P
wqML(wjv) +3P
wqML(w)
=1+2+3
= 1
(Can show also that q(wju;v)0for allw2V0)
Linear Interpolation (continued)
Our estimate correctly denes a distribution (dene
V0=V[f STOPg):
P
w2V0q(wju;v)
=P
w2V0[1qML(wju;v) +2qML(wjv) +3qML(w)]=1P
wqML(wju;v) +2P
wqML(wjv) +3P
wqML(w)
=1+2+3
= 1
(Can show also that q(wju;v)0for allw2V0)
Linear Interpolation (continued)
Our estimate correctly denes a distribution (dene
V0=V[f STOPg):
P
w2V0q(wju;v)
=P
w2V0[1qML(wju;v) +2qML(wjv) +3qML(w)]
=1P
wqML(wju;v) +2P
wqML(wjv) +3P
wqML(w)=1+2+3
= 1
(Can show also that q(wju;v)0for allw2V0)
Linear Interpolation (continued)
Our estimate correctly denes a distribution (dene
V0=V[f STOPg):
P
w2V0q(wju;v)
=P
w2V0[1qML(wju;v) +2qML(wjv) +3qML(w)]
=1P
wqML(wju;v) +2P
wqML(wjv) +3P
wqML(w)
=1+2+3= 1
(Can show also that q(wju;v)0for allw2V0)
Linear Interpolation (continued)
Our estimate correctly denes a distribution (dene
V0=V[f STOPg):
P
w2V0q(wju;v)
=P
w2V0[1qML(wju;v) +2qML(wjv) +3qML(w)]
=1P
wqML(wju;v) +2P
wqML(wjv) +3P
wqML(w)
=1+2+3
= 1(Can show also that q(wju;v)0for allw2V0)
Linear Interpolation (continued)
Our estimate correctly denes a distribution (dene
V0=V[f STOPg):
P
w2V0q(wju;v)
=P
w2V0[1qML(wju;v) +2qML(wjv) +3qML(w)]
=1P
wqML(wju;v) +2P
wqML(wjv) +3P
wqML(w)
=1+2+3
= 1
(Can show also that q(wju;v)0for allw2V0)
How to estimate the values?
IHold out part of training set as \validation" dataIDenec0(w1;w2;w3)to be the number of times the
trigram (w1;w2;w3)is seen in validation set
IChoose1;2;3to maximize:
L(1;2;3) =X
w1;w2;w3c0(w1;w2;w3) logq(w3jw1;w2)
such that1+2+3= 1, andi0for alli, and
where
q(wijwi 2;wi 1) =1qML(wijwi 2;wi 1)
+2qML(wijwi 1)
+3qML(wi)
How to estimate the values?
IHold out part of training set as \validation" data
IDenec0(w1;w2;w3)to be the number of times the
trigram (w1;w2;w3)is seen in validation setIChoose1;2;3to maximize:
L(1;2;3) =X
w1;w2;w3c0(w1;w2;w3) logq(w3jw1;w2)
such that1+2+3= 1, andi0for alli, and
where
q(wijwi 2;wi 1) =1qML(wijwi 2;wi 1)
+2qML(wijwi 1)
+3qML(wi)
How to estimate the values?
IHold out part of training set as \validation" data
IDenec0(w1;w2;w3)to be the number of times the
trigram (w1;w2;w3)is seen in validation set
IChoose1;2;3to maximize:
L(1;2;3) =X
w1;w2;w3c0(w1;w2;w3) logq(w3jw1;w2)
such that1+2+3= 1, andi0for alli, and
where
q(wijwi 2;wi 1) =1qML(wijwi 2;wi 1)
+2qML(wijwi 1)
+3qML(wi)
Allowing the 's to vary
ITake a function that partitions histories
e.g.,
(wi 2;wi 1) =8
>><
>>:1If Count (wi 1;wi 2) = 0
2If1Count (wi 1;wi 2)2
3If3Count (wi 1;wi 2)5
4Otherwise
IIntroduce a dependence of the 's on the partition:
q(wijwi 2;wi 1) =(wi 2;wi 1)
1qML(wijwi 2;wi 1)
+(wi 2;wi 1)
2qML(wijwi 1)
+(wi 2;wi 1)
3qML(wi)
where(wi 2;wi 1)
1 +(wi 2;wi 1)
2 +(wi 2;wi 1)
3 = 1,
and(wi 2;wi 1)
i0for alli.
Overview
IThe language modeling problem
ITrigram models
IEvaluating language models: perplexity
IEstimation techniques:
ILinear interpolation
IDiscounting methods
Discounting Methods
ISay we've seen the following counts:
x Count (x)qML(wijwi 1)
the 48
the, dog 15 15/48
the, woman 11 11/48
the, man 10 10/48
the, park 5 5/48
the, job 2 2/48
the, telescope 1 1/48
the, manual 1 1/48
the, afternoon 1 1/48
the, country 1 1/48
the, street 1 1/48
IThe maximum-likelihood estimates are high
(particularly for low count items)
Discounting Methods
INow dene \discounted" counts,
Count(x) =Count (x) 0:5
INew estimates:
x Count (x)Count(x)Count(x)
Count (the)
the 48
the, dog 15 14.5 14.5/48
the, woman 11 10.5 10.5/48
the, man 10 9.5 9.5/48
the, park 5 4.5 4.5/48
the, job 2 1.5 1.5/48
the, telescope 1 0.5 0.5/48
the, manual 1 0.5 0.5/48
the, afternoon 1 0.5 0.5/48
the, country 1 0.5 0.5/48
the, street 1 0.5 0.5/48
Discounting Methods (Continued)
IWe now have some \missing probability mass":
(wi 1) = 1 X
wCount(wi 1;w)
Count (wi 1)
e.g., in our example, (the) = 100:5=48 = 5=48
Katz Back-O Models (Bigrams)
IFor a bigram model, dene two sets
A(wi 1) =fw:Count (wi 1;w)>0g
B(wi 1) =fw:Count (wi 1;w) = 0g
IA bigram model
qBO(wijwi 1) =8
>><
>>:Count(wi 1;wi)
Count (wi 1)Ifwi2A(wi 1)
(wi 1)qML(wi)P
w2B(wi 1)qML(w)Ifwi2B(wi 1)
where
(wi 1) = 1 X
w2A(wi 1)Count(wi 1;w)
Count (wi 1)
Katz Back-O Models (Trigrams)
IFor a trigram model, rst dene two sets
A(wi 2;wi 1) =fw:Count (wi 2;wi 1;w)>0g
B(wi 2;wi 1) =fw:Count (wi 2;wi 1;w) = 0g
IA trigram model is dened in terms of the bigram model:
qBO(wijwi 2;wi 1) =8
>>>>>><
>>>>>>:Count(wi 2;wi 1;wi)
Count (wi 2;wi 1)
Ifwi2A(wi 2;wi 1)
(wi 2;wi 1)qBO(wijwi 1)P
w2B(wi 2;wi 1)qBO(wjwi 1)
Ifwi2B(wi 2;wi 1)
where
(wi 2;wi 1) = 1 X
w2A(wi 2;wi 1)Count(wi 2;wi 1;w)
Count (wi 2;wi 1)
Summary
IThree steps in deriving the language model probabilities:
1. Expandp(w1;w2:::wn)using Chain rule.
2. Make Markov Independence Assumptions
p(wijw1;w2:::wi 2;wi 1) =p(wijwi 2;wi 1)
3. Smooth the estimates using low order counts.
IOther methods used to improve language models:
I\Topic" or \long-range" features.
ISyntactic models.
It's generally hard to improve on trigram models though!!
Loss-augmented	Structured	PredictionCMSC	723	/	LING	723	/	INST	725Marine	CarpuatFigures,	algorithms	&	equationsfrom	CIML	chap	17
POS	taggingSequence	labeling	with	the	perceptronSequence	labeling	problem•Input:•sequence	of	tokens	x	=	[x1…xL]•Variable	length	L•Output	(aka	label):	•sequence	of	tags	y	=	[y1…yL]•#	tags	=	K•Size	of	output	space?Structured	Perceptron•Perceptron	algorithm	can	be	used	for	sequence	labeling•But	there	are	challenges•How	to	compute	argmaxefficiently?•What	are	appropriate	features?•Approach:	leverage	structure	of	output	space
Solving	the	argmaxproblem	for	sequences	with	dynamic	programming•Efficient	algorithms	possible	if	the	feature	function	decomposes	over	the	input•This	holds	for	unary	and	markovfeatures	used	for	POS	tagging

Feature	functions	for	sequence	labeling•Standard	features	of	POS	tagging•Unary	features:#	times	word	w	has	been	labeled	with	tag	l	for	all	words	w	and	all	tags	l•Markov	features:#	times	tag	l	is	adjacent	to	tag	l’	in	output	for	all	tags	l	and	l’•Size	of	feature	representation	is	constant	wrtinput	length

Solving	the	argmaxproblem	for	sequences•Trellis	sequence	labeling•Any	path	represents	a	labeling	of	input	sentence	•Gold	standard	path	in	red•Each	edge	receives	a	weight	such	that	adding	weights	along	the	path	corresponds	to	score	for	input/ouputconfiguration•Any	max-weight	max-weight	path	algorithm	can	find	the	argmax•e.g.	Viterbi	algorithm	O(LK2)

Defining	weights	of	edge	in	treillis
•Weight	of	edge	that	goes	from	time	l-1	to	time	l,	and	transitions	from	y	to	y’Unary	features	at	position	l	together	with	Markov	features	that	end	at	position	l

Dynamic	program•Define:	the	score	of	best	possible	output	prefix	up	to	and	including	position	l	that	labels	the	l-thword	with	label	k•With	decomposable	features,	alphas	can	be	computed	recursively


A	more	general	approach	for	argmaxInteger	Linear	Programming•ILP:	optimization	problem	of	the	form,	for	a	fixed	vector	a•With	integer	constraints•Pro:	can	leverage	well-engineered	solvers	(e.g.,	Gurobi)•Con:	not	always	most	efficient

POS	tagging	as	ILP•Markov	features	as	binary	indicator	variables•Output	sequence:	y(z)	obtained	by	reading	off	variables	z•Define	a	such	that	a.zis	equal	to	score•Enforcing	constraints	for	well	formed	solutions

Sequence	labeling•Structured	perceptron•A	general	algorithm	for	structured	prediction	problems	such	as	sequence	labeling•The	Argmaxproblem•Efficient	argmaxfor	sequences	with	Viterbi	algorithm,	given	some	assumptions	on	feature	structure•A	more	general	solution:	Integer	Linear	Programming•Loss-augmented	structured	prediction•Training	algorithm•Loss-augmented	argmax
In	structured	perceptron,	all	errors	are	equally	bad

All	bad	output	sequences	are	not	equally	bad
•Consider	•𝑦"#=𝐴,𝐴,𝐴,𝐴•𝑦'#=[𝑁,𝑉,𝑁,𝑁]
•Hamming	Loss•Gives	a	more	nuanced	evaluation	of	output	than	0–1	loss

Loss	functions	for	structured	prediction•Recall	learning	as	optimization	for	classification•e.g.,	•Let’s	define	a	structure-aware	optimization	objective•e.g.,	
Structured	hinge	loss•0	if		true	output	beats	score	of	every	imposter	output•Otherwise:	scales	linearly	as	function	of	score	diff	between	most	confusing	imposter	and	true	output
Optimization:	stochastic	subgradientdescent•Subgradientsof	structured	hinge	loss?

Optimization:	stochastic	subgradientdescent•subgradientsof	structured	hinge	loss

Optimization:	stochastic	subgradientdescentResulting	training	algorithm
Only	2	differences	compared	to	structured	perceptron!
Loss-augmented	inference/searchRecall	dynamic	programming	solution	without	Hamming	loss

Loss-augmented	inference/search	Dynamic	programming	with	Hamming	loss
We	can	use	Viterbi	algorithm	as	before	as	long	as	the	loss	function	decomposes	over	the	input	consistently	w	features!

Sequence	labeling•Structured	perceptron•A	general	algorithm	for	structured	prediction	problems	such	as	sequence	labeling•The	Argmaxproblem•Efficient	argmaxfor	sequences	with	Viterbi	algorithm,	given	some	assumptions	on	feature	structure•A	more	general	solution:	Integer	Linear	Programming•Loss-augmented	structured	prediction•Training	algorithm•Loss-augmented	argmax
Syntax	&	GrammarsFrom	Sequences	to	Trees

Syntax	&	Grammar•Syntax•From	Greek	syntaxis,	meaning	“setting	out	together”•refers	to	the	way	words	are	arranged	together.	•Grammar•Set	of	structural	rules	governing	composition	of		clauses,	phrases,	and	words	in	any	given	natural	language•Descriptive,	not	prescriptive•Panini’s	grammar	of	Sanskrit	~2000	years	ago
Syntax	and	Grammar•Goal	of	syntactic	theory•“explain	how	people	combine	words	to	form	sentences	and	how	children	attain	knowledge	of	sentence	structure”•Grammar	•implicit	knowledge	of	a	native	speaker•acquired	without	explicit	instruction•minimally	able	to	generate	all	and	only	the	possible	sentences	of	the	language[Philips,	2003]
Syntax	in	NLP•Syntactic	analysis	often	a	key	component	in	applications•Grammar	checkers•Dialogue	systems•Question	answering	•Information	extraction•Machine	translation•…
Two	views	of	syntactic	structure•Constituency	(phrase	structure)•Phrase	structure	organizes	words	in	nested	constituents•Dependency	structure•Shows	which	words	depend	on	(modify	or	are	arguments	of)	which	on	other	words
Constituency•Basic	idea:	groups	of	words	act	as	a	single	unit•Constituents	form	coherent	classes	that	behave	similarly•With	respect	to	their	internal	structure:	e.g.,	at	the	core	of	a	noun	phrase	is	a	noun•With	respect	to	other	constituents:	e.g.,	noun	phrases	generally	occur	before	verbs
Constituency:	Example•The	following	are	all	noun	phrases	in	English...•Why?	•They	can	all	precede	verbs•They	can	all	be	preposed/postposed•…

Grammars	and	Constituency•For	a	particular	language:•What	are	the	“right”	set	of	constituents?•What	rules	govern	how	they	combine?•Answer:	not	obvious	and	difficult•That’s	why	there	are	many	different	theories	of	grammar	and	competing	analyses	of	the	same	data!•Our	approach•Focus	primarily	on	the	“machinery”
Context-Free	Grammars•Context-free	grammars	(CFGs)•Aka	phrase	structure	grammars•Aka	Backus-Naur	form	(BNF)•Consist	of•Rules	•Terminals•Non-terminals
Context-Free	Grammars•Terminals•We’ll	take	these	to	be	words•Non-Terminals•The	constituents	in	a	language	(e.g.,	noun	phrase)•Rules•Consist	of	a	single	non-terminal	on	the	left	and	any	number	of	terminals	and	non-terminals	on	the	right
An	Example	Grammar

Parse	Tree:	Example
Note:	equivalence	between	parse	trees	and	bracket	notation
Dependency	Grammars•CFGs	focus	on	constituents•Non-terminals	don’t	actually	appear	in	the	sentence•In	dependency	grammar,	a	parse	is	a	graph	(usually	a	tree)	where:•Nodes	represent	words•Edges	represent	dependency	relations	between	words	(typed	or	untyped,	directed	or	undirected)
Dependency	Grammars•Syntactic	structure	=	lexical	items	linked	by	binary	asymmetrical	relations	called	dependencies	

Dependency	Relations

Example	Dependency	Parse
They	hid	the	letter	on	the	shelfCompare	with	constituent	parse…	What’s	the	relation?


Universal	Dependencies	project•Set	of	dependency	relations	that	are•Linguistically	motivated•Computationally	useful•Cross-linguistically	applicable•[Nivreet	al.	2016]•Universaldependencies.org
Summary•Syntax	&	Grammar•Two	views	of	syntactic	structures•Context-Free	Grammars•Dependency	grammars•Can	be	used	to	capture	various	facts	about	the	structure	of	language	(but	not	all!)•Treebanksas	an	important	resource	for	NLP
CS224d:	Deep	NLPLecture	13:Convolutional	Neural	Networks	(for	NLP)Richard	Socherrichard@metamind.io
Overview	of	today•From	RNNs	to	CNNs•CNN	Variant	1:	Simple	single	layer•Application:	Sentence	classification•More	details	and	tricks•Evaluation•Comparison	between	sentence	models:	BoV,	RNNs2,	CNNs•CNN	Variant	2:	Complex	multi	layer5/12/16Richard	Socher
From	RNNs	to	CNNs	
5/12/16Richard	Socherthe		country							of							my		birth0.40.32.33.644.5772.13.32.53.85.56.113.515
the		country							of							my		birth0.40.32.33.644.5772.13.34.53.85.56.113.5152.53.8
From	RNNs	to	CNNs	
5/12/16Richard	Socher•Recursive	neural	netsrequire	a	parser	to	gettree	structure•Recurrent	neural	netscannot	capture	phraseswithout	prefix	contextand	often	capture	too	muchof	last	words	in	final	vectorthe		country							of							my		birth0.40.32.33.644.5772.13.32.53.85.56.113.515
the		country							of							my		birth0.40.32.33.644.5772.13.34.53.85.56.113.5152.53.8
From	RNNs	to	CNNs	
5/12/16Richard	Socher•RNN:	Get	compositional	vectors	for	grammatical	phrases	only	•CNN:	What	if	we	compute	vectors	for	every	possible	phrase?•Example:	“the	country	of	my	birth”	computes	vectors	for:•the	country,	country	of,	of	my,	my	birth,	the	country	of,	country	of	my,	of	my	birth,	the	country	of	my,	country	of	my	birth•Regardless	of	whether	it	is	grammatical•Wouldn’t	need	parser•Not	very	linguistically	or	cognitively	plausible
What	is	convolution	anyway?
5/12/16Richard	Socher•1d	discrete	convolution	generally:•Convolution	is	great	to	extract	features	from	images•2d	example	à•Yellow	shows	filter	weights•Green	shows	input
Stanford	UFLDL	wiki

From	RNNs	to	CNNs	
5/12/16Richard	Socher•First	layer:	compute	all	bigram	vectors
•Same	computation	as	in	RNN	but	for	every	pair•This	can	be	interpreted	as	a	convolution	over	the	word	vectorsthe		country							of							my		birth0.40.32.33.644.5772.13.32.53.85.56.113.515

From	RNNs	to	CNNs	
5/12/16Richard	Socher•Now	multiple	options	to	compute	higher	layers.•First	option	(simple	to	understand	but	not	necessarily	best)•Just	repeat	with	different	weights:
the		country							of							my		birth0.40.32.33.644.5772.13.32.53.85.56.113.51523.545.513.5

From	RNNs	to	CNNs	
5/12/16Richard	Socher•First	option	(simple	to	understand	but	not	necessarily	best)
the		country							of							my		birth0.40.32.33.644.5772.13.32.53.85.56.113.51523.545.513.523.545.5
From	RNNs	to	CNNs	
5/12/16Richard	Socher•First	option	(simple	to	understand	but	not	necessarily	best)
the		country							of							my		birth0.40.32.33.644.5772.13.32.53.85.56.113.51523.545.513.523.545.533.5
Single	Layer	CNN•A	simple	variant	using	one	convolutional	layer	and	pooling	•Based	on	Collobertand	Weston	(2011)	and	Kim	(2014)	“Convolutional	Neural	Networks	for	Sentence	Classification”•Word	vectors:	•Sentence:(vectors	concatenated)•Concatenation	of	words	in	range:	•Convolutional	filter:	(goes	over	window	of	h	words)•Could	be	2	(as	before)	higher,	e.g.	3:
5/12/16Richard	SocherProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751,October 25-29, 2014, Doha, Qatar.c 2014 Association for Computational LinguisticsConvolutional Neural Networks for Sentence ClassiﬁcationYoon KimNew York Universityyhk255@nyu.eduAbstractWe report on a series of experiments withconvolutional neural networks (CNN)trained on top of pre-trained word vec-tors for sentence-level classiﬁcation tasks.We show that a simple CNN with lit-tle hyperparameter tuning and static vec-tors achieves excellent results on multi-ple benchmarks. Learning task-speciﬁcvectors through ﬁne-tuning offers furthergains in performance. We additionallypropose a simple modiﬁcation to the ar-chitecture to allow for the use of bothtask-speciﬁc and static vectors. The CNNmodels discussed herein improve upon thestate of the art on 4 out of 7 tasks, whichinclude sentiment analysis and questionclassiﬁcation.1 IntroductionDeep learning models have achieved remarkableresults in computer vision (Krizhevsky et al.,2012) and speech recognition (Graves et al., 2013)in recent years. Within natural language process-ing, much of the work with deep learning meth-ods has involved learning word vector representa-tions through neural language models (Bengio etal., 2003; Yih et al., 2011; Mikolov et al., 2013)and performing composition over the learned wordvectors for classiﬁcation (Collobert et al., 2011).Word vectors, wherein words are projected from asparse, 1-of-Vencoding (hereVis the vocabularysize) onto a lower dimensional vector space via ahidden layer, are essentially feature extractors thatencode semantic features of words in their dimen-sions. In such dense representations, semanticallyclose words are likewise close—in euclidean orcosine distance—in the lower dimensional vectorspace.Convolutional neural networks (CNN) utilizelayers with convolving ﬁlters that are applied tolocal features (LeCun et al., 1998). Originallyinvented for computer vision, CNN models havesubsequently been shown to be effective for NLPand have achieved excellent results in semanticparsing (Yih et al., 2014), search query retrieval(Shen et al., 2014), sentence modeling (Kalch-brenner et al., 2014), and other traditional NLPtasks (Collobert et al., 2011).In the present work, we train a simple CNN withone layer of convolution on top of word vectorsobtained from an unsupervised neural languagemodel. These vectors were trained by Mikolov etal. (2013) on 100 billion words of Google News,and are publicly available.1We initially keep theword vectors static and learn only the other param-eters of the model. Despite little tuning of hyper-parameters, this simple model achieves excellentresults on multiple benchmarks, suggesting thatthe pre-trained vectors are ‘universal’ feature ex-tractors that can be utilized for various classiﬁca-tion tasks. Learning task-speciﬁc vectors throughﬁne-tuning results in further improvements. Weﬁnally describe a simple modiﬁcation to the archi-tecture to allow for the use of both pre-trained andtask-speciﬁc vectors by having multiple channels.Our work is philosophically similar to Razavianet al. (2014) which showed that for image clas-siﬁcation, feature extractors obtained from a pre-trained deep learning model perform well on a va-riety of tasks—including tasks that are very dif-ferent from the original task for which the featureextractors were trained.2 ModelThe model architecture, shown in ﬁgure 1, is aslight variant of the CNN architecture of Collobertet al. (2011). Letxi2Rkbe thek-dimensionalword vector corresponding to thei-th word in thesentence. A sentence of lengthn(padded where1https://code.google.com/p/word2vec/1746the		country							of							my		birth0.40.32.33.644.5772.13.3
Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747
Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747
Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.17471.1
Single	layer	CNN•Convolutional	filter:	(goes	over	window	of	h	words)•Note,	filter	is	vector!•Window	size	h	could	be	2	(as	before)	or	higher,	e.g.	3:•To	compute	feature	for	CNN	layer:
5/12/16Richard	Socher
Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747the		country							of							my		birth0.40.32.33.644.5772.13.31.1

Single	layer	CNN•Filter	w	is	applied	to	all	possible	windows	(concatenated	vectors)•Sentence:•All	possible	windows	of	length	h:•Result	is	a	feature	map:	
5/12/16Richard	Socher
Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747the		country							of							my		birth0.40.32.33.644.5772.13.31.13.5…2.4??????????Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747
Single	layer	CNN•Filter	w	is	applied	to	all	possible	windows	(concatenated	vectors)•Sentence:•All	possible	windows	of	length	h:•Result	is	a	feature	map:	
5/12/16Richard	SocherFigure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747
Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747the		country							of							my		birth0.40.32.33.644.5772.13.31.13.5…2.40000
Single	layer	CNN:	Pooling	layer•New	building	block:	Pooling•In	particular:	max-over-time	pooling	layer•Idea:	capture	most	important	activation	(maximum	over	time)•From	feature	map•Pooled	single	number:•But	we	want	more	features!5/12/16Richard	SocherFigure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747
Solution:	Multiple	filters•Use	multiple	filter	weights	w	•Useful	to	have	different	window	sizes	h•Because	of	max	pooling																								,	length	of	cirrelevant•So	we	can	have	some	filters	that	look	at	unigrams,	bigrams,	tri-grams,	4-grams,	etc.
5/12/16Richard	SocherFigure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747
Multi-channel	idea•Initialize	with	pre-trained	word	vectors	(word2vec	or	Glove)•Start	with	two	copies•Backpropinto	only	one	set,	keep	other	“static”•Both	channels	are	added	to	cibefore	max-pooling
5/12/16Richard	Socher
Classification	after	one	CNN	layer•First	one	convolution,	followed	by	one	max-pooling•To	obtain	final	feature	vector:(assuming	m	filters	w)•Simple	final	softmaxlayer	
5/12/16Richard	SocherFigure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.1747

Figure	from	Kim	(2014)
wait for the video and do n't rent it n x k representation of sentence with static and non-static channels Convolutional layer with multiple filter widths and feature maps Max-over-time pooling Fully connected layer with dropout and  softmax output 
Figure 1: Model architecture with two channels for an example sentence.necessary) is represented asx1:n=x1 x2 ... xn,(1)where is the concatenation operator. In gen-eral, letxi:i+jrefer to the concatenation of wordsxi,xi+1,...,xi+j. A convolution operation in-volves aﬁlterw2Rhk, which is applied to awindow ofhwords to produce a new feature. Forexample, a featureciis generated from a windowof wordsxi:i+h 1byci=f(w·xi:i+h 1+b).(2)Hereb2Ris a bias term andfis a non-linearfunction such as the hyperbolic tangent. This ﬁlteris applied to each possible window of words in thesentence{x1:h,x2:h+1,...,xn h+1:n}to produceafeature mapc=[c1,c2,...,cn h+1],(3)withc2Rn h+1. We then apply a max-over-time pooling operation (Collobert et al., 2011)over the feature map and take the maximum valueˆc= max{c}as the feature corresponding to thisparticular ﬁlter. The idea is to capture the most im-portant feature—one with the highest value—foreach feature map. This pooling scheme naturallydeals with variable sentence lengths.We have described the process by whichonefeature is extracted fromoneﬁlter. The modeluses multiple ﬁlters (with varying window sizes)to obtain multiple features. These features formthe penultimate layer and are passed to a fully con-nected softmax layer whose output is the probabil-ity distribution over labels.In one of the model variants, we experimentwith having two ‘channels’ of word vectors—onethat is kept static throughout training and one thatis ﬁne-tuned via backpropagation (section 3.2).2In the multichannel architecture, illustrated in ﬁg-ure 1, each ﬁlter is applied to both channels andthe results are added to calculateciin equation(2). The model is otherwise equivalent to the sin-gle channel architecture.2.1 RegularizationFor regularization we employ dropout on thepenultimate layer with a constraint onl2-norms ofthe weight vectors (Hinton et al., 2012). Dropoutprevents co-adaptation of hidden units by ran-domly dropping out—i.e., setting to zero—a pro-portionpof the hidden units during foward-backpropagation. That is, given the penultimatelayerz=[ ˆc1,...,ˆcm](note that here we havemﬁlters), instead of usingy=w·z+b(4)for output unityin forward propagation, dropoutusesy=w·(z r)+b,(5)where is the element-wise multiplication opera-tor andr2Rmis a ‘masking’ vector of Bernoullirandom variables with probabilitypof being 1.Gradients are backpropagated only through theunmasked units. At test time, the learned weightvectors are scaled bypsuch thatˆw=pw, andˆwis used (without dropout) to score unseen sen-tences. We additionally constrainl2-norms of theweight vectors by rescalingwto have||w||2=swhenever||w||2>safter a gradient descent step.2We employ language from computer vision where a colorimage has red, green, and blue channels.17475/12/16Richard	Sochern	words	(possibly	 zero	padded)	 and	each	word	vector	has	k	dimensions
Tricks	to	make	it	work	better:	Dropout•Idea:	randomly	mask/dropout/set	to	0	some	of	the	feature	weights	z•Create	masking	vector	r	of	Bernoulli	random	variables	with	probability	p	(a	hyperparameter)	of	being	1•Delete	features	during	training:•Reasoning:	Prevents	co-adaptation	(overfittingto	seeing	specific	feature	constellations)5/12/16Richard	Socher

Tricks	to	make	it	work	better:	Dropout•At	training	time,	gradients	are	backpropagatedonly	through	those	elements	of	z	vector	for	which	ri=	1•At	test	time,	there	is	no	dropout,	so	feature	vectors	z	are	larger.•Hence,	we	scale	final	vector	by	Bernoulli	probability	p	•Kim	(2014)	reports	2	–4%	improved	accuracy	and	ability	to	use	very	large	networks	without	overfitting5/12/16Richard	Socher

Another	regularization	trick•Somewhat	less	common•Constrain	l2norms	of	weight	vectors	of	each	class	(row	in	softmaxweight	W(S))	to	fixed	number	s	(also	a	hyperparameter)•If	,	then	rescale	it	so	that:	
5/12/16Richard	Socher

All	hyperparametersin	Kim	(2014)•Find	hyperparametersbased	on	devset•Nonlinearity:	reLu•Window	filter	sizes	h	=	3,4,5•Each	filter	size	has	100	feature	maps•Dropout	p	=	0.5•L2	constraint	s	for	rows	of	softmaxs	=	3•Mini	batch	size	for	SGD	training:	50•Word	vectors:	pre-trained	with	word2vec,	k	=	300•During	training,	keep	checking	performance	on	devset	and	pick	highest	accuracy	weights	for	final	evaluation5/12/16Richard	Socher
ExperimentsModelMRSST-1SST-2SubjTRECCRMPQACNN-rand76.145.082.789.691.279.883.4CNN-static81.045.586.893.092.884.789.6CNN-non-static81.548.087.293.493.684.389.5CNN-multichannel81.147.488.193.292.285.089.4RAE (Socher et al., 2011)77.743.282.4   86.4MV-RNN (Socher et al., 2012)79.044.482.9    RNTN (Socher et al., 2013) 45.785.4    DCNN (Kalchbrenner et al., 2014) 48.586.8 93.0  Paragraph-Vec (Le and Mikolov, 2014) 48.787.8    CCAE (Hermann and Blunsom, 2013)77.8     87.2Sent-Parser (Dong et al., 2014)79.5     86.3NBSVM (Wang and Manning, 2012)79.4  93.2 81.886.3MNB (Wang and Manning, 2012)79.0  93.6 80.086.3G-Dropout (Wang and Manning, 2013)79.0  93.4 82.186.1F-Dropout (Wang and Manning, 2013)79.1  93.6 81.986.3Tree-CRF (Nakagawa et al., 2010)77.3    81.486.1CRF-PR (Yang and Cardie, 2014)     82.7 SVMS(Silva et al., 2011)    95.0  Table 2: Results of our CNN models against other methods.RAE: Recursive Autoencoders with pre-trained word vectors fromWikipedia (Socher et al., 2011).MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012).RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013).DCNN:Dynamic Convolutional Neural Network with k-max pooling (Kalchbrenner et al., 2014).Paragraph-Vec: Logistic regres-sion on top of paragraph vectors (Le and Mikolov, 2014).CCAE: Combinatorial Category Autoencoders with combinatorialcategory grammar operators (Hermann and Blunsom, 2013).Sent-Parser: Sentiment analysis-speciﬁc parser (Dong et al.,2014).NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012).G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013).Tree-CRF: Dependency treewith Conditional Random Fields (Nakagawa et al., 2010).CRF-PR: Conditional Random Fields with Posterior Regularization(Yang and Cardie, 2014).SVMS: SVM with uni-bi-trigrams, wh word, head word, POS, parser, hypernyms, and 60 hand-codedrules as features from Silva et al. (2011).to both channels, but gradients are back-propagated only through one of the chan-nels. Hence the model is able to ﬁne-tuneone set of vectors while keeping the otherstatic. Both channels are initialized withword2vec.In order to disentangle the effect of the abovevariations versus other random factors, we elim-inate other sources of randomness—CV-fold as-signment, initialization of unknown word vec-tors, initialization of CNN parameters—by keep-ing them uniform within each dataset.4 Results and DiscussionResults of our models against other methods arelisted in table 2. Our baseline model with all ran-domly initialized words (CNN-rand) does not per-form well on its own. While we had expected per-formance gains through the use of pre-trained vec-tors, we were surprised at the magnitude of thegains. Even a simple model with static vectors(CNN-static) performs remarkably well, givingcompetitive results against the more sophisticateddeep learning models that utilize complex pool-ing schemes (Kalchbrenner et al., 2014) or requireparse trees to be computed beforehand (Socheret al., 2013). These results suggest that the pre-trained vectors are good, ‘universal’ feature ex-tractors and can be utilized across datasets. Fine-tuning the pre-trained vectors for each task givesstill further improvements (CNN-non-static).4.1 Multichannel vs. Single Channel ModelsWe had initially hoped that the multichannel ar-chitecture would prevent overﬁtting (by ensuringthat the learned vectors do not deviate too farfrom the original values) and thus work better thanthe single channel model, especially on smallerdatasets. The results, however, are mixed, and fur-ther work on regularizing the ﬁne-tuning processis warranted. For instance, instead of using anadditional channel for the non-static portion, onecould maintain a single channel but employ extradimensions that are allowed to be modiﬁed duringtraining.17495/12/16Richard	Socher
Problem	with	comparison?•Dropout	gives	2	–4	%	accuracy	improvement•Several	baselines	didn’t	use	dropout	•Still	remarkable	results	and	simple	architecture!•Difference	to	window	and	RNN	architectures	we	described	in	previous	lectures:	pooling,	many	filters	and	dropout•Ideas	can	be	used	in	RNN2s	too	•Tree-LSTMs	obtain	better	performance	on	sentence	datasets5/12/16Richard	Socher
•Fixed	tree	RNNs	explored	in	computer	vision:	Socheret	al	(2012):	“Convolutional-Recursive	Deep	Learning	for	3D	Object	Classification”
5/12/16Richard	SocherLecture	1,	Slide	26

Relationship	between	RNNs	and	CNNs•CNNRNN
5/12/16Richard	Socher
Relationship	between	RNNs	and	CNNs•CNNRNN
5/12/16Richard	Socher
Relationship	between	RNNs	and	CNNs•CNNRNN
•Stride	size	flexible	in	CNNs,	RNNs	“weighted	average	pool”•Tying	(sharing)	weights	of	filters	inside	vsacross	different	layers•CNN:	multiple	filters,	additional	layer	type:	max-pooling•Balanced	input	independent	structure	vsinput	specific	tree5/12/16Richard	Socher
CNN	alternatives•Narrow	vswide	convolution•Complex	pooling	schemes(over	sequences)and	deeper	convolutional	layers•Kalchbrenneret	al.	(2014)
5/12/16Richard	Socherlayer to the network, the TDNN can be adopted asa sentence model (Collobert and Weston, 2008).2.1 Related Neural Sentence ModelsVarious neural sentence models have been de-scribed. A general class of basic sentence modelsis that of Neural Bag-of-Words (NBoW) models.These generally consist of a projection layer thatmaps words, sub-word units orn-grams to highdimensional embeddings; the latter are then com-bined component-wise with an operation such assummation. The resulting combined vector is clas-siﬁed through one or more fully connected layers.A model that adopts a more general structureprovided by an external parse tree is the RecursiveNeural Network (RecNN) (Pollack, 1990; K¨uchlerand Goller, 1996; Socher et al., 2011; Hermannand Blunsom, 2013). At every node in the tree thecontexts at the left and right children of the nodeare combined by a classical layer. The weights ofthe layer are shared across all nodes in the tree.The layer computed at the top node gives a repre-sentation for the sentence. The Recurrent NeuralNetwork (RNN) is a special case of the recursivenetwork where the structure that is followed is asimple linear chain (Gers and Schmidhuber, 2001;Mikolov et al., 2011). The RNN is primarily usedas a language model, but may also be viewed as asentence model with a linear structure. The layercomputed at the last word represents the sentence.Finally, a further class of neural sentence mod-els is based on the convolution operation and theTDNN architecture (Collobert and Weston, 2008;Kalchbrenner and Blunsom, 2013b). Certain con-cepts used in these models are central to theDCNN and we describe them next.2.2 ConvolutionTheone-dimensional convolutionis an operationbetween a vector of weightsm2Rmand a vectorof inputs viewed as a sequences2Rs. The vectormis theﬁlterof the convolution. Concretely, wethink ofsas the input sentence andsi2Ris a sin-gle feature value associated with thei-th word inthe sentence. The idea behind the one-dimensionalconvolution is to take the dot product of the vectormwith eachm-gram in the sentencesto obtainanother sequencec:cj=m|sj m+1:j(1)Equation 1 gives rise to two types of convolutiondepending on the range of the indexj. Thenarrowtype of convolution requires thats mand yieldss1s1ssssc1c5c5Figure 2: Narrow and wide types of convolution.The ﬁltermhas sizem=5.a sequencec2Rs m+1withjranging frommtos. Thewidetype of convolution does not haverequirements onsormand yields a sequencec2Rs+m 1where the indexjranges from1tos+m 1. Out-of-range input valuessiwherei<1ori>sare taken to be zero. The result of thenarrow convolution is a subsequence of the resultof the wide convolution. The two types of one-dimensional convolution are illustrated in Fig. 2.The trained weights in the ﬁltermcorrespondto a linguistic feature detector that learns to recog-nise a speciﬁc class ofn-grams. Thesen-gramshave sizenm, wheremis the width of theﬁlter. Applying the weightsmin a wide convo-lution has some advantages over applying them ina narrow one. A wide convolution ensures that allweights in the ﬁlter reach the entire sentence, in-cluding the words at the margins. This is particu-larly signiﬁcant whenmis set to a relatively largevalue such as 8 or 10. In addition, a wide convo-lution guarantees that the application of the ﬁltermto the input sentencesalways produces a validnon-empty resultc, independently of the widthmand the sentence lengths. We next describe theclassical convolutional layer of a TDNN.2.3 Time-Delay Neural NetworksA TDNN convolves a sequence of inputsswith aset of weightsm. As in the TDNN for phonemerecognition (Waibel et al., 1990), the sequencesis viewed as having a time dimension and the con-volution is applied over the time dimension. Eachsjis often not just a single value, but a vector ofdvalues so thats2Rd⇥s. Likewise,mis a ma-trix of weights of sized⇥m. Each row ofmisconvolved with the corresponding row ofsand theconvolution is usually of the narrow type. Multi-ple convolutional layers may be stacked by takingthe resulting sequencecas input to the next layer.The Max-TDNN sentence model is based on thearchitecture of a TDNN (Collobert and Weston,2008). In the model, a convolutional layer of thenarrow type is applied to the sentence matrixs,where each column corresponds to the feature vec-torwi2Rdof a word in the sentence:s=24w1...ws35(2)To address the problem of varying sentencelengths, the Max-TDNN takes the maximum ofeach row in the resulting matrixcyielding a vectorofdvalues:cmax=264max(c1,:)...max(cd,:)375(3)The aim is to capture the most relevant feature, i.e.the one with the highest value, for each of thedrows of the resulting matrixc. The ﬁxed-sizedvectorcmaxis then used as input to a fully con-nected layer for classiﬁcation.The Max-TDNN model has many desirableproperties. It is sensitive to the order of the wordsin the sentence and it does not depend on externallanguage-speciﬁc features such as dependency orconstituency parse trees. It also gives largely uni-form importance to the signal coming from eachof the words in the sentence, with the exceptionof words at the margins that are considered fewertimes in the computation of the narrow convolu-tion. But the model also has some limiting as-pects. The range of the feature detectors is lim-ited to the spanmof the weights. Increasingmorstacking multiple convolutional layers of the nar-row type makes the range of the feature detectorslarger; at the same time it also exacerbates the ne-glect of the margins of the sentence and increasesthe minimum sizesof the input sentence requiredby the convolution. For this reason higher-orderand long-range feature detectors cannot be easilyincorporated into the model. The max pooling op-eration has some disadvantages too. It cannot dis-tinguish whether a relevant feature in one of therows occurs just one or multiple times and it for-gets the order in which the features occur. Moregenerally, the pooling factor by which the signalof the matrix is reduced at once corresponds tos m+1; even for moderate values ofsthe pool-ing factor can be excessive. The aim of the nextsection is to address these limitations while pre-serving the advantages.3 Convolutional Neural Networks withDynamick-Max PoolingWe model sentences using a convolutional archi-tecture that alternates wide convolutional layers
K-Max pooling(k=3)Fully connected layer
FoldingWideconvolution(m=2)Dynamick-max pooling (k= f(s) =5)
 Projectedsentence matrix(s=7)Wideconvolution(m=3)
 The cat sat on the red matFigure 3: A DCNN for the seven word input sen-tence. Word embeddings have sized=4. Thenetwork has two convolutional layers with twofeature maps each. The widths of the ﬁlters at thetwo layers are respectively 3 and 2. The (dynamic)k-max pooling layers have valueskof5and 3.with dynamic pooling layers given bydynamick-max pooling. In the network the width of a featuremap at an intermediate layer varies depending onthe length of the input sentence; the resulting ar-chitecture is the Dynamic Convolutional NeuralNetwork. Figure 3 represents a DCNN. We pro-ceed to describe the network in detail.3.1 Wide ConvolutionGiven an input sentence, to obtain the ﬁrst layer ofthe DCNN we take the embeddingwi2Rdforeach word in the sentence and construct the sen-tence matrixs2Rd⇥sas in Eq. 2. The valuesin the embeddingswiare parameters that are op-timised during training. A convolutional layer inthe network is obtained by convolving a matrix ofweightsm2Rd⇥mwith the matrix of activationsat the layer below. For example, the second layeris obtained by applying a convolution to the sen-tence matrixsitself. Dimensiondand ﬁlter widthmare hyper-parameters of the network. We let theoperations bewideone-dimensional convolutionsas described in Sect. 2.2. The resulting matrixchas dimensionsd⇥(s+m 1).
CNN	application:	Translation•One	of	the	first	successful	neural	machine	translation	efforts•Uses	CNN	for	encoding	and	RNN	for	decoding•Kalchbrennerand	Blunsom(2013)“Recurrent	Continuous	Translation	Models”
5/12/16Richard	SocherRCTM IIRCTM IP( f | e ) P( f | m, e )
e e e F T S S 
csm cgm icgm E F g g 
Figure 3: A graphical depiction of the two RCTMs. Arrows represent full matrix transformations while lines arevector transformations corresponding to columns of weight matrices.represented byEei. For example, for a sufﬁcientlylong sentencee,gram(Ee2)=2,gram(Ee3)=4,gram(Ee4)=7. We denote bycgm(e,n)that matrixEeifrom the CSM that represents then-grams of thesource sentencee.The CGM can also be inverted to obtain a repre-sentation for a sentence from the representation ofitsn-grams. We denote byicgmthe inverse CGM,which depends on the size of then-gram represen-tationcgm(e,n)and on the target sentence lengthm. The transformationicgmunfolds then-gramrepresentation onto a representation of a target sen-tence withmwords. The architecture correspondsto an inverted CGM or, equivalently, to an invertedtruncated CSM (Fig. 3). Given the transformationscgmandicgm, we now detail the computation of theRCTM II.4.2 RCTM IIThe RCTM II models the conditional probabilityP(f|e)by factoring it as follows:P(f|e)=P(f|m,e)·P(m|e)(9a)=mYi=1P(fi+1|f1:i, m,e)·P(m|e)(9b)and computing the distributionsP(fi+1|f1:i, m,e)andP(m|e). The architecture of the RCTM IIcomprises all the elements of the RCTM I togetherwith the following additional elements: a translationtransformationTq⇥qand two sequences of weightmatrices(Ji)2isand(Hi)2isthat are part oftheicgm3.The computation of the RCTM II proceeds recur-sively as follows:Eg=cgm(e,4)(10a)Fg:,j= (T·Eg:,j)(10b)F=icgm(Fg,m)(10c)h1= (I·v(f1)+S·F:,1)(10d)hi+1= (R·hi+I·v(fi+1)+S·F:,i+1)(10e)oi+1=O·hi(10f)and the conditional distributionsP(fi+1|f1:i,e)areobtained fromoias in Eq. 4. Note how each re-constructed vectorF:,iis added successively to thecorresponding layerhithat predicts the target wordfi. The RCTM II is illustrated in Fig. 3.3Just likerthe valuesis small and depends on the lengthof the source and target sentences in the training set. SeeSect. 5.1.2.
Model	comparison•Bag	of	Vectors:	Surprisingly	good	baseline	for	simple	classification	problems.	Especially	if	followed	by	a	few	layers!•Window	Model:	Good	for	single	word	classification	for	problems	that	do	not	need	wide	context•CNNs:	good	for	classification,	unclear	how	to	incorporate	phrase	level	annotation	(can	only	take	a	single	label),	need	zero	padding	for	shorter	phrases,	hard	to	interpret,	easy	to	parallelize	on	GPUs5/12/16Richard	Socher
Model	comparison	•Recursive	Neural	Networks:	most	linguistically	plausible,	interpretable,	provide	most	important	phrases	(for	visualization),	need	parse	trees•Recurrent	Neural	Networks:	Most	cognitively	plausible	(reading	from	left	to	right),	not	usually	the	highest	classification	performance	but	lots	of	improvements	right	now	with	gates	(GRUs,	LSTMs,	etc).•Best	but	also	most	complex	models:	Hierarchical	recurrent	neural	networks	with	attention	mechanisms	and	additional	memory	àLast	week	of	class	:)	5/12/16Richard	Socher
Next	week:•Guest	lectures	next	week:	•Speech	recognition	and	state	of	the	art	machine	translation
5/12/16Richard	Socher
Maria -Florina  Balcan  
04/06/2015  Clustering.  
Unsupervised Learning  
Additional resources:  
•Center Based Clustering: A Foundational Perspective.  
     Awasthi , Balcan . Handbook of Clustering Analysis. 2015.   Reading:  
•Chapter 14.3: Hastie, Tibshirani , Friedman.  
Logistics  
•Midway Review due today.  
•Final Report, May 8. 
•Poster Presentation, May 11. 
•Exam #2 on April 29th. •Project:  
•Communicate with your mentor TA!  
Clustering, Informal Goals  
Goal: Automatically partition unlabeled  data into groups of 
similar datapoints . 
 Question : When and why would we want to do this?  
• Automatically organizing data.   Useful for:  
• Representing high -dimensional data in a low -dimensional space 
(e.g., for visualization purposes).  • Understanding hidden structure in data.  
• Preprocessing for further analysis.  
•Cluster news articles or web pages or search results by topic.  
 
Applications  (Clustering comes up everywhere…)  
 
•Cluster protein sequences by function or genes according to expression 
profile.  
 
•Cluster users of social networks by interest (community detection).  
Facebook network  
Twitter Network  
•Cluster customers according to purchase history.  
 
Applications (Clustering comes up everywhere…)  
 
•Cluster galaxies or nearby stars  (e.g. Sloan Digital Sky Survey)  
 
•And many many  more applications….  
Clustering  
[March 4th: EM-style algorithm for clustering for mixture of Gaussians (specific 
probabilistic model).]  Today : 
•Objective based clustering  
•Hierarchical clustering  
•Mention overlapping clusters  
Objective Based Clustering  
Goal: output a partition  of the data.  
Input : A set  S of n points, also a distance/dissimilarity 
measure specifying the distance d(x,y) between pairs (x,y). 
E.g., # keywords in common, edit distance, wavelets coef., etc.  
–k-median : find center pts 𝐜𝟏,𝐜𝟐,…,𝐜𝐤 to  
                minimize  ∑i=1 n minj∈1,…,kd(𝐱𝐢,𝐜𝐣) –k-means : find center pts 𝒄𝟏,𝒄𝟐,…,𝒄𝒌 to  
                minimize   ∑i=1 n minj∈1,…,kd2(𝐱𝐢,𝐜𝐣) 
–K-center : find partition to minimize the maxim radius  z   x y 
c1 c2 s c3 
Input : A set of  n datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝒏 in Rd Euclidean k -means Clustering  
target #clusters k 
Output : k representatives 𝒄𝟏,𝐜𝟐,…,𝒄𝒌∈Rd 
Objective : choose 𝒄𝟏,𝐜𝟐,…,𝒄𝒌∈Rd to minimize  
∑i=1 n minj∈1,…,k𝐱𝐢−𝐜𝐣2
 

Input : A set of  n datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝒏 in Rd  Euclidean k -means Clustering  
target #clusters k 
Output : k representatives 𝒄𝟏,𝐜𝟐,…,𝒄𝒌∈Rd 
Objective : choose 𝒄𝟏,𝐜𝟐,…,𝒄𝒌∈Rd to minimize  
∑i=1 n minj∈1,…,k𝐱𝐢−𝐜𝐣2
 
Natural assignment: each point assigned to its 
closest center, leads to a Voronoi  partition.  

Input : A set of  n datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝒏 in Rd  Euclidean k -means Clustering  
target #clusters k 
Output : k representatives 𝒄𝟏,𝐜𝟐,…,𝒄𝒌∈Rd 
Objective : choose 𝒄𝟏,𝐜𝟐,…,𝒄𝒌∈Rd to minimize  
∑i=1 n minj∈1,…,k𝐱𝐢−𝐜𝐣2
 
Computational complexity : 
NP hard: even for k=2 [Dagupta’08]  or 
d=2 [Mahajan -Nimbhorkar -Varadarajan09]  
There are a couple of easy cases…  
An Easy Case for k -means: k=1  
Output : 𝒄∈Rd to minimize  ∑i=1 n 𝐱𝐢−𝐜2
 
Solution : 
1
n∑i=1 n 𝐱𝐢−𝐜2
= 𝛍−𝐜2+1
n∑i=1 n 𝐱𝐢−𝛍2
 
So, the optimal choice for 𝐜 is 𝛍. The optimal choice is 𝛍=1
n∑i=1 n 𝐱𝐢 Input : A set of  n datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝒏 in Rd 
Avg k-means cost wrt c Avg k -means cost wrt μ Idea: bias/variance like decomposition  
Another Easy Case for k -means: d=1  
Output : 𝒄∈Rd to minimize  ∑i=1 n 𝐱𝐢−𝐜2
 
Extra-credit homework question  
Hint: dynamic programming in time O(n2k). Input : A set of  n datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝒏 in Rd 
Input : A set of  n datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝐧 in Rd Common Heuristic in Practice:  
The Lloyd’s method  
Repeat  until there is no further change in the cost.  
•For each j:  Cj←{𝑥∈𝑆 whose closest center is 𝐜𝐣} 
•For each j: 𝐜𝐣←mean of Cj Initialize  centers 𝒄𝟏,𝐜𝟐,…,𝒄𝒌∈Rd and 
               clusters C1,C2,…,Ck in any way.  [Least squares quantization in PCM, Lloyd, IEEE Transactions on Information Theory, 1982]  
Input : A set of  n datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝐧 in Rd Common Heuristic in Practice:  
The Lloyd’s method  
Repeat  until there is no further change in the cost.  
•For each j:  Cj←{𝑥∈𝑆 whose closest center is 𝐜𝐣} 
•For each j: 𝐜𝐣←mean of Cj Initialize  centers 𝒄𝟏,𝐜𝟐,…,𝒄𝒌∈Rd and 
               clusters C1,C2,…,Ck in any way.  [Least squares quantization in PCM, Lloyd, IEEE Transactions on Information Theory, 1982]  
Holding 𝒄𝟏,𝐜𝟐,…,𝒄𝒌 fixed, 
pick optimal C1,C2,…,Ck Holding C1,C2,…,Ck fixed, 
pick optimal 𝒄𝟏,𝐜𝟐,…,𝒄𝒌  
Input : A set of  n datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝐧 in Rd Common Heuristic: The Lloyd’s method  
Initialize  centers 𝐜𝟏,𝐜𝟐,…,𝐜𝐤∈Rd and 
               clusters C1,C2,…,Ck in any way.  
Repeat  until there is no further change in the cost.  
•For each j:  Cj←{𝑥∈𝑆 whose closest center is 𝐜𝐣} 
•For each j: 𝐜𝐣←mean of Cj 
Note : it always converges.  
•the cost always drops and  
•there is only a finite #s of Voronoi  partitions 
(so a finite # of values the cost could take)  
Input : A set of  n datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝐧 in Rd Initialization  for the Lloyd’s method  
Initialize  centers 𝐜𝟏,𝐜𝟐,…,𝐜𝐤∈Rd and 
               clusters C1,C2,…,Ck in any way.  
Repeat  until there is no further change in the cost.  
•For each j:  Cj←{𝑥∈𝑆 whose closest center is 𝐜𝐣} 
•For each j: 𝐜𝐣←mean of Cj 
•Initialization is crucial (how fast it converges, quality of solution output)  
•Discuss techniques  commonly used in practice  
•Random centers from the datapoints  (repeat a few times)  
•K-means ++ (works well and has provable guarantees)  •Furthest traversal  
Lloyd’s method: Random Initialization  
Example : Given a set of datapoints  Lloyd’s method: Random Initialization  
Select initial centers at random  Lloyd’s method: Random Initialization  
Assign each point to its nearest center  Lloyd’s method: Random Initialization  
Recompute  optimal centers given a fixed clustering  Lloyd’s method: Random Initialization  
Assign each point to its nearest center  Lloyd’s method: Random Initialization  
Recompute  optimal centers given a fixed clustering  Lloyd’s method: Random Initialization  
Assign each point to its nearest center  Lloyd’s method: Random Initialization  
Recompute  optimal centers given a fixed clustering  Lloyd’s method: Random Initialization  
Get a good  quality solution in this example.  
Lloyd’s method: Performance  
It always converges, but it may converge at a local optimum 
that is different from the global optimum, and in fact could 
be arbitrarily worse in terms of its score . 
Lloyd’s method: Performance  
Local optimum: every point is assigned to its nearest center 
and every center is the mean value of its points.  
Lloyd’s method: Performance  
.It is arbitrarily worse than optimum solution….  

Lloyd’s method: Performance  
This bad performance, can happen 
even with well separated Gaussian 
clusters.  
Lloyd’s method: Performance  
This bad performance, can 
happen even with well 
separated Gaussian clusters.  
Some Gaussian are 
combined…..  
Lloyd’s method: Performance  
•For k equal -sized Gaussians, Pr[each initial center is in a 
different Gaussian] ≈𝑘!
𝑘𝑘≈1
𝑒𝑘 
•Becomes unlikely as k gets large.  •If we do random initialization, as k increases, it becomes 
more likely we won’t have perfectly picked one center per 
Gaussian in our initialization (so Lloyd’s method will output 
a bad solution ). 
Another Initialization Idea: Furthest 
Point Heuristic  
Choose 𝐜𝟏 arbitrarily (or at random).  
•Pick 𝐜𝐣 among datapoints 𝐱𝟏,𝐱𝟐,…,𝐱𝐝 that is 
farthest from previously chosen 𝐜𝟏,𝐜𝟐,…,𝐜𝒋−𝟏 •For j=2,…,k 
Fixes the Gaussian problem. But it can be thrown 
off by outliers….  
Furthest point heuristic does well on 
previous example  
(0,1) 
(0,-1) (-2,0) (3,0) Furthest point initialization heuristic 
sensitive to outliers  
Assume k=3 
(0,1) 
(0,-1) (-2,0) (3,0) Furthest point initialization heuristic 
sensitive to outliers  
Assume k=3 
K-means++ Initialization: D2 sampling [AV07]  
•Choose 𝐜𝟏 at random.  
•Pick 𝐜𝐣 among 𝐱𝟏,𝐱𝟐,…,𝐱𝐝 according to the distribution  •For j=2,…,k •Interpolate between random and furthest point initialization  
𝐏𝐫(𝐜𝐣=𝐱𝐢)∝𝐦𝐢𝐧𝐣′<𝐣 𝐱𝐢−𝐜𝐣′𝟐
 •Let D(x) be the distance between a point 𝑥 and its nearest 
center. Chose the next center proportional to D2(𝐱). 
D2(𝐱𝐢) 
Theorem: K-means++ always attains an O(log k) approximation to 
optimal k -means solution in expectation . 
Running Lloyd’s  can only further improve the cost . 
K-means++ Idea: D2 sampling  
•Interpolate between random and furthest point initialization  
•Let D(x) be the distance between a point 𝑥 and its nearest 
center. Chose the next center proportional to D𝛼(𝐱). 
•𝛼=0, random sampling  
•𝛼=∞, furthest point  (Side note: it actually works well for k -center)  
•𝛼=2, k-means ++  
Side note: 𝛼=1, works well for k -median  
(0,1) 
(0,-1) (-2,0) (3,0) K-means ++ Fix  
K-means++/  Lloyd’s  Running Time  
Repeat  until there is no change in the cost.  
•For each j:  Cj←{𝑥∈𝑆 whose closest center is 𝐜𝐣} 
•For each j: 𝐜𝐣←mean of Cj Each round takes 
time O(nkd). •K-means ++ initialization: O(nd) and one pass over data to 
select next center. So O(nkd) time in total . 
•Lloyd’s method  
•Exponential # of rounds in the worst case [AV07 ]. 
•Expected polynomial time in the smoothed analysis model!  
K-means++/  Lloyd’s Summary  
•Exponential # of rounds in the worst case [AV07 ]. 
•Expected polynomial time in the smoothed analysis model!  •K-means ++ always attains an O(log k) approximation to optimal 
k-means solution in expectation . 
•Running Lloyd’s can only further improve the cost . 
•Does well in practice.  
What value of k???  
•Hold-out validation/cross -validation on auxiliary 
task (e.g ., supervised learning task).  •Heuristic : Find large gap between k -1-means cost 
and k -means cost.  
•Try hierarchical clustering.  
soccer  sports  fashion  
Gucci  tennis  Lacoste   All topics  Hierarchical Clustering  
•A hierarchy might be more natural.  
•Different users might care about different levels of 
granularity or even prunings . 
•Partition data into 2 -groups (e.g., 2 -means)  Top-down (divisive)  Hierarchical Clustering  
•Recursively cluster each group.  
Bottom -Up (agglomerative)  
soccer  sports  fashion  
Gucci  tennis  Lacoste   All topics  •Start with every point in its own cluster.  
•Repeatedly merge the “closest” two clusters.  
•Different defs of “closest” give different 
algorithms.  
Bottom -Up (agglomerative)  
•  Single  linkage : dist A,𝐵= min
x∈A,x′∈B′dist(x,x′) 
dist A,B= avg
x∈A,x′∈B′dist(x,x′) soccer  sports  fashion  
Gucci  tennis  Lacoste  All topics  Have a distance measure on pairs of objects.  
d(x,y) – distance between x and y 
•  Average  linkage : •  Complete  linkage : 
•  Wards’  method  E.g., # keywords in common, edit distance, etc 
dist A,B= max
x∈A,x′∈B′dist(x,x′) 
Single Linkage  
Bottom -up (agglomerative)  
•Start with every point in its own cluster.  
•Repeatedly merge the “closest” two clusters.  
Single linkage: dist A,𝐵= min
x∈A,x′∈𝐵dist(x,x′) 
0 6 2.1 3.2 -2 -3 
A B C D E F 3 4 5 
A B D E 1 2 A B C  A B C D E  A B C D E F  Dendogram  
Single Linkage  
Bottom -up (agglomerative)  
•Start with every point in its own cluster.  
•Repeatedly merge the “closest” two clusters.  
1 2 3 4 5 One way to think of it: at any moment, we see connected components 
of the graph where connect any two pts of distance < r.  
0 6 2.1 3.2 -2 -3 
A B C D E F Watch as r grows (only n -1 relevant values because we only we merge 
at value of r corresponding to values of r in different clusters).  Single linkage: dist A,𝐵= min
x∈A,x′∈𝐵dist(x,x′) 
Complete Linkage  
Bottom -up (agglomerative)  
•Start with every point in its own cluster.  
•Repeatedly merge the “closest” two clusters.  
Complete linkage: dist A,B= max
x∈A,x′∈Bdist(x,x′) 
One way to think of it: keep max diameter as small as possible at 
any level.  
0 6 2.1 3.2 -2 -3 
A B C D E F 3 4 5 
A B D E 1 2 A B C  DEF A B C D E F  
Complete Linkage  
Bottom -up (agglomerative)  
•Start with every point in its own cluster.  
•Repeatedly merge the “closest” two clusters.  
One way to think of it: keep max diameter as small as possible.  
0 6 2.1 3.2 -2 -3 
A B C D E F 1 2 3 4 5 Complete linkage: dist A,B= max
x∈A,x′∈Bdist(x,x′) 
Ward’s Method  
Bottom -up (agglomerative)  
•Start with every point in its own cluster.  
•Repeatedly merge the “closest” two clusters.  
Ward’s method: dist C,C′=C⋅C′
C+C′mean C−mean C′ 2 
Merge the two clusters such that the increase in k -means cost is 
as small as possible.  
Works well in practice.  
0 6 2.1 3.2 -2 -3 
A B C D E F 1 2 4 5 
3 
Running time  
In fact, can run all these algorithms in time 𝑂(𝑁2log𝑁). •Each algorithm starts with N clusters, and performs N -1 merges.  
•For each algorithm, computing 𝑑𝑖𝑠𝑡 (𝐶,𝐶′) can be done in time 
𝑂(𝐶⋅𝐶′).  (e.g., examining 𝑑𝑖𝑠𝑡 (𝑥,𝑥′) for all 𝑥∈𝐶,𝑥′∈𝐶′) 
•Time to compute all pairwise distances and take smallest is 𝑂(𝑁2). 
See: Christopher D. Manning, Prabhakar  Raghavan  and Hinrich  Schütze , Introduction to 
Information Retrieval, Cambridge University Press. 2008. http://www -nlp.stanford.edu/IR -book/ •Overall time is 𝑂(𝑁3). 
Hierarchical Clustering Experiments 
[BLG, JMLR’15]  
Ward’s method does the best among classic techniques.  
Hierarchical Clustering Experiments 
[BLG, JMLR’15]  
Ward’s method does the best among classic techniques.  
What You Should Know  
•Partitional  Clustering. k -means  and k -means ++  
•Hierarchical Clustering.  •Lloyd’s method  
• Single linkage, Complete Linkge , Ward’s method  •Initialization techniques (random, furthest 
traversal, k -means ++) 
Additional Slides  
Smoothed analysis model  
•Imagine a worst -case input.  
•But then add small Gaussian perturbation to each data point.  
Smoothed analysis model  
•Imagine a worst -case input.  
•But then add small Gaussian perturbation to each data point.  
•Theorem [Arthur -Manthey -Roglin  2009]:  
 
•Might still find local opt that is far from global opt. -E[number of rounds until Lloyd’s converges] if add Gaussian 
perturbation with variance 𝜎2 is polynomial in 𝑛, 1/𝜎. 
-The actual bound is : 𝑂𝑛34𝑘34𝑑8
𝜎6 
TCS Christos 
Papadimitriou  Colleagues at 
Berkeley  
Databases 
Systems  
Algorithmic Game 
Theory  Overlapping Clusters: Communities  

Overlapping Clusters: Communities  
•Social networks  •Professional networks  
•Product Purchasing Networks, Citation Networks, 
Biological Networks, etc 
Kids CDs 
lullabies  
Electronics  Overlapping Clusters: Communities  
Baby's Favorite 
Songs  
Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  February 4, 2015 
Today: • Generative – discriminative classifiers • Linear regression • Decomposition of error into bias, variance, unavoidable Readings:  • Mitchell: “Naïve Bayes and Logistic Regression”      (required) • Ng and Jordan paper (optional) • Bishop, Ch 9.1, 9.2 (optional) 
•  Consider learning f: X à Y, where •  X is a vector of real-valued features, < X1 … Xn > •  Y is boolean •  assume all Xi are conditionally independent given Y •  model P(Xi | Y = yk) as Gaussian N(µik,σi) •  model P(Y) as Bernoulli (π) •  Then P(Y|X) is of this form, and we can directly estimate W •  Furthermore, same holds if the Xi are boolean •  trying proving that to yourself •  Train by gradient ascent estimation of w’s (no assumptions!) 
Logistic Regression 
MLE vs MAP  • Maximum conditional likelihood estimate • Maximum a posteriori estimate with prior W~N(0,σI) 

MAP estimates and Regularization • Maximum a posteriori estimate with prior W~N(0,σI) 
called a “regularization” term •  helps reduce overfitting, especially when training data is sparse •  keep weights nearer to zero (if P(W) is zero mean Gaussian prior), or whatever the prior suggests •  used very frequently in Logistic Regression 
Generative vs. Discriminative Classifiers Training classifiers involves estimating f: X à Y, or P(Y|X)  Generative classifiers (e.g., Naïve Bayes) • Assume some functional form for P(Y), P(X|Y)  • Estimate parameters of P(X|Y), P(Y) directly from training data • Use Bayes rule to calculate P(Y=y |X= x) Discriminative classifiers (e.g., Logistic regression) • Assume some functional form for P(Y|X) • Estimate parameters of P(Y|X) directly from training data • NOTE: even though our derivation of the form of P(Y|X) made GNB-style assumptions, the training procedure for Logistic Regression does not! 
Use Naïve Bayes or Logisitic Regression? Consider • Restrictiveness of modeling assumptions (how well can we learn with infinite data?)   • Rate of convergence (in amount of training data)  toward asymptotic (infinite data) hypothesis  – i.e., the learning curve 
Naïve Bayes vs Logistic Regression Consider Y boolean, Xi continuous, X=<X1 ... Xn>  Number of parameters: • NB: 4n +1 • LR: n+1 Estimation method: • NB parameter estimates are uncoupled • LR parameter estimates are coupled  

Gaussian Naïve Bayes – Big Picture assume P(Y=1) = 0.5 
Gaussian Naïve Bayes – Big Picture assume P(Y=1) = 0.5 
G.Naïve Bayes vs. Logistic Regression Recall two assumptions deriving form of LR from GNBayes: 1. Xi conditionally independent of Xk given Y 2. P(Xi | Y = yk)  =  N(µik,σi),   ß not N(µik,σik) Consider three learning methods: • GNB (assumption 1 only) • GNB2 (assumption 1 and 2) • LR   Which method works better if we have infinite training data, and...  • Both (1) and (2) are satisfied • Neither (1) nor (2) is satisfied • (1) is satisfied, but not (2)  [Ng & Jordan, 2002] 
G.Naïve Bayes vs. Logistic Regression Recall two assumptions deriving form of LR from GNBayes: 1. Xi conditionally independent of Xk given Y 2. P(Xi | Y = yk)  =  N(µik,σi),   ß not N(µik,σik) Consider three learning methods: • GNB (assumption 1 only)     -- decision surface can be non-linear • GNB2 (assumption 1 and 2) – decision surface linear • LR                                         -- decision surface linear, trained differently  Which method works better if we have infinite training data, and...  • Both (1) and (2) are satisfied:    LR = GNB2 = GNB • Neither (1) nor (2) is satisfied:   LR > GNB2,   GNB>GNB2 • (1) is satisfied, but not (2) :        GNB > LR,   LR > GNB2  [Ng & Jordan, 2002] 
G.Naïve Bayes vs. Logistic Regression What if we have only finite training data?  They converge at different rates to their asymptotic (∞ data) error  Let          refer to expected error of learning algorithm A after n training examples  Let d be the number of features: <X1 … Xd>          So, GNB requires n = O(log d) to converge, but LR requires n = O(d) [Ng & Jordan, 2002] 

Some experiments from UCI data sets 
[Ng & Jordan, 2002]  
Naïve Bayes vs. Logistic Regression The bottom line:  GNB2 and LR both use linear decision surfaces, GNB need not  Given infinite data, LR is better than GNB2 because training procedure does not make assumptions 1 or 2 (though our derivation of the form of P(Y|X) did).  But GNB2 converges more quickly to its perhaps-less-accurate asymptotic error  And GNB is both more biased (assumption1) and less (no assumption 2) than LR, so either might beat the other 
Rate of covergence: logistic regression 
Let hDis,m be logistic regression trained on m examples in n dimensions.  Then with high probability: Implication: if we want  for some constant      , it suffices to pick order n examples   à Convergences to its asymptotic classifier, in order n examples (result follows from Vapnik’s structural risk bound, plus fact that VCDim of n dimensional linear separators is n ) 
[Ng & Jordan, 2002] 
Rate of covergence: naïve Bayes parameters 
[Ng & Jordan, 2002] 
What you should know: • Logistic regression – Functional form follows from Naïve Bayes assumptions • For Gaussian Naïve Bayes assuming variance σi,k = σi • For discrete-valued Naïve Bayes too – But training procedure picks parameters without the conditional independence assumption – MCLE training: pick W to maximize P(Y | X, W) – MAP training: pick W to maximize P(W | X,Y) • regularization:   e.g., P(W)  ~ N(0,σ) • helps reduce overfitting  • Gradient ascent/descent – General approach when closed-form solutions for MLE, MAP are unavailable • Generative vs. Discriminative classifiers – Bias vs. variance tradeoff 
Machine Learning 10-701  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  February 4, 2015 
Today: • Linear regression • Decomposition of error into bias, variance, unavoidable Readings: • Mitchell: “Naïve Bayes and Logistic Regression”      (see class website) • Ng and Jordan paper (class website) • Bishop, Ch 9.1, 9.2 
Regression So far, we’ve been interested in learning P(Y|X) where Y has discrete values (called ‘classification’)  What if Y is continuous? (called ‘regression’) • predict weight from gender, height, age, … • predict Google stock price today from Google, Yahoo, MSFT prices yesterday • predict each pixel intensity in robot’s current camera image, from previous image and previous action 
Regression Wish to learn f:XàY, where Y is real, given {<x1,y1>…<xn,yn>}  Approach:  1. choose some parameterized form for P(Y|X; θ) ( θ is the vector of parameters)  2. derive learning algorithm as MCLE or MAP estimate for θ 
1. Choose parameterized form for P(Y|X; θ)  Assume Y is some deterministic f(X), plus random noise   Therefore Y is a random variable that follows the distribution   and the expected value of y for any given x is f(x)  Y X 
where 

1. Choose parameterized form for P(Y|X; θ)  Assume Y is some deterministic f(X), plus random noise   Therefore Y is a random variable that follows the distribution   and the expected value of y for any given x is f(x)  Y X 
where 

Consider Linear Regression    E.g., assume f(x) is linear function of x      Notation: to make our parameters explicit, let’s write     

Training Linear Regression    How can we learn W from the training data?   

Training Linear Regression    How can we learn W from the training data?  Learn Maximum Conditional Likelihood Estimate!     where   

Training Linear Regression  Learn Maximum Conditional Likelihood Estimate   where        

Training Linear Regression  Learn Maximum Conditional Likelihood Estimate   where        

Training Linear Regression  Learn Maximum Conditional Likelihood Estimate   where      so:   

Training Linear Regression  Learn Maximum Conditional Likelihood Estimate   Can we derive gradient descent rule for training?          

How about MAP instead of MLE estimate? 

Regression – What you should know Under general assumption  1. MLE corresponds to minimizing sum of squared prediction errors 2. MAP estimate minimizes SSE plus sum of squared weights 3. Again, learning is an optimization problem once we choose our objective function • maximize data likelihood • maximize posterior prob of W 4. Again, we can use gradient descent as a general learning algorithm • as long as our objective fn is differentiable wrt W • though we might learn local optima ins  5. Almost nothing we said here required that f(x) be linear in x             

Bias/Variance Decomposition of Error 
Bias and Variance given some estimator Y for some parameter θ, we define  the bias of estimator Y =  the variance of estimator Y =  e.g., define Y as the MLE estimator for probability of heads, based on n independent coin flips  biased or unbiased?  variance decreases as sqrt(1/n) 

• Consider simple regression problem f:XàY  y = f(x) + εWhat are sources of prediction error?   noise N(0,σ) deterministic Bias – Variance decomposition of error  Reading: Bishop chapter 9.1, 9.2 
learned estimate of f(x)  
Sources of error • What if we have perfect learner, infinite data? – Our learned h(x) satisfies h(x)=f(x) – Still have remaining, unavoidable error                                                                   σ2 
Sources of error • What if we have only n training examples? • What is our expected error – Taken over random training sets of size n, drawn from distribution D=p(x,y) 

Sources of error 

COMS 4721: Machine Learning for Data Science
Lecture 15, 3/23/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
MAXIMUM LIKELIHOOD
APPROACHES TO DATA MODELING
Our approaches to modeling data thus far have been either probabilistic or
non-probabilistic in motivation.
IProbabilistic models: Probability distributions deﬁned on data, e.g.,
1. Bayes classiﬁers
2. Logistic regression
3. Least squares and ridge regression (using ML and MAP interpretation)
4. Bayesian linear regression
INon-probabilistic models: No probability distributions involved, e.g.,
1. Perceptron
2. Support vector machine
3. Decision trees
4. K-means
Inevery case, we have some objective function we are trying to optimize
(greedily vs non-greedily, locally vs globally).
MAXIMUM LIKELIHOOD
As we’ve seen, one probabilistic objective function is maximum likelihood.
Setup: In the most basic scenario, we start with
1. some set of model parameters 
2. a set of datafx1;:::; xng
3. a probability distribution p(xj)
4. an i.i.d. assumption, xiiidp(xj)
Maximum likelihood seeks the that maximizes the likelihood
ML=arg max
p(x1;:::; xnj)(a)=arg max
nY
i=1p(xij)(b)=arg max
nX
i=1lnp(xij)
(a) follows from i.i.d. assumption.
(b) follows since f(y)>f(x))lnf(y)>lnf(x).
MAXIMUM LIKELIHOOD
We’ve discussed maximum likelihood for a few models, e.g., least squares
linear regression and the Bayes classiﬁer.
Both of these models were “nice” because we could ﬁnd their respective ML
analytically by writing an equation and plugging in data to solve.
Gaussian with unknown mean and covariance
In the ﬁrst lecture, we saw if xiiidN(;), where=f;g, then
rlnnY
i=1p(xij) =0
gives the following maximum likelihood values for and:
ML=1
nnX
i=1xi; ML=1
nnX
i=1(xi ML)(xi ML)T
COORDINATE ASCENT AND MAXIMUM LIKELIHOOD
In more complicated models, we might split the parameters into groups
1;2and try to maximize the likelihood over both of these,
1;ML;2;ML=arg max
1;2nX
i=1lnp(xij1;2);
Although we can solve one given the other, we can’t solve it simultaneously .
Coordinate ascent (probabilistic version)
We saw how K-means presented a similar situation, and that we could
optimize using coordinate ascent. This technique is generalizable.
Algorithm : For iteration t=1;2;:::;
1. Optimize (t)
1=arg max1Pn
i=1lnp(xij1;(t 1)
2)
2. Optimize (t)
2=arg max2Pn
i=1lnp(xij(t)
1;2)
COORDINATE ASCENT AND MAXIMUM LIKELIHOOD
There is a third (subtly) different situation, where we really want to ﬁnd
1;ML=arg max
1nX
i=1lnp(xij1):
Except this function is “tricky” to optimize directly. However, we ﬁgure out
that we can add a second variable 2such that
nX
i=1lnp(xi;2j1) ( Function 2 )
is easier to work with. We’ll make this clearer later.
INotice in this second case that 2is on the leftside of the conditioning
bar. This implies a prior on 2, (whatever “ 2” turns out to be).
IWe will next discuss a fundamental technique called the EM algorithm
for ﬁnding1;MLby using Function 2 instead.
EXPECTATION -MAXIMIZATION
ALGORITHM
AMOTIVATING EXAMPLE
Letxi2Rd, be a vector with missing data . Split this vector into two parts:
1.xo
i– observed portion (the sub-vector of xithat is measured)
2.xm
i– missing portion (the sub-vector of xithat is still unknown)
3. The missing dimensions can be different for different xi.
We assume that xiiidN(;), and want to solve
ML;ML=arg max
;nX
i=1lnp(xo
ij;):
This is tricky. However, if we knew xm
i(and therefore xi), then
ML;ML=arg max
;nX
i=1lnp(xo
i;xm
ij;)|{z}
=p(xij;)
is very easy to optimize (we just did it on a previous slide).
CONNECTING TO A MORE GENERAL SETUP
We will discuss a method for optimizingPn
i=1lnp(xo
ij;)and imputing its
missing valuesfxm
1;:::; xm
ng. This is a very general technique.
General setup
Imagine we have two parameter sets 1;2, where
p(xj1) =Z
p(x;2j1)d2 (marginal distribution )
Example: For the previous example we can show that
p(xo
ij;) =Z
p(xo
i;xm
ij;)dxm
i=N(o
i;o
i);
whereo
iando
iare the sub-vector/sub-matrix of anddeﬁned by xo
i.
THEEM OBJECTIVE FUNCTION
We need to deﬁne a general objective function that gives us what we want:
1. It lets us optimize the marginal p(xj1)over1,
2. It uses p(x;2j1)in doing so purely for computational convenience.
The EM objective function
Before picking it apart, we claim that this objective function is
lnp(xj1) =Z
q(2)lnp(x;2j1)
q(2)d2+Z
q(2)lnq(2)
p(2jx;1)d2
Some immediate comments:
Iq(2)isanyprobability distribution (assumed continuous for now)
IWe assume we know p(2jx;1). That is, given the data xand ﬁxed
values for1, we can solve the conditional posterior distribution of 2.
DERIVING THE EM OBJECTIVE FUNCTION
Let’s show that this equality is actually true
lnp(xj1) =Z
q(2)lnp(x;2j1)
q(2)d2+Z
q(2)lnq(2)
p(2jx;1)d2
=Z
q(2)lnp(x;2j1)q(2)
p(2jx;1)q(2)d2
Remember some rules of probability:
p(a;bjc) =p(ajb;c)p(bjc)) p(bjc) =p(a;bjc)
p(ajb;c):
Letting a=1,b=xandc=1, we conclude
lnp(xj1) =Z
q(2)lnp(xj1)d2
=lnp(xj1)
THEEM OBJECTIVE FUNCTION
The EM objective function splits our desired objective into two terms:
lnp(xj1) =Z
q(2)lnp(x;2j1)
q(2)d2
|{z}
A function only of 1, we’ll call itL+Z
q(2)lnq(2)
p(2jx;1)d2
|{z}
Kullback-Leibler divergence
Some more observations about the right hand side:
1. The KL diverence is always0 and only =0 when q=p.
2. We are assuming that the integral in Lcan be calculated, leaving a
function only of 1(for a particular setting of the distribution q).
BIGGER PICTURE
Q: What does it mean to iteratively optimize ln p(xj1)w.r.t.1?
A: One way to think about it is that we want a method for generating:
1. A sequence of values for 1such that ln p(xj(t)
1)lnp(xj(t 1)
1).
2. We want (t)
1to converge to a local maximum of ln p(xj1).
It doesn’t matter how we generate the sequence (1)
1;(2)
1;(3)
1;:::
We will show how EM generates #1 and just mention that EM satisﬁes #2.
THEEM ALGORITHM
The EM objective function
lnp(xj1) =Z
q(2)lnp(x;2j1)
q(2)d2
|{z}
deﬁne this to beL(x;1)+Z
q(2)lnq(2)
p(2jx;1)d2
|{z}
Kullback-Leibler divergence
Deﬁnition: The EM algorithm
Given the value(t)
1,ﬁnd the value(t+1)
1 as follows:
E-step : Set qt(2) =p(2jx;(t)
1)and calculate
Lqt(x;1) =Z
qt(2)lnp(x;2j1)d2 Z
qt(2)lnqt(2)d2
|{z}
can ignore this term:
M-step : Set(t+1)
1 =arg max1Lqt(x;1).
PROOF OF MONOTONIC IMPROVEMENT
Once we’re comfortable with the moving parts, the proof that the sequence
(t)
1monotonically improves ln p(xj1)just requires analysis :
lnp(xj(t)
1) =L(x;(t)
1) + KL
q(2)kp(2jx1;(t)
1)
|{z}
=0 by setting q=p
=Lqt(x;(t)
1) E-step
 L qt(x;(t+1)
1) M-step
 L qt(x;(t+1)
1) + KL
qt(2)kp(2jx1;(t+1)
1)
|{z }
>0 because q6=p
=L(x;(t+1)
1) + KL
q(2)kp(2jx1;(t+1)
1)
=lnp(xj(t+1)
1)
ONE ITERATION OF EM
Start : Current setting of 1andq(2)
LKL(q| |p)}lnp(X|θ1)
(X|θ1)
Some arbitrary point < 0
For reference :
lnp(xj1) =L+KL
L =Z
q(2)lnp(x;2j1)
q(2)d2
KL =Z
q(2)lnq(2)
p(2jx;1)d2
ONE ITERATION OF EM
E-step : Set q(2) =p(2jx;1)and updateL.
Some arbitrary point < 0lnp(X|θ1) LKL(q| |p) = 0
(X|θ1)
For reference :
lnp(xj1) =L+KL
L =Z
q(2)lnp(x;2j1)
q(2)d2
KL =Z
q(2)lnq(2)
p(2jx;1)d2
ONE ITERATION OF EM
M-step : MaximizeLwrt1. Now q6=p.
lnp(X|θ1up) LKL(q| |p)}
Some arbitrary point < 0(X|θ1up)
For reference :
lnp(xj1) =L+KL
L =Z
q(2)lnp(x;2j1)
q(2)d2
KL =Z
q(2)lnq(2)
p(2jx;1)d2
EM FOR MISSING DATA
THE PROBLEM
We have a data matrix with missing entries. We model the columns as
xiiidN(;):
Our goal could be to
1. Learnandusing maximum likelihood
2. Fill in the missing values “intelligently” (e.g., using a model)
3. Both
We will see how to achieve both of these goals using the EM algorithm.
EM FOR SINGLE GAUSSIAN MODEL WITH MISSING DATA
The original, generic EM objective is
lnp(xj1) =Z
q(2)lnp(x;2j1)
q(2)d2+Z
q(2)lnq(2)
p(2jx;1)d2
The EM objective for this speciﬁc problem and notation is
nX
i=1lnp(xo
ij;) =nX
i=1Z
q(xm
i)lnp(xo
i;xm
ij;)
q(xm
i)dxm
i+
nX
i=1Z
q(xm
i)lnq(xm
i)
p(xm
ijxo
i;;)dxm
i
We can calculate everything required to do this.
E-STEP (PART ONE )
Setq(xm
i) =p(xm
ijxo
i;;)using current ,
Letxo
iandxm
irepresent the observed and missing dimensions of xi. For
notational convenience, think
xi=xo
i
xm
i
No
i
m
i
;oo
iom
i
mo
imm
i
Then we can show that p(xm
ijxo
i;;) = N(bi;bi), where
bi=m
i+ mo
i(oo
i) 1(xo
i o
i);bi= mm
i mo
i(oo
i) 1om
i:
It doesn’t look nice, but these are just functions of sub-vectors of and
sub-matrices of using the relevant dimensions deﬁned by xi.
E-STEP (PART TWO )
E-step: Eq(xm
i)[lnp(xo
i;xm
ij;)]
For each iwe will need to calculate the following term,
Eq[(xi )T 1(xi )] = Eq[tracef 1(xi )(xi )Tg]
=tracef 1Eq[(xi )(xi )T]g
The expectation is calculated using q(xm
i) =p(xm
ijxo
i;;). So only the xm
i
portion of xiwill be integrated.
To this end, recall q(xm
i) =N(bi;bi). We deﬁne
1.bxi: A vector where we replace the missing values in xiwithbi.
2.bVi: A matrix of 0’s, plus sub-matrix biin the missing dimensions.
M-STEP
M-step: MaximizePn
i=1Eq[lnp(xo
i;xm
ij;)]
We’ll omit the derivation, but the expectation can now be solved and
up;up=arg max
;nX
i=1Eq[lnp(xo
i;xm
ij;)]
can be found. Recalling the bnotation,
up=1
nnX
i=1bxi;
up=1
nnX
i=1f(bxi up)(bxi up)T+bVig
Then return to the E-step to calculate the new p(xm
ijxo
i;up;up).
IMPLEMENTATION DETAILS
We need to initialize and, for example, by setting missing values to zero
and calculating MLandML. (We can also use random initialization.)
The EM objective function is then calculated after each update to and
and will look like the ﬁgure above. Stop when the change is “small.”
The output is ML,MLandq(xm
i)for all missing entries.
CMSC 422 Introduction to Machine LearningLecture 2 Decision TreesFurong Huang / furongh@cs.umd.edu
Last week: introducing machine learningWhat does “learning by example” mean?Classification tasksLearning requires examples + inductive biasGeneralization vs. memorizationFormalizing the learning problemFunction approximationLearning as minimizing expected loss0poo
Supervised LearningInput!∈#An item !drawn from an input space #Outputy∈%An item ydrawn from an output space %
Systemy='(!)Consider systems that apply a function '() to input items !and return outputs *='(!).
Supervised LearningInput!∈#An item !drawn from an input space #Outputy∈%An item ydrawn from an output space %
Systemy='(!)In (supervised) machine learning, we deal with systems whose '(!)is learned from examples.
Supervised LearningInput!∈#An item !drawn from an input space #Outputy∈%An item ydrawn from an output space %
Systemy='(!)We typically use machine learning when the function '(!)we want the system to apply is unknown to us, and we cannot “think” about it.
Supervised LearningInput!∈#An item !drawn from an instance space #Outputy∈%An item &drawn from a label space %
Learned Modely=((!)
Target functiony=+(!)
Supervised Learning: TrainingLabeledTraining Data!train("#,%1)("(,%2)⋮("+,%,)Learnedmodel-(")Learning Algorithm
Given the training examples in !trainThe learner returns a model -(")
Supervised Learning: TestingLabeledTest Data!test("#′,&1′)(")′,&2′)⋮(",′,&-′)
Supervised Learning: TestingLabeledTest Data!test("#′,&1′)(")′,&2′)⋮(",′,&-′)RawTest Data.test"#′")′⋮",′TestLabels/test&1′&2′⋮&-′
Supervised Learning: Testing
•Apply the model to the raw test data•Evaluate by comparing predicted labels against the test labelsRawTest Data!test"#′"%′⋮"'′Learnedmodel((")
TestLabels+test,1′,2′⋮,/′PredictedLabels(	(!test)(("#′)(("%′)⋮(("'′)
Can we use the test data otherwise?
Learning formally•Given: examples (",$("))of unknown function $•Find: A good approximation of $•"provides some representationof the input ØFeature extraction: the process of mapping a domain element into a representation.Ø"∈0,1),"∈ℝ)•The target function $() (label)Ø$"∈{−1,+1}Binary classificationØ$"∈1,2,3,…,2−1Multi-class classificationØ$"∈ℝRegression
Learning formally -continued•Hypothesis spaceØSet of possible instances X={$∈&}ØSet of possible outputs Y={y∈*}ØThe set of function hypotheses that provide some mapping from the input to output space +=ℎ		ℎ:$→y,$∈&,y∈*}ØSize of hypothesis space
Machine Learning as Function Approximation•Problem settingØSet of possible instancesX={$∈&}ØUnknown target function (:Y={y∈,}ØSet of function hypotheses -=ℎ		ℎ:$→y,$∈&,y∈,}•InputØTraining examples {$2,32,…$5,35}of unknown target function (•OutputØHypothesis ℎ	∈-	that best approximates target function (
Formalizing induction:Loss Function!(#,%(&))where #	is the truth and %&	is the system’s predictione.g.	!#,%(&)	=	*0					,%	#=%(&)1							./ℎ123,41Captures our notion of what is important to learn
Formalizing induction:Data generating distributionWhere does the data come from?•Data generating distributionA probability distribution !over (#,%)pairs•We don’t know what !is!We only get a random sample from it: our training data
Formalizing induction:Expected loss•!should make good predictions•as measured by loss "•on futureexamples that are also drawn from #•Formally•$	, the expected loss of !	over #with respect to "should be small$≜'(,*~,"(.,!(/))=	∑#/,."(.,!(/))((,*)
Formalizing induction:Training error•We can’t compute expected loss because we don’t know what !is•We only have a sample of !•training examples {#$,&$,…#(,&(}•All we can compute is the training error*̂≜	.101(&3,4(#3))(36$
Formalizing Induction•Given•a loss function !•a sample from some unknowndata distribution "•Our task is to compute a function f that has low expected error over "with respect to !.#$,&~(!(*,+(,))=		0",,*!(*,+(,))($,&)
Today: Decision Trees•What is a decision tree?•How to learn a decision tree from data?•What is the inductive bias?•Generalization?
An example training set

A decision treeto decide whether to play tennis

Decision Trees•RepresentationØEach internal node tests a featureØEach branch corresponds to a feature valueØEach leaf node assigns a classificationvor a probability distribution over classifications•Decision trees represent functions that map examples in X to classes in Y

ExerciseHow would you represent the following Boolean functions with decision trees?!∩#!∪#Take home exercise: !∩#∪(&∩¬()
Today: Decision Trees•What is a decision tree?•How to learn a decision tree from data?•What is the inductive bias?•Generalization?
Function Approximationwith Decision Trees•Problem settingØSet of possible instances !vEach instance "	∈!is a feature vector "=["',…,"*]ØUnknown target function ,:!	→/v/is discrete valued ØSet of function hypotheses 0=ℎ		ℎ:!	→/}vEach hypothesis ℎis a decision tree•InputØTraining examples {4',5',…46,56}of unknown target function ,•OutputØHypothesis ℎ	∈0	that best approximates target function ,
Decision Trees Learning•Finding the hypothesis ℎ	∈$ØThat minimizes training errorØOr maximizes training accuracy•How? Ø$	is too large for exhaustive search!ØWe will use a heuristic search algorithm whichvPicks questions to ask, in ordervSuch that classification accuracy is maximized
Top-down Inductionof Decision TreesCurrentNode= RootDTtrain(examples for CurrentNode, features at CurrentNode):1.Find F, the “best” decision feature for next node2.For each value of F, create new descendant of node3.Sort training examples to leaf nodes4.If training examples perfectly classifiedStopElseRecursively apply DTtrainover new leaf nodes 
How to select the “best” feature?•A good feature is a feature that lets us make correct classification decision•One way to do this:Øselect features based on their classification accuracy•Let’s try it on the PlayTennisdataset
Let’s build a decision tree using features O, T, H, W

Partitioning examples according to Humidity feature

Partitioning examples:H  = Normal

Partitioning examples:H = Normal and W = Strong

Decision Trees
•Can represent any Boolean Function•Can be viewed as a way to compactly represent a lot of data.  •The evaluation of the Classifier is easy•Clearly, given data, there are many ways to represent it as a decision tree.•Learning a good representation from data is the challenge.

Will I play tennis today?•FeaturesØOutlook:{Sun, Overcast, Rain}ØTemperature:{Hot, Mild, Cool}ØHumidity:{High, Normal, Low}ØWind:{Strong, Weak}•LabelsØBinary classification task: Y = {+, -}
Will I play tennis today?OTHWPlay?1SHHW-2SHHS-3OHHW+4RMHW+5RCNW+6RCNS-7OCNS+8SMHW-9SCNW+10RMNW+11SMNS+12OMHS+13OHNW+14RMHS-ØOutlook:S(unny),O(vercast),R(ainy)ØTemperature:H(ot), M(ild), C(ool)ØHumidity:H(igh), N(ormal), L(ow)ØWind:S(trong), W(eak)
Will I play tennis today?OTHWPlay?1SHHW-2SHHS-3OHHW+4RMHW+5RCNW+6RCNS-7OCNS+8SMHW-9SCNW+10RMNW+11SMNS+12OMHS+13OHNW+14RMHS-
ØData is processed in Batch (i.e. all the data available)ØRecursively build a decision tree top down.

Top-down Inductionof Decision TreesCurrentNode= RootDTtrain(examples for CurrentNode, features at CurrentNode):1.Find F, the “best” decision feature for next node2.For each value of F, create new descendant of node3.Sort training examples to leaf nodes4.If training examples perfectly classifiedStopElseRecursively apply DTtrainover new leaf nodes 
Picking the Root Attribute•The goal is to have the resulting decision tree as small as possible (Occam’s Razor)ØHowever, finding the minimal decision tree consistent with the data is NP-hard•The recursive algorithm is a greedy heuristic search for a simple tree, but cannot guarantee optimality.•The main decision in the algorithm is the selection of the next attribute to condition on.
Picking the Root AttributeTraining Data with 2 Boolean attributes (A,B)( (A=0,B=0), -): 50 examples( (A=0,B=1), -): 50 examples( (A=1,B=0), -): 0 examples( (A=1,B=1), +): 100 examplesWhat should be the first attribute we select?Splitting on A: we get purely labeled nodesSplitting on B: we don’t get purely labeled nodes

Picking the Root AttributeTraining Data with 2 Boolean attributes (A,B)( (A=0,B=0), -): 50 examples( (A=0,B=1), -): 50 examples( (A=1,B=0), -): 3examples( (A=1,B=1), +): 100 examplesTrees looks structurally similar; which attribute should we choose?

Another feature selection criterion: Entropy•Used in the ID3 algorithm [Quinlan, 1963]Øpick feature with smallest entropy to split the examples at current iteration•Entropy measures impurity of a sample of examples


Sample Entropy
HighEntropy–Highlevel of uncertaintyLowEntropy –Lowlevel of uncertainty
Information Gain•The information gain of an attribute !is the expected reduction in entropy caused by partitioning on this attribute"!#$%,!=($)*+,-%−/|%1||%|($)*+,-(%1)1∈156789(5)ØOriginal set is %.	Ø	%1	is the subset of %for which attribute !has value <. ØThe entropy of partitioning the data is calculated by weighing the entropy of each partition by its size relative to the original set.vPartitions of low entropy (imbalanced splits) lead to high gainØTake Home Exercise: go back and check which of the A, B splits is better. 
Will I play tennis today?OTHWPlay?1SHHW-2SHHS-3OHHW+4RMHW+5RCNW+6RCNS-7OCNS+8SMHW-9SCNW+10RMNW+11SMNS+12OMHS+13OHNW+14RMHS-ØOutlook:S(unny),O(vercast),R(ainy)ØTemperature:H(ot), M(ild), C(ool)ØHumidity:H(igh), N(ormal), L(ow)ØWind:S(trong), W(eak)
Will I play tennis today?OTHWPlay?1SHHW-2SHHS-3OHHW+4RMHW+5RCNW+6RCNS-7OCNS+8SMHW-9SCNW+10RMNW+11SMNS+12OMHS+13OHNW+14RMHS-ØCurrent entropy:P = 9/14N =5/14ØH(Y)=-(9/14) log2(9/14)-(5/14) log2(5/14)≈0.94
Will I play tennis today?OTHWPlay?1SHHW-2SHHS-3OHHW+4RMHW+5RCNW+6RCNS-7OCNS+8SMHW-9SCNW+10RMNW+11SMNS+12OMHS+13OHNW+14RMHS-ØOutlook= sunny:p=2/5n =3/5 HS=0.971ØOutlook=overcast:p=4/4 n =0 HO=0ØOutlook= rainy:p=3/5 n =2/5 HR=0.971ØExpected entropy514×0.971+414×0+514×0.971=+.,-.ØInformation gain:0.940-0.694 = 0.246
Will I play tennis today?OTHWPlay?1SHHW-2SHHS-3OHHW+4RMHW+5RCNW+6RCNS-7OCNS+8SMHW-9SCNW+10RMNW+11SMNS+12OMHS+13OHNW+14RMHS-ØHumidity= sunny:p=3/7n =4/7 HH=0.985ØHumidity=overcast:p=6/7 n =1/7 HO=0.592ØExpected entropy714×0.985+714×0.592=-.../0ØInformation gain:0.940-0.7785 = 0.1515
Which feature to split on?OTHWPlay?1SHHW-2SHHS-3OHHW+4RMHW+5RCNW+6RCNS-7OCNS+8SMHW-9SCNW+10RMNW+11SMNS+12OMHS+13OHNW+14RMHS-ØInformation gain:Outlook: 0.246Humidity: 0.151Wind: 0.048Temperature: 0.029üSplit on Outlook
An Illustrative Example (III)OTHWPlay?1SHHW-2SHHS-3OHHW+4RMHW+5RCNW+6RCNS-7OCNS+8SMHW-9SCNW+10RMNW+11SMNS+12OMHS+13OHNW+14RMHS-
Continue until:•Every attribute is included in path, or,•All examples in the leaf have same label
An Illustrative Example (IV)

An Illustrative Example (V)

induceDecisionTree(S)1.Does !uniquely define a class?ifall "∈!have the same label y: return!;2.Find the feature with the most information gain:%=argmax,-.%/(!,2,)3.Add children to !	:for5inValues(2,):!6="∈!2,=5addChild(!	,!6)induceDecisionTree(!6)returnS;
An Illustrative Example (VI)

Hypothesis Space in Decision Tree Induction•Conduct a search of the space of decision trees which can represent all possible discrete functions. (pros and cons)•Goal: to find the bestdecision tree •Finding a minimal decision tree consistent with a set of data is NP-hard.•Performs a greedy heuristic search: hill climbing without backtracking.•Makes statistically based decisions using alldata.
A decision tree to distinguish homes in New York from homes in San FranciscoTake a look at home:http://www.r2d3.us/visual-intro-to-machine-learning-part-1/
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.eduvCheck out course webpage, Canvas, PiazzavSubmit HW01qdue Thursday 10:30amvDo the readings!
COMS 4721: Machine Learning for Data Science
Lecture 13, 3/2/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
BOOSTING
Robert E. Schapire and Yoav Freund, Boosting: Foundations and Algorithms , MIT Press, 2012.
See this textbook for many more details. (I borrow some ﬁgures from that book.)
BAGGING CLASSIFIERS
Algorithm: Bagging binary classiﬁers
Given (x1;y1);:::; (xn;yn),x2X,y2f  1;+1g
IForb=1;:::; B
ISample a bootstrap dataset Bbof size n. For each entry in Bb, select (xi;yi)
with probability1
n. Some (xi;yi)will repeat and some won’t appear in Bb.
ILearn a classiﬁer fbusing data in Bb.
IDeﬁne the classiﬁcation rule to be
fbag(x0) =sign BX
b=1fb(x0)!
:
IWith bagging, we observe that a committee of classiﬁers votes on a label.
IEach classiﬁer is learned on a bootstrap sample from the data set.
ILearning a collection of classiﬁers is referred to as an ensemble method .
BOOSTING
How is it that a committee of blockheads can somehow arrive at highly reasoned decisions,
despite the weak judgment of the individual members?
- Schapire & Freund, “Boosting: Foundations and Algorithms”
Boosting is another powerful method for ensemble learning. It is similar to
bagging in that a set of classiﬁers are combined to make a better one.
It works for any classiﬁer, but a “weak” one that is easy to learn is usually
chosen. (weak =accuracy a little better than random guessing)
Short history
1984 : Leslie Valiant and Michael Kearns ask if “boosting” is possible.
1989 : Robert Schapire creates ﬁrst boosting algorithm.
1990 : Yoav Freund creates an optimal boosting algorithm.
1995 : Freund and Schapire create AdaBoost (Adaptive Boosting),
the major boosting algorithm.
BAGGING VS BOOSTING (OVERVIEW )
Training sampleWeighted sampleWeighted sampleWeighted sample
Training sampleBootstrap sampleBootstrap sampleBootstrap sample f3(x)
f2(x)f3(x)
f2(x)
f1(x) f1(x)
Bagging Boosting
THEADABOOST ALGORITHM (SAMPLING VERSION )
Training sampleWeighted sampleWeighted sampleWeighted sample α3, f3(x)
α2, f2(x)
α1, f1(x)
BoostingSample and 
classify B 3
weighted
error ε1 Sample and 
classify B 2
Sample and 
classify B 1weighted
error ε2
fboost(x0) =sign TX
t=1tft(x0)!
THEADABOOST ALGORITHM (SAMPLING VERSION )
Algorithm: Boosting a binary classiﬁer
Given (x1;y1);:::; (xn;yn),x2X,y2f  1;+1g, setw1(i) =1
nfori=1 :n
IFort=1;:::; T
1. Sample a bootstrap dataset Btof size naccording to distribution wt.
Notice we pick (xi;yi)with probability wt(i)and not1
n.
2. Learn a classiﬁer ftusing data in Bt.
3. Sett=Pn
i=1wt(i)1fyi6=ft(xi)gandt=1
2ln
1 t
t
.
4. Scale ^wt+1(i) =wt(i)e tyift(xi)and set wt+1(i) =^wt+1(i)P
j^wt+1(j):
ISet the classiﬁcation rule to be
fboost(x0) =signPT
t=1tft(x0)
:
Comment : Description usually simpliﬁed to “learn classiﬁer ftusing distribution wt.”
BOOSTING A DECISION STUMP (EXAMPLE 1)
+
+++
+
--
-
--
Original data
Uniform distribution, w1
Learn weak classiﬁer
Here: Use a decision stump
x1>1.7
ˆy= 1 ˆy= 3
BOOSTING A DECISION STUMP (EXAMPLE 1)
+
+++
+
--
-
--
Round 1 classiﬁer
Weighted error: 1=0:3
Weight update: 1=0:42
BOOSTING A DECISION STUMP (EXAMPLE 1)
+
+++
+
-
-
-
--
Weighted data
After round 1
BOOSTING A DECISION STUMP (EXAMPLE 1)
+
+++
+
-
-
-
--
Round 2 classiﬁer
Weighted error: 2=0:21
Weight update: 2=0:65
BOOSTING A DECISION STUMP (EXAMPLE 1)
+
+++
+
--
-
--
Weighted data
After round 2
BOOSTING A DECISION STUMP (EXAMPLE 1)
+
+++
+
--
-
--
Round 2 classiﬁer
Weighted error: 3=0:14
Weight update: 3=0:92
BOOSTING A DECISION STUMP (EXAMPLE 1)
+
+++
+
--
-
--
Classiﬁer after three rounds
+
+0.42x
0.65x
0.92x
BOOSTING A DECISION STUMP (EXAMPLE 2)
Example problem
Random guessing
50% error
Decision stump
45.8% error
Full decision tree
24.7% error
Boosted stump
5.8% error
BOOSTING
Point = one dataset. Location = error rate w/ and w/o boosting. The boosted
version of the same classiﬁer almost always produces better results.
BOOSTING
(left) Boosting a bad classiﬁer is often better than not boosting a good one.
(right) Boosting a good classiﬁer is often better, but can take more time.
BOOSTING AND FEATURE MAPS
Q: What makes boosting work so well?
A: This is a well-studied question. We will present one analysis later, but
we can also give intuition by tying it in with what we’ve already learned.
The classiﬁcation for a new x0from boosting is
fboost(x0) =sign TX
t=1tft(x0)!
:
Deﬁne(x) = [ f1(x);:::; fT(x)]>, where each ft(x)2f  1;+1g.
IWe can think of (x)as a high dimensional feature map of x.
IThe vector = [1;:::; T]>corresponds to a hyperplane.
ISo the classiﬁer can be written fboost(x0) =sign((x0)>).
IBoosting learns the feature mapping and hyperplane simultaneously.
APPLICATION : FACE DETECTION
FACE DETECTION (VIOLA & J ONES , 2001)
Problem : Locate the faces in an image or video.
Processing : Divide image into patches of different scales, e.g., 24 24,
4848, etc. Extract features from each patch.
Classify each patch as face or no face using a boosted decision stump . This
can be done in real-time, for example by your digital camera (at 15 fps).
144Viola and Jones
Figure 5.The ﬁrst and second features selected by AdaBoost. Thetwo features are shown in the top row and then overlayed on a typ-ical training face in the bottom row. The ﬁrst feature measures thedifference in intensity between the region of the eyes and a regionacross the upper cheeks. The feature capitalizes on the observationthat the eye region is often darker than the cheeks. The second featurecompares the intensities in the eye regions to the intensity across thebridge of the nose.features to the classiﬁer, directly increases computationtime.4. The Attentional CascadeThis section describes an algorithm for constructing acascade of classiﬁers which achieves increased detec-tion performance while radically reducing computationtime. The key insight is that smaller, and therefore moreefﬁcient, boosted classiﬁers can be constructed whichreject many of the negative sub-windows while detect-ing almost all positive instances. Simpler classiﬁers areused to reject the majority of sub-windows before morecomplex classiﬁers are called upon to achieve low falsepositive rates.Stages in the cascade are constructed by trainingclassiﬁers using AdaBoost. Starting with a two-featurestrong classiﬁer, an effective face ﬁlter can be obtainedby adjusting the strong classiﬁer threshold to mini-mize false negatives. The initial AdaBoost threshold,12/summationtextTt=1αt,isdesigned to yield a low error rate on thetraining data. A lower threshold yields higher detec-tion rates and higher false positive rates. Based on per-formance measured using a validation training set, thetwo-feature classiﬁer can be adjusted to detect 100% ofthe faces with a false positive rate of 50%. See Fig. 5 foradescription of the two features used in this classiﬁer.The detection performance of the two-feature clas-siﬁer is far from acceptable as a face detection system.Nevertheless the classiﬁer can signiﬁcantly reduce thenumber of sub-windows that need further processingwith very few operations:1. Evaluate the rectangle features (requires between 6and 9 array references per feature).2. Compute the weak classiﬁer for each feature (re-quires one threshold operation per feature).3. Combine the weak classiﬁers (requires one multiplyper feature, an addition, and ﬁnally a threshold).Atwofeature classiﬁer amounts to about 60 mi-croprocessor instructions. It seems hard to imaginethat any simpler ﬁlter could achieve higher rejectionrates. By comparison, scanning a simple image tem-plate would require at least 20 times as many operationsper sub-window.The overall form of the detection process is that ofadegenerate decision tree, what we call a “cascade”(Quinlan, 1986) (see Fig. 6). A positive result fromthe ﬁrst classiﬁer triggers the evaluation of a secondclassiﬁer which has also been adjusted to achieve veryhigh detection rates. A positive result from the secondclassiﬁer triggers a third classiﬁer, and so on. A negativeoutcome at any point leads to the immediate rejectionof the sub-window.The structure of the cascade reﬂects the fact thatwithin any single image an overwhelming majority ofsub-windows are negative. As such, the cascade at-tempts to reject as many negatives as possible at theearliest stage possible. While a positive instance will
Figure 6.Schematic depiction of a the detection cascade. A seriesof classiﬁers are applied to every sub-window. The initial classiﬁereliminates a large number of negative examples with very little pro-cessing. Subsequent layers eliminate additional negatives but requireadditional computation. After several stages of processing the num-ber of sub-windows have been reduced radically. Further processingcan take any form such as additional stages of the cascade (as in ourdetection system) or an alternative detection system.
IOne patch from a larger image. Mask it with many “feature extractors.”
IEach pattern gives one number, which is the sum of all pixels in black
region minus sum of pixels in white region (total of 45,000+ features).
FACE DETECTION (EXAMPLE RESULTS )
152Viola and Jones
Figure 10.Output of our face detector on a number of test images from the MIT+CMU test set.6. ConclusionsWehave presented an approach for face detectionwhich minimizes computation time while achievinghigh detection accuracy. The approach was used to con-struct a face detection system which is approximately15 times faster than any previous approach. Preliminaryexperiments, which will be described elsewhere, showthat highly efﬁcient detectors for other objects, such aspedestrians or automobiles, can also be constructed inthis way.This paper brings together new algorithms, represen-tations, and insights which are quite generic and maywell have broader application in computer vision andimage processing.The ﬁrst contribution is a new a technique for com-puting a rich set of image features using the integralimage. In order to achieve true scale invariance, almostall face detection systems must operate on multipleimage scales. The integral image, by eliminating theneed to compute a multi-scale image pyramid, reducesthe initial image processing required for face detection
ANALYSIS OF BOOSTING
ANALYSIS OF BOOSTING
Training error theorem
We can use analysis to make a statement about the accuracy of boosting on
the training data .
Theorem : Under the AdaBoost framework, if tis the weighted error of
classiﬁer ft, then for the classiﬁer fboost(x0) =sign(PT
t=1tft(x0)),
training error =1
nnX
i=11fyi6=fboost(xi)g  exp
 2TX
t=1(1
2 t)2
:
Even if each tis only a little better than random guessing, the sum over T
classiﬁers can lead to a large negative value in the exponent when Tis large.
For example, if we set:
t=0:45;T=1000!training error0:0067.
PROOF OF THEOREM
Setup
We break the proof into three steps. It is an application of the fact that
ifa<b|{z}
Step 2and b<c|{z}
Step 3then a<c|{z}
conclusion
IStep 1 calculates the value of b.
ISteps 2 and 3 prove the two inequalities.
Also recall the following step from AdaBoost:
IUpdate ^wt+1(i) =wt(i)e tyift(xi).
INormalize wt+1(i) =^wt+1(i)P
j^wt+1(j) ! Deﬁne Zt=P
j^wt+1(j).
PROOF OF THEOREM (abc)
Step 1
We ﬁrst want to expand the equation of the weights to show that
wT+1(i) =1
ne yiPT
t=1tft(xi)
QT
t=1Zt:=1
ne yihT(xi)
QT
t=1Zt!hT(x) :=TX
t=1tft(xi)
Derivation of Step 1 :
Notice the update rule: wt+1(i) =1
Ztwt(i)e tyift(xi)
Do the same expansion for wt(i)and continue until reaching w1(i) =1
n,
wT+1(i) =w1(i)e 1yif1(xi)
Z1e TyifT(xi)
ZT
The productQT
t=1Ztis “b” above. We use this form of wT+1(i)in Step 2.
PROOF OF THEOREM (abc)
Step 2
Next show the training error of f(T)
boost(boosting after Tsteps) isQT
t=1Zt.
Currently we know
wT+1(i) =1
ne yihT(xi)
QT
t=1Zt)wT+1(i)TY
t=1Zt=1
ne yihT(xi)& f(T)
boost(x) =sign(hT(x))
Derivation of Step 2 :
Observe that 0 <ez1and 1<ez2for any z1<0<z2. Therefore
1
nnX
i=11fyi6=f(T)
boost(xi)g
|{z}
a1
nnX
i=1e yihT(xi)
=nX
i=1wT+1(i)TY
t=1Zt=TY
t=1Zt
|{z}
b“a” is the training error – the quantity we care about.
PROOF OF THEOREM (abc)
Step 3
The ﬁnal step is to calculate an upper bound on Zt, and by extensionQT
t=1Zt.
Derivation of Step 3 :
This step is slightly more involved. It also shows why t:=1
2ln
1 t
t
.
Zt=nX
i=1wt(i)e tyift(xi)
=X
i:yi=ft(xi)e twt(i) +X
i:yi6=ft(xi)etwt(i)
=e t(1 t) +ett
Remember we deﬁnedt=P
i:yi6=ft(xi)wt(i), the probability of error for wt.
PROOF OF THEOREM (abc)
Derivation of Step 3 (continued):
Remember from Step 2 that
training error =1
nnX
i=11fyi6=fboost(xi)g TY
t=1Zt:
and we just showed that Zt=e t(1 t) +ett.
We want the training error to be small, so we pick ttominimize Z t.
Minimizing, we get the value of tused by AdaBoost:
t=1
2ln1 t
t
:
Plugging this value back in gives Zt=2p
t(1 t).
PROOF OF THEOREM (abc)
Derivation of Step 3 (continued):
Next, re-write Ztas
Zt=2p
t(1 t)
=r
1 4(1
2 t)2
−2 −1 0 1 2−1.5−1−0.500.511.522.533.5
e-x
1-x
Then, use the inequality 1  xe xto conclude that
Zt= 
1 4(1
2 t)21
2
e 4(1
2 t)21
2=e 2(1
2 t)2:
PROOF OF THEOREM
Concluding the right inequality (abc)
Because both sides of Zte 2(1
2 t)2are positive, we can say that
TY
t=1ZtTY
t=1e 2(1
2 t)2=e 2PT
t=1(1
2 t)2:
This concludes the “ bc” portion of the proof.
Combining everything
training error =az}|{
1
nnX
i=11fyi6=fboost(xi)g bz}|{
TY
t=1Ztcz}|{
e 2PT
t=1(1
2 t)2:
We set out to prove “ a<c” and we did so by using “ b” as a stepping-stone.
TRAINING VS TESTING ERROR
Q: Driving the training error to zero leads one to ask, does boosting overﬁt?
A: Sometimes, but very often it doesn’t!
C4.5 (tree) testing error
AdaBoost testing error
AdaBoost training error
Rounds of boostingError
CMSC 422 Introduction to Machine LearningLecture 9 Imbalanced Data and ReductionsFurong Huang / furongh@cs.umd.eduSlides adapted from Prof Carpuatand Duraiswami
Practical Issues: Evaluation, beyond accuracy•So far we’ve measured classification performance using accuracy•But this is not a good metric when some errors matter mode than others•Given medical record, predict whether patient has cancer or not•Given a document collection and a query, find documents in collection that are relevant to query

!"#$%&%'(=*+*++-+.#$/00=*+*++-(Denominator: # of positive predictions
Denominator: # of positive gold labels
A combined measure: F•A combined measure that assesses the P/R tradeoff is F measure !=1$1%+1−$1(=)!+1%()!%+(•People usually use balanced F-1 measure •i.e.,with"=1(that is, %=*+)•Harmonic mean &=+,-,.-'=()()++),=()()++-
Formalizing Errors
The learned classifierset of all possible classifiers using a fixed representation
How far is the learned classifier f from the optimal classifier f*?Quality of the model familyaka hypothesis class∗
The bias/variance trade-off•Trade-off between•approximation error (bias)•estimation error (variance)•Example:•Consider the always positive classifier•Low variance as a function of a random draw of the training set•Strongly biased toward predicting +1 no matter what the input
Recap: practical issues•Learning algorithm is only one of many steps in designing a ML application•Many things can go wrong, but there are practical strategies for•Improving inputs•Evaluating•Tuning•Debugging•Fundamental ML concepts:  estimation vs. approximation error
Imbalanced data distributions•Sometimes training examples are drawn from an imbalanced distribution•This results in an imbalanced training set•“needle in a haystack” problems•E.g., find fraudulent transactions in credit card histories•Why is this a big problem for the ML algorithms we know?
Recall: Machine Learning as Function Approximation•Problem setting•Set of possible instances !•Unknown target function ":!→%•Set of function hypotheses &=ℎℎ:!→%}•Input•Training examples {+,,.,,…+0,.0}of unknown target function "•Output•Hypothesis ℎ∈&that best approximates target function "
Recall:Loss Function!(#,%(&))where #is the truth and %&is the system’s predictione.g.!#,%(&)=)0+%#=%(&)1-.ℎ012+30Captures our notion of what is important to learn
Recall: Expected loss•!should make good predictions•as measured by loss "•on futureexamples that are also drawn from #•Formally•$, the expected loss of !over #with respect to "should be small•$≜&',)~+"(-,!(.))=∑(',))#.,-"(-,!(.))

We define cost of mispredictionas:α> 1for y = +1 1 for y = -1Given a good algorithm for solving the binary classification problem, how can I solve the α-weighted binary classification problem?
Solution: Train a binary classifier on an induced distribution

Subsampling optimalityTheorem:If the binary classifier achieves a binary error rate of ε, then the error rate of the α-weighted classifier is αεLet’s prove it.(see also CIML 6.1)

Strategies for inducing a new binary distribution•Undersamplethe negative class (dominant class)•Oversample the positive class (subordinate class)
Strategies for inducing a new binary distribution•Undersamplethe negative class•More computationally efficient•Oversample the positive class•Base binary classifier might do better with more training examples•Efficient implementations incorporate weight in algorithm, instead of explicitly duplicating data!
ReductionsIdea is to re-use simple and efficient algorithms for binary classification to perform more complex tasksWorks great in practice: E.g., VowpalWabbit
Learning with Imbalanced Datais an Example of Reduction
Subsampling Optimality Theorem:If the binary classifier achieves a binary error rate of ε, then the error rate of the α-weighted classifier is α ε
Multiclass classification•Real world problems often have multiple classes (text, speech, image, biological sequences…)•How can we perform multiclass classification?•Straightforward with decision trees or KNN•Can we use the perceptron algorithm?
Reductions for Multiclass Classification


How many classes can we handle in practice?•In most tasks, number of classes K < 100•For much larger K•we need to frame the problem differently•e.g, machine translation or automatic speech recognition
What you should know•How can we take the standard binary classifier and adapt it to handle problems with•Imbalanced data distributions•Multiclass  classification problems•Algorithms & guarantees on error rate•Fundamental ML concept: reduction
Reduction 1: OVA•“One versus all” (aka “one versus rest”)•Train K-many binary classifiers•classifier k predicts whether an example belong to class k or not•At test time, •If only one classifier predicts positive, predict that class•Break ties randomly

Time complexity•Suppose you have N training examples, in K classes. How long does it take to train an OVA classifier•if the base binary classifier takes O(N) time to learn?•if the base binary classifier takes O(N^2) time to learn?
Error bound•Theorem:Suppose that the average error of the K binary classifiers is ε, then the error rate of the OVA multiclass classifier is at most (K-1)ε•To prove this: how do different errors affect the maximum ratio of the probability of a multiclass error to the number of binary errors (“efficiency”)?
Error bound proof1. If we have a false negativeon one of the binary classifiers (assuming all other classifiers correctly output negative) What is the probability that we will make an incorrect multiclass prediction?(K –1) / KEfficiency: [( K –1) / K] / 1 = (K –1 ) / K
Error bound proof2. If we have mfalse positiveswith the binary classifiersWhat is the probability that we will make an incorrect multiclass prediction?If there is also a false negative: 1Efficiency =1 / (m + 1)Otherwise  m / ( m + 1)Efficiency = [m / (m + 1)] / m = 1 / ( m + 1)
Error bound proof3. What is the worst case scenario?False negative case: efficiency is (K-1)/KLarger than false positive efficiencies There are K-many opportunities to get false negative, overall error bound is (K-1)ε
Reduction 2:  AVAAll versus all (aka all pairs)How many binary classifiers does this require?

Time complexity•Suppose you have N training examples, in K classes. How long does it take to train an AVA classifier•if the base binary classifier takes O(N) time to learn?•if the base binary classifier takes O(N^2) time to learn?
Error boundTheorem:Suppose that the average error of the K binary classifiers is ε, then the error rate of the AVA multiclass classifier is at most 2(K-1)εQuestion: Does this mean that AVA is always worse than OVA?
ExtensionsDivide and conquerOrganize classes into binary tree structuresUse confidence to weight predictions of binary classifiersInstead of using majority vote
TopicsGiven an arbitrary method for binary classification, how can we learn to make multiclass predictions?OVA, AVAFundamental ML concept: reductions
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
Coreference&	Entity	LinkingProf.	Sameer	SinghCS	295:	STATISTICAL	NLPWINTER	2017March	9,	2017Based	on	slides	from	Dan	Klein,	Mark	Greenwood,	and	everyone	else	they	copied	from.
Upcoming…•Homework	4	is	due	on	March	13•Lowest	grade	of	the	homeworkswill	be	droppedHomework
•Final	report	due:	March	20,	2017•Instructions	coming	soon,	only	5	pagesProject•Paper	summaries:	March	14•Summary	2	gradedSummaries
CS	295:	STATISTICAL	NLP	(WINTER	2017)2TA/Instructor	Evaluations	are	available!
OutlineCoreferenceResolutionEntity	LinkingQuestion	Answering
CS	295:	STATISTICAL	NLP	(WINTER	2017)3
OutlineCoreferenceResolutionEntity	LinkingQuestion	Answering
CS	295:	STATISTICAL	NLP	(WINTER	2017)4
My	girlfriend	and	I	met	my	lawyer	for	a	drink,
CoreferenceResolutionbut	she	became	ill	and	had	to	leave.
CS	295:	STATISTICAL	NLP	(WINTER	2017)5
WinogradSchemaThe	city	councilmen	refused	the	demonstrators	a	permit	because	they	feared	violence.
The	city	councilmen	refused	the	demonstrators	a	permit	because	they	advocated	violence.CS	295:	STATISTICAL	NLP	(WINTER	2017)6
CoreferenceAmbiguities
CS	295:	STATISTICAL	NLP	(WINTER	2017)7

At	a	Document	Level
CS	295:	STATISTICAL	NLP	(WINTER	2017)8

At	a	Document	Level
CS	295:	STATISTICAL	NLP	(WINTER	2017)9

At	a	Document	Level
CS	295:	STATISTICAL	NLP	(WINTER	2017)10

Mentions	and	Entities
CS	295:	STATISTICAL	NLP	(WINTER	2017)11

Applications
CS	295:	STATISTICAL	NLP	(WINTER	2017)12Relation	ExtractionHe	married	her	in	1927.Sentiment	AnalysisI	really	loved	the	movie…I	liked	how	evil	the	villain's	plan	was.It	was	abhorrent!QA/Dialog	SystemsFind	me	a	flight	from	LA	to	London,	make	sure	it	is	not	too	long.SummarizationA	shooting	took	place	in	Wallingford	this	morning…The	shooter	targeted	two	women..	He	is	a	34-year	old	…
Semantics	vs	Pragmatics
CS	295:	STATISTICAL	NLP	(WINTER	2017)13One	tries	to	be	as	informative	as	one	possibly	can,and	gives	as	much	information	as	is	needed,	and	no	more.-Grice’s	Maxim	of	QuantitySemanticsWhat	does	the	sentence	mean?PragmaticsWhat	does	the	sentence	imply?
Example:	Semantic/Pragmatic
CS	295:	STATISTICAL	NLP	(WINTER	2017)14
Semantics•“UC	Irvine”	is	a	team/player•It	has	not	lost	yet•The	event	in	question	might	be	the	“Big	West	Tournament”Pragmatics•UC	Irvine	is	likely	to	win•A	team	has	to	win	for	UC	Irvine	to	lose•Other	team	is	not	good?•BW	Tournament	is	a	college	sport•Must	be	a	big	deal,	all	of	west	coast?•Must	be	happening	soon?•“Primer”:	Must	be	important!http://www.midmajormadness.com/2017/3/8/14837458/big-west-tournament-primer-uc-irvine-anteaters-how-to-watch-prediction-bracket-champ-week
Reverse	Pragmatics
CS	295:	STATISTICAL	NLP	(WINTER	2017)15

Antecedents	/	Anaphor
CS	295:	STATISTICAL	NLP	(WINTER	2017)16CataphorAfter she won the lottery, Susan quit her job.

Types:	Proper	Names
CS	295:	STATISTICAL	NLP	(WINTER	2017)17
Lexical	similarity
Types:	Pronouns
CS	295:	STATISTICAL	NLP	(WINTER	2017)18President	Barack	Obamareceived	the	Serve	America	Act	after	Congress’s	vote.	He	…	President	Barack	Obamamet	with	Chancellor	Merkel.He	…	President	Barack	Obamamet	with	President	Hollandeafter	…he	signed	the	bill.he	flew	in	from	Paris.“agreement”,salience
Types:	Nominals
CS	295:	STATISTICAL	NLP	(WINTER	2017)19lexical	semantics,	world	knowledge,salience

Learning-based	Methods
CS	295:	STATISTICAL	NLP	(WINTER	2017)20

Learning-based	Methods
CS	295:	STATISTICAL	NLP	(WINTER	2017)21

Evaluation
CS	295:	STATISTICAL	NLP	(WINTER	2017)22
https://xkcd.com/927/
Evaluation	Metrics
CS	295:	STATISTICAL	NLP	(WINTER	2017)23CONLLMUC“How	many	antecedents	did	you	get	right?”Pairwise“How	many	total	edges	did	you	get	right?”B3Metric“How	many	edges	in	predicted	clusters	did	you	get	right?”CEAF“Do	a	maximum	matching	between	predicted	and	gold	entities;how	close	are	they?”
OutlineCoreferenceResolutionEntity	LinkingQuestion	Answering
CS	295:	STATISTICAL	NLP	(WINTER	2017)24
25...during the late 60's and early 70's, Kevin Smithworked with several local......the term hip-hop is attributed to LovebugStarski. What does it actually mean...The filmmaker Kevin Smithreturns to the role of Silent Bob...Nothing could be more irrelevant to Kevin Smith's audacious ''Dogma'' than ticking off...Like Back in 2008, the Lions drafted Kevin Smith, even though Smith was badly...... backfield in the wake of Kevin Smith's knee injury, and the addition of Haynesworth...... The Physiological Basis of Politics,” by Kevin Smith, Douglas Oxley, Matthew Hibbing...
Entity	Resolution	&	Linking
CS	295:	STATISTICAL	NLP	(WINTER	2017)
World	Knowledge
CS	295:	STATISTICAL	NLP	(WINTER	2017)26

World	Knowledge
CS	295:	STATISTICAL	NLP	(WINTER	2017)27

Entity	Names:	Two	ProblemsDifferent	Names	for	Entities
Inconsistent	ReferencesMSFT,	APPL,	GOOG…Typos/MisspellingsBaarak,	Barak,	Barrack,	…Nick	NamesBam	Bam,	Drumpf,	…Entities	with	Same	Name
Partial	ReferenceThings	named	after	each	otherFirst	names	of	people,	Location	instead	of	team	name,	Nick	namesClinton,	Washington,	Paris,	Amazon,	Princeton,	Kingston,	…Same	type	of	entities	share	namesKevin	Smith,	John	Smith,	Springfield,	…
28CS	295:	STATISTICAL	NLP	(WINTER	2017)
Evaluating	Entity	Linking
CS	295:	STATISTICAL	NLP	(WINTER	2017)29
Baseline:	Link	Probabilities
CS	295:	STATISTICAL	NLP	(WINTER	2017)30Washington drops 10 points after game with UCLA Bruins.Washington	DC,George	Washington,Washington	state,Lake	Washington,Washington	Huskies,Denzel	Washington,University	of	Washington,Washington	High	School,…Washington
Entity	Linking	Approach
31Washington drops 10 points after game with UCLA Bruins.Candidate	GenerationWashington	DC,	George	Washington,	Washington	state,Lake	Washington,	Washington	Huskies,	Denzel	Washington,University	of	Washington,	Washington	High	School,	…Entity	TypesWashington	DC,	George	Washington,	Washington	state,Lake	Washington,	Washington	Huskies,	Denzel	Washington,University	of	Washington,	Washington	High	School,	…LOC/ORGCoreferenceWashington	DC,	George	Washington,	Washington	state,Lake	Washington,	Washington	Huskies,	Denzel	Washington,University	of	Washington,	Washington	High	School,	…UWashington,HuskiesCoherenceUCLA	Bruins,USC	TrojansWashington	DC,	George	Washington,	Washington	state,Lake	Washington,	Washington	Huskies,	Denzel	Washington,University	of	Washington,	Washington	High	School,	…Vinculum,	Ling,	Singh,	Weld,	TACL	(2015)CS	295:	STATISTICAL	NLP	(WINTER	2017)
Global	Inference
CS	295:	STATISTICAL	NLP	(WINTER	2017)32

OutlineCoreferenceResolutionEntity	LinkingQuestion	Answering
CS	295:	STATISTICAL	NLP	(WINTER	2017)33
Questions	are	very	common
CS	295:	STATISTICAL	NLP	(WINTER	2017)34who	invented	surf	music?	how	to	make	stink	bombs	where	are	the	snowdensof	yesteryear?	which	englishtranslation	of	the	bible	is	used	in	official	catholic	liturgies?	how	to	do	clayarthow	to	copy	psxhow	tall	is	the	sears	tower?	how	can	ifind	someone	in	texaswhere	can	ifind	information	on	puritan	religion?	what	are	the	7	wonders	of	the	world	how	can	ieliminate	stress	What	vacuum	cleaner	does	Consumers	Guide	recommendAround	10-15%	of	search	queries
Applications	of	QASchema-specific	matching	of	text	to	SQL	queries
CS	295:	STATISTICAL	NLP	(WINTER	2017)35“List	the	authors	who	have	written	books	about	business”SELECTfirstname, lastnameFROMauthors, titleauthor,titles WHEREauthors.id= titleauthor.authors_idANDtitleauthor.title_id= titles.idNatural	Language	Database	Systems
Early	Systems:	BASESBALL	(1961)	and	LUNAR	(1977)
Applications	of	QADomain-specific	dialogs	from	an	environment
CS	295:	STATISTICAL	NLP	(WINTER	2017)36Spoken	Dialog	Systems
Early	Work:	SHRDLU,	Winograd(1972)
Applications	of	QAQuestions	from	a	paragraph,	answers	in	them.
CS	295:	STATISTICAL	NLP	(WINTER	2017)37Reading	Comprehension
Early	Work:	QUALM,	Lehnert(1977)How	Maple	Syrup	is	MadeMaple	syrup	comes	from	sugar	maple	trees.		At	one	time,	maple	syrup	was	used	to	make	sugar.		This	is	why	the	tree	is	called	a	"sugar"	maple	tree.		Sugar	maple	trees	make	sap.		Farmerscollect	the	sap.		The	best	time	to	collect	sap	is	in	February	and	March.		The	nights	must	be	cold	and	the	days	warm.		The	farmer	drills	a	few	small	holes	in	each	tree.		He	puts	a	spout	in	each	hole.		Then	he	hangs	a	bucket	on	the	end	of	each	spout.		The	bucket	has	a	cover	to	keep	rain	and	snow	out.		The	sap	drips	into	the	bucket.		About	10	gallons	of	sap	come	from	each	hole.	•Who	collects	maple	sap?		(Farmers)•What	does	the	farmer	hang	from	a	spout?		(A	bucket)	•When	is	sap	collected?		(February	and	March)•Where	does	the	maple	sap	come	from?		(Sugar	maple	trees)	•Why	is	the	bucket	covered?		(to	keep	rain	and	snow	out)
Applications	of	QAQuestions	about	anything,	have	huge	corpus	available!
CS	295:	STATISTICAL	NLP	(WINTER	2017)38Open-domain	QALast	15	years	or	so,	the	whole	web!TREC•Annual	competition	of	open-ended	question	answering•Provides	a	text	corpus,	and	a	collection	of	factoidquestions•Made	more	difficult	every	year•Questions	more	“realistic”•Answer	may	not	be	in	the	corpus•Submit	only	one	answer,	not	a	ranked	list,	…“When	was	Mozart	born?”
TREC	Example	Questions
CS	295:	STATISTICAL	NLP	(WINTER	2017)39•Who	is	the	author	of	the	book,	"The	Iron	Lady:	A	Biography	of	Margaret	Thatcher"?	•What	was	the	monetary	value	of	the	Nobel	Peace	Prize	in	1989?	•What	does	the	Peugeot	company	manufacture?•How	much	did	Mercury	spend	on	advertising	in	1993?•What	is	the	name	of	the	managing	director	of	Apricot	Computer?	•Why	did	David	Koresh	ask	the	FBI	for	a	word	processor?•What	debts	did	Qintexgroup	leave?•What	is	the	name	of	the	rare	neurological	disease	with	symptoms	such	as:	involuntary	movements	(tics),	swearing,	and	incoherent	vocalizations	(grunts,	shouts,	etc.)?
Sequence	LabelingProf.	Sameer	SinghCS	295:	STATISTICAL	NLPWINTER	2017January	31,	2017Based	on	slides	from	Nathan	Schneider,	Noah	Smith,	YejinChoi,	and	everyone	else	they	copied	from.
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)2Sequence	Labelling	and	POS	TaggingGenerative	Modeling:	HMMsInference	in	HMMs:	Viterbi	and	F/BUnsupervised	Tagging	using	EM
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)3Sequence	Labelling	and	POS	TaggingGenerative	Modeling:	HMMsInference	in	HMMs:	Viterbi	and	F/BUnsupervised	Tagging	using	EM
Classification
CS	295:	STATISTICAL	NLP	(WINTER	2017)4Sentiment	AnalysisIdentify	TopicLanguage	Model
Sequence	Labeling
CS	295:	STATISTICAL	NLP	(WINTER	2017)5
Parts	of	Speech
CS	295:	STATISTICAL	NLP	(WINTER	2017)6This			is					a						simple				sentence			.DET			VB		DET					ADJ								NOUN							.Applications:•Text	to	speech:	record,	lead,	…•Machine	translation:	run,	walk,	…•Noun	phrases:	`grep{JJ	|	NN}*	{NN	|	NNS}`•andmanyothers…
Parts	of	Speech:	Tags
CS	295:	STATISTICAL	NLP	(WINTER	2017)7
“Open	classes”Nouns,	verbs,	adjectives,adverbs,	numbers“Closed	classes”•Modal	verbs•Prepositions	(on,	to)•Particles	(off,	up)•Determiners	(the,	some)•Pronouns	(she,	they)•Conjunctions	(and,	or)
Named	Entity	Recognition
CS	295:	STATISTICAL	NLP	(WINTER	2017)8Barack	Obama	spoke	from	the	White	House	today		.PER							PER							O									O				O					LOC				LOC								O				O
Field	Segmentation:	Ads
CS	295:	STATISTICAL	NLP	(WINTER	2017)93BR		flat			in	Bruntsfield,		near	main	roads		.			Bright	,	well	maintained	...SIZE	TYPE	O							LOC								O		LOC		LOC			LOC			O			FEAT	O	FEAT				FEAT								...
Field	Segmentation:	Citations
CS	295:	STATISTICAL	NLP	(WINTER	2017)10
AuthorsTitlePublication	Venue
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)11Sequence	Labelling	and	POS	TaggingGenerative	Modeling:	HMMsInference	in	HMMs:	Viterbi	and	F/BUnsupervised	Tagging	using	EM
Naïve	Bayes	Classifier
CS	295:	STATISTICAL	NLP	(WINTER	2017)12
“Transitions”	matter
CS	295:	STATISTICAL	NLP	(WINTER	2017)13
How	do	we	select	a	“consistent”	set	of	POS	tags?“Impossible”	Transitions•Two	determiners	never	follow	each	other•Two	base	form	verbs	never	follow	each	other•Determiner	is	followed	by	adjective	or	nounFruit	flies	likea	bird.Fruit	flies	likebananas.Based	on	semantics
“Transitions”	matter
CS	295:	STATISTICAL	NLP	(WINTER	2017)14

“Transitions”	matter
CS	295:	STATISTICAL	NLP	(WINTER	2017)15Transition	on	Words	versus	Tags•Too	many	words,	learn	the	same	thing	again•Support	for	unseen	words:	“I	like	tenguizino!”
Hidden	Markov	Models
CS	295:	STATISTICAL	NLP	(WINTER	2017)16SE
Example	Sentence
CS	295:	STATISTICAL	NLP	(WINTER	2017)17This			is					a						simple				sentenceDET			VB		DET					ADJ								NOUNSE
Estimating	Emissions
CS	295:	STATISTICAL	NLP	(WINTER	2017)18SESmoothing•Unknown/rare	words	get	inaccurate	probabilities•Reminder:	Laplace	Smoothing	(Add-k)•Next	lecture:	we	will	look	at	“features”
Estimating	Transitions
CS	295:	STATISTICAL	NLP	(WINTER	2017)19SEInterpolation•If	there	are	too	many	tags,	or	too	little	data,	some	combinations	are	too	rare•Same	as	N-gram	language	models,	“backoff”	to	simpler	models
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)20Sequence	Labelling	and	POS	TaggingGenerative	Modeling:	HMMsInference	in	HMMs:	Viterbi	and	F/BUnsupervised	Tagging	using	EM
Predicting	from	HMMs
CS	295:	STATISTICAL	NLP	(WINTER	2017)21
Brute	Force	Inference
CS	295:	STATISTICAL	NLP	(WINTER	2017)22
Conditional	Independence
CS	295:	STATISTICAL	NLP	(WINTER	2017)23SE
Dynamic	Programming
CS	295:	STATISTICAL	NLP	(WINTER	2017)24
State	Lattice
CS	295:	STATISTICAL	NLP	(WINTER	2017)25FruitflieslikebananasR(1,N)R(1,V)R(1,IN)R(2,N)R(2,V)R(2,IN)R(3,N)R(3,V)R(3,IN)R(4,N)R(4,V)R(4,IN)SE
Viterbi	Decoding	Algorithm
CS	295:	STATISTICAL	NLP	(WINTER	2017)26InitializationIterative	Computation	(forward)Follow	pointers	(backward)
Computational	Complexity
CS	295:	STATISTICAL	NLP	(WINTER	2017)27
Outline
CS	295:	STATISTICAL	NLP	(WINTER	2017)28Sequence	Labelling	and	POS	TaggingGenerative	Modeling:	HMMsInference	in	HMMs:	Viterbi	and	F/BUnsupervised	Tagging	using	EM
Unsupervised	Tagging
CS	295:	STATISTICAL	NLP	(WINTER	2017)29Supervision	is	not	always	appropriate•Linguist	has	to	read	and	understand	each	sentence•Time	consuming	and	expensive•Contains	domain	specific	signal	in	the	labels•WSJ	doesn’t	generalize	to	Twitter,	for	example•Difficult	to	agree	on	the	universal	part-of-speech	tags	(C5	tags:	61,	Brown:	87)•Want	to	apply	it	to	low-resource/unknown	languagesGeneralize	the	notion	of	“clustering”	to	sequence	labeling.
Expectation	Maximization
CS	295:	STATISTICAL	NLP	(WINTER	2017)30K-MeansInitializationPick	K	random	centroidsCompute	ExpectationsCluster	all	the	pointsUpdate	ParametersUpdate	centroids
Upcoming…
CS	295:	STATISTICAL	NLP	(WINTER	2017)31•Homework	2	is	due	(~10	days):	February	9,	2017•Write-up,	data,	and	code	for	Homework	2	is	up•Ask	questions	early!Homework•Proposal	is	due	in	a	week:	February	7,	2017•Only	2	pagesProject•Paper	summaries:	February	17,	February	28,	March	14•Only	1page	eachSummaries
CMSC 422 Introduction to Machine LearningLecture 5 K-Means Clustering (Unsupervised Learning)Furong Huang / furongh@cs.umd.eduSlides adapted from Prof. Carpuat
Question•When applying a learning algorithm, some things are properties of the problem you are trying to solve, and some things are up to you to choose as the ML programmer. •Which of the following are properties of the problem?ØThe data generating distributionØThe train/dev/test splitØThe learning modelØThe loss function
Recap•Nearest Neighbors (NN) algorithms for classificationØK-NN, Epsilon ball NNØTake a geometric view of learning•Fundamental Machine Learning ConceptsØDecision boundaryvVisualizes predictions over entire feature spacevCharacterizes complexity of learned modelvIndicates overfitting/underfitting
Exercise: When are DT vs kNNappropriate?Propertiesof classification problemCan DecisionTrees handle them?Can K-NN handle them?Binary featuresNumericfeaturesCategorical featuresRobustto noisytraining examplesFast classification is crucialMany irrelevantfeaturesRelevant features have very differentscale
Exercise: When are DT vs kNNappropriate?Propertiesof classification problemCan DecisionTrees handle them?Can K-NN handle them?Binary featuresyesyesNumericfeaturesyesyesCategorical featuresyesyesRobustto noisytraining examplesno(for default algorithm)yes (when k > 1)Fast classification is crucialyesnoMany irrelevantfeaturesyesnoRelevant features have very differentscaleyesno
Today’s Topics•A new algorithmØK-Means Clustering•Fundamental Machine Learning ConceptsØUnsupervised vs. supervised learningØDecision boundary
Clustering•Goal: automatically partition examples into groups of similar examples•Why? It is useful forØAutomatically organizing dataØUnderstanding hidden structure in dataØPreprocessing for further analysis
What can we cluster in practice?•news articles or web pages by topic•protein sequences by function, or genes according to expression profile•users of social networks by interest•customers according to purchase history•…
Clustering•InputØa set !of "points in feature spaceØa distance measure specifying distance #(%!,%")between pairs (%!,%")•OutputØA partition {	!#,!$,…,!%}of !
SupervisedMachine Learning as Function ApproximationProblem setting•Set of possible instances !•Unknown target function ":!	→&•Set of function hypotheses '=ℎ		ℎ:!	→&}Input•Training examples {,-,/-,…,1,/1}of unknown target function "Output•Hypothesis ℎ	∈'	that best approximates target function "
Supervised vs. unsupervised learning•Clustering is an example of unsupervised learning•We are not given examples of classes y•Instead we have to discover classes in data
2 datasets with very different underlying structure!

The K-Means Algorithm
Training DataK: number of clusters to discover
K-means1.Ask user how many clusters they’d like. (e.g. k=5) 

K-means1.Ask user how many clusters they’d like. (e.g. k=5) 2.Randomly guess k cluster Center locations
K-means1.Ask user how many clusters they’d like. (e.g. k=5) 2.Randomly guess k cluster Center locations3.Each datapoint finds out which Center it’s closest to. (Thus each Center “owns” a set of datapoints)
K-means1.Ask user how many clusters they’d like. (e.g. k=5) 2.Randomly guess k cluster Center locations3.Each datapoint finds out which Center it’s closest to.4.Each Center finds the centroid of the points it owns
K-means1.Ask user how many clusters they’d like. (e.g. k=5) 2.Randomly guess k cluster Center locations3.Each datapoint finds out which Center it’s closest to.4.Each Center finds the centroid of the points it owns…5.…and jumps there6.…Repeat until terminated!
K-Means properties•Time complexity: O(KNL) whereØK is the number of clustersØN is number of examplesØL is the number of iterations•K is a hyperparameterØNeeds to be set in advance (or learned on dev set)•Different initializations yield different results!ØDoesn’t necessarily converge to best partition•“Global” view of data: revisits all examples at every iteration
K-means QuestionsAre we sure it will terminate?Are we sure it will find an optimal clustering?How should we start it?How could we automatically choose the number of centers?….we’ll deal with these questions over the next few slides
Can K-means always win?

Impact of initialization

Impact of initialization

Optimization View of K-meansGivena set of observations ("!,"",…,"#)where each observation is a &-dimensional real vectork-means clustering aims to partition the 'observations into k(≤")sets S=S!,S",…,,$so as to minimize the within-cluster sum of squares.Formally, the objective is to find:argmin%33"−5&"=argmin'3,&	Var(,&)(&)∈%+(&,!	where 5&is the mean of points in ,&. 
Trying to find good optimaIdea 1: Be careful about where you startIdea 2: Do many runs of k-means, each from a different random start configurationMany other ideas floating around.
Common uses of K-means•Often used as an exploratory data analysis tool•In one-dimension, a good way to quantize real-valued variables into k non-uniform buckets•Used on acoustic data in speech understanding to convert waveforms into one of k categories (known as Vector Quantization)•Also used for choosing color palettes on old fashioned graphical display devices!
Questions for you…•Are there clusters that cannot be discovered using k-means?•Do you know any other clustering algorithms?
Single Linkage Hierarchical Clustering1.Say “Every point is its own cluster”
Single Linkage Hierarchical Clustering1.Say “Every point is its own cluster”2.Find “most similar” pair of clusters
Single Linkage Hierarchical Clustering1.Say “Every point is its own cluster”2.Find “most similar” pair of clusters3.Merge it into a parent cluster
Single Linkage Hierarchical Clustering1.Say “Every point is its own cluster”2.Find “most similar” pair of clusters3.Merge it into a parent cluster4.Repeat
Single Linkage Hierarchical Clustering1.Say “Every point is its own cluster”2.Find “most similar” pair of clusters3.Merge it into a parent cluster4.Repeat
Single Linkage Hierarchical Clustering1.Say “Every point is its own cluster”2.Find “most similar” pair of clusters3.Merge it into a parent cluster4.Repeat…until you’ve merged the whole dataset into one clusterYou’re left with a nice dendrogram, or taxonomy, or hierarchy of datapoints (not shown here)How do we define similarity between clusters?•Minimum distance between points in clusters •Maximum distance between points in clusters•Average distance between points in clusters 
Aside: Curse of dimensionality•Challenges of working with high dimensional spacesØHard to visualizeØComputational costØMany of our intuitions  about 2D or 3D spaces don’t holdvHigh dimensional hyperspheres “look more like porcupines than balls” vDistances between two random points in high dimensions are approximately the same(CIML Section 3.5 + HW #3)
What you should know•New AlgorithmsØK-NN classificationØK-means clustering•Fundamental ML conceptsØHow to draw decision boundariesØWhat decision boundaries tells us about the underlying classifiersØThe difference between supervised and unsupervised learning
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
COMS 4721: Machine Learning for Data Science
Lecture 1, 1/17/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
OVERVIEW
This class will cover model-based techniques for extracting information
from data with an end-task in mind. Such tasks include:
Ipredicting an unknown “output” given its corresponding “input”
Iuncovering information within the data to better understand it
Idata-driven recommendation, grouping, classiﬁcation, ranking, etc.
There are a few ways we can divide up the material as we go along, e.g.,
supervised learning junsupervised learning
probabilistic models jnon-probabilistic models
modeling approach joptimization techniques
We’ll adopt the ﬁrst method and work in the second two along the way.
OVERVIEW : SUPERVISED LEARNING
xt
0 1−101
(a) Regression
 (b) Classiﬁcation
Regression : Using set of inputs, predict real-valued output.
Classiﬁcation : Using set of inputs, predict a discrete label (aka class).
EXAMPLE CLASSIFICATION PROBLEM
Given a set of inputs characterizing an item, assign it a label.
Is this spam?
hi everyone,
i saw that close to my hotel there is a pub with bowling
(it’s on market between 9th and 10th avenue). meet
there at 8:30?
What about this?
Enter for a chance to win a trip to Universal Orlando to
celebrate the arrival of Dr. Seuss’s The Lorax on Movies
On Demand on August 21st! Click here now!
OVERVIEW : UNSUPERVISED LEARNING
government
law            
politics
legislation
 . . .0.04
0.02
0.01
0.01
team
basketball         
points
score
 . . .0.03
0.02
0.01
0.01
business
money    
economic
company
. . .0.04
0.02
0.02
0.01
health
medical       
disease
hospital
 . . .0.03
0.03
0.02
0.01
computer
system  
software
program
 . . .0.03
0.02
0.02
0.01T opicsDocuments
T opic 
assignmentsT opic 
proportions
(c) topic modeling
 (d) recommendations1
With unsupervised learning our goal is often to uncover structure in the data.
This helps with predictions, recommendations, efﬁcient data exploration.
1Figure from Koren, Y ., Robert B., and V olinsky, C.. “Matrix factorization techniques for recommender systems.” Computer 42.8 (2009): 30-37.
EXAMPLE UNSUPERVISED PROBLEM
Goal : Learn the dominant topics from a set of news articles.

DATA MODELING
ISupervised vs. unsupervised: Blocks #1 and #4
IProbabilistic vs. non-probabilistic: Primarily Block #2 (Some Block #3)
IModel development (Block #2) vs. Optimization techniques (Block #3)
GAUSSIAN DISTRIBUTION (MULTIVARIATE )
Gaussian density in ddimensions
IBlock #1: Data x1;:::; xn. Each xi2Rd
IBlock #2: An i.i.d. Gaussian model
IBlock #3: Maximum likelihood
IBlock #4: Leave undeﬁned
The density function is
p(xj;) :=1
(2)d
2p
det()exp
 1
2(x )T 1(x )
The central moments are:
E[x] =R
Rdx p(xj;)dx=;
Cov(x) =E[(x E[x])(x E[x])T] =E[xxT] E[x]E[x]T= .
BLOCK #2: A P ROBABILISTIC MODEL
Probabilistic Models
IAprobabilistic model is a set of probability distributions, p(xj).
IWe pick the distribution family p (), but don’t know the parameter .
Example : Model data with a Gaussian distribution p(xj),=f;g.
The i.i.d. assumption
Assume data is independent and identically distributed (iid) . This is written
xiiidp(xj);i=1;:::; n:
Writing the density as p(xj), then the joint density decomposes as
p(x1;:::; xnj) =nY
i=1p(xij):
BLOCK #3: M AXIMUM LIKELIHOOD ESTIMATION
Maximum Likelihood approach
We now need to ﬁnd .Maximum likelihood seeks the value of that
maximizes the likelihood function:
^ML:=arg max
p(x1;:::; xnj);
This value best explains the data according to the chosen distribution family.
Maximum Likelihood equation
The analytic criterion for this maximum likelihood estimator is:
rnY
i=1p(xij) =0:
Simply put, the maximum is at a peak. There is no “upward” direction.
BLOCK #3: L OGARITHM TRICK
Logarithm trick
CalculatingrQn
i=1p(xij)can be complicated. We use the fact that the
logarithm is monotonically increasing on R+, and the equality
lnY
ifi
=X
iln(fi):
Consequence: Taking the logarithm does not change the location of a
maximum or minimum:
max
ylng(y)6=max
yg(y) Thevalue changes.
arg max
ylng(y) =arg max
yg(y) Thelocation does not change.
BLOCK #3: A NALYTIC MAXIMUM LIKELIHOOD
Maximum likelihood and the logarithm trick
^ML=arg max
nY
i=1p(xij) =arg max
lnnY
i=1p(xij)
=arg max
nX
i=1lnp(xij)
To then solve for ^ML, ﬁnd
rnX
i=1lnp(xij) =nX
i=1rlnp(xij) =0:
Depending on the choice of the model, we will be able to solve this
1. analytically (via a simple set of equations)
2. numerically (via an iterative algorithm using different equations)
3. approximately (typically when #2 converges to a local optimal solution)
EXAMPLE : M ULTIVARIATE GAUSSIAN MLE
Block #2: Multivariate Gaussian data model
Model: Set of all Gaussians on Rdwith unknown mean 2Rdand
covariance 2Sd
++(positive deﬁnite ddmatrix).
We assume that x1;:::; xnare i.i.d. p(xj;), written xiiidp(xj;).
Block #3: Maximum likelihood solution
We have to solve the equation
nX
i=1r(;)lnp(xij;) = 0
forand. (Try doing this without the log to appreciate it’s usefulness.)
EXAMPLE : GAUSSIAN MEAN MLE
First take the gradient with respect to .
0=rnX
i=1ln1p
(2)djjexp
 1
2(xi )T 1(xi )
=rnX
i=1 1
2ln(2)djj 1
2(xi )T 1(xi )
= 1
2nX
i=1r
xT
i 1xi 2T 1xi+T 1
=  1nX
i=1(xi )
Since is positive deﬁnite, the only solution is
nX
i=1(xi ) =0) ^ML=1
nnX
i=1xi
Since this solution is independent of , it doesn’t depend on ^ML.
EXAMPLE : GAUSSIAN COVARIANCE MLE
Now take the gradient with respect to .
0=rnX
i=1 1
2ln(2)djj 1
2(xi )T 1(xi )
= n
2rlnjj 1
2rtrace
 1nX
i=1(xi )(xi )T
= n
2 1+1
2 2nX
i=1(xi )(xi )T
Solving for and plugging in = ^ML,
^ML=1
nnX
i=1(xi ^ML)(xi ^ML)T:
EXAMPLE : GAUSSIAN MLE (S UMMARY )
So if we have data x1;:::; xninRdthat we hypothesize is i.i.d. Gaussian, the
maximum likelihood values of the mean and covariance matrix are
^ML=1
nnX
i=1xi;^ML=1
nnX
i=1(xi ^ML)(xi ^ML)T:
Are we done? There are many assumptions/issues with this approach that
makes ﬁnding the “best” parameter values not a complete victory.
IWe made a model assumption (multivariate Gaussian).
IWe made an i.i.d. assumption.
IWe assumed that maximizing the likelihood is the best thing to do.
Comment: We often use MLto make predictions about xnew(Block #4).
How does MLgeneralize to xnew?
Ifx1:ndon’t “capture the space” well, MLcanoverﬁt the data.
CMSC 422 Introduction to Machine LearningLecture 12 Bias and FairnessFurong Huang / furongh@cs.umd.eduSlides adapted from Prof Carpuatand Duraiswami
Some ML issues in the real world
Midterm –grade distribution (histogram)
Mean: 70.5, Median: 70
Midterm –makeup policyØResubmit youranswertoquestion(s)ØUp to8pointstomakeupØI trustyou, don’t seek help from othersØFor T/F problems, give detailedjustification/proofØGrading will be very strict, you will get 0 points if the justification is flawedØDetailed printout of justifications/proofsØYour name, your session ID, your UIDØDeadline:March15 (Thursday), 11:00 am before class.
Requirements from my first lectureØDo the reading before classØalready familiar with high-level concepts and mathematical notation by the time you come to class. Øyou can get more out of class time by focusing on understanding the reasoningand clarifyingwhat didn't make sense when you first read itØUnderstandingØbeing able to precisely defineand manipulatethe mathematicalconcepts Øbeing able to discuss the intuition behind algorithms with words is necessarybut not sufficient.Why?
Final Exam –how to prepare•We will assign more questions that require mathematical reasoning in the homework•Youwillreadthetextbookbeforecomingtoclass•Youwillaskquestionsduringthe lecture•Youcouldformstudygroups if need to review linear algebraic knowledge, logical reasoning techniques
Midterm –regrading requestsØWritten only requestsØList the problems that should be regradedØSuggest the points you should be gettingØJustify whyØSubmit a detailed printoutØYour name, your session ID, your UIDØDeadline: March 27, 11:00 am before class
RankingCanonical example: web searchGiven all the documents on the webFor a user query, retrieve relevant documents, ranked from most relevant to least relevant
How can we reduce ranking to binary classification?
Preference function•Given a query q and documents diand dj, the preference function outputs whether•dishould be preferred to dj•Or djshould be preferred to di•That’s a binary classification problem!
Specifying the reduction from ranking to binary classification•How to train classifier that predicts preferences?•How to turn the predicted preferences into a ranking?
Features associated with comparing document j and document j for query n
Naïve approachWorks well for bipartite problems“is this document relevant or not?”Not ideal for full ranking problems, becauseBinary preference problems are not all equally importantSeparates preference function and sorting
Improving on naïve approach

Example of cost functions

Resulting Ranking Algorithms

Exercise: understand this offline
RankTestA probabilistic version of the quicksort algorithmOnly O(M log2M) calls to f in expectationBetter error bound than naïve algorithm(see CIML for theorem)
What you should know•What are reductions and why they are useful•Implement, analyze and prove error bounds of algorithms for•Weighted binary classification•Multiclass classification (OVA, AVA)•Understand algorithms for•!−ranking
Bias and Fairness
Word EmbeddingsCould be Biased
Bolukbasiet al. NIPS 2016.
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing

Recall: Formal Definition of Binary Classification (from CIML)

Train/Test Mismatch•When working with real world data,  training sample•reflects human biases•is influenced by practical concerns•e.g., what  kind of data is easy to obtain•Train/test distribution mismatch is frequent issue•aka sample selection bias, domain adaptation
Domain Adaptation
•What does it mean for 2 distributions to be related?•When 2 distributions are related how can we build models that effectively share information between them?
Unsupervised adaptationGoal:learn a classifier f that achieves low expected loss under new distributionGiven labeled training data from old distribution And unlabeled examples from new distribution

Relation between test loss in new domain and old domain

How can we estimate the ratio between Dnewand Dold?
Fixed base distributionS = selection variable
We can estimate P(s=1|x) using a binary classifier!
Clarifications•Note Znewis the same forallthedatapoints (i.e., ∀(#,%)). It is also a normalization to make sure ∑(#,())*+,-,%=1.1*+,=2(#,()[)456+-,%78=0-]•Therefore,Znewis not a function of (#,%), in fact it is a constant. •SimilarlyforZold.•All examples are drawn from fixed base distribution, some are selected to go into Dnew, some are selected to go into Doldaccording to a selection variable s.
We will revisit logistic regression! 
Supervised adaptationGoal:learn a classifier f that achieves low expected loss under new distributionGiven labeled training data from old distribution And labeled examples from new distribution

One solution: feature augmentationMap inputs to a new augmented representation

One solution: feature augmentation•Transform Doldand Dnewtraining examples•Train a classifier on new representations•Done!
One solution: feature augmentation•Adding instance weighting might be useful if N >> M•Most effective when distributions are “not too close but not too far”•In practice, always try “old only”, “new only”, “union of old and new” as well!

Bias is pervasive•Bias in the labeling•Sample selection bias•Bias in choice of labels•Bias in features or model structure•Bias in loss function•Deployed systems create feedback loops
Bias and how to deal with it•Train/test mismatch•Unsupervised adaptation•Supervised adaptation
ACM Code of Ethics“To minimize the possibility of indirectly harming others, computing professionals must minimize malfunctions by following generally accepted standards for system design and testing. Furthermore, it is often necessary to assess the social consequences of systems to project the likelihood of any serious harm to others. If system features are misrepresented to users, coworkers, or supervisors, the individual computing professional is responsible for any resulting injury.”https://www.acm.org/about-acm/acm-code-of-ethics-and-professional-conduct
Furong Huang3251 A.V. Williams, College Park, MD 20740301.405.8010 / furongh@cs.umd.edu
COMS 4721: Machine Learning for Data Science
Lecture 18, 4/4/2017
Prof. John Paisley
Department of Electrical Engineering
& Data Science Institute
Columbia University
TOPIC MODELING
MODELS FOR TEXT DATA
Given text data we want to:
IOrganize
IVisualize
ISummarize
ISearch
IPredict
IUnderstand
Topic models allow us to
1. Discover themes in text
2. Annotate documents
3. Organize, summarize, etc.
TOPIC MODELING

TOPIC MODELING
A probabilistic topic model
ILearns distributions on words called “topics” shared by documents
ILearns a distribution on topics for each document
IAssigns every word in a document to a topic
TOPIC MODELING
However, none of these things are known in advance and must be learned
IEach document is treated as a “bag of words”
INeed to deﬁne (1) a model, and (2) an algorithm to learn it
IWe will review the standard topic model, but won’t cover inference
LATENT DIRICHLET ALLOCATION
There are two essential ingredients to latent Dirichlet allocation (LDA).
1. A collection of distributions on words (topics).
2. A distribution on topics for each document.
youth
vote
politics
rate
ball
reason
interest
power
sense
proof
boy
score
brain
order
set
season
senate
taxβ1
β2
β3
LATENT DIRICHLET ALLOCATION
There are two essential ingredients to latent Dirichlet allocation (LDA).
1. A collection of distributions on words (topics).
2. A distribution on topics for each document.
θ1
LATENT DIRICHLET ALLOCATION
There are two essential ingredients to latent Dirichlet allocation (LDA).
1. A collection of distributions on words (topics).
2. A distribution on topics for each document.
θ2
LATENT DIRICHLET ALLOCATION
There are two essential ingredients to latent Dirichlet allocation (LDA).
1. A collection of distributions on words (topics).
2. A distribution on topics for each document.
θ3
LATENT DIRICHLET ALLOCATION
There are two essential ingredients to latent Dirichlet allocation (LDA).
1. A collection of distributions on words (topics).
2. A distribution on topics for each document.
The generative process for LDA is:
1. Generate each topic, which is a distribution on words
kDirichlet (
);k=1;:::; K
2. For each document, generate a distribution on topics
dDirichlet ();d=1;:::; D
3. For the nth word in the dth document,
a) Allocate the word to a topic, cdnDiscrete (d)
b) Generate the word from the selected topic, xdnDiscrete (cdn)
DIRICHLET DISTRIBUTION
A continuous distribution on discrete probability vectors. Let kbe a
probability vector and 
a positive parameter vector,
p(kj
) = (P
v
v)QV
v=1 (
v)VY
v=1
v 1
k;v
This deﬁnes the Dirichlet distribution. Some examples of kgenerated from
this distribution for a constant value of 
andV=10 are given below.

=1
DIRICHLET DISTRIBUTION
A continuous distribution on discrete probability vectors. Let kbe a
probability vector and 
a positive parameter vector,
p(kj
) = (P
v
v)QV
v=1 (
v)VY
v=1
v 1
k;v
This deﬁnes the Dirichlet distribution. Some examples of kgenerated from
this distribution for a constant value of 
andV=10 are given below.

=10
DIRICHLET DISTRIBUTION
A continuous distribution on discrete probability vectors. Let kbe a
probability vector and 
a positive parameter vector,
p(kj
) = (P
v
v)QV
v=1 (
v)VY
v=1
v 1
k;v
This deﬁnes the Dirichlet distribution. Some examples of kgenerated from
this distribution for a constant value of 
andV=10 are given below.

=100
DIRICHLET DISTRIBUTION
A continuous distribution on discrete probability vectors. Let kbe a
probability vector and 
a positive parameter vector,
p(kj
) = (P
v
v)QV
v=1 (
v)VY
v=1
v 1
k;v
This deﬁnes the Dirichlet distribution. Some examples of kgenerated from
this distribution for a constant value of 
andV=10 are given below.

=1
DIRICHLET DISTRIBUTION
A continuous distribution on discrete probability vectors. Let kbe a
probability vector and 
a positive parameter vector,
p(kj
) = (P
v
v)QV
v=1 (
v)VY
v=1
v 1
k;v
This deﬁnes the Dirichlet distribution. Some examples of kgenerated from
this distribution for a constant value of 
andV=10 are given below.

=0:1
DIRICHLET DISTRIBUTION
A continuous distribution on discrete probability vectors. Let kbe a
probability vector and 
a positive parameter vector,
p(kj
) = (P
v
v)QV
v=1 (
v)VY
v=1
v 1
k;v
This deﬁnes the Dirichlet distribution. Some examples of kgenerated from
this distribution for a constant value of 
andV=10 are given below.

=0:01
LDA OUTPUT
LDA outputs two main things:
1. A set of distributions on words (topics). Shown above are ten topics
from NYT data. We list the ten words with the highest probability.
2. A distribution on topics for each document (not shown). This indicates
its thematic breakdown and provides a compact representation.
LDA AND MATRIX FACTORIZATION
Q: For a particular document, what is P(xdn=ij; d)?
A: Find this by integrating out the cluster assignment,
P(xdn=ij;) =KX
k=1P(xdn=i;cdn=kj; d)
=KX
k=1P(xdn=i;j;cdn=k)| {z }
=kiP(cdn=kjd)|{z}
=dk
LetB= [1;:::; K]and = [1;:::; D], then P(xdn=ij;) = ( B)id
In other words, we can read the probabilities from a matrix formed by taking
the product of two matrices that have nonnegative entries.
NONNEGATIVE MATRIX
FACTORIZATION
NONNEGATIVE MATRIX FACTORIZATION
LDA can be thought of as an instance of nonnegative matrix factorization.
IIt is a probabilistic model.
IInference involves techniques not taught in this course.
We will discuss two other related models and their algorithms. These two
models are called nonnegative matrix factorization (NMF)
IThey can be used for the same tasks as LDA
IThough “nonnegative matrix factorization” is a general technique,
“NMF” usually just refers to the following two methods.
NONNEGATIVE MATRIX FACTORIZATION
N2 "objects"N1 dimensions{{
(i,j)-th entry, X ij> _0~~
Wik>_0Hkj> _ 0
{rank = k
We use notation and think about the problem slightly differently from PMF
IData Xhas nonnegative entries. None missing, but likely many zeros.
IThe learned factorization WandHalso have nonnegative entries.
IThe value XijP
kWikHkj, but we won’t write this with vector notation
ILater we interpret the output in terms of columns of WandH.
NONNEGATIVE MATRIX FACTORIZATION
What are some data modeling problems that can constitute X?
IText data:
IWord term frequencies
IXijcontains the number of times word iappears in document j.
IImage data:
IFace identiﬁcation data sets
IPut each vectorized N Mimage of a face on a column ofX.
IOther discrete grouped data:
IQuantize continuous sets of features using K-means
IXijcounts how many times group juses cluster i.
IFor example: group = song, features = dnspectral information matrix
TWO OBJECTIVE FUNCTIONS
NMF minimizes one of the following two objective functions over WandH.
Choice 1: Squared error objective
kX WHk2=X
iX
j(Xij (WH)ij)2
Choice 2: Divergence objective
D(XkWH) = X
iX
j[Xijln(WH)ij (WH)ij]
IBoth have the constraint that WandHcontain nonnegative values.
INMF uses a fast, simple algorithm for optimizing these two objectives.
MINIMIZATION AND MULTIPLICATIVE ALGORITHMS1
Recall what we should look for in minimizing an objective “min
hF(h)”:
1. A way to generate a sequence of values h1;h2;:::, such that
F(h1)F(h2)F(h3)
2. Convergence of the sequence to a local minimum of F
The following algorithms fulﬁll these
requirements. In this case:
IMinimization is done via an
“auxiliary function.”
ILeads to a “multiplicative
algorithm” for WandH.
IWe’ll skip details (see reference).
1For details, see D.D. Lee and H.S. Seung (2001). “Algorithms for non-negative matrix
factorization.” Advances in Neural Information Processing Systems.
MULTIPLICATIVE UPDATE FOR kX WHk2
Problem
minP
ij(Xij (WH)ij)2subject to Wik0,Hkj0.
Algorithm
IRandomly initialize HandWwith nonnegative values.
IIterate the following, ﬁrst for all values in H, then all in W:
Hkj Hkj(WTX)kj
(WTWH)kj;
Wik Wik(XHT)ik
(WHHT)ik;
until the change in kX WHk2is “small.”
VISUALIZATION AND MAXIMUM LIKELIHOOD
~~XWH
A visualization that may be helpful. Use the color-coded deﬁnition above.
IUse element-wise multiplication/division across three columns below.
IUse matrix multiplication within each outlined box.
.* . /
.* . /
Probabilistically, the squared error penalty implies a Gaussian distribution,
XijN(P
kWikHkj;2)
Since Xij0 (and often isn’t continuous), we are making an incorrect
modeling assumption. Nevertheless, as with PMF it still works well.
MULTIPLICATIVE UPDATE FOR D(XkWH )
Problem
minP
ijh
Xijln1
(WH)ij+ (WH)iji
subject to Wik0,Hkj0.
Algorithm
IRandomly initialize HandWwith nonnegative values.
IIterate the following, ﬁrst for all values in H, then all in W:
Hkj HkjP
iWikXij=(WH)ijP
iWik;
Wik WikP
jHkjXij=(WH)ijP
jHkj;
until the change in D(XkWH)is “small.”
VISUALIZATION
__ ./defXWH
Visualizing the update for the divergence penalty is more complicated.
IUse the color-coded deﬁnition above.
I“Purple” is the data matrix “dot-divided” by the approximation of it.
normalize the rows of
this transposed matrix
so they sum to one
.*
.*normalize the columns 
of this matrix so they 
sum to one
MAXIMUM LIKELIHOOD
The maximum likelihood interpretation of the divergence penalty is more
interesting than for the squared error penalty.
If we model the data as independent Poisson random variables
XijPois((WH)ij); Pois(xj) =x
x!e ;x2f0;1;2;:::g;
then the negative divergence penalty is maximum likelihood for WandH.
 D(XkWH) =X
ij[Xijln(WH)ij (WH)ij]
=X
ijlnP(XijjW;H) +constant
We use: P(XjW;H) =Q
ijP(XijjW;H) =Q
ijPois(Xijj(WH)ij).
NMF AND TOPIC MODELING
As discussed, NMF can be used for topic modeling. In fact, one can show
that the divergence penalty is closely related mathematically to LDA.
Step 1. Form the term-frequency matrix X. (Xij= # times word iin doc j)
Step 2. Run NMF to learn WandHusing D(XkWH)penalty
Step 3. As an added step, after Step 2 is complete, for k=1;:::; K
1. Set ak=P
iWik
2. Divide Wikbyakfor all i
3. Multiply Hkjbyakfor all j
Notice that this is does not change the matrix multiplication WH.
Interpretation: The kthcolumn ofWcan be interpreted as the kthtopic . The
jthcolumn ofHcan be interpreted as how much document juses each topic.
NMF AND FACE MODELING
For face modeling, put the face images along the columns of Xand factorize.
Show columns of Was image. Compare this with K-means and SVD.
K-means (i.e., VQ): Equivalent to each column of Hhaving a single 1.
K-means learns averages of full faces.
NMF AND FACE MODELING
For face modeling, put the face images along the columns of Xand factorize.
Show columns of Was image. Compare this with K-means and SVD.
SVD: Finds the singular value decomposition of X.
Results not interpretable because of values and orthogonality constraint
NMF AND FACE MODELING
For face modeling, put the face images along the columns of Xand factorize.
Show columns of Was image. Compare this with K-means and SVD.
NMF learns a “parts-based” representation. Each column captures something
interpretable. This is a result of the nonnegativity constraint.
1 Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  January 12, 2015 
Today: • What is machine learning? • Decision tree learning • Course logistics Readings: • “The Discipline of ML” • Mitchell, Chapter 3 • Bishop, Chapter 14.4 
Machine Learning: Study of algorithms that •  improve their performance P •  at some task T •  with experience E well-defined learning task: <P,T,E> 
2 Learning to Predict Emergency C-Sections 
9714 patient records, each with 215 features [Sims et al., 2000] 
Learning to classify text documents 
spam  vs not spam 
3 Learning to detect objects in images 
Example training images for each orientation 
(Prof. H. Schneiderman) 
Learn to classify the word a person is thinking about, based on fMRI brain activity 
4 Learning prosthetic control from neural implant 
[R. Kass L. Castellanos A. Schwartz] 
Machine Learning - Practice 
Object recognition  
Mining Databases 
Speech Recognition 
Control learning •  Support Vector Machines •  Bayesian networks •  Hidden Markov models •  Deep neural networks •  Reinforcement learning •  .... 
Text analysis 
5 Machine Learning - Theory PAC Learning Theory  # examples (m) representational complexity (H) error rate (ε) failure probability (δ) 
Other theories for •  Reinforcement skill learning •  Semi-supervised learning •  Active student querying •  … … also relating: •  # of mistakes during learning •  learner’s query strategy •  convergence rate •  asymptotic performance •  bias, variance (supervised concept learning)  
Machine Learning in Computer Science • Machine learning already the preferred approach to – Speech recognition, Natural language processing – Computer vision – Medical outcomes analysis – Robot control – … • This ML niche is growing (why?) All software apps. ML apps. 
6 • Machine learning already the preferred approach to – Speech recognition, Natural language processing – Computer vision – Medical outcomes analysis – Robot control – … • This ML niche is growing – Improved machine learning algorithms  – Increased volume of online data  – Increased demand for self-customizing software All software apps. Machine Learning in Computer Science 
ML apps. 
Tom’s prediction: ML will be fastest-growing part of CS this century 
Animal learning (Cognitive science, Psychology, Neuroscience) Machine learning Statistics  Computer science Adaptive Control Theory Evolution Economics and Organizational Behavior 
7 What You’ll Learn in This Course • The primary Machine Learning algorithms – Logistic regression, Bayesian methods, HMM’s, SVM’s, reinforcement learning, decision tree learning, boosting, unsupervised clustering, … • How to use them on real data – text, image, structured data – your own project • Underlying statistical and computational theory • Enough to read and understand ML research papers 
Course logistics 
8 Machine Learning 10-601 Faculty • Maria Balcan • Tom Mitchell  TA’s • Travis Dick • Kirsten Early • Ahmed Hefny • Micol Marchetti-Bowick • Willie Neiswanger • Abu Saparov  Course assistant • Sharon Cavlovich  website: www.cs.cmu.edu/~ninamf/courses/601sp15            See webpage for  • Office hours • Syllabus details • Recitation sessions • Grading policy • Honesty policy • Late homework policy • Piazza pointers • ... Highlights of Course Logistics On the wait list? • Hang in there for first few weeks Homework 1 • Available now, due friday Grading: • 30% homeworks (~5-6) • 20% course project • 25% first midterm (March 2) • 25% final midterm (April 29)  Academic integrity: • Cheating à Fail class, be expelled from CMU Late homework: • full credit when due • half credit next 48 hrs • zero credit after that • we’ll  delete your lowest HW score • must turn in at least n-1 of the n homeworks, even if late Being present at exams: • You must be there – plan now. • Two in-class exams, no other final  
9 Maria-Florina Balcan: Nina • Foundations for Modern Machine Learning • Theoretical Computer Science, especially connections between learning theory & other fields 
Game Theory Approx. Algorithms Matroid  Theory Machine Learning  Theory Discrete Optimization Mechanism Design Control Theory 
• E.g., interactive, distributed, life-long learning 
Travis Dick 
• When can we learn many concepts from mostly unlabeled data by exploiting relationships between between concepts. • Currently: Geometric relationships 
10 Kirstin Early • Analyzing and predicting  energy consumption • Reduce costs/usage and help  people make informed decisions 
Energy disaggregation: decomposing total electric signal  into individual appliances Predicting energy costs from features of home  and occupant behavior 
Ahmed Hefny • How can we learn to track and predict the state of a dynamical system only from noisy observations ?  •  Can we exploit supervised learning  methods to devise a flexible, local minima-free approach ? 
observations (oscillating pendulum) Extracted 2D state trajectory 

11 Micol Marchetti-Bowick 
How can we use machine learning for biological and medical research? • Using genotype data to build personalized models that can predict clinical outcomes • Integrating data from multiple sources to perform cancer subtype analysis • Structured sparse regression models for genome-wide association studies 
xyxyxyxyxyxyxyyxxy
sample weightgenetic relatednessGene expression data w/ dendrogram (or have one picture per task) 
Willie Neiswanger • If we want to apply machine learning     algorithms to BIG datasets… • How can we develop parallel, low-communication machine learning algorithms?  • Such as embarrassingly parallel algorithms, where machines work independently, without communication. 

12 Abu Saparov • How can knowledge about the world help computers understand natural language? • What kinds of machine learning tools are needed to understand sentences?​ 
“Carolyn ate the cake with a fork.” “Carolyn ate the cake with vanilla.” person_eats_food consumer Carolyn food cake instrument fork person_eats_food consumer Carolyn food cake topping vanilla Tom Mitchell How can we build never-ending learners? Case study: never-ending language learner (NELL) runs 24x7 to learn to read the web 
see  http://rtw.ml.cmu.edu 
reading accuracy  vs.  time (5 years)  
mean avg. precision top 1000 
# of beliefs vs.  time (5 years)  
13 Function Approximation and Decision tree learning 
Function approximation Problem Setting: • Set of possible instances X• Unknown target function f : XàY• Set of function hypotheses H={ h | h : XàY }Input: • Training examples {<x(i),y(i)>} of unknown target function f Output: • Hypothesis h ∈ H that best approximates target function fsuperscript: ith training example
14 
Day   Outlook  Temperature  Humidity   Wind   PlayTennis? Simple Training Data Set 
Each internal node: test one discrete-valued attribute Xi Each branch from a node: selects one value for Xi Each leaf node: predict Y  (or P(Y|X ∈ leaf)) A Decision tree for  f: <Outlook, Temperature, Humidity, Wind> à PlayTennis? 
15 
Problem Setting: • Set of possible instances X – each instance x in X is a feature vector – e.g., <Humidity=low, Wind=weak, Outlook=rain, Temp=hot> • Unknown target function f : XàY– Y=1 if we play tennis on this day, else 0 • Set of function hypotheses H={ h | h : XàY }– each hypothesis h is a decision tree – trees sorts x to leaf, which assigns y Decision Tree Learning 
Decision Tree Learning Problem Setting: • Set of possible instances X – each instance x in X is a feature vector  x = < x1, x2 … xn> • Unknown target function f : XàY– Y is discrete-valued • Set of function hypotheses H={ h | h : XàY }– each hypothesis h is a decision tree Input: • Training examples {<x(i),y(i)>} of unknown target function fOutput: • Hypothesis h ∈ H that best approximates target function f
16 Decision Trees Suppose X = <X1,… Xn>  where Xi are boolean-valued variables   How would you represent Y = X2 X5 ?     Y = X2 ∨ X5 How would you represent  X2 X5  ∨ X3X4(¬X1)  

17 
node = Root [ID3, C4.5, Quinlan] 
Sample Entropy 

18 Entropy Entropy H(X) of a random variable X    H(X) is the expected number of bits needed to encode a randomly drawn value of X  (under most efficient code)   Why?  Information theory: • Most efficient possible code assigns  -log2 P(X=i)  bits to encode the message X=i • So, expected number of bits to code one random X is:   
# of possible values for X
Entropy Entropy H(X) of a random variable X      
Specific conditional entropy H(X|Y=v) of X given Y=v : 
Mutual information (aka Information Gain) of X and Y : 
Conditional entropy H(X|Y) of X given Y : 

19 
Information Gain is the mutual information between input attribute A and target variable Y  Information Gain is the expected reduction in entropy of target variable Y for data sample S, due to sorting on variable A   
Day   Outlook    Temperature   Humidity    Wind   PlayTennis? Simple Training Data Set 
20 

21 
Each internal node: test one discrete-valued attribute Xi Each branch from a node: selects one value for Xi Each leaf node: predict Y Final Decision Tree for f: <Outlook, Temperature, Humidity, Wind> à PlayTennis? 
Which Tree Should We Output? • ID3 performs heuristic search through space of decision trees • It stops at smallest acceptable tree. Why? 
Occam’s razor: prefer the simplest hypothesis that fits the data 
22 Why Prefer Short Hypotheses? (Occam’s Razor) Arguments in favor:     Arguments opposed:  
Why Prefer Short Hypotheses? (Occam’s Razor) Argument in favor: • Fewer short hypotheses than long ones à a short hypothesis that fits the data is less likely to be a statistical coincidence à highly probable that a sufficiently complex hypothesis will fit the data  Argument opposed: • Also fewer hypotheses with prime number of nodes and attributes beginning with “Z” • What’s so special about “short” hypotheses? 
23 
Overfitting Consider a hypothesis h and its • Error rate over training data: • True error rate over all data:   We say h overfits the training data if   Amount of overfitting =  

24 

25 
Split data into training and validation setCreate tree that classiﬁes training set correctly

26 

27 You should know: • Well posed function approximation problems: – Instance space, X – Sample of labeled training data { <x(i), y(i)>} – Hypothesis space, H = { f: XàY } • Learning is a search/optimization problem over H – Various objective functions • minimize training error (0-1 loss)  • among hypotheses that minimize training error, select smallest (?) • Decision tree learning – Greedy top-down learning of decision trees (ID3, C4.5, ...) – Overfitting and tree/rule post-pruning – Extensions… Questions to think about (1) • ID3 and C4.5 are heuristic algorithms that search through the space of decision trees.  Why not just do an exhaustive search? 
28 Questions to think about (2) • Consider target function f: <x1,x2> à y, where x1 and x2 are real-valued, y is boolean.  What is the set of decision surfaces describable with decision trees that use each attribute at most once? 
Questions to think about (3) • Why use Information Gain to select attributes in decision trees?  What other criteria seem reasonable, and what are the tradeoffs in making this choice?   
29 Questions to think about (4) • What is the relationship between learning decision trees, and learning IF-THEN rules 

Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  January 14, 2015  
Today: • The Big Picture • Overfitting • Review: probability Readings: Decision trees, overfiting • Mitchell, Chapter 3 Probability review • Bishop Ch. 1 thru 1.2.3 • Bishop, Ch. 2 thru 2.2 • Andrew Moore’s online tutorial 
Function Approximation:   Problem Setting: • Set of possible instances X  • Unknown target function f : XàY• Set of function hypotheses H={ h | h : XàY }Input: • Training examples {<x(i),y(i)>} of unknown target function fOutput: • Hypothesis h ∈ H that best approximates target function f
Function Approximation: Decision Tree Learning Problem Setting: • Set of possible instances X – each instance x in X is a feature vector  x = < x1, x2 … xn> • Unknown target function f : XàY– Y is discrete valued • Set of function hypotheses H={ h | h : XàY }– each hypothesis h is a decision tree Input: • Training examples {<x(i),y(i)>} of unknown target function fOutput: • Hypothesis h ∈ H that best approximates target function f
Function approximation as Search for the best hypothesis • ID3 performs heuristic search through space of decision trees 

Function Approximation: The Big Picture 

Which Tree Should We Output? • ID3 performs heuristic search through space of decision trees • It stops at smallest acceptable tree. Why? 
Occam’s razor: prefer the simplest hypothesis that fits the data 
Why Prefer Short Hypotheses? (Occam’s Razor) Arguments in favor:      Arguments opposed:  
Why Prefer Short Hypotheses? (Occam’s Razor) Argument in favor: • Fewer short hypotheses than long ones à a short hypothesis that fits the data is less likely to be a statistical coincidence   Argument opposed: • Also fewer hypotheses containing a prime number of nodes and attributes beginning with “Z” • What’s so special about “short” hypotheses, instead of “prime number of nodes and edges”? 

Overfitting Consider a hypothesis h and its • Error rate over training data: • True error rate over all data:  

Overfitting Consider a hypothesis h and its • Error rate over training data: • True error rate over all data:   We say h overfits the training data if   Amount of overfitting =  



Split data into training and validation setCreate tree that classiﬁes training set correctly

Decision Tree Learning, Formal Guarantees  
   Labeled Examples   Supervised Learning or Function Approximation  Learning Algorithm  Expert / Oracle  Data Source  
Alg.outputs  Distribution D on X 
c* : X ! Y (x1,c*(x1)),…, (xm,c*(xm)) h : X ! Y x1 > 5 x6 > 2 +1 -1 +1 
   Labeled Examples   Learning Algorithm  Expert/Oracle  Data Source  
Alg.outputs  c* : X ! Y h : X ! Y (x1,c*(x1)),…, (xm,c*(xm)) • Algo sees training sample S: (x1,c*(x1)),…, (xm,c*(xm)), xi i.i.d. from D Distribution D on X 
  err(h)=Prx 2 D(h(x) ≠ c*(x)) •    Does optimization over S, finds hypothesis h (e.g., a decision tree). •    Goal:  h has small error over D. Supervised Learning or Function Approximation  
Two Core Aspects of Machine Learning  Algorithm Design. How to optimize? Automatically generate rules that do well on observed data. Confidence Bounds, Generalization Confidence for rule effectiveness on future data. Computation 
• Very well understood: Occam’s bound, VC theory, etc. (Labeled) Data • Decision trees: if we were able to find a small decision tree that explains data well, then good generalization guarantees.  • NP-hard [Hyafil-Rivest’76]   
Top Down Decision Trees Algorithms  • Decision trees: if we were able to find a small decision tree consistent with the data, then good generalization guarantees.  • NP-hard [Hyafil-Rivest’76]   • Very nice practical heuristics;  top down algorithms, e.g, ID3 • Natural greedy approaches where we grow the tree from the root to the leaves by repeatedly replacing an existing leaf with an internal node. • Key point: splitting criterion. • ID3: split the leaf that decreases the entropy the most. • Why not split according to error rate --- this is what we care about after all? • There are examples where we can get stuck in local minima!!! 
𝑓(𝑥)=​𝑥↓1 ∧​𝑥↓2  0 0 0 −0 0 1 −0 1 0 −0 1 1 −1 0 0 −1 0 1 −1 1 0 +1 1 1 +​𝑥↓1  𝑞=1/4 
𝑝=0 𝑟=1/2 Initial error rate is 1/4  (25% positive, 75% negative) Error rate after split is 0.5∗0+0.5∗0.5=1/4  (left leaf is 100% negative; right leaf is 50/50) Overall error doesn’t decrease! Entropy as a better splitting measure  
𝑓(𝑥)=​𝑥↓1 ∧​𝑥↓2  0 0 0 −0 0 1 −0 1 0 −0 1 1 −1 0 0 −1 0 1 −1 1 0 +1 1 1 +​𝑥↓1  𝑞=1/4 
𝑝=0 𝑟=1/2 Initial entropy is ​1/4 (​​log↓2 ⁠4)+​3/4 (​​log↓2 ⁠​4/3 )=0.81   Entropy after split is ​1/2 ∗0+​1/2 ∗1=0.5 Entropy decreases! Entropy as a better splitting measure  
• Natural greedy approaches where we grow the tree from the root to the leaves by repeatedly replacing an existing leaf with an internal node. • Key point: splitting criterion. • ID3: split the leaf that decreases the entropy the most. • Why not split according to error rate --- this is what we care about after all? • There are examples where you can get stuck!!! Top Down Decision Trees Algorithms  
• [Kearns-Mansour’96]: if measure of progress is entropy, we can always guarantees success under some formal relationships between the class of splits and the target (the class of splits can weakly approximate the target function).   • Provides a way to think about the effectiveness of various top down algos. 
Top Down Decision Trees Algorithms  • Key: strong concavity of the splitting crieterion h Pr[c*=1]=q 
Pr[c*=1| h=0]=p Pr[c*=1| h=1]=r 0 1 Pr[h=0]=u Pr[h=1]=1-u v v1 v2 • q=up + (1-u) r. p q r Want to lower bound: G(q) – [uG(p) + (1-u)G(r)] • If: G(q) =min(q,1-q) (error rate), then G(q) = uG(p) + (1-u)G(r)  • If: G(q) =H(q) (entropy), then G(q) – [uG(p) + (1-u)G(r)] >0 if r-p> 0 and u ≠1, u ≠0 (this happens under the weak learning assumption)
Two Core Aspects of Machine Learning  Algorithm Design. How to optimize? Automatically generate rules that do well on observed data. Confidence Bounds, Generalization Confidence for rule effectiveness on future data. Computation (Labeled) Data 
What you should know: • Well posed function approximation problems: – Instance space, X – Sample of labeled training data { <x(i), y(i)>} – Hypothesis space, H = { f: XàY } • Learning is a search/optimization problem over H – Various objective functions • minimize training error (0-1 loss)  • among hypotheses that minimize training error, select smallest (?) – But inductive learning without some bias is futile ! • Decision tree learning – Greedy top-down learning of decision trees (ID3, C4.5, ...) – Overfitting and tree post-pruning – Extensions… 
Extra slides extensions to decision tree learning  

  



Questions to think about (1) • ID3 and C4.5 are heuristic algorithms that search through the space of decision trees.  Why not just do an exhaustive search? 
Questions to think about (2) • Consider target function f: <x1,x2> à y, where x1 and x2 are real-valued, y is boolean.  What is the set of decision surfaces describable with decision trees that use each attribute at most once? 
Questions to think about (3) • Why use Information Gain to select attributes in decision trees?  What other criteria seem reasonable, and what are the tradeoffs in making this choice?   
Questions to think about (4) • What is the relationship between learning decision trees, and learning IF-THEN rules 

Machine Learning 10-601  Tom M. Mitchell Machine Learning Department Carnegie Mellon University  January 14, 2015  
Today: • Review: probability Readings:  Probability review • Bishop Ch. 1 thru 1.2.3 • Bishop, Ch. 2 thru 2.2 • Andrew Moore’s online tutorial many of these slides are derived from William Cohen, Andrew Moore, Aarti Singh, Eric Xing. Thanks! 
Probability Overview • Events  – discrete random variables, continuous random variables, compound events • Axioms of probability – What defines a reasonable theory of uncertainty • Independent events • Conditional probabilities • Bayes rule and beliefs • Joint probability distribution • Expectations • Independence, Conditional independence 
Random Variables • Informally, A is a random variable if – A denotes something about which we are uncertain – perhaps the outcome of a randomized experiment  • Examples A = True if a randomly drawn person from our class is female A = The hometown of a randomly drawn person from our class A = True if two randomly drawn persons from our class have same birthday  • Define P(A) as “the fraction of possible worlds in which A is true” or       “the fraction of times A holds, in repeated runs of the random experiment” – the set of possible worlds is called the sample space, S – A random variable A is a function defined over S                         A: S à {0,1}  
A little formalism More formally, we have • a sample space S (e.g., set of students in our class) – aka the set of possible worlds • a random variable is a function defined over the sample space – Gender: S à { m, f } – Height: S à Reals • an event is a subset of S – e.g., the subset of S for which Gender=f – e.g., the subset of S for which (Gender=m) AND (eyeColor=blue) • we’re often interested in probabilities of specific events • and of specific events conditioned on other specific events  
Visualizing A Sample space of all possible worlds Its area is 1             Worlds in which A is False Worlds in which A is true P(A) = Area of reddish oval 
The Axioms of Probability • 0 <= P(A) <= 1 • P(True) = 1 • P(False) = 0 • P(A or B) = P(A) + P(B) - P(A and B)  [di Finetti 1931]:  when gambling based on “uncertainty formalism A” you can be exploited by an opponent  iff  your uncertainty formalism A violates these axioms 
Elementary Probability in Pictures • P(~A) + P(A) = 1  A  ~A 
A useful theorem • 0 <= P(A) <= 1, P(True) = 1, P(False) = 0,     P(A or B) = P(A) + P(B) - P(A and B)  è P(A) = P(A ^ B) + P(A ^ ~B)  A =  [A and (B or ~B)]  =  [(A and B) or (A and ~B)] P(A) = P(A and B) + P(A and ~B) – P((A and B) and (A and ~B)) P(A) = P(A and B) + P(A and ~B) – P(A and B and A and ~B)  
Elementary Probability in Pictures • P(A) = P(A ^ B) + P(A ^ ~B)  B  A ^ ~B A ^ B 
Definition of Conditional Probability                      P(A ^ B)  P(A|B)  =  -----------                     P(B)  A  B  
Definition of Conditional Probability                      P(A ^ B)  P(A|B)  =  -----------                     P(B)  Corollary: The Chain Rule P(A ^ B) = P(A|B) P(B)  
Bayes Rule • let’s write 2 expressions for P(A ^ B)   B  A A ^ B 
P(B|A) * P(A) P(B) P(A|B) = Bayes, Thomas (1763) An essay towards solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London, 53:370-418 
…by no means merely a curious speculation in the doctrine of chances, but necessary to be solved in order to a sure foundation for all our reasonings concerning past facts, and what is likely to be hereafter…. necessary to be considered by any that would give a clear account of the strength of analogical or inductive reasoning… Bayes’ rule we call P(A) the “prior”  and P(A|B) the “posterior” 
Other Forms of Bayes Rule )(~)|~()()|()()|()|(APABPAPABPAPABPBAP+=)()()|()|(XBPXAPXABPXBAP∧∧∧=∧
Applying Bayes Rule P(A|B)=P(B|A)P(A)P(B|A)P(A)+P(B|~A)P(~A)A = you have the flu,   B = you just coughed  Assume: P(A) = 0.05 P(B|A) = 0.80 P(B| ~A) = 0.2  what is P(flu | cough) = P(A|B)? 
what does all this have to do with function approximation? 
The Joint Distribution Recipe for making a joint distribution of M variables: Example: Boolean variables A, B, C 
The Joint Distribution Recipe for making a joint distribution of M variables:  1. Make a truth table listing all combinations of values of your variables (if there are M Boolean variables then the table will have 2M rows). Example: Boolean variables A, B, C A B C 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 
The Joint Distribution Recipe for making a joint distribution of M variables:  1. Make a truth table listing all combinations of values of your variables (if there are M Boolean variables then the table will have 2M rows). 2. For each combination of values, say how probable it is. Example: Boolean variables A, B, C A B C Prob 0 0 0 0.30 0 0 1 0.05 0 1 0 0.10 0 1 1 0.05 1 0 0 0.05 1 0 1 0.10 1 1 0 0.25 1 1 1 0.10 
The Joint Distribution Recipe for making a joint distribution of M variables:  1. Make a truth table listing all combinations of values of your variables (if there are M Boolean variables then the table will have 2M rows). 2. For each combination of values, say how probable it is. 3. If you subscribe to the axioms of probability, those numbers must sum to 1. A B C Prob 0 0 0 0.30 0 0 1 0.05 0 1 0 0.10 0 1 1 0.05 1 0 0 0.05 1 0 1 0.10 1 1 0 0.25 1 1 1 0.10    A B C 0.05 0.25 0.10 0.05 0.05 0.10 0.10 0.30 
Using the Joint Distribution 
One you have the JD you can ask for the probability of any logical expression involving your attribute ∑=EPEP matching rows)row()(
Using the Joint 
P(Poor Male) = 0.4654 ∑=EPEP matching rows)row()(
Using the Joint 
P(Poor) = 0.7604 ∑=EPEP matching rows)row()(
Inference with the Joint 
∑∑=∧=2 2 1 matching rows and matching rows22121)row()row()()()|(EEEPPEPEEPEEP
Inference with the Joint 
∑∑=∧=2 2 1 matching rows and matching rows22121)row()row()()()|(EEEPPEPEEPEEPP(Male | Poor) = 0.4654 / 0.7604 = 0.612   
You should know • Events  – discrete random variables, continuous random variables, compound events • Axioms of probability – What defines a reasonable theory of uncertainty • Conditional probabilities • Chain rule • Bayes rule • Joint distribution over multiple random variables – how to calculate other quantities from the joint distribution 
Expected values Given discrete random variable X, the expected value of  X, written E[X] is     We also can talk about the expected value of functions of X 

Covariance Given two discrete r.v.’s X and Y, we define the  covariance of X and Y as   e.g., X=gender, Y=playsFootball or     X=gender, Y=leftHanded   Remember: 

CS224d	Deep	Learning		for	Natural	Language	Processing					Richard	Socher,	PhD	
Welcome	
3/31/16	Richard	Socher	2	1. CS224d	logis7cs		2. Introduc7on	to	NLP,	deep	learning	and	their	intersec7on		
Course	Logis>cs	• Instructor:	Richard	Socher		(Stanford	PhD,	2014;	now	Founder/CEO	at	MetaMind)	• TAs:	James	Hong,	Bharath	Ramsundar,	Sameep	Bagadia,	David	Dindi,	++	• Time:	Tuesday,	Thursday	3:00-4:20	• Loca7on:	Gates	B1	• There	will	be	3	problem	sets	(with	lots	of	programming),		a	midterm	and	a	ﬁnal	project		• For	syllabus	and	oﬃce	hours,	see	h\p://cs224d.stanford.edu/	• Slides	uploaded	before	each	lecture,	video	+	lecture	notes	a]er	3/31/16	Richard	Socher	Lecture	1,	Slide	3	
Pre-requisites	• Proﬁciency	in	Python	• All	class	assignments	will	be	in	Python.	There	is	a	tutorial	here		• College	Calculus,	Linear	Algebra	(e.g.	MATH	19	or	41,	MATH	51)	• Basic	Probability	and	Sta7s7cs	(e.g.	CS	109	or	other	stats	course)		• Equivalent	knowledge	of	CS229	(Machine	Learning)	• cost	func7ons,		• taking	simple	deriva7ves		• performing	op7miza7on	with	gradient	descent.	3/31/16	Richard	Socher	Lecture	1,	Slide	4	
Grading	Policy	• 3	Problem	Sets:	15%	x	3	=	45%		• Midterm	Exam:	15%		• Final	Course	Project:	40%	• Milestone:	5%	(2%	bonus	if	you	have	your	data	and	ran	an	experiment!)	• A\end	at	least	1	project	advice	oﬃce	hour:	2%	• Final	write-up,	project	and	presenta7on:	33%	• Bonus	points	for	excep7onal	poster	presenta7on		• Late	policy	• 7	free	late	days	–	use	as	you	please	• A]erwards,	25%	oﬀ	per	day	late	• PSets	Not	accepted	a]er	3	late	days	per	PSet	• Does	not	apply	to	Final	Course	Project		• Collabora7on	policy:	Read	the	student	code	book	and	Honor	Code!	• Understand	what	is	‘collabora7on’	and	what	is	‘academic	infrac7on’		3/31/16	Richard	Socher	Lecture	1,	Slide	5	
High	Level	Plan	for	Problem	Sets	• The	ﬁrst	half	of	the	course	and	the	ﬁrst	2	PSets	will	be	hard	• PSet	1	is	in	pure	python	code	(numpy	etc.)	to	really	understand	the	basics	• Released	on	April	4th	• New:	PSets	2	&	3	will	be	in	TensorFlow,	a	library	for	punng	together	new	neural	network	models	quickly	(à	special	lecture)	• PSet	3	will	be	shorter	to	increase	7me	for	ﬁnal	project	• Libraries	like	TensorFlow	(or	Torch)	are	becoming	standard	tools	• But	s7ll	some	problems	3/31/16	Richard	Socher	Lecture	1,	Slide	6	
What	is	Natural	Language	Processing	(NLP)?	• Natural	language	processing	is	a	ﬁeld	at	the	intersec7on	of		• computer	science	• ar7ﬁcial	intelligence	• and	linguis7cs.		• Goal:	for	computers	to	process	or	“understand”	natural	language	in	order	to	perform	tasks	that	are	useful,	e.g.	• Ques7on	Answering	• Fully	understanding	and	represen>ng	the	meaning	of	language	(or	even		deﬁning	it)	is	an	illusive	goal.	• Perfect	language	understanding	is		AI-complete		3/31/16	Richard	Socher	Lecture	1,	Slide	7	

NLP	Levels	
3/31/16	Richard	Socher	Lecture	1,	Slide	8	

(A	>ny	sample	of)	NLP	Applica>ons		• Applica7ons	range	from	simple	to	complex:	• Spell	checking,	keyword	search,	ﬁnding	synonyms	• Extrac7ng	informa7on	from	websites	such	as		• product	price,	dates,	loca7on,	people	or	company	names	• Classifying,	reading	level	of	school	texts,	posi7ve/nega7ve	sen7ment	of	longer	documents	• Machine	transla7on	• Spoken	dialog	systems	• Complex	ques7on	answering	3/31/16	Richard	Socher	Lecture	1,	Slide	9	
NLP	in	Industry	• Search	(wri\en	and	spoken)	• Online	adver7sement	• Automated/assisted	transla7on	• Sen7ment	analysis	for	marke7ng	or	ﬁnance/trading	• Speech	recogni7on	• Automa7ng	customer	support	3/31/16	Richard	Socher	Lecture	1,	Slide	10	

Why	is	NLP	hard?	• Complexity	in	represen7ng,	learning	and	using	linguis7c/situa7onal/world/visual	knowledge	• Jane	hit	June	and	then	she	[fell/ran].	• Ambiguity:	“I	made	her	duck”	
3/31/16	Richard	Socher	Lecture	1,	Slide	11	
What’s	Deep	Learning	(DL)?	• Deep	learning	is	a	subﬁeld	of	machine	learning	• Most	machine	learning	methods	work		well	because	of	human-designed		representa7ons	and	input	features	• For	example:	features	for	ﬁnding		named	en77es	like	loca7ons	or		organiza7on	names	(Finkel,	2010):	• Machine	learning	becomes	just	op7mizing	weights	to	best	make	a	ﬁnal	predic7on	3/31/16	Richard	Socher	Lecture	1,	Slide	12	
3.3. APPROACH35
Feature
NER
TF
Current Word
/check
/check
Previous Word
/check
/check
Next Word
/check
/check
Current Word Character n-gram
all
length≤6
Current POS Tag
/check
Surrounding POS Tag Sequence
/check
Current Word Shape
/check
/check
Surrounding Word Shape Sequence
/check
/check
Presence of Word in Left Window
size 4
size 9
Presence of Word in Right Window
size 4
size 9
Table 3.1: Features used by the CRF for the two tasks: named entity recognition (NER)and template ﬁlling (TF).can go beyond imposing just exact identity conditions). I illustrate this by modeling twoforms of non-local structure:label consistencyin the named entity recognition task, andtemplate consistencyin the template ﬁlling task. One could imagine many ways of deﬁningsuch models; for simplicity I use the formPM(y|x)∝∏λ∈Λθ#(λ,y,x)λ(3.1)where the product is over a set of violation typesΛ,a n df o re a c hv i o l a t i o nt y p eλwespecify a penalty parameterθλ.T h ee x p o n e n t#(λ,s,o)is the count of the number of timesthat the violationλoccurs in the state sequenceswith respect to the observation sequenceo.T h i s h a s t h e e f f e c t o f a s s i g n i n g s e q u e n c e s w i t h m o r e v i o l a tions a lower probability.The particular violation types are deﬁned speciﬁcally for each task, and are described insections 3.4.1 and 3.5.2.This model, as deﬁned above, is not normalized, and clearly itw o u l db ee x p e n s i v et od oso. As we will see in the discussion of Gibbs sampling, this will not actually be a problemfor us.
Machine	Learning	vs	Deep	Learning	Machine Learning in Practice Describing your data with features a computer can understand Learning algorithm 
Domain	speciﬁc,	requires	Ph.D.	level	talent	
Op7mizing	the	weights	on	features	
What’s	Deep	Learning	(DL)?	• Representa7on	learning	a\empts		to	automa7cally	learn	good		features	or	representa7ons	• Deep	learning	algorithms	a\empt	to	learn	(mul7ple	levels	of)		representa7on	and	an	output	• From	“raw”	inputs	x	(e.g.	words)	3/31/16	Richard	Socher	Lecture	1,	Slide	14	

On	the	history	and	term	of	“Deep	Learning”	• We	will	focus	on	diﬀerent	kinds	of	neural	networks		• The	dominant	model	family	inside	deep	learning	• Only	clever	terminology	for	stacked	logis7c	regression	units?	• Somewhat,	but	interes7ng	modeling	principles	(end-to-end)	and	actual	connec7ons	to	neuroscience	in	some	cases	• We	will	not	take	a	historical	approach	but	instead	focus	on	methods	which	work	well	on	NLP	problems	now	• For	history	of	deep	learning	models	(star7ng	~1960s),	see:		Deep	Learning	in	Neural	Networks:	An	Overview		by	Schmidhuber	3/31/16	Richard	Socher	Lecture	1,	Slide	15	
Reasons	for	Exploring	Deep	Learning	• Manually	designed	features	are	o]en	over-speciﬁed,	incomplete	and	take	a	long	7me	to	design	and	validate	• Learned	Features	are	easy	to	adapt,	fast	to	learn	• Deep	learning	provides	a	very	ﬂexible,	(almost?)	universal,	learnable	framework	for	represen>ng	world,	visual	and	linguis7c	informa7on.	• Deep	learning	can	learn	unsupervised	(from	raw	text)	and	supervised	(with	speciﬁc	labels	like	posi7ve/nega7ve)	3/31/16	Richard	Socher	Lecture	1,	Slide	16	
Reasons	for	Exploring	Deep	Learning	• In	2006	deep	learning	techniques	started	outperforming	other	machine	learning	techniques.	Why	now?	• DL	techniques	beneﬁt	more	from	a	lot	of	data	• Faster	machines	and	mul7core	CPU/GPU	help	DL		• New	models,	algorithms,	ideas		à	Improved	performance	(ﬁrst	in	speech	and	vision,	then	NLP)	
3/31/16	Richard	Socher	Lecture	1,	Slide	17	
Deep	Learning	for	Speech	• The	ﬁrst	breakthrough	results	of	“deep	learning”	on	large	datasets	happened	in	speech	recogni7on	• Context-Dependent	Pre-trained	Deep	Neural	Networks	for	Large	Vocabulary	Speech	Recogni7on		Dahl	et	al.	(2010)	
3/31/16	Richard	Socher	Lecture	1,	Slide	18	
Phonemes/Words	
Acous>c	model	Recog	\	WER	RT03S	FSH	Hub5	SWB	Tradi7onal	features	1-pass	−adapt	27.4	23.6	Deep	Learning	1-pass	−adapt	18.5	(−33%)	16.1	(−32%)	
Deep	Learning	for	Computer	Vision	• Most	deep	learning	groups	have	(un7l	2	years	ago)		focused	on	computer	vision	• Break	through	paper:	ImageNet	Classiﬁca7on	with	Deep	Convolu7onal	Neural	Networks	by	Krizhevsky	et	al.	2012	
Richard	Socher	Lecture	1,	Slide	19	19	Zeiler	and	Fergus	(2013)	8Olga Russakovsky* et al.PASCALILSVRCbirds
···cats
···dogs
···Fig. 2The ILSVRC dataset contains many more ﬁne-grained classes compared to the standard PASCAL VOC benchmark;for example, instead of the PASCAL “dog” category there are 120 di↵erent breeds of dogs in ILSVRC2012-2014 classiﬁcationand single-object localization tasks.are 1000 object classes and approximately 1.2 milliontraining images, 50 thousand validation images and 100thousand test images. Table 2 (top) documents the sizeof the dataset over the years of the challenge.3.2 Single-object localization dataset constructionThe single-object localization task evaluates the abilityof an algorithm to localize one instance of an objectcategory. It was introduced as a taster task in ILSVRC2011, and became an o cial part of ILSVRC in 2012.The key challenge was developing a scalable crowd-sourcing method for object bounding box annotation.Our three-step self-verifying pipeline is described in Sec-tion 3.2.1. Having the dataset collected, we performdetailed analysis in Section 3.2.2 to ensure that thedataset is su ciently varied to be suitable for evalu-ation of object localization algorithms.Object classes and candidate images.The object classesfor single-object localization task are the same as theobject classes for image classiﬁcation task describedabove in Section 3.1. The training images for localiza-tion task are a subset of the training images used forimage classiﬁcation task, and the validation and testimages are the same between both tasks.Bounding box annotation.Recall that for the imageclassiﬁcation task every image was annotated with oneobject class label, corresponding to one object that ispresent in an image. For the single-object localizationtask, every validation and test image and a subset of thetraining images are annotated with axis-aligned bound-ing boxes around every instance of this object.Every bounding box is required to be as small aspossible while including all visible parts of the objectinstance. An alternate annotation procedure could beto annotate thefull (estimated) extentof the object:e.g., if a person’s legs are occluded and only the torsois visible, the bounding box could be drawn to includethe likely location of the legs. However, this alterna-tive procedure is inherently ambiguous and ill-deﬁned,leading to disagreement among annotators and amongresearchers (what is the true “most likely” extent ofthis object?). We follow the standard protocol of onlyannotating visible object parts (Russell et al., 2007; Ev-eringham et al., 2010).53.2.1 Bounding box object annotation systemWe summarize the crowdsourced bounding box anno-tation system described in detail in (Su et al., 2012).The goal is to build a system that is fully automated,5Some datasets such as PASCAL VOC (Everingham et al.,2010) and LabelMe (Russell et al., 2007) are able to providemore detailed annotations: for example, marking individualobject instances as beingtruncated.W ec h o s en o tt op r o v i d ethis level of detail in favor of annotating more images andmore object instances.)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE
7KHZHLJKWVRIWKLVQHXURQYLVXDOL]HG)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE
9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV=HLOHU	)HUJXV
)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE
9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV=HLOHU	)HUJXV)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS
)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE
9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV=HLOHU	)HUJXV
)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE)HL)HL/L	$QGUHM.DUSDWK\/HFWXUH)HE
9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV=HLOHU	)HUJXV
Deep	Learning	+	NLP	=	Deep	NLP	• Combine	ideas	and	goals	of	NLP	and	use	representa7on	learning	and	deep	learning	methods	to	solve	them	• Several	big	improvements	in	recent	years	across	diﬀerent	NLP		• levels:	speech,	morphology,	syntax,	seman7cs	• applica>ons:	machine	transla7on,	sen7ment	analysis	and	ques7on	answering	
3/31/16	Richard	Socher	Lecture	1,	Slide	20	
Representa>ons	at	NLP	Levels:	Phonology	• Tradi7onal:	Phonemes	
• DL:	trains	to	predict	phonemes	(or	words	directly)	from	sound	features	and	represent	them	as	vectors	3/31/16	Richard	Socher	Lecture	1,	Slide	21	              THE INTERNATIONAL PHONETIC ALPHABET (revised to 2005)CONSONANTS (PULMONIC)
´AÅiyÈ Ë ¨ uPeeØoE{ ‰ øOa”åIY U F r o n t                         C e n t r a l                             B a c kCloseClose-midOpen-midOpenWhere symbols appear in pairs, the one to the right represents a rounded vowel.œòBilabialLabiodentalDentalAlveolarPost alveolarRetroflexPalatalVelarUvularPharyngealGlottalPlosivep  bt  dÊ  c  Ôk  gq  G/Nasalmµn=N–TrillırRTap or Flap    v| «FricativeF  Bf   vT  D s ¬¬zS  Zß  ç  Jx  VX  Â©  ?h  HLateralfricativeÒ  LApproximant¥® ’j˜Lateralapproximantl Ò¥KWhere symbols appear in pairs, the one to the right represents a voiced consonant. Shaded areas denote articulations judged impossible.CONSONANTS (NON-PULMONIC)
SUPRASEGMENTALSVOWELS
OTHER SYMBOLSClicksVoiced implosivesEjectives>BilabialBilabial’Examples:˘DentalÎDental/alveolarp’Bilabial!(Post)alveolar˙Palatalt’Dental/alveolar¯PalatoalveolarƒVelark’VelarAlveolar lateralÏUvulars’Alveolar fricative "Primary stress ÆSecondary stressÆfoUn´"tIS´n …Long              e… ÚHalf-long       eÚ  *Extra-short     e*˘Minor (foot) groupMajor (intonation) group .Syllable break    ®i.œkt   § Linking (absence of a break)          TONES AND WORD ACCENTS       LEVELCONTOURe¬_orâExtrahigheˆoräRisinge!êHighe$ëFallinge@îMide%üHighrisinge~ôLoweﬁïLowrisinge—ûExtralowe& ñ$Rising-fallingÕDownstepãGlobal riseõUpstepÃGlobal fall© 2005 IPA
 DIACRITICS     Diacritics may be placed above a symbol with a descender, e.g. N(  9Voiceless                n9    d9  ªBreathy voiced      bª  aª  1Dental                     t¬1 d1  3Voiced                 s3  t¬3  0Creaky voiced       b0  a0  ¡Apical                     t¬¡ d¡ ÓAspirated             tÓ dÓ  £Linguolabial          t¬£  ¬d£     4Laminal                  t¬4 d4  7More rounded     O7 WLabialized             tW dW  )Nasalized                      e)  ¶Less rounded      O¶ ¨Palatalized            t¨  d¨ ˆNasal release                dˆ  ™Advanced           u™ ¹Velarized              t¹ ¬d¹ ¬Lateral release              d¬  2Retracted            e2 Pharyngealized     t ¬¬d }No audible release        d}¬  ¬·Centralized         e· ùVelarized or pharyngealized      :  +Mid-centralized  e+  6Raised                  e6        ¬( ®6    = voiced alveolar fricative)  `Syllabic              n`  §Lowered              e§       ( B§  = voiced bilabial approximant)  8Non-syllabic       e8  5Advanced Tongue Root          e5 ±Rhoticity             ´± a±  Retracted Tongue Root           e    Voiceless labial-velar fricativeÇ ÛAlveolo-palatal fricativesw  ¬ Voiced labial-velar approximant  »Voiced alveolar lateral flapÁ     Voiced labial-palatal approximantÍSimultaneous  S  and   xÌVoiceless epiglottal fricative¬¿     ¬Voiced epiglottal fricativeAffricates and double articulationscan be represented by two symbols¬÷  ¬   Epiglottal plosive joined by a tie bar if necessary.kp  ts((
Representa>ons	at	NLP	Levels:	Morphology	• Tradi7onal:	Morphemes		preﬁx		stem					suﬃx						un		interest			ed	• DL:		• every	morpheme	is	a	vector	• a	neural	network	combines		two	vectors	into	one	vector	• Thang	et	al.	2013	3/31/16	Richard	Socher	Lecture	1,	Slide	22	/g437/g374/g87/g90/g28/g296/g381/g396/g410/g437/g374/g258/g410/g286/g94/g100/g68
/g367/g455/g94/g104/g38/g437/g374/g296/g381/g396/g410/g437/g374/g258/g410/g286/g367/g455/g94/g100/g68
/g1849/g3040/g481/g1854/g3040/g1849/g3040/g481/g1854/g3040/g437/g374/g296/g381/g396/g410/g437/g374/g258/g410/g286/g94/g100/g68Figure 1:Morphological Recursive Neural Net-work. A vector representation for the word “un-fortunately” is constructed from morphemic vec-tors:unpre,fortunatestm,lysuf. Dotted nodes arecomputed on-the-ﬂy and not in the lexicon.3 Morphological RNNsOur morphological Recursive Neural Network(morphoRNN) is similar to (Socher et al., 2011b),but operates at the morpheme level instead of atthe word level. Speciﬁcally, morphemes, the mini-mum meaning-bearing unit in languages, are mod-eled as real-valued vectors of parameters, and areused to build up more complex words. We assumeaccess to a dictionary of morphemic analyses ofwords, which will be detailed in Section 4.Following (Collobert and Weston, 2008), dis-tinct morphemes are encoded by column vectorsin a morphemic embedding matrixWe∈Rd×|M|,wheredis the vector dimension andMis an or-dered set of all morphemes in a language.As illustrated in Figure 1, vectors of morpho-logically complex words are gradually built upfrom their morphemic representations. At any lo-cal decision (a dotted node), a new parent wordvector (p) is constructed by combining a stem vec-tor (xstem) and an afﬁx vector (xaffix) as follow:p=f(Wm[xstem;xaffix]+bm)(1)Here,Wm∈Rd×2dis a matrix of morphemic pa-rameters whilebm∈Rd×1is an intercept vector.We denote an element-wise activation function asf, such astanh. This forms the basis of our mor-phoRNN models withθ={We,Wm,bm}beingthe parameters to be learned.3.1 Context-insensitive Morphological RNNOur ﬁrst model examines how well morphoRNNscould construct word vectors simply from the mor-phemic representationwithout referring to anycontext information. Input to the model is a refer-ence embedding matrix, i.e. word vectors trainedby an NLM such as (Collobert and Weston, 2008)and (Huang et al., 2012). By assuming that thesereference vectors are right, the goal of the modelis to construct new representations for morpholog-ically complex words from their morphemes thatclosely match the corresponding reference ones.Speciﬁcally, the structure of the context-insensitive morphoRNN (cimRNN) is the same asthe basic morphoRNN. For learning, we ﬁrst de-ﬁne a cost functionsfor each wordxias thesquared Euclidean distance between the newly-constructedrepresentationpc(xi)and itsrefer-encevectorpr(xi):s(xi)=∥pc(xi)−pr(xi)∥22.The objective function is then simply the sum ofall individual costs overNtraining examples, plusa regularization term, which we try to minimize:J(θ)=N/summationdisplayi=1s(xi)+λ2∥θ∥22(2)3.2 Context-sensitive Morphological RNNThe cimRNN model, though simple, is interestingto attest if morphemic semantics could be learnedsolely from an embedding. However, it is lim-ited in several aspects. Firstly, the model hasno chance of improving representations for rarewords which might have been poorly estimated.For example, “distinctness” and “unconcerned”are very rare, occurring only 141 and 340 timesin Wikipedia documents, even though their corre-sponding stems “distinct” and “concern” are veryfrequent (35323 and 26080 respectively). Tryingto construct exactly those poorly-estimated wordvectors might result in a bad model with parame-ters being pushed in wrong directions.Secondly, though word embeddings learnedfrom an NLM could, in general, blend well boththe semantic and syntactic information, it wouldbe useful to explicitly model another kind of syn-tactic information, the word structure, as we trainour embeddings. Motivated by these limitations,we propose a context-sensitive morphoRNN (csm-RNN) which integrates RNN structures into NLMtraining, allowing for contextual information be-ing taken into account in learning morphemiccompositionality. Speciﬁcally, we adopt the NLMtraining approach proposed in (Collobert et al.,2011) to learn word embeddings, but build rep-resentations for complex words from their mor-phemes. During learning, updates at the top levelof the neural network will be back-propagated allthe way till the morphemic layer.
Neural	word	vectors	-	visualiza>on	
23	

Representa>ons	at	NLP	Levels:	Syntax	• Tradi7onal:	Phrases	Discrete	categories	like	NP,	VP	• DL:		• Every	word	and	every	phrase	is	a	vector	• a	neural	network	combines		two	vectors	into	one	vector	• Socher	et	al.	2011	3/31/16	Richard	Socher	Lecture	1,	Slide	24	

Representa>ons	at	NLP	Levels:	Seman>cs	• Tradi7onal:	Lambda	calculus	• Carefully	engineered	func7ons	• Take	as	inputs	speciﬁc	other	func7ons	• No	no7on	of	similarity	or	fuzziness	of	language	• DL:		• Every	word	and	every	phrase	and	every	logical	expression		is	a	vector	• a	neural	network	combines		two	vectors	into	one	vector	• Bowman	et	al.	2014	3/31/16	Richard	Socher	Lecture	1,	Slide	25	
Much of the theoretical work on natural lan-guage inference (and some successful imple-mented models; MacCartney and Manning 2009;Watanabe et al. 2012) involvesnatural logics,which are formal systems that deﬁne rules of in-ference between natural language words, phrases,and sentences without the need of intermediaterepresentations in an artiﬁcial logical language.In our ﬁrst three experiments, we test our mod-els’ ability to learn the foundations of natural lan-guage inference by training them to reproduce thebehavior of the natural logic of MacCartney andManning (2009) on artiﬁcial data. This logic de-ﬁnes seven mutually-exclusive relations of syn-onymy, entailment, contradiction, and mutual con-sistency, as summarized in Table 1, and it pro-vides rules of semantic combination for project-ing these relations from the lexicon up to com-plex phrases. The formal properties of this sys-tem are now well-understood (Icard and Moss,2013a; Icard and Moss, 2013b). The ﬁrst exper-iment using this logic covers reasoning with thebare logical relations (§3), the second extends thisto reasoning with statements constructed compo-sitionally from recursive functions (§4), and thethird covers the additional complexity that resultsfrom quantiﬁcation (§5). Though the performanceof the plain TreeRNN model is somewhat poorin our ﬁrst experiment, we ﬁnd that the strongerTreeRNTN model generalizes well in every case,suggesting that it has learned to simulate our targetlogical concepts.The experiments with simulated data provide aconvincing demonstration of the ability of neuralnetworks to learn to build and use semantic repre-sentations for complex natural language sentencesfrom reasonably-sized training sets. However, weare also interested in the more practical question ofwhether they can learn these representations fromnaturalistic text. To address this question, we ap-ply our models to the SICK entailment challengedata in§6. The small size of this corpus puts data-hungry NN models like ours at a disadvantage,but we are nonetheless able to achieve competi-tive performance on it, surpassing several submit-ted models with signiﬁcant hand-engineered task-speciﬁc features and our own NN baseline. Thissuggests that the representational abilities that weobserve in the previous sections are not limited tocarefully circumscribed tasks. We conclude thatTreeRNTN models are adequate for typical casesP(@)=0.8all reptiles walkvs.some turtles moveSoftmax classiﬁerComparisonN(T)N layerCompositionRN(T)NlayersPre-trained or randomly initialized learned word vectorsallreptilesall reptileswalkall reptiles walksometurtlessome turtlesmovesome turtles moveFigure 1: In our model, two separate tree-structured networks build up vector representa-tions for each of two sentences using either NNor NTN layer functions. A comparison layer thenuses the resulting vectors to produce features for aclassiﬁer.of natural language inference, and that there is notyet any clear level of inferential complexity forwhich other approaches work and NN models fail.2 Tree-structured neural networksWe limit the scope of our experiments in this paperto neural network models that adhere to the lin-guisticprinciple of compositionality, which saysthat the meanings for complex expressions are de-rived from the meanings of their parts via speciﬁccomposition functions (Partee, 1984; Janssen,1997). In our distributed setting, word meaningsare embedding vectors of dimensionn. A learnedcomposition function maps pairs of them to singlephrase vectors of dimensionn, which can then bemerged again to represent more complex phrases,forming a tree structure. Once the entire sentence-level representation has been derived at the top ofthe tree, it serves as a ﬁxed-dimensional input forsome subsequent layer function.To apply these recursive models to our task, wepropose the tree pair model architecture depictedin Fig. 1. In it, the two phrases being compared areprocessed separately using a pair of tree-structurednetworks that share a single set of parameters. Theresulting vectors are fed into a separate compari-son layer that is meant to generate a feature vec-tor capturing the relation between the two phrases.The output of this layer is then given to a softmaxclassiﬁer, which produces a distribution over theseven relations represented in Table 1.For the sentence embedding portions of the net-work, we evaluate both TreeRNN models with thestandard NN layer function (1) and those with the
NLP	Applica>ons:	Sen>ment	Analysis	• Tradi7onal:	Curated	sen7ment	dic7onaries	combined	with	either	bag-of-words	representa7ons	(ignoring	word	order)	or	hand-designed	nega7on	features	(ain’t	gonna	capture	everything)	• Same	deep	learning	model	that	was	used	for	morphology,	syntax	and	logical	seman7cs	can	be	used!	à	RecursiveNN			
3/31/16	Richard	Socher	Lecture	1,	Slide	26	

Ques>on	Answering	• Common:	A	lot	of	feature	engineering	to	capture	world	and	other	knowledge,		e.g.	regular	expressions,	Berant	et	al.	(2014)	• DL:	Same	deep	learning	model	that	was	used	for	morphology,	syntax,	logical	seman7cs	and	sen7ment	can	be	used!	• Facts	are	stored	in	vectors	
Lecture	1,	Slide	27	TypeExample# (%)DependencyQ: What can the splitting of water lead to?407 (69.57%)a: Light absorptionb: Transfer of ionsTemporalQ: What is the correct order of events?57 (9.74%)a: PDGF binds to tyrosine kinases, then cells divide, then wound healingb: Cells divide, then PDGF binds to tyrosine kinases, then wound healingTrue-FalseQ: Cdk associates with MPF to become cyclin121 (20.68%)a: Trueb: FalseTable 3:Examples and statistics for each of the three coarse types of questions.Is main verb trigger?ConditionRegular Exp.Wh-word subjective?AGENTWh-word object?THEMEConditionRegular Exp.default(ENABLE|SUPER)+DIRECT(ENABLE|SUPER)PREVENT(ENABLE|SUPER)⇤PREVENT(ENABLE|SUPER)⇤YesNoFigure 3:Rules for determining the regular expressions for queries concerning two triggers. In each table, theconditioncolumn decides the regular expression to be chosen. In the left table, we make the choice based on the path from the root totheWh-word in the question. In the right table, if the worddirectlymodiﬁes the main trigger, theDIRECTregular expressionis chosen. If the main verb in the question is in the synset ofprevent,inhibit,stoporprohibit, we select thePREVENTregularexpression. Otherwise, the default one is chosen. We omit the relation label SAMEfrom the expressions, but allow goingthrough any number of edges labeled by SAMEwhen matching expressions to the structure.that we expand using WordNet.The ﬁnal step in constructing the query is toidentify the regular expression for the path con-necting the source and the target. Due to paucityof data, we do not map a question and an answerto arbitrary regular expressions. Instead, we con-struct a small set of regular expressions, and builda rule-based system that selects one. We used thetraining set to construct the regular expressionsand we found that they answer most questions (seeSection 6.4). We determine the regular expressionbased on whether the main verb in the sentence isa trigger and whether the source and target of thepath are triggers or arguments. Figure 3 shows thepossible regular expressions and the procedure forchoosing one when both the source and target aretriggers. If either of them are argument nodes, weappend the appropriate semantic role to the regu-lar expression, based on whether the argument isthe source or the target of the path (or both).True-false questions are treated similarly, ex-cept that both source and target are chosen fromthe question. For temporal questions, we seek toidentify the ordering of events in the answers. Weuse the keywordsﬁrst,then, orsimultaneouslytoidentify the implied order in the answer. We usethe regular expression SUPER+for questions ask-ing about simultaneous events and ENABLE+forthose asking about sequential events.5.3 Answering QuestionsWe match the query of an answer to the processstructure to identify the answer. In case of a match,the corresponding answer is chosen. The matchingpath can be thought of as aprooffor the answer.If neither query matches the graph (or both do),we check if either answer contradicts the struc-ture. To do so, we ﬁnd an undirected path fromthe source to the target. In the event of a match, ifthe matching path traverses any ENABLEedge inthe incorrect direction, we treat this as arefutationfor the corresponding answer and select the otherone. In our running example, in addition to thevalid path for the second query, for the ﬁrst querywe see that there is an undirected path fromsplittoabsorbthroughtransferthat matches the ﬁrstquery. This tells us thatlight absorptioncannotbe the answer because it is not along a causal pathfromsplit.Finally, if none of the queries results in a match,we look for any unlabeled path between the sourceand the target, before backing off to a dependency-based proximity baseline described in Section 6.When there are multiple aligning nodes in thequestion and answer, we look for any proof orrefutation before backing off to the baselines.

Machine	Transla>on	• Many	levels	of	transla7on		have	been	tried	in	the	past:	• Tradi7onal	MT	systems	are		very	large	complex	systems		• What	do	you	think	is	the	interlingua	for	the	DL	approach	to	transla7on?	3/31/16	Richard	Socher	Lecture	1,	Slide	28	

Machine	Transla>on	
3/31/16	Richard	Socher	Lecture	1,	Slide	29	

Machine	Transla>on	• Source	sentence	mapped	to	vector,	then	output	sentence	generated.		• Sequence	to	Sequence	Learning	with	Neural	Networks	by	Sutskever	et	al.	2014;	Luong	et	al.	2016	• About	to	replace	very	complex	hand	engineered	architectures	3/31/16	Richard	Socher	Lecture	1,	Slide	30	sequence of words representing the answer. It is therefore clear that a domain-independent methodthat learns to map sequences to sequences would be useful.Sequences pose a challenge for DNNs because they require thatt h ed i m e n s i o n a l i t yo ft h ei n p u t sa n doutputs is known and ﬁxed. In this paper, we show that a straightforward application of the LongShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large ﬁxed-dimensional vector representation, and then to use anotherLSTM to extract the output sequencefrom that vector (ﬁg. 1). The second LSTM is essentially a recurrent neural network language model[28, 23, 30] except that it is conditioned on the input sequence. The LSTM’s ability to successfullylearn on data with long range temporal dependencies makes itan a t u r a lc h o i c ef o rt h i sa p p l i c a t i o ndue to the considerable time lag between the inputs and theircorresponding outputs (ﬁg. 1).There have been a number of related attempts to address the general sequence to sequence learningproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]who were the ﬁrst to map the entire input sentence to vector, and is related to Cho et al. [5] althoughthe latter was used only for rescoring hypotheses produced byap h r a s e - b a s e ds y s t e m . G r a v e s[ 1 0 ]introduced a novel differentiable attention mechanism thata l l o w sn e u r a ln e t w o r k st of o c u so nd i f -ferent parts of their input, and an elegant variant of this idea was successfully applied to machinetranslation by Bahdanau et al. [2]. The Connectionist Sequence Classiﬁcation is another populartechnique for mapping sequences to sequences with neural networks, but it assumes a monotonicalignment between the inputs and the outputs [11].
Figure 1:Our model reads an input sentence “ABC” and produces “WXYZ” ast h eo u t p u ts e n t e n c e . T h emodel stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads theinput sentence in reverse, because doing so introduces manyshort term dependencies in the data that make theoptimization problem much easier.The main result of this work is the following. On the WMT’14 English to French translation task,we obtained a BLEU score of34.81by directly extracting translations from an ensemble of 5 deepLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-search decoder. This is by far the best result achieved by direct translation with large neural net-works. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalizedwhenever the reference translation contained a word not covered by these 80k. This result showsthat a relatively unoptimized small-vocabulary neural network architecture which has much roomfor improvement outperforms a phrase-based SMT system.Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline onthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).Surprisingly, the LSTM did not suffer on very long sentences,d e s p i t et h er e c e n te x p e r i e n c eo fo t h e rresearchers with related architectures [26]. We were able tod ow e l lo nl o n gs e n t e n c e sb e c a u s ew ereversed the order of words in the source sentence but not thetarget sentences in the training and testset. By doing so, we introduced many short term dependenciesthat made the optimization problemmuch simpler (see sec. 2 and 3.3). As a result, SGD could learnLSTMs that had no trouble withlong sentences. The simple trick of reversing the words in thes o u r c es e n t e n c ei so n eo ft h ek e ytechnical contributions of this work.Au s e f u lp r o p e r t yo ft h eL S T Mi st h a ti tl e a r n st om a pa ni n p u tsentence of variable length intoaﬁ x e d - d i m e n s i o n a lv e c t o rr e p r e s e n t a t i o n . G i v e nt h a tt r a nslations tend to be paraphrases of thesource sentences, the translation objective encourages theL S T Mt oﬁ n ds e n t e n c er e p r e s e n t a t i o n sthat capture their meaning, as sentences with similar meanings are close to each other while different2
3/31/16	Richard	Socher	Lecture	1,	Slide	31	
Representa>on	for	all	levels:	Vectors	• We	will	learn	in	the	next	lecture	how	we	can	learn	vector	representa7ons	for	words	and	what	they	actually	represent.	
• Next	week:	neural	networks	and	how	they	can	use	these	vectors	for	all	NLP	levels	and	many	diﬀerent	applica7ons	3/31/16	Richard	Socher	Lecture	1,	Slide	32	

Vector'SemanticsDense%Vectors%
Dan%JurafskySparse'versus'dense'vectors•PPMI%vectors%are•long(length%|V|=%20,000%to%50,000)•sparse'(most%elements%are%zero)•Alternative:%learn%vectors%which%are•short(length%200F1000)•dense(most%elements%are%nonFzero)2
Dan%JurafskySparse'versus'dense'vectors•Why%dense%vectors?•Short%vectors%may%be%easier%to%use%as%features%in%machine%learning%(less%weights%to%tune)•Dense%vectors%may%generalize%better%than%storing%explicit%counts•They%may%do%better%at%capturing%synonymy:•carand%automobileare%synonyms;%but%are%represented%as%distinct%dimensions;%this%fails%to%capture%similarity%between%a%word%with%caras%a%neighbor%and%a%word%with%automobileas%a%neighbor3
Dan%JurafskyThree'methods'for'getting'short'dense'vectors•Singular%Value%Decomposition%(SVD)•A%special%case%of%this%is%called%LSA%–Latent%Semantic%Analysis•“Neural%Language%Model”Finspired%predictive%models•skipFgrams%and%CBOW•Brown%clustering4
Vector'SemanticsDense%Vectors%via%SVD
Dan%JurafskyIntuition•Approximate%an%NFdimensional%dataset%using%fewer%dimensions•By%first%rotating%the%axes%into%a%new%space•In%which%the%highest%order%dimension%captures%the%most%variance%in%the%original%dataset•And%the%next%dimension%captures%the%next%most%variance,%etc.•Many%such%(related)%methods:•PCA%–principle%components%analysis•Factor%Analysis•SVD6
Dan%Jurafsky
123456123456
7123456123456PCA dimension 1PCA dimension 2Dimensionality'reduction
Dan%JurafskySingular'Value'Decomposition
8Any/rectangular/w/x/c/matrix/X/equals/the/product/of/3/matrices:W:%rows%corresponding%to%original%but%m%columns%represents%a%dimension%in%a%new%latent%space,%such%that%•M%column%vectors%are%orthogonal%to%each%other•Columns%are%ordered%by%the%amount%of%variance%in%the%dataset%each%new%dimension%accounts%forS:%%diagonal%mx%mmatrix%of%singular'values'expressing%the%importance%of%each%dimension.C:%columns%corresponding%to%original%but%m%rows%corresponding%to%singular%values
Dan%JurafskySingular'Value'Decomposition238 LANDAUER AND DUMAIS Appendix An Introduction to Singular Value Decomposition and an LSA Example Singular Value Decomposition (SVD) A well-known proof in matrix algebra asserts that any rectangular matrix (X) is equal to the product of three other matrices (W, S, and C) of a particular form (see Berry, 1992, and Golub et al., 1981, for the basic math and computer algorithms of SVD). The first of these (W) has rows corresponding to the rows of the original, but has m columns corresponding to new, specially derived variables such that there is no correlation between any two columns; that is, each is linearly independent of the others, which means that no one can be constructed as a linear combination of others. Such derived variables are often called principal components, basis vectors, factors, or dimensions. The third matrix (C) has columns corresponding to the original columns, but m rows composed of derived singular vectors. The second matrix (S) is a diagonal matrix; that is, it is a square m × m matrix with nonzero entries only along one central diagonal. These are derived constants called singular values. Their role is to relate the scale of the factors in the first two matrices to each other. This relation is shown schematically in Figure A1. To keep the connection to the concrete applications of SVD in the main text clear, we have labeled the rows and columns words (w) and contexts (c). The figure caption defines SVD more formally. The fundamental proof of SVD shows that there always exists a decomposition of this form such that matrix mu!tiplication of the three derived matrices reproduces the original matrix exactly so long as there are enough factors, where enough is always less than or equal to the smaller of the number of rows or columns of the original matrix. The number actually needed, referred to as the rank of the matrix, depends on (or expresses) the intrinsic dimensionality of the data contained in the cells of the original matrix. Of critical importance for latent semantic analysis (LSA), if one or more factor is omitted (that is, if one or more singular values in the diagonal matrix along with the corresponding singular vectors of the other two matrices are deleted), the reconstruction is a least-squares best approximation to the original given the remaining dimensions. Thus, for example, after constructing an SVD, one can reduce the number of dimensions systematically by, for example, remov- ing those with the smallest effect on the sum-squared error of the approx- imation simply by deleting those with the smallest singular values. The actual algorithms used to compute SVDs for large sparse matrices of the sort involved in LSA are rather sophisticated and are not described here. Suffice it to say that cookbook versions of SVD adequate for small (e.g., 100 × 100) matrices are available in several places (e.g., Mathematica, 1991 ), and a free software version (Berry, 1992) suitable Contexts 3= mxm mxc wxc w xm Figure A1. Schematic diagram of the singular value decomposition (SVD) of a rectangular word (w) by context (c) matrix (X). The original matrix is decomposed into three matrices: W and C, which are orthonormal, and S, a diagonal matrix. The m columns of W and the m rows of C ' are linearly independent. for very large matrices such as the one used here to analyze an encyclope- dia can currently be obtained from the WorldWideWeb (http://www.net- lib.org/svdpack/index.html). University-affiliated researchers may be able to obtain a research-only license and complete software package for doing LSA by contacting Susan Dumais. A~ With Berry's software and a high-end Unix work-station with approximately 100 megabytes of RAM, matrices on the order of 50,000 × 50,000 (e.g., 50,000 words and 50,000 contexts) can currently be decomposed into representations in 300 dimensions with about 2-4 hr of computation. The computational complexity is O(3Dz), where z is the number of nonzero elements in the Word (w) × Context (c) matrix and D is the number of dimensions returned. The maximum matrix size one can compute is usually limited by the memory (RAM) requirement, which for the fastest of the methods in the Berry package is (10 + D + q)N + (4 + q)q, where N = w + c and q = min (N, 600), plus space for the W × C matrix. Thus, whereas the computational difficulty of methods such as this once made modeling and simulation of data equivalent in quantity to human experi- ence unthinkable, it is now quite feasible in many cases. Note, however, that the simulations of adult psycholinguistic data reported here were still limited to corpora much smaller than the total text to which an educated adult has been exposed. An LSA Example Here is a small example that gives the flavor of the analysis and demonstrates what the technique can accomplish. A2 This example uses as text passages the titles of nine technical memoranda, five about human computer interaction (HCI), and four about mathematical graph theory, topics that are conceptually rather disjoint. The titles are shown below. cl: Human machine interface for ABC computer applications c2: A survey of user opinion of computer system response time c3: The EPS user interface management system c4: System and human system engineering testing of EPS c5: Relation of user perceived response time to error measurement ml: The generation of random, binary, ordered trees m2: The intersection graph of paths in trees m3: Graph minors IV: Widths of trees and well-quasi-ordering m4: Graph minors: A survey The matrix formed to represent this text is shown in Figure A2. (We discuss the highlighted parts of the tables in due course.) The initial matrix has nine columns, one for each title, and we have given it 12 rows, each corresponding to a content word that occurs in at least two contexts. These are the words in italics. In LSA analyses of text, includ- ing some of those reported above, words that appear in only one context are often omitted in doing the SVD. These contribute little to derivation of the space, their vectors can be constructed after the SVD with little loss as a weighted average of words in the sample in which they oc- curred, and their omission sometimes greatly reduces the computation. See Deerwester, Dumais, Furnas, Landauer, and Harshman (1990) and Dumais (1994) for more on such details. For simplicity of presentation, A~ Inquiries about LSA computer programs should be addressed to Susan T. Dumais, Bellcore, 600 South Street, Morristown, New Jersey 07960. Electronic mail may be sent via Intemet to std@bellcore.com. A2 This example has been used in several previous publications (e.g., Deerwester et al., 1990; Landauer & Dumais, 1996). 
9Landuaerand%Dumais1997
Dan%JurafskySVD'applied'to'term<document'matrix:Latent'Semantic'Analysis•If%instead%of%keeping%all%m%dimensions,%we%just%keep%the%top%k%singular%values.%Let’s%say%300.•The%result%is%a%leastFsquares%approximation%to%the%original%X•But%instead%of%multiplying,%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%we’ll%just%make%use%of%W.•Each%row%of%W:•A%kFdimensional%vector•Representing%word%W10238 LANDAUER AND DUMAIS Appendix An Introduction to Singular Value Decomposition and an LSA Example Singular Value Decomposition (SVD) A well-known proof in matrix algebra asserts that any rectangular matrix (X) is equal to the product of three other matrices (W, S, and C) of a particular form (see Berry, 1992, and Golub et al., 1981, for the basic math and computer algorithms of SVD). The first of these (W) has rows corresponding to the rows of the original, but has m columns corresponding to new, specially derived variables such that there is no correlation between any two columns; that is, each is linearly independent of the others, which means that no one can be constructed as a linear combination of others. Such derived variables are often called principal components, basis vectors, factors, or dimensions. The third matrix (C) has columns corresponding to the original columns, but m rows composed of derived singular vectors. The second matrix (S) is a diagonal matrix; that is, it is a square m × m matrix with nonzero entries only along one central diagonal. These are derived constants called singular values. Their role is to relate the scale of the factors in the first two matrices to each other. This relation is shown schematically in Figure A1. To keep the connection to the concrete applications of SVD in the main text clear, we have labeled the rows and columns words (w) and contexts (c). The figure caption defines SVD more formally. The fundamental proof of SVD shows that there always exists a decomposition of this form such that matrix mu!tiplication of the three derived matrices reproduces the original matrix exactly so long as there are enough factors, where enough is always less than or equal to the smaller of the number of rows or columns of the original matrix. The number actually needed, referred to as the rank of the matrix, depends on (or expresses) the intrinsic dimensionality of the data contained in the cells of the original matrix. Of critical importance for latent semantic analysis (LSA), if one or more factor is omitted (that is, if one or more singular values in the diagonal matrix along with the corresponding singular vectors of the other two matrices are deleted), the reconstruction is a least-squares best approximation to the original given the remaining dimensions. Thus, for example, after constructing an SVD, one can reduce the number of dimensions systematically by, for example, remov- ing those with the smallest effect on the sum-squared error of the approx- imation simply by deleting those with the smallest singular values. The actual algorithms used to compute SVDs for large sparse matrices of the sort involved in LSA are rather sophisticated and are not described here. Suffice it to say that cookbook versions of SVD adequate for small (e.g., 100 × 100) matrices are available in several places (e.g., Mathematica, 1991 ), and a free software version (Berry, 1992) suitable Contexts 3= mxm mxc wxc w xm Figure A1. Schematic diagram of the singular value decomposition (SVD) of a rectangular word (w) by context (c) matrix (X). The original matrix is decomposed into three matrices: W and C, which are orthonormal, and S, a diagonal matrix. The m columns of W and the m rows of C ' are linearly independent. for very large matrices such as the one used here to analyze an encyclope- dia can currently be obtained from the WorldWideWeb (http://www.net- lib.org/svdpack/index.html). University-affiliated researchers may be able to obtain a research-only license and complete software package for doing LSA by contacting Susan Dumais. A~ With Berry's software and a high-end Unix work-station with approximately 100 megabytes of RAM, matrices on the order of 50,000 × 50,000 (e.g., 50,000 words and 50,000 contexts) can currently be decomposed into representations in 300 dimensions with about 2-4 hr of computation. The computational complexity is O(3Dz), where z is the number of nonzero elements in the Word (w) × Context (c) matrix and D is the number of dimensions returned. The maximum matrix size one can compute is usually limited by the memory (RAM) requirement, which for the fastest of the methods in the Berry package is (10 + D + q)N + (4 + q)q, where N = w + c and q = min (N, 600), plus space for the W × C matrix. Thus, whereas the computational difficulty of methods such as this once made modeling and simulation of data equivalent in quantity to human experi- ence unthinkable, it is now quite feasible in many cases. Note, however, that the simulations of adult psycholinguistic data reported here were still limited to corpora much smaller than the total text to which an educated adult has been exposed. An LSA Example Here is a small example that gives the flavor of the analysis and demonstrates what the technique can accomplish. A2 This example uses as text passages the titles of nine technical memoranda, five about human computer interaction (HCI), and four about mathematical graph theory, topics that are conceptually rather disjoint. The titles are shown below. cl: Human machine interface for ABC computer applications c2: A survey of user opinion of computer system response time c3: The EPS user interface management system c4: System and human system engineering testing of EPS c5: Relation of user perceived response time to error measurement ml: The generation of random, binary, ordered trees m2: The intersection graph of paths in trees m3: Graph minors IV: Widths of trees and well-quasi-ordering m4: Graph minors: A survey The matrix formed to represent this text is shown in Figure A2. (We discuss the highlighted parts of the tables in due course.) The initial matrix has nine columns, one for each title, and we have given it 12 rows, each corresponding to a content word that occurs in at least two contexts. These are the words in italics. In LSA analyses of text, includ- ing some of those reported above, words that appear in only one context are often omitted in doing the SVD. These contribute little to derivation of the space, their vectors can be constructed after the SVD with little loss as a weighted average of words in the sample in which they oc- curred, and their omission sometimes greatly reduces the computation. See Deerwester, Dumais, Furnas, Landauer, and Harshman (1990) and Dumais (1994) for more on such details. For simplicity of presentation, A~ Inquiries about LSA computer programs should be addressed to Susan T. Dumais, Bellcore, 600 South Street, Morristown, New Jersey 07960. Electronic mail may be sent via Intemet to std@bellcore.com. A2 This example has been used in several previous publications (e.g., Deerwester et al., 1990; Landauer & Dumais, 1996). 
k//k/k/kDeerwesteret%al%(1988)
Dan%JurafskyLSA'more'details•300%dimensions%are%commonly%used•The%cells%are%commonly%weighted%by%a%product%of%two%weights•Local%weight:%%Log%term%frequency•Global%weight:%either%idfor%an%entropy%measure
11
Dan%JurafskyLet’s'return'to'PPMI'word<word'matrices•Can%we%apply%to%SVD%to%them?
12
Dan%JurafskySVD'applied'to'term<term'matrix19.3•DENSEVECTORS ANDSVD13Singular Value Decomposition (SVD) is a method for ﬁnding the most impor-tant dimensions of a data set, those dimensions along which the data varies the most.It can be applied to any rectangular matrix and in language processing it was ﬁrstapplied to the task of generating embeddings from term-document matrices byDeer-wester et al. (1988)in a model calledLatent Semantic Indexing. In this sectionlet’s look just at its application to a square term-context matrixMwith|V|rows (onefor each word) and columns (one for each context word)SVD factorizesMinto the product of three square|V|⇥|V|matricesW,S, andCT. InWeach row still represents a word, but the columns do not; each columnnow represents a dimension in a latent space, such that the|V|column vectors areorthogonal to each other and the columns are ordered by the amount of variancein the original dataset each accounts for. S is a diagonal|V|⇥|V|matrix, withsingular valuesalong the diagonal, expressing the importance of each dimension.The|V|⇥|V|matrixCTstill represents contexts, but the rows now represent the newlatent dimensions and the|V|row vectors are orthogonal to each other.By using only the ﬁrstkdimensions, of W, S, and C instead of all|V|dimen-sions, the product of these 3 matrices becomes a least-squares approximation to theoriginalM. Since the ﬁrst dimensions encode the most variance, one way to viewthe reconstruction is thus as modeling the most important information in the originaldataset.SVD applied to co-occurrence matrix X:2666664X3777775|V|⇥|V|=2666664W3777775|V|⇥|V|2666664s100...00s20...000s3...0...............000...sV3777775|V|⇥|V|2666664C3777775|V|⇥|V|Taking only the top k dimensions after SVD applied to co-occurrence matrix X:2666664X3777775|V|⇥|V|=2666664W3777775|V|⇥k2666664s100...00s20...000s3...0...............000...sk3777775k⇥khCik⇥|V|Figure 19.11SVD factors a matrix X into a product of three matrices, W,S, and C. Takingthe ﬁrstkdimensions gives a|V|⇥kmatrixWkthat has onek-dimensioned row per word thatcan be used as an embedding.Using only the topkdimensions (corresponding to thekmost important singularvalues), leads to a reduced|V|⇥kmatrixWk, with onek-dimensioned row per word.This row now acts as a densek-dimensional vector (embedding) representing thatword, substituting for the very high-dimensional rows of the originalM.33Note that early systems often instead weightedWkby the singular values, using the productWk·Skasan embedding instead of just the matrixWk, but this weighting leads to signiﬁcantly worse embeddings(Levy et al., 2015).13(I’m%simplifying%here%by%assuming%the%matrix%has%rank%|V|)
Dan%JurafskyTruncated'SVD'on'term<term'matrix19.3•DENSEVECTORS ANDSVD13Singular Value Decomposition (SVD) is a method for ﬁnding the most impor-tant dimensions of a data set, those dimensions along which the data varies the most.It can be applied to any rectangular matrix and in language processing it was ﬁrstapplied to the task of generating embeddings from term-document matrices byDeer-wester et al. (1988)in a model calledLatent Semantic Indexing. In this sectionlet’s look just at its application to a square term-context matrixMwith|V|rows (onefor each word) and columns (one for each context word)SVD factorizesMinto the product of three square|V|⇥|V|matricesW,S, andCT. InWeach row still represents a word, but the columns do not; each columnnow represents a dimension in a latent space, such that the|V|column vectors areorthogonal to each other and the columns are ordered by the amount of variancein the original dataset each accounts for. S is a diagonal|V|⇥|V|matrix, withsingular valuesalong the diagonal, expressing the importance of each dimension.The|V|⇥|V|matrixCTstill represents contexts, but the rows now represent the newlatent dimensions and the|V|row vectors are orthogonal to each other.By using only the ﬁrstkdimensions, of W, S, and C instead of all|V|dimen-sions, the product of these 3 matrices becomes a least-squares approximation to theoriginalM. Since the ﬁrst dimensions encode the most variance, one way to viewthe reconstruction is thus as modeling the most important information in the originaldataset.SVD applied to co-occurrence matrix X:2666664X3777775|V|⇥|V|=2666664W3777775|V|⇥|V|2666664s100...00s20...000s3...0...............000...sV3777775|V|⇥|V|2666664C3777775|V|⇥|V|Taking only the top k dimensions after SVD applied to co-occurrence matrix X:2666664X3777775|V|⇥|V|=2666664W3777775|V|⇥k2666664s100...00s20...000s3...0...............000...sk3777775k⇥khCik⇥|V|Figure 19.11SVD factors a matrix X into a product of three matrices, W,S, and C. Takingthe ﬁrstkdimensions gives a|V|⇥kmatrixWkthat has onek-dimensioned row per word thatcan be used as an embedding.Using only the topkdimensions (corresponding to thekmost important singularvalues), leads to a reduced|V|⇥kmatrixWk, with onek-dimensioned row per word.This row now acts as a densek-dimensional vector (embedding) representing thatword, substituting for the very high-dimensional rows of the originalM.33Note that early systems often instead weightedWkby the singular values, using the productWk·Skasan embedding instead of just the matrixWk, but this weighting leads to signiﬁcantly worse embeddings(Levy et al., 2015).14
Dan%JurafskyTruncated'SVD'produces'embeddings
15•Each%row%of%W%matrix%is%a%kFdimensional%representation%of%each%word%w•K%might%range%from%50%to%1000•Generally%we%keep%the%top%k%dimensions,%but%some%experiments%suggest%that%getting%rid%of%the%top%1%dimension%or%%even%the%top%50%dimensions%is%helpful%(Lapesaand%Evert%2014).19.3•DENSEVECTORS ANDSVD13Singular Value Decomposition (SVD) is a method for ﬁnding the most impor-tant dimensions of a data set, those dimensions along which the data varies the most.It can be applied to any rectangular matrix and in language processing it was ﬁrstapplied to the task of generating embeddings from term-document matrices byDeer-wester et al. (1988)in a model calledLatent Semantic Indexing. In this sectionlet’s look just at its application to a square term-context matrixMwith|V|rows (onefor each word) and columns (one for each context word)SVD factorizesMinto the product of three square|V|⇥|V|matricesW,S, andCT. InWeach row still represents a word, but the columns do not; each columnnow represents a dimension in a latent space, such that the|V|column vectors areorthogonal to each other and the columns are ordered by the amount of variancein the original dataset each accounts for. S is a diagonal|V|⇥|V|matrix, withsingular valuesalong the diagonal, expressing the importance of each dimension.The|V|⇥|V|matrixCTstill represents contexts, but the rows now represent the newlatent dimensions and the|V|row vectors are orthogonal to each other.By using only the ﬁrstkdimensions, of W, S, and C instead of all|V|dimen-sions, the product of these 3 matrices becomes a least-squares approximation to theoriginalM. Since the ﬁrst dimensions encode the most variance, one way to viewthe reconstruction is thus as modeling the most important information in the originaldataset.SVD applied to co-occurrence matrix X:2666664X3777775|V|⇥|V|=2666664W3777775|V|⇥|V|2666664s100...00s20...000s3...0...............000...sV3777775|V|⇥|V|2666664C3777775|V|⇥|V|Taking only the top k dimensions after SVD applied to co-occurrence matrix X:2666664X3777775|V|⇥|V|=2666664W3777775|V|⇥k2666664s100...00s20...000s3...0...............000...sk3777775k⇥khCik⇥|V|Figure 19.11SVD factors a matrix X into a product of three matrices, W,S, and C. Takingthe ﬁrstkdimensions gives a|V|⇥kmatrixWkthat has onek-dimensioned row per word thatcan be used as an embedding.Using only the topkdimensions (corresponding to thekmost important singularvalues), leads to a reduced|V|⇥kmatrixWk, with onek-dimensioned row per word.This row now acts as a densek-dimensional vector (embedding) representing thatword, substituting for the very high-dimensional rows of the originalM.33Note that early systems often instead weightedWkby the singular values, using the productWk·Skasan embedding instead of just the matrixWk, but this weighting leads to signiﬁcantly worse embeddings(Levy et al., 2015).embeddingfor word i
Dan%JurafskyEmbeddingsversus'sparse'vectors•Dense%SVD%embeddingssometimes%work%better%than%sparse%PPMI%matrices%at%tasks%like%word%similarity•Denoising:%lowForder%dimensions%may%represent%unimportant%information•Truncation%may%help%the%models%generalize%better%to%unseen%data.•Having%a%smaller%number%of%dimensions%may%make%it%easier%for%classifiers%to%properly%weight%the%dimensions%for%the%task.•Dense%models%may%do%better%at%capturing%higher%order%coFoccurrence.%16
Vector'SemanticsEmbeddingsinspired%by%neural%language%models:%skipFgrams%and%CBOW
Dan%JurafskyPrediction<based'models:An'alternative'way'to'get'dense'vectors•Skip<gram(Mikolovet%al.%2013a)%%CBOW(Mikolovet%al.%2013b)•Learn%embeddingsas%part%of%the%process%of%word%prediction.•Train%a%neural%network%to%predict%neighboring%words•Inspired%by%neural'net'language'models.•In%so%doing,%learn%dense%embeddingsfor%the%words%in%the%training%corpus.•Advantages:•Fast,%easy%to%train%(much%faster%than%SVD)•Available%online%in%the%word2vecpackage•Including%sets%of%pretrainedembeddings!18
Dan%JurafskySkip<grams•Predict%each%neighboring%word%•in%a%context%window%of%2C/words%•from%the%current%word.%•So%for%C=2,%we%are%given%word%wtand%predicting%these%4%words:1914CHAPTER19•VECTORSEMANTICSThis method is sometimes calledtruncated SVD. SVD is parameterized byk,truncated SVDthe number of dimensions in the representation for each word, typically rangingfrom 500 to 1000. Usually, these are the highest-order dimensions, although forsome tasks, it seems to help to actually throw out a small number of the most high-order dimensions, such as the ﬁrst 50(Lapesa and Evert, 2014).The dense embeddings produced by SVD sometimes perform better than theraw PPMI matrices on semantic tasks like word similarity. Various aspects of thedimensionality reduction seem to be contributing to the increased performance. Iflow-order dimensions represent unimportant information, the truncated SVD may beacting to removing noise. By removing parameters, the truncation may also help themodels generalize better to unseen data. When using vectors in NLP tasks, havinga smaller number of dimensions may make it easier for machine learning classiﬁersto properly weight the dimensions for the task. And the models may do better atcapturing higher order co-occurrence.Nonetheless, there is a signiﬁcant computational cost for the SVD for a large co-occurrence matrix, and performance is not always better than using the full sparsePPMI vectors, so for some applications the sparse vectors are the right approach.Alternatively, the neural embeddings we discuss in the next section provide a popularefﬁcient solution to generating dense embeddings.19.4 Embeddings from prediction: Skip-gram and CBOWAn alternative to applying dimensionality reduction techniques like SVD to co-occurrence matrices is to apply methods that learn embeddings for words as partof the process of word prediction. Two methods for generating dense embeddings,skip-gramandCBOW(continuous bag of words)(Mikolov et al. 2013,Mikolovskip-gramCBOWet al. 2013a), draw inspiration from the neural methods for language modeling intro-duced in Chapter 5. Like the neural language models, these models train a networkto predict neighboring words, and while doing so learn dense embeddings for thewords in the training corpus. The advantage of these methods is that they are fast,efﬁcient to train, and easily available online in theword2vecpackage; code andpretrained embeddings are both available.We’ll begin with the skip-gram model. The skip-gram model predicts eachneighboring word in a context window of 2Cwords from the current word. Sofor a context windowC=2 the context is[wt 2,wt 1,wt+1,wt+2]and we are pre-dicting each of these from wordwt. Fig.17.12sketches the architecture for a samplecontextC=1.The skip-gram model actually learns twod-dimensional embeddings for eachwordw: theinput embeddingvand theoutput embeddingv0. These embeddingsinputembeddingoutputembeddingare encoded in two matrices, theinput matrixWand theoutput matrixW0. Eachcolumniof the input matrixWis the 1⇥dvector embeddingvifor wordiin thevocabulary. Each rowiof the output matrixW0is ad⇥1 vector embeddingv0iforwordiin the vocabularyLet’s consider the prediction task. We are walking through a corpus of lengthTand currently pointing at thetth wordw(t), whose index in the vocabulary isj, sowe’ll call itwj(1<j<|V|). Let’s consider predicting one of the 2Ccontext words,for examplew(t+1), whose index in the vocabulary isk(1<k<|V|). Hence our taskis to computeP(wk|wj).
Dan%JurafskySkip<grams'learn'2'embeddingsfor'each'winput'embedding'v,/in%the%input%matrix%W•Column%iof%the%input%matrix%W/is%the%1d/embedding%vifor%word%iin%the%vocabulary.%output'embedding'vl,%in%output%matrix%W’•Row%iof%the%output%matrix%Wl%is%a%d/1%vector%embedding%vlifor%word%iin%the%vocabulary.20 |V| x dW’12
|V|i12d…........d x  |V|W12|V|i12d....…
Dan%JurafskySetup•Walking%through%corpus%pointing%at%word%w(t),%whose%index%in%the%vocabulary%is%j,%so%we’ll%call%it%wj(1%<%j/<%|V/|).%•Let’s%predict%w(t+1)%,%whose%index%in%the%vocabulary%is%k/(1%<%k/<%|V/|).%Hence%our%task%is%to%compute%P(wk|wj).%
21
Dan%JurafskyIntuition:'similarity'as'dot<productbetween'a'target'vector'and'context'vector
1..k..|Vw|1.2…….j………|Vw|1...dW
context embeddingfor word kC1. ..    …   dtarget embeddingscontext embeddingsSimilarity( j , k)target embeddingfor word j
22
Dan%JurafskySimilarity'is'computed'from'dot'product•Remember:%two%vectors%are%similar%if%they%have%a%high%dot%product•Cosine%is%just%a%normalized%dot%product•So:•Similarity(j,k)cko%vj•We’ll%need%to%normalize%to%get%a%probability23
Dan%JurafskyTurning'dot'products'into'probabilities•Similarity(j,k) = ck· vj•We%use%softmaxto%turn%into%probabilities
246CHAPTER16•SEMANTICS WITHDENSEVECTORScontext words, for examplew(t+1), whose index in the vocabulary isk(1<k<|V|).Hence our task is to computeP(wk|wj).The heart of the skip-gram computation of the probabilityp(wk|wj)is computingthe dot product between the vectors forwkandwj, thecontext vectorforwkand thetarget vectorforwj. We’ll represent this dot product asck·vj, whereckis the contextvector of wordkandvjis the target vector for wordj. As we saw in the previouschapter, the higher the dot product between two vectors, the more similar they are.(That was the intuition of using the cosine as a similarity metric, since cosine is justa normalized dot product). Fig.16.4shows the intuition that the similarity functionrequires selecting out a target vectorvjfromW, and a context vectorckfromC.
1..k..|Vw|1.2…….j………|Vw|1...dW
context embeddingfor word kC1. ..    …   dtarget embeddingscontext embeddingsSimilarity( j , k)target embeddingfor word j
Figure 16.4Of course, the dot productck·vjis not a probability, it’s just a number rangingfrom •to•. We can use thesoftmaxfunction from Chapter 7 to normalize the dotproduct into probabilities. Computing this denominator requires computing the dotproduct between each other wordwin the vocabulary with the target wordwi:p(wk|wj)=exp(ck·vj)Pi2|V|exp(ci·vj)(16.1)In summary, the skip-gram computes the probabilityp(wk|wj)by taking the dotproduct between the word vector forj(vj) and the context vector fork(ck), andturning this dot productvj·ckinto a probability by passing it through a softmaxfunction.This version of the algorithm, however, has a problem: the time it takes to com-pute the denominator. For each wordwt, the denominator requires computing thedot product with all other words. As we’ll see in the next section, we generally solvethis by using an approximation of the denominator.CBOWThe CBOW (continuous bag of words) model is roughly the mirror im-age of the skip-gram model. Like skip-grams, it is based on a predictive model,but this time predicting the current wordwtfrom the context window of 2Lwordsaround it, e.g. forL=2 the context is[wt 2,wt 1,wt+1,wt+2]While CBOW and skip-gram are similar algorithms and produce similar embed-dings, they do have slightly different behavior, and often one of them will turn outto be the better choice for any particular task.16.2.1 Learning the word and context embeddingsWe already mentioned the intuition for learning the word embedding matrixWandthe context embedding matrixC: iteratively make the embeddings for a word more
Dan%JurafskyEmbeddingsfrom'W'and'W’•Since%we%have%two%embeddings,%vjand%cjfor%each%word%wj•We%can%either:•Just%use%vj•Sum%them•Concatenate%them%to%make%a%doubleFlength%embedding
25
Dan%JurafskyLearning•Start%with%some%initial%embeddings(e.g.,%random)•iteratively%make%the%embeddingsfor%a%word%•more%like%the%embeddingsof%its%neighbors%•less%like%the%embeddingsof%other%words.%
26
Dan%JurafskyVisualizing'W'and'C'as'a'network'for'doing'error'backpropInput layerProjection layerOutput layerwtwt+11-hot input vector1⨉d1⨉|V|embedding for wtprobabilities ofcontext wordsC d ⨉ |V|x1x2xjx|V|y1y2yky|V|W|V|⨉d1⨉|V|27
Dan%JurafskyOne<hot'vectors•A%vector%of%length%|V|%•1%for%the%target%word%and%0%for%other%words•So%if%“popsicle”%is%vocabulary%word%5•The%one<hot'vector'is•[0,0,0,0,1,0,0,0,0…….0]2800000…0000100000…0000w0wjw|V|w1
Dan%Jurafsky
29Skip<gramh%=%vjo%=%ChInput layerProjection layerOutput layerwtwt+11-hot input vector1⨉d1⨉|V|embedding for wtprobabilities ofcontext wordsC d ⨉ |V|x1x2xjx|V|y1y2yky|V|W|V|⨉d1⨉|V|ok=%ckhok=%ckovj
Dan%JurafskyProblem'with'the'softamx•The%denominator:%have%to%compute%over%every%word%in%vocab•Instead:%just%sample%a%few%of%those%negative%words306CHAPTER16•SEMANTICS WITHDENSEVECTORScontext words, for examplew(t+1), whose index in the vocabulary isk(1<k<|V|).Hence our task is to computeP(wk|wj).The heart of the skip-gram computation of the probabilityp(wk|wj)is computingthe dot product between the vectors forwkandwj, thecontext vectorforwkand thetarget vectorforwj. We’ll represent this dot product asck·vj, whereckis the contextvector of wordkandvjis the target vector for wordj. As we saw in the previouschapter, the higher the dot product between two vectors, the more similar they are.(That was the intuition of using the cosine as a similarity metric, since cosine is justa normalized dot product). Fig.16.4shows the intuition that the similarity functionrequires selecting out a target vectorvjfromW, and a context vectorckfromC.
1..k..|Vw|1.2…….j………|Vw|1...dW
context embeddingfor word kC1. ..    …   dtarget embeddingscontext embeddingsSimilarity( j , k)target embeddingfor word j
Figure 16.4Of course, the dot productck·vjis not a probability, it’s just a number rangingfrom •to•. We can use thesoftmaxfunction from Chapter 7 to normalize the dotproduct into probabilities. Computing this denominator requires computing the dotproduct between each other wordwin the vocabulary with the target wordwi:p(wk|wj)=exp(ck·vj)Pi2|V|exp(ci·vj)(16.1)In summary, the skip-gram computes the probabilityp(wk|wj)by taking the dotproduct between the word vector forj(vj) and the context vector fork(ck), andturning this dot productvj·ckinto a probability by passing it through a softmaxfunction.This version of the algorithm, however, has a problem: the time it takes to com-pute the denominator. For each wordwt, the denominator requires computing thedot product with all other words. As we’ll see in the next section, we generally solvethis by using an approximation of the denominator.CBOWThe CBOW (continuous bag of words) model is roughly the mirror im-age of the skip-gram model. Like skip-grams, it is based on a predictive model,but this time predicting the current wordwtfrom the context window of 2Lwordsaround it, e.g. forL=2 the context is[wt 2,wt 1,wt+1,wt+2]While CBOW and skip-gram are similar algorithms and produce similar embed-dings, they do have slightly different behavior, and often one of them will turn outto be the better choice for any particular task.16.2.1 Learning the word and context embeddingsWe already mentioned the intuition for learning the word embedding matrixWandthe context embedding matrixC: iteratively make the embeddings for a word more
Dan%JurafskyGoal'in'learning•Make%the%word%like%the%context%words•We%want%this%to%be%high:•And%not%like%krandomly%selected%“noise%words”•We%want%this%to%be%low:3116.2•EMBEDDINGS FROM PREDICTION:SKIP-GRAM ANDCBOW7like the embeddings of its neighbors and less like the embeddings of other words.In the version of the prediction algorithm suggested in the previous section, theprobability of a word is computed by normalizing the dot-product between a wordand each context word by the dot products for all words. This probability is opti-mized when a word’s vector is closest to the words that occur near it (the numerator),and further from every other word (the denominator). Such a version of the algo-rithm is very expensive; we need to compute a whole lot of dot products to make thedenominator.Instead, the most commonly used version of skip-gram,skip-gram with negativesampling, approximates this full denominator.This section offers a brief sketch of how this works. In the training phase, thealgorithm walks through the corpus, at each target word choosing the surroundingcontext words as positive examples, and for each positive example also choosingknoisesamples ornegative samples: non-neighbor words. The goal will be to movenegativesamplesthe embeddings toward the neighbor words and away from the noise words.For example, in walking through the example text below we come to the wordapricot, and letL=2 so we have 4 context words c1 through c4:lemon, a [tablespoon of apricot preserves or] jamc1 c2 w c3 c4The goal is to learn an embedding whose dot product with each context wordis high. In practice skip-gram uses a sigmoid functionsof the dot product, wheres(x)=11+ex. So for the above example we wants(c1·w)+s(c2·w)+s(c3·w)+s(c4·w)to be high.In addition, for each context word the algorithm choosesknoise words accordingto their unigram frequency. If we letk=2, for each target/context pair, we’ll have 2noise words for each of the 4 context words:[cement metaphysical dear coaxial apricot attendant whence forever puddle]n1 n2 n3 n4 n5 n6 n7 n8We’d like these noise wordsnto have a low dot-product with our target embed-dingw; in other words we wants(n1·w)+s(n2·w)+...+s(n8·w)to be low.More formally, the learning objective for one word/context pair(w,c)islogs(c·w)+kXi=1Ewi⇠p(w)[logs( wi·w)](16.2)That is, we want to maximize the dot product of the word with the actual contextword, and minimize the dot products of the word with theknegative sampled non-neighbor words. The noise wordswiare sampled from the vocabularyVaccordingto their weighted unigram probability; in practice rather thanp(w)it is common touse the weightingp34(w).The learning algorithm starts with randomly initializedWandCmatrices, andthen walks through the training corpus movingWandCso as to maximize the objec-tive in Eq.16.2. An algorithm like stochastic gradient descent is used to iterativelyshift each value so as to maximize the objective, using error backpropagation topropagate the gradient back through the network as described in Chapter 5(Mikolovet al., 2013a).In summary, the learning objective in Eq.16.2is not the same as thep(wk|wj)deﬁned in Eq.16.3. Nonetheless, although negative sampling is a different objectivethan the probability objective, and so the resulting dot products will not produce16.2•EMBEDDINGS FROM PREDICTION:SKIP-GRAM ANDCBOW7like the embeddings of its neighbors and less like the embeddings of other words.In the version of the prediction algorithm suggested in the previous section, theprobability of a word is computed by normalizing the dot-product between a wordand each context word by the dot products for all words. This probability is opti-mized when a word’s vector is closest to the words that occur near it (the numerator),and further from every other word (the denominator). Such a version of the algo-rithm is very expensive; we need to compute a whole lot of dot products to make thedenominator.Instead, the most commonly used version of skip-gram,skip-gram with negativesampling, approximates this full denominator.This section offers a brief sketch of how this works. In the training phase, thealgorithm walks through the corpus, at each target word choosing the surroundingcontext words as positive examples, and for each positive example also choosingknoisesamples ornegative samples: non-neighbor words. The goal will be to movenegativesamplesthe embeddings toward the neighbor words and away from the noise words.For example, in walking through the example text below we come to the wordapricot, and letL=2 so we have 4 context words c1 through c4:lemon, a [tablespoon of apricot preserves or] jamc1 c2 w c3 c4The goal is to learn an embedding whose dot product with each context wordis high. In practice skip-gram uses a sigmoid functionsof the dot product, wheres(x)=11+ex. So for the above example we wants(c1·w)+s(c2·w)+s(c3·w)+s(c4·w)to be high.In addition, for each context word the algorithm choosesknoise words accordingto their unigram frequency. If we letk=2, for each target/context pair, we’ll have 2noise words for each of the 4 context words:[cement metaphysical dear coaxial apricot attendant whence forever puddle]n1 n2 n3 n4 n5 n6 n7 n8We’d like these noise wordsnto have a low dot-product with our target embed-dingw; in other words we wants(n1·w)+s(n2·w)+...+s(n8·w)to be low.More formally, the learning objective for one word/context pair(w,c)islogs(c·w)+kXi=1Ewi⇠p(w)[logs( wi·w)](16.2)That is, we want to maximize the dot product of the word with the actual contextword, and minimize the dot products of the word with theknegative sampled non-neighbor words. The noise wordswiare sampled from the vocabularyVaccordingto their weighted unigram probability; in practice rather thanp(w)it is common touse the weightingp34(w).The learning algorithm starts with randomly initializedWandCmatrices, andthen walks through the training corpus movingWandCso as to maximize the objec-tive in Eq.16.2. An algorithm like stochastic gradient descent is used to iterativelyshift each value so as to maximize the objective, using error backpropagation topropagate the gradient back through the network as described in Chapter 5(Mikolovet al., 2013a).In summary, the learning objective in Eq.16.2is not the same as thep(wk|wj)deﬁned in Eq.16.3. Nonetheless, although negative sampling is a different objectivethan the probability objective, and so the resulting dot products will not produce16.2•EMBEDDINGS FROM PREDICTION:SKIP-GRAM ANDCBOW7like the embeddings of its neighbors and less like the embeddings of other words.In the version of the prediction algorithm suggested in the previous section, theprobability of a word is computed by normalizing the dot-product between a wordand each context word by the dot products for all words. This probability is opti-mized when a word’s vector is closest to the words that occur near it (the numerator),and further from every other word (the denominator). Such a version of the algo-rithm is very expensive; we need to compute a whole lot of dot products to make thedenominator.Instead, the most commonly used version of skip-gram,skip-gram with negativesampling, approximates this full denominator.This section offers a brief sketch of how this works. In the training phase, thealgorithm walks through the corpus, at each target word choosing the surroundingcontext words as positive examples, and for each positive example also choosingknoisesamples ornegative samples: non-neighbor words. The goal will be to movenegativesamplesthe embeddings toward the neighbor words and away from the noise words.For example, in walking through the example text below we come to the wordapricot, and letL=2 so we have 4 context words c1 through c4:lemon, a [tablespoon of apricot preserves or] jamc1 c2 w c3 c4The goal is to learn an embedding whose dot product with each context wordis high. In practice skip-gram uses a sigmoid functionsof the dot product, wheres(x)=11+ex. So for the above example we wants(c1·w)+s(c2·w)+s(c3·w)+s(c4·w)to be high.In addition, for each context word the algorithm choosesknoise words accordingto their unigram frequency. If we letk=2, for each target/context pair, we’ll have 2noise words for each of the 4 context words:[cement metaphysical dear coaxial apricot attendant whence forever puddle]n1 n2 n3 n4 n5 n6 n7 n8We’d like these noise wordsnto have a low dot-product with our target embed-dingw; in other words we wants(n1·w)+s(n2·w)+...+s(n8·w)to be low.More formally, the learning objective for one word/context pair(w,c)islogs(c·w)+kXi=1Ewi⇠p(w)[logs( wi·w)](16.2)That is, we want to maximize the dot product of the word with the actual contextword, and minimize the dot products of the word with theknegative sampled non-neighbor words. The noise wordswiare sampled from the vocabularyVaccordingto their weighted unigram probability; in practice rather thanp(w)it is common touse the weightingp34(w).The learning algorithm starts with randomly initializedWandCmatrices, andthen walks through the training corpus movingWandCso as to maximize the objec-tive in Eq.16.2. An algorithm like stochastic gradient descent is used to iterativelyshift each value so as to maximize the objective, using error backpropagation topropagate the gradient back through the network as described in Chapter 5(Mikolovet al., 2013a).In summary, the learning objective in Eq.16.2is not the same as thep(wk|wj)deﬁned in Eq.16.3. Nonetheless, although negative sampling is a different objectivethan the probability objective, and so the resulting dot products will not produce16.2•EMBEDDINGS FROM PREDICTION:SKIP-GRAM ANDCBOW7like the embeddings of its neighbors and less like the embeddings of other words.In the version of the prediction algorithm suggested in the previous section, theprobability of a word is computed by normalizing the dot-product between a wordand each context word by the dot products for all words. This probability is opti-mized when a word’s vector is closest to the words that occur near it (the numerator),and further from every other word (the denominator). Such a version of the algo-rithm is very expensive; we need to compute a whole lot of dot products to make thedenominator.Instead, the most commonly used version of skip-gram,skip-gram with negativesampling, approximates this full denominator.This section offers a brief sketch of how this works. In the training phase, thealgorithm walks through the corpus, at each target word choosing the surroundingcontext words as positive examples, and for each positive example also choosingknoisesamples ornegative samples: non-neighbor words. The goal will be to movenegativesamplesthe embeddings toward the neighbor words and away from the noise words.For example, in walking through the example text below we come to the wordapricot, and letL=2 so we have 4 context words c1 through c4:lemon, a [tablespoon of apricot preserves or] jamc1 c2 w c3 c4The goal is to learn an embedding whose dot product with each context wordis high. In practice skip-gram uses a sigmoid functionsof the dot product, wheres(x)=11+ex. So for the above example we wants(c1·w)+s(c2·w)+s(c3·w)+s(c4·w)to be high.In addition, for each context word the algorithm choosesknoise words accordingto their unigram frequency. If we letk=2, for each target/context pair, we’ll have 2noise words for each of the 4 context words:[cement metaphysical dear coaxial apricot attendant whence forever puddle]n1 n2 n3 n4 n5 n6 n7 n8We’d like these noise wordsnto have a low dot-product with our target embed-dingw; in other words we wants(n1·w)+s(n2·w)+...+s(n8·w)to be low.More formally, the learning objective for one word/context pair(w,c)islogs(c·w)+kXi=1Ewi⇠p(w)[logs( wi·w)](16.2)That is, we want to maximize the dot product of the word with the actual contextword, and minimize the dot products of the word with theknegative sampled non-neighbor words. The noise wordswiare sampled from the vocabularyVaccordingto their weighted unigram probability; in practice rather thanp(w)it is common touse the weightingp34(w).The learning algorithm starts with randomly initializedWandCmatrices, andthen walks through the training corpus movingWandCso as to maximize the objec-tive in Eq.16.2. An algorithm like stochastic gradient descent is used to iterativelyshift each value so as to maximize the objective, using error backpropagation topropagate the gradient back through the network as described in Chapter 5(Mikolovet al., 2013a).In summary, the learning objective in Eq.16.2is not the same as thep(wk|wj)deﬁned in Eq.16.3. Nonetheless, although negative sampling is a different objectivethan the probability objective, and so the resulting dot products will not produce16.2•EMBEDDINGS FROM PREDICTION:SKIP-GRAM ANDCBOW7like the embeddings of its neighbors and less like the embeddings of other words.In the version of the prediction algorithm suggested in the previous section, theprobability of a word is computed by normalizing the dot-product between a wordand each context word by the dot products for all words. This probability is opti-mized when a word’s vector is closest to the words that occur near it (the numerator),and further from every other word (the denominator). Such a version of the algo-rithm is very expensive; we need to compute a whole lot of dot products to make thedenominator.Instead, the most commonly used version of skip-gram,skip-gram with negativesampling, approximates this full denominator.This section offers a brief sketch of how this works. In the training phase, thealgorithm walks through the corpus, at each target word choosing the surroundingcontext words as positive examples, and for each positive example also choosingknoisesamples ornegative samples: non-neighbor words. The goal will be to movenegativesamplesthe embeddings toward the neighbor words and away from the noise words.For example, in walking through the example text below we come to the wordapricot, and letL=2 so we have 4 context words c1 through c4:lemon, a [tablespoon of apricot preserves or] jamc1 c2 w c3 c4The goal is to learn an embedding whose dot product with each context wordis high. In practice skip-gram uses a sigmoid functionsof the dot product, wheres(x)=11+ex. So for the above example we wants(c1·w)+s(c2·w)+s(c3·w)+s(c4·w)to be high.In addition, for each context word the algorithm choosesknoise words accordingto their unigram frequency. If we letk=2, for each target/context pair, we’ll have 2noise words for each of the 4 context words:[cement metaphysical dear coaxial apricot attendant whence forever puddle]n1 n2 n3 n4 n5 n6 n7 n8We’d like these noise wordsnto have a low dot-product with our target embed-dingw; in other words we wants(n1·w)+s(n2·w)+...+s(n8·w)to be low.More formally, the learning objective for one word/context pair(w,c)islogs(c·w)+kXi=1Ewi⇠p(w)[logs( wi·w)](16.2)That is, we want to maximize the dot product of the word with the actual contextword, and minimize the dot products of the word with theknegative sampled non-neighbor words. The noise wordswiare sampled from the vocabularyVaccordingto their weighted unigram probability; in practice rather thanp(w)it is common touse the weightingp34(w).The learning algorithm starts with randomly initializedWandCmatrices, andthen walks through the training corpus movingWandCso as to maximize the objec-tive in Eq.16.2. An algorithm like stochastic gradient descent is used to iterativelyshift each value so as to maximize the objective, using error backpropagation topropagate the gradient back through the network as described in Chapter 5(Mikolovet al., 2013a).In summary, the learning objective in Eq.16.2is not the same as thep(wk|wj)deﬁned in Eq.16.3. Nonetheless, although negative sampling is a different objectivethan the probability objective, and so the resulting dot products will not produce16.2•EMBEDDINGS FROM PREDICTION:SKIP-GRAM ANDCBOW7like the embeddings of its neighbors and less like the embeddings of other words.In the version of the prediction algorithm suggested in the previous section, theprobability of a word is computed by normalizing the dot-product between a wordand each context word by the dot products for all words. This probability is opti-mized when a word’s vector is closest to the words that occur near it (the numerator),and further from every other word (the denominator). Such a version of the algo-rithm is very expensive; we need to compute a whole lot of dot products to make thedenominator.Instead, the most commonly used version of skip-gram,skip-gram with negativesampling, approximates this full denominator.This section offers a brief sketch of how this works. In the training phase, thealgorithm walks through the corpus, at each target word choosing the surroundingcontext words as positive examples, and for each positive example also choosingknoisesamples ornegative samples: non-neighbor words. The goal will be to movenegativesamplesthe embeddings toward the neighbor words and away from the noise words.For example, in walking through the example text below we come to the wordapricot, and letL=2 so we have 4 context words c1 through c4:lemon, a [tablespoon of apricot preserves or] jamc1 c2 w c3 c4The goal is to learn an embedding whose dot product with each context wordis high. In practice skip-gram uses a sigmoid functionsof the dot product, wheres(x)=11+ex. So for the above example we wants(c1·w)+s(c2·w)+s(c3·w)+s(c4·w)to be high.In addition, for each context word the algorithm choosesknoise words accordingto their unigram frequency. If we letk=2, for each target/context pair, we’ll have 2noise words for each of the 4 context words:[cement metaphysical dear coaxial apricot attendant whence forever puddle]n1 n2 n3 n4 n5 n6 n7 n8We’d like these noise wordsnto have a low dot-product with our target embed-dingw; in other words we wants(n1·w)+s(n2·w)+...+s(n8·w)to be low.More formally, the learning objective for one word/context pair(w,c)islogs(c·w)+kXi=1Ewi⇠p(w)[logs( wi·w)](16.2)That is, we want to maximize the dot product of the word with the actual contextword, and minimize the dot products of the word with theknegative sampled non-neighbor words. The noise wordswiare sampled from the vocabularyVaccordingto their weighted unigram probability; in practice rather thanp(w)it is common touse the weightingp34(w).The learning algorithm starts with randomly initializedWandCmatrices, andthen walks through the training corpus movingWandCso as to maximize the objec-tive in Eq.16.2. An algorithm like stochastic gradient descent is used to iterativelyshift each value so as to maximize the objective, using error backpropagation topropagate the gradient back through the network as described in Chapter 5(Mikolovet al., 2013a).In summary, the learning objective in Eq.16.2is not the same as thep(wk|wj)deﬁned in Eq.16.3. Nonetheless, although negative sampling is a different objectivethan the probability objective, and so the resulting dot products will not produce
Dan%JurafskySkipgramwith'negative'sampling:Loss'function16.2•EMBEDDINGS FROM PREDICTION:SKIP-GRAM ANDCBOW7like the embeddings of its neighbors and less like the embeddings of other words.In the version of the prediction algorithm suggested in the previous section, theprobability of a word is computed by normalizing the dot-product between a wordand each context word by the dot products for all words. This probability is opti-mized when a word’s vector is closest to the words that occur near it (the numerator),and further from every other word (the denominator). Such a version of the algo-rithm is very expensive; we need to compute a whole lot of dot products to make thedenominator.Instead, the most commonly used version of skip-gram,skip-gram with negativesampling, approximates this full denominator.This section offers a brief sketch of how this works. In the training phase, thealgorithm walks through the corpus, at each target word choosing the surroundingcontext words as positive examples, and for each positive example also choosingknoisesamples ornegative samples: non-neighbor words. The goal will be to movenegativesamplesthe embeddings toward the neighbor words and away from the noise words.For example, in walking through the example text below we come to the wordapricot, and letL=2 so we have 4 context words c1 through c4:lemon, a [tablespoon of apricot preserves or] jamc1 c2 w c3 c4The goal is to learn an embedding whose dot product with each context wordis high. In practice skip-gram uses a sigmoid functionsof the dot product, wheres(x)=11+ex. So for the above example we wants(c1·w)+s(c2·w)+s(c3·w)+s(c4·w)to be high.In addition, for each context word the algorithm choosesknoise words accordingto their unigram frequency. If we letk=2, for each target/context pair, we’ll have 2noise words for each of the 4 context words:[cement metaphysical dear coaxial apricot attendant whence forever puddle]n1 n2 n3 n4 n5 n6 n7 n8We’d like these noise wordsnto have a low dot-product with our target embed-dingw; in other words we wants(n1·w)+s(n2·w)+...+s(n8·w)to be low.More formally, the learning objective for one word/context pair(w,c)islogs(c·w)+kXi=1Ewi⇠p(w)[logs( wi·w)](16.2)That is, we want to maximize the dot product of the word with the actual contextword, and minimize the dot products of the word with theknegative sampled non-neighbor words. The noise wordswiare sampled from the vocabularyVaccordingto their weighted unigram probability; in practice rather thanp(w)it is common touse the weightingp34(w).The learning algorithm starts with randomly initializedWandCmatrices, andthen walks through the training corpus movingWandCso as to maximize the objec-tive in Eq.16.2. An algorithm like stochastic gradient descent is used to iterativelyshift each value so as to maximize the objective, using error backpropagation topropagate the gradient back through the network as described in Chapter 5(Mikolovet al., 2013a).In summary, the learning objective in Eq.16.2is not the same as thep(wk|wj)deﬁned in Eq.16.3. Nonetheless, although negative sampling is a different objectivethan the probability objective, and so the resulting dot products will not produce32
Dan%JurafskyRelation'between'skipgramsand'PMI!•If%we%multiply%WW’T•We%get%a%|V|x|V|%matrix%M,%each%entry%mijcorresponding%to%some%association%between%input%word%iand%output%word%j/•Levy%and%Goldberg%(2014b)%show%that%skipFgram%reaches%its%optimum%just%when%this%matrix%is%a%shifted%version%of%PMI:WWlT/=MPMI%−log%k/•So%skipFgram%is%implicitly%factoring%a%shifted%version%of%the%PMI%matrix%into%the%two%embedding%matrices.33
Dan%JurafskyProperties'of'embeddings
34•Nearest%words%to%some%embeddings(Mikolovet%al.%20131)18CHAPTER19•VECTORSEMANTICSmatrix is repeated between each one-hot input and the projection layerh. For thecase ofC=1, these two embeddings must be combined into the projection layer,which is done by multiplying each one-hot context vectorxbyWto give us twoinput vectors (let’s sayviandvj). We then average these vectorsh=W·12CX cjc,j6=0v(j)(19.31)As with skip-grams, the the projection vectorhis multiplied by the output matrixW0. The resulto=W0his a 1⇥|V|dimensional output vector giving a score foreach of the|V|words. In doing so, the elementokwas computed by multiplyingh by theoutput embeddingfor wordwk:ok=v0kh. Finally we normalize this scorevector, turning the score for each elementokinto a probability by using the soft-maxfunction.19.5 Properties of embeddingsWe’ll discuss in Section17.8how to evaluate the quality of different embeddings.But it is also sometimes helpful to visualize them. Fig.17.14shows the words/phrasesthat are most similar to some sample words using the phrase-based version of theskip-gram algorithm(Mikolov et al., 2013a).target:Redmond Havel ninjutsu grafﬁti capitulateRedmond Wash. Vaclav Havel ninja spray paint capitulationRedmond Washington president Vaclav Havel martial arts graﬁtti capitulatedMicrosoft Velvet Revolution swordsmanship taggers capitulatingFigure 19.14Examples of the closest tokens to some target words using a phrase-basedextension of the skip-gram algorithm(Mikolov et al., 2013a).One semantic property of various kinds of embeddings that may play in theirusefulness is their ability to capture relational meaningsMikolov et al. (2013b)demonstrates that theoffsetsbetween vector embeddingscan capture some relations between words, for example that the result of the ex-pression vector(‘king’) - vector(‘man’) + vector(‘woman’) is a vector close to vec-tor(‘queen’); the left panel in Fig.17.15visualizes this by projecting a representationdown into 2 dimensions. Similarly, they found that the expression vector(‘Paris’)- vector(‘France’) + vector(‘Italy’) results in a vector that is very close to vec-tor(‘Rome’).Levy and Goldberg (2014a)shows that various other kinds of em-beddings also seem to have this property. We return in the next section to theserelationalproperties of embeddings and how they relate tomeaning compositional-ity: the way the meaning of a phrase is built up out of the meaning of the individualvectors.19.6 Compositionality in Vector Models of MeaningTo be written.
Dan%JurafskyEmbeddingscapture'relational'meaning!vector(‘king’)%Fvector(‘man’)%+%vector(‘woman’)%≈"vector(‘queen’)vector(‘Paris’)%Fvector(‘France’)%+%vector(‘Italy’)%≈vector(‘Rome’)
35

Cross-lingual Embeddings•Skip-gram allows us learning embeddings for words in a single languageVectors in L1 
children money law life world country war peace energy market 
Slides courtesy Shyam Upadhyay
Cross-lingual Embeddings•Skip-gram allows us learning embeddings for words in a single language
•But what if we want to work with multiple languages?Vectors in L1 
Vectors in L2 children enfants money argent loi law life vie monde world 
pays country war guerre peace paix energy energie market marche 
Slides courtesy Shyam Upadhyay
General Schema for Cross-lingual Embeddings
Cross-lingual!Supervision!L1 and L2!Cross-lingual Word Vector Model!Initial embedding (Optional)!W!
Initial embedding (Optional)!V!Vectors in L1 
Vectors in L2 
Slides courtesy Shyam Upadhyay
General Schema for Cross-lingual Embeddings
Cross-lingual!Supervision!L1 and L2!Cross-lingual Word Vector Model!Initial embedding (Optional)!W!
Initial embedding (Optional)!V!Vectors in L1 
Vectors in L2 
Slides courtesy Shyam Upadhyay
Sources of Cross-Lingual Supervision
Decreasing	Cost		(You, t’)  (Love, aime)  (I, je)  word 
Je	
I	
t’	
aime	
love	
You	word + sentence BiSkip Luong et al. 15  BiCVM Hermann et al. 14 BiCCA Faruqui et al. 14 BiVCD Vulic et al. 15 
Je	t’	aime	
I	love	you	
Bonjour!	Je	t’	aime	
Hello!	How	are	you?	I	love	you	sentence document 
Slides courtesy Shyam Upadhyay
BiSparse - Sparse Bilingual Embeddings•A method to learn embeddings, that are
Bilingual
Sparse
Non-negative
•Starting from 
Monolingual embeddings in two languages
A “seed” dictionary
BiSparse•Method based on matrix factorization
DftAeXe
XfAfDeT≈
≈SMonolingual corpus statisticsCross-lingualknowledge
BiSparse•Method based on matrix factorization
DftAeXe
XfAfDeT≈
≈SMonolingual corpus statisticsCross-lingualknowledge

BiSparse•Method based on matrix factorization
DftAeXe
XfAfDeT≈
≈SMonolingual corpus statisticsCross-lingualknowledge

BiSparse•Method based on matrix factorization
DftAeXe
XfAfDeT≈
≈SMonolingual corpus statisticsCross-lingualknowledge

Building the S Matrix•…
•nuit —> night
•dog —> chien
•cake —> gateau
•…dog[                                       ]                                       chien0 .. 0 1 .. .. 0 0   0  ..  0   0  ..  
Interpreting Embeddings

Summary•Vector Semantics with Dense Vectors
•Singular Value Decomposition
•Skip-gram embeddings
•Cross-lingual embeddings
•BiSparse model
Lexicalized Probabilistic Context-Free
Grammars
Michael Collins, Columbia University
Overview
ILexicalization of a treebank
ILexicalized probabilistic context-free grammars
IParameter estimation in lexicalized probabilistic context-free
grammars
IAccuracy of lexicalized probabilistic context-free grammars
Heads in Context-Free Rules
Add annotations specifying the \head" of each rule:
S) NP VP
VP) Vi
VP) Vt NP
VP) VP PP
NP) DT NN
NP) NP PP
PP) IN NPVi) sleeps
Vt) saw
NN) man
NN) woman
NN) telescope
DT) the
IN) with
IN) in
More about Heads
IEach context-free rule has one \special" child that is the
head of the rule. e.g.,
S) NP VP (VP is the head)
VP) Vt NP (Vt is the head)
NP) DT NN NN (NN is the head)
IA core idea in syntax
(e.g., see X-bar Theory, Head-Driven Phrase Structure
Grammar)
ISome intuitions:
IThe central sub-constituent of each rule.
IThe semantic predicate in each rule.
Rules which Recover Heads: An Example for NPs
Ifthe rule contains NN, NNS, or NNP:
Choose the rightmost NN, NNS, or NNP
Else If the rule contains an NP: Choose the leftmost NP
Else If the rule contains a JJ: Choose the rightmost JJ
Else If the rule contains a CD: Choose the rightmost CD
Else Choose the rightmost child
e.g.,
NP) DT NNP NN
NP) DT NN NNP
NP) NP PP
NP) DT JJ
NP) DT
Rules which Recover Heads: An Example for VPs
Ifthe rule contains Vi or Vt: Choose the leftmost Vi or Vt
Else If the rule contains an VP: Choose the leftmost VP
Else Choose the leftmost child
e.g.,
VP) Vt NP
VP) VP PP
Adding Headwords to Trees
S
NP
DT
theNN
lawyerVP
Vt
questionedNP
DT
theNN
witness
+
S(questioned)
NP(lawyer)
DT(the)
theNN(lawyer)
lawyerVP(questioned)
Vt(questioned)
questionedNP(witness)
DT(the)
theNN(witness)
witness
Adding Headwords to Trees (Continued)
S(questioned)
NP(lawyer)
DT(the)
theNN(lawyer)
lawyerVP(questioned)
Vt(questioned)
questionedNP(witness)
DT(the)
theNN(witness)
witness
IA constituent receives its headword from its head child .
S) NP VP (S receives headword from VP)
VP) Vt NP (VP receives headword from Vt)
NP) DT NN (NP receives headword from NN)
Overview
ILexicalization of a treebank
ILexicalized probabilistic context-free grammars
IParameter estimation in lexicalized probabilistic context-free
grammars
IAccuracy of lexicalized probabilistic context-free grammars
Chomsky Normal Form
A context free grammar G= (N;;R;S )in Chomsky Normal
Form is as follows
INis a set of non-terminal symbols
Iis a set of terminal symbols
IRis a set of rules which take one of two forms:
IX!Y1Y2forX2N, andY1;Y22N
IX!YforX2N, andY2
IS2Nis a distinguished start symbol
We can nd the highest scoring parse under a PCFG in
this form, in O(n3jNj3)time where nis the length of the
string being parsed.
Lexicalized Context-Free Grammars in Chomsky
Normal Form
INis a set of non-terminal symbols
Iis a set of terminal symbols
IRis a set of rules which take one of three forms:
IX(h)!1Y1(h)Y2(w)forX2N, andY1;Y22N, and
h;w2
IX(h)!2Y1(w)Y2(h)forX2N, andY1;Y22N, and
h;w2
IX(h)!hforX2N, andh2
IS2Nis a distinguished start symbol
An Example
S(saw)!2NP(man) VP(saw)
VP(saw)!1Vt(saw) NP(dog)
NP(man)!2DT(the) NN(man)
NP(dog)!2DT(the) NN(dog)
Vt(saw)! saw
DT(the)! the
NN(man)! man
NN(dog)! dog
Parameters in a Lexicalized PCFG
IAn example parameter in a PCFG:
q(S!NP VP )
IAn example parameter in a Lexicalized PCFG:
q(S(saw)!2NP(man) VP(saw) )
Parsing with Lexicalized CFGs
IThe new form of grammar looks just like a Chomsky normal
form CFG, but with potentially O(jj2jNj3)possible rules.
INaively, parsing an nword sentence using the dynamic
programming algorithm will take O(n3jj2jNj3)time. But
jjcan be huge!!
ICrucial observation: at most O(n2jNj3)rules can be
applicable to a given sentence w1;w2;:::w nof lengthn.
This is because any rules which contain a lexical item that is
not one ofw1:::w n, can be safely discarded.
IThe result: we can parse in O(n5jNj3)time.
Overview
ILexicalization of a treebank
ILexicalized probabilistic context-free grammars
IParameter estimation in lexicalized probabilistic context-free
grammars
IAccuracy of lexicalized probabilistic context-free grammars
S(saw)
NP(man)
DT(the)
theNN(man)
manVP(saw)
VP(saw)
Vt(saw)
sawNP(dog)
DT(the)
theNN(dog)
dogPP(with)
IN(with)
withNP(telescope)
DT(the)
theNN(telescope)
telescope
p(t) =q(S(saw)!2NP(man) VP(saw) )
q(NP(man)!2DT(the) NN(man) )
q(VP(saw)!1VP(saw) PP(with) )
q(VP(saw)!1Vt(saw) NP(dog) )
q(PP(with)!1IN(with) NP(telescope) )
:::
A Model from Charniak (1997)
IAn example parameter in a Lexicalized PCFG:
q(S(saw)!2NP(man) VP(saw) )
IFirst step: decompose this parameter into a product of two
parameters
q(S(saw)!2NP(man) VP(saw) )
=q(S!2NP VPjS, saw )q(manjS!2NP VP, saw )
A Model from Charniak (1997) (Continued)
q(S(saw)!2NP(man) VP(saw) )
=q(S!2NP VPjS, saw )q(manjS!2NP VP, saw )
ISecond step: use smoothed estimation for the two parameter
estimates
q(S!2NP VPjS, saw )
=1qML(S!2NP VPjS, saw ) +2qML(S!2NP VPjS)q(manjS!2NP VP, saw )
=3qML(manjS!2NP VP, saw ) +4qML(manjS!2NP VP )
+5qML(manjNP)
A Model from Charniak (1997) (Continued)
q(S(saw)!2NP(man) VP(saw) )
=q(S!2NP VPjS, saw )q(manjS!2NP VP, saw )
ISecond step: use smoothed estimation for the two parameter
estimates
q(S!2NP VPjS, saw )
=1qML(S!2NP VPjS, saw ) +2qML(S!2NP VPjS)
q(manjS!2NP VP, saw )
=3qML(manjS!2NP VP, saw ) +4qML(manjS!2NP VP )
+5qML(manjNP)
Other Important Details
INeed to deal with rules with more than two children, e.g.,
VP(told)!V(told) NP(him) PP(on) SBAR(that)INeed to incorporate parts of speech (useful in smoothing)
VP-V(told)!V(told) NP-PRP(him) PP-IN(on) SBAR-COMP(that)
INeed to encode preferences for close attachment
John was believed to have been shot by Bill
IFurther reading:
Michael Collins. 2003. Head-Driven Statistical Models
for Natural Language Parsing. In Computational
Linguistics.
Other Important Details
INeed to deal with rules with more than two children, e.g.,
VP(told)!V(told) NP(him) PP(on) SBAR(that)
INeed to incorporate parts of speech (useful in smoothing)
VP-V(told)!V(told) NP-PRP(him) PP-IN(on) SBAR-COMP(that)INeed to encode preferences for close attachment
John was believed to have been shot by Bill
IFurther reading:
Michael Collins. 2003. Head-Driven Statistical Models
for Natural Language Parsing. In Computational
Linguistics.
Other Important Details
INeed to deal with rules with more than two children, e.g.,
VP(told)!V(told) NP(him) PP(on) SBAR(that)
INeed to incorporate parts of speech (useful in smoothing)
VP-V(told)!V(told) NP-PRP(him) PP-IN(on) SBAR-COMP(that)
INeed to encode preferences for close attachment
John was believed to have been shot by BillIFurther reading:
Michael Collins. 2003. Head-Driven Statistical Models
for Natural Language Parsing. In Computational
Linguistics.
Other Important Details
INeed to deal with rules with more than two children, e.g.,
VP(told)!V(told) NP(him) PP(on) SBAR(that)
INeed to incorporate parts of speech (useful in smoothing)
VP-V(told)!V(told) NP-PRP(him) PP-IN(on) SBAR-COMP(that)
INeed to encode preferences for close attachment
John was believed to have been shot by Bill
IFurther reading:
Michael Collins. 2003. Head-Driven Statistical Models
for Natural Language Parsing. In Computational
Linguistics.
Overview
ILexicalization of a treebank
ILexicalized probabilistic context-free grammars
IParameter estimation in lexicalized probabilistic context-free
grammars
IAccuracy of lexicalized probabilistic context-free grammars
Evaluation: Representing Trees as Constituents
S
NP
DT
theNN
lawyerVP
Vt
questionedNP
DT
theNN
witness
Label Start Point End Point
NP 1 2
NP 4 5
VP 3 5
S 1 5
Precision and Recall
Label Start Point End Point
NP 1 2
NP 4 5
NP 4 8
PP 6 8
NP 7 8
VP 3 8
S 1 8Label Start Point End Point
NP 1 2
NP 4 5
PP 6 8
NP 7 8
VP 3 8
S 1 8
IG= number of constituents in gold standard = 7
IP= number in parse output = 6
IC= number correct = 6
Recall = 100%C
G= 100%6
7Precision = 100%C
P= 100%6
6
Results
ITraining data: 40,000 sentences from the Penn Wall Street
Journal treebank. Testing: around 2,400 sentences from the
Penn Wall Street Journal treebank.
IResults for a PCFG: 70.6% Recall, 74.8% Precision
IMagerman (1994): 84.0% Recall, 84.3% Precision
IResults for a lexicalized PCFG: 88.1% recall, 88.3% precision
(from Collins (1997, 2003))
IMore recent results: 90.7% Recall/91.4% Precision (Carreras
et al., 2008); 91.7% Recall, 92.0% Precision (Petrov 2010);
91.2% Recall, 91.8% Precision (Charniak and Johnson, 2005)
S(saw)
NP(man)
DT(the)
theNN(man)
manVP(saw)
VP(saw)
Vt(saw)
sawNP(dog)
DT(the)
theNN(dog)
dogPP(with)
IN(with)
withNP(telescope)
DT(the)
theNN(telescope)
telescope
hROOT 0, saw 3, ROOTi
hsaw 3, man 2, S!2NP VPi
hman 2, the 1, NP!2DT NNi
hsaw 3, with 6, VP!1VP PPi
hsaw 3, dog 5, VP!1Vt NPi
hdog 5, the 4, NP!2DT NNi
hwith 6, telescope 8, PP!1IN NPi
htelescope 8, the 7, NP!2DT NNi
Dependency Accuracies
IAll parses for a sentence with nwords have ndependencies
Report a single gure, dependency accuracy
IResults from Collins, 2003: 88.3% dependency accuracy
ICan calculate precision/recall on particular dependency types
e.g., look at all subject/verb dependencies )
all dependencies with label S !2NP VP
Recall =
number of subject/verb dependencies correct
number of subject/verb dependencies in gold standard
Precision =
number of subject/verb dependencies correct
number of subject/verb dependencies in parser's output
Strengths and Weaknesses of Modern Parsers
(Numbers taken from Collins (2003))
ISubject-verb pairs: over 95% recall and precision
IObject-verb pairs: over 92% recall and precision
IOther arguments to verbs: 93% recall and precision
INon-recursive NP boundaries: 93% recall and precision
IPP attachments:82% recall and precision
ICoordination ambiguities: 61% recall and precision
Summary
IKey weakness of PCFGs: lack of sensitivity to lexical
information
ILexicalized PCFGs:
ILexicalize a treebank using head rules
IEstimate the parameters of a lexicalized PCFG using
smoothed estimation
IAccuracy of lexicalized PCFGs: around 88% in recovering
constituents or depenencies
Optimization  for  ML+Linear  Regression
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  7February  8,  2016Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Optimization  Readings:Lecture  notes  from  10-­‐600  (see  Piazza  note)“Convex  Optimization”  Boyd  and  Vandenberghe(2009)    [See  Chapter  9.  This  advanced  reading  is  entirely  optional.]Linear  Regression  Readings:Murphy  7.1  –7.3Bishop  3.1HTF  3.1  –3.4Mitchell  4.1-­‐4.3
Reminders•Homework2:  NaiveBayes–Release:  Wed,  Feb.  1–Due:  Mon,  Feb.  13  at  5:30pm•Homework3:  Linear  /  LogisticRegression–Release:  Mon,  Feb.  13–Due:  Wed,  Feb.  22  at  5:30pm
2
Optimization  Outline•Optimization  for  ML–Differences  –Types  of  optimization  problems–Unconstrained  optimization–Convex,  concave,  nonconvex•Optimization:  Closed  form  solutions–Example:  1-­‐D  function–Example:  higher  dimensions–Gradient  and  Hessian•Gradient  Descent–Example:  2D  gradients–Algorithm–Details:  starting  point,  stopping  criterion,  line  search•Stochastic  Gradient  Descent  (SGD)–Expectations  of  gradients–Algorithm–Mini-­‐batches–Details:  mini-­‐batches,  step  size,  stopping  criterion–Problematic  cases  for  SGD•Convergence–Comparison  of  Newton’s  method,  Gradient  Descent,  SGD–Asymptotic  convergence–Convergence  in  practice3
Optimization  for  MLNot  quite  the  same  setting  as  other  fields…–Function  we  are  optimizing  might  not  be  the  true  goal  (e.g.  likelihood  vs  generalization  error)–Precision  might  not  matter  (e.g.  data  is  noisy,  so  optimal  up  to  1e-­‐16  might  not  help)–Stopping  early  can  help  generalization  error(i.e.  “early  stopping”  is  a  technique  for  regularization  –discussed  more  next  time)4
Optimization  for  MLWhiteboard–Differences  –Types  of  optimization  problems–Unconstrained  optimization–Convex,  concave,  nonconvex
5
Convexity
6
There  is  only  one  local  optimum  if  the  function  is  convex
Slide  adapted  from  William  Cohen

Optimization:  Closed  form  solutionsWhiteboard–Example:  1-­‐D  function–Example:  higher  dimensions–Gradient  and  Hessian
7
Gradients
8
Gradients
9These  are  the  gradientsthat  Gradient  Ascent  would  follow.
Negative  Gradients
10These  are  the  negativegradients  that  Gradient  Descentwould  follow.
Negative  Gradient  Paths
11Shown  are  the  paths  that  Gradient  Descent  would  follow  if  it  were  making  infinitesimally  small  steps.
Gradient  DescentWhiteboard–Example:  2D  gradients–Algorithm–Details:  starting  point,  stopping  criterion,  line  search
12
Gradient  ascent
13
To	
  find	
  argminxf(x):•Start	
  with	
  x0•For	
  t=1….•xt+1	
  =	
  xt+	
  λ	
  f’(xt)where	
  λ	
  is	
  small
Slide  courtesy  of  William  Cohen
Gradient  descent
14
Likelihood:  ascentLoss:  descent
Slide  courtesy  of  William  Cohen
Pros  and  cons  of  gradient  descent•Simple  and  often  quite  effective  on  ML  tasks•Often  very  scalable  •Only  applies  to  smooth  functions  (differentiable)•Might  find  a  local  minimum,  rather  than  a  global  one
15Slide  courtesy  of  William  Cohen
Gradient  Descent
16Algorithm 1Gradient Descent1:procedureGD(D, (0))2:   (0)3:whilenot convergeddo4:   +   J( )5:return 
In  order  to  apply  GD  to  Linear  Regression  all  we  need  is  the  gradientof  the  objective  function  (i.e.  vector  of  partial  derivatives).    J( )=     dd 1J( )dd 2J( )...dd NJ( )     —
Gradient  Descent
17Algorithm 1Gradient Descent1:procedureGD(D, (0))2:   (0)3:whilenot convergeddo4:   +   J( )5:return 
There  are  many  possible  ways  to  detect  convergence.    For  example,  we  could  check  whether  the  L2  norm  of  the  gradient  is  below  some  small  tolerance.||  J( )||2  Alternatively  we  could  check  that  the  reduction  in  the  objective  function  from  one  iteration  to  the  next  is  small.—
Stochastic  Gradient  Descent  (SGD)Whiteboard–Expectations  of  gradients–Algorithm–Mini-­‐batches–Details:  mini-­‐batches,  step  size,  stopping  criterion–Problematic  cases  for  SGD
18
Stochastic  Gradient  Descent  (SGD)
19Algorithm 2Stochastic Gradient Descent (SGD)1:procedureSGD(D, (0))2:   (0)3:whilenot convergeddo4:fori shu e({1,2,...,N})do5:   +   J(i)( )6:return 
We  need  a  per-­‐example  objective:LetJ( )= Ni=1J(i)( )whereJ(i)( )=12( Tx(i) y(i))2.—
Stochastic  Gradient  Descent  (SGD)
We  need  a  per-­‐example  objective:20LetJ( )= Ni=1J(i)( )whereJ(i)( )=12( Tx(i) y(i))2.Algorithm 2Stochastic Gradient Descent (SGD)1:procedureSGD(D, (0))2:   (0)3:whilenot convergeddo4:fori shu e({1,2,...,N})do5:fork {1,2,...,K}do6: k  k+ dd kJ(i)( )7:return 
—
ConvergenceWhiteboard–Comparison  of  Newton’s  method,  Gradient  Descent,  SGD–Asymptotic  convergence–Convergence  in  practice
21
Linear  Regression  Outline•Regression  Problems–Definition–Linear  functions–Residuals–Notation  trick:  fold  in  the  intercept•Linear  Regression  as  Function  Approximation–Objective  function:  Mean  squared  error–Hypothesis  space:  Linear  Functions•Optimization  for  Linear  Regression–Normal  Equations  (Closed-­‐form  solution)•Computational  complexity•Stability–SGD  for  Linear  Regression•Partial  derivatives•Update  rule–Gradient  Descent  for  Linear  Regression•Probabilistic  Interpretation  of  Linear  Regression–Generative  vs.  Discriminative–Conditional  Likelihood–Background:  Gaussian  Distribution–Case  #1:  1D  Linear  Regression–Case  #2:  Multiple  Linear  Regression22
Regression  ProblemsWhiteboard–Definition–Linear  functions–Residuals–Notation  trick:  fold  in  the  intercept
23
Linear  Regression  as  Function  ApproximationWhiteboard–Objective  function:  Mean  squared  error–Hypothesis  space:  Linear  Functions
24
Optimization  for  Linear  RegressionWhiteboard–Normal  Equations  (Closed-­‐form  solution)•Computational  complexity•Stability–SGD  for  Linear  Regression•Partial  derivatives•Update  rule–Gradient  Descent  for  Linear  Regression
25
Probabilistic  Interpretation  of  Linear  RegressionWhiteboard–Generative  vs.  Discriminative–Conditional  Likelihood–Background:  Gaussian  Distribution–Case  #1:  1D  Linear  Regression–Case  #2:  Multiple  Linear  Regression
26
Convergence  Curves
•For  the  batch  method,  the  training  MSE  is  initially  large  due  to  uninformed  initialization•In  the  online  update,  N  updates  for  every  epoch  reduces  MSE  to  a  much  smaller  value.
27©  Eric  Xing  @  CMU,  2006-­‐2011
Clustering(K-­‐Means)
110-­‐601  Introduction  to  Machine  Learning
Matt  GormleyLecture  15March  8,  2017Machine  Learning  DepartmentSchool  of  Computer  ScienceCarnegie  Mellon  University
Clustering  Readings:Murphy  25.5Bishop  12.1,  12.3HTF  14.3.0Mitchell  -­‐-­‐
Reminders•Homework5:  Readings/  Applicationof  ML–Release:  Wed,  Mar.  08–Due:  Wed,  Mar.  22  at  11:59pm
2
Outline•Clustering:  Motivation  /  Applications•Optimization  Background–Coordinate  Descent–Block  Coordinate  Descent•Clustering–Inputs  and  Outputs–Objective-­‐based  Clustering•K-­‐Means–K-­‐Means  Objective–Computational  Complexity–K-­‐Means  Algorithm  /  Lloyd’s  Method•K-­‐Means  Initialization–Random–Farthest  Point–K-­‐Means++3
Clustering,  Informal  GoalsGoal:  Automatically  partition  unlabeleddata  into  groups  of  similar  datapoints.Question:  When  and  why  would  we  want  to  do  this?•  Automatically  organizing  data.Useful  for:•  Representing  high-­‐dimensional  data  in  a  low-­‐dimensional  space  (e.g.,  for  visualization  purposes).•  Understanding  hidden  structure  in  data.•  Preprocessing  for  further  analysis.
Slide  courtesy  of  Nina  Balcan
•Cluster  news  articles  or  web  pages  or  search  results  by  topic.
Applications
(Clustering  
comes  up  
everywhere…)
•Cluster  protein  sequences  by  function  or  genes  according  to  expression  profile.•Cluster  users  of  social  networks  by  interest  (community  detection).
Facebook network
Twitter NetworkSlide  courtesy  of  Nina  Balcan
•Cluster  customers  according  to  purchase  history.
Applications  
(Clustering  comes  up  everywhere…)
•Cluster  galaxies  or  nearby  stars(e.g.  Sloan  Digital  Sky  Survey)
•And  many  manymore  applications….Slide  courtesy  of  Nina  Balcan
Optimization  BackgroundWhiteboard:–Coordinate  Descent–Block  Coordinate  Descent
7
ClusteringWhiteboard:–Inputs  and  Outputs–Objective-­‐based  Clustering
8
K-­‐MeansWhiteboard:–K-­‐Means  Objective–Computational  Complexity–K-­‐Means  Algorithm  /  Lloyd’s  Method
9
K-­‐Means  InitializationWhiteboard:–Random–Furthest  Traversal–K-­‐Means++
10
Lloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Example:  Given  a  set  of  datapointsLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Select  initial  centers  at  randomLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Assign  each  point  to  its  nearest  centerLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Recomputeoptimal  centers  given  a  fixed  clusteringLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Assign  each  point  to  its  nearest  centerLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Recomputeoptimal  centers  given  a  fixed  clusteringLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Assign  each  point  to  its  nearest  centerLloyd’s  method:  Random  Initialization
Slide  courtesy  of  Nina  Balcan
Recomputeoptimal  centers  given  a  fixed  clusteringLloyd’s  method:  Random  Initialization
Get  a  good    quality  solution  in  this  example.Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
It  always  converges,  but  it  may  converge  at  a  local  optimum  that  is  different  from  the  global  optimum,  and  in  fact  could  be  arbitrarily  worse  in  terms  of  its  score.Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
Local  optimum:  every  point  is  assigned  to  its  nearest  center  and  every  center  is  the  mean  value  of  its  points.Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
.It  is  arbitrarily  worse  than  optimum  solution….
Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
This  bad  performance,  can  happen  even  with  well  separated  Gaussian  clusters.Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
This  bad  performance,  can  happen  even  with  well  separated  Gaussian  clusters.
Some  Gaussian  are  combined…..Slide  courtesy  of  Nina  Balcan
Lloyd’s  method:  Performance
•For  k  equal-­‐sized  Gaussians,  Pr[each  initial  center  is  in  a  different  Gaussian]  ≈"!"$≈%&$•Becomes  unlikely  as  k  gets  large.  •If  we  do  random  initialization,  as  kincreases,  it  becomes  more  likely  we  won’t  have  perfectly  picked  one  center  per  Gaussian  in  our  initialization  (so  Lloyd’s  method  will  output  a  bad  solution).
Slide  courtesy  of  Nina  Balcan
Another  Initialization  Idea:  Furthest  Point  HeuristicChoose  𝐜𝟏arbitrarily  (or  at  random).•Pick  𝐜𝐣among  datapoints  𝐱𝟏,𝐱𝟐,…,𝐱𝐧that  is  farthest  from  previously  chosen  𝐜𝟏,𝐜𝟐,…,𝐜𝒋0𝟏•For  j=2,…,kFixes  the  Gaussian  problem.  But  it  can  be  thrown  off  by  outliers….Slide  courtesy  of  Nina  Balcan
Furthest  point  heuristic  does  well  on  previous  example
Slide  courtesy  of  Nina  Balcan
(0,1)(0,-­‐1)(-­‐2,0)(3,0)Furthest  point  initialization  heuristic  sensitive  to  outliersAssume  k=3
Slide  courtesy  of  Nina  Balcan
(0,1)(0,-­‐1)(-­‐2,0)(3,0)Furthest  point  initialization  heuristic  sensitive  to  outliersAssume  k=3
Slide  courtesy  of  Nina  Balcan
K-­‐means++  Initialization:  D6sampling  [AV07]•Choose  𝐜𝟏at  random.•Pick  𝐜𝐣among  𝐱𝟏,𝐱𝟐,…,𝐱𝒏according  to  the  distribution•For  j=2,…,k•Interpolate  between  random  and  furthest  point  initialization
𝐏𝐫(𝐜𝐣=𝐱𝐢)∝𝐦𝐢𝐧𝐣?@𝐣	
  𝐱𝐢−𝐜𝐣?𝟐•Let  D(x)be  the  distance  between  a  point  𝑥and  its  nearest  center.  Chose  the  next  center  proportional  to  D6(𝐱).D6(𝐱𝐢)Theorem:  K-­‐means++  always  attains  an  O(log  k)  approximation  to  optimal  k-­‐means  solution  in  expectation.Running  Lloyd’scan  only  further  improve  the  cost.Slide  courtesy  of  Nina  Balcan
K-­‐means++  Idea:  D6sampling•Interpolate  between  random  and  furthest  point  initialization•Let  D(x)be  the  distance  between  a  point  𝑥and  its  nearest  center.  Chose  the  next  center  proportional  to  DD(𝐱).•𝛼=0,  random  sampling•𝛼=∞,  furthest  point  (Side  note:  it  actually  works  well  for  k-­‐center)•𝛼=2,  k-­‐means++Side  note:  𝛼=1,  works  well  for  k-­‐median  Slide  courtesy  of  Nina  Balcan
(0,1)(0,-­‐1)(-­‐2,0)(3,0)K-­‐means  ++  Fix
Slide  courtesy  of  Nina  Balcan
K-­‐means++/Lloyd’sRunning  Time
Repeatuntil  there  is  no  change  in  the  cost.•For  each  j:    CJ←{𝑥∈𝑆whose  closest  center  is  𝐜𝐣}•For  each  j:  𝐜𝐣←mean  of  CJEach  round  takes  time  O(nkd).•K-­‐means  ++  initialization:  O(nd)  and  one  pass  over  data  to  select  next  center.  So  O(nkd)  time  in  total.•Lloyd’s  method•Exponential  #  of  rounds  in  the  worst  case  [AV07].•Expected  polynomial  time  in  the  smoothed  analysis  (non  worst-­‐case)  model!Slide  courtesy  of  Nina  Balcan
K-­‐means++/Lloyd’s  Summary•Exponential  #  of  rounds  in  the  worst  case  [AV07].•Expected  polynomial  time  in  the  smoothed  analysis  model!•K-­‐means++  always  attains  an  O(log  k)  approximation  to  optimal  k-­‐means  solution  in  expectation.•Running  Lloyd’s  can  only  further  improve  the  cost.•Does  well  in  practice.
Slide  courtesy  of  Nina  Balcan
What  value  of  k???•Hold-­‐out  validation/cross-­‐validation  on  auxiliary  task  (e.g.,  supervised  learning  task).•Heuristic:  Find  large  gap  between  k  -­‐1-­‐means  cost  and  k-­‐means  cost.•Try  hierarchical  clustering.
Slide  courtesy  of  Nina  Balcan
CSC321 Lecture 22: Q-Learning
Roger Grosse
Roger Grosse CSC321 Lecture 22: Q-Learning 1 / 21
Overview
Second of 3 lectures on reinforcement learning
Last time: policy gradient (e.g. REINFORCE)
Optimize a policy directly, don't represent anything about the
environment
Today: Q-learning
Learn an action-value function that predicts future returns
Next time: AlphaGo uses both a policy network and a value network
This lecture is review if you've taken 411
This lecture has more new content than I'd intended. If there is an
exam question about this lecture or next one, it won't be a hard
question.
Roger Grosse CSC321 Lecture 22: Q-Learning 2 / 21
Overview
Agent interacts with an environment, which we treat as a black box
Your RL code accesses it only through an API since it's external to
the agent
I.e., you're not \allowed" to inspect the transition probabilities, reward
distributions, etc.
Roger Grosse CSC321 Lecture 22: Q-Learning 3 / 21
Recap: Markov Decision Processes
The environment is represented as a Markov decision process (MDP)
M.
Markov assumption: all relevant information is encapsulated in the
current state
Components of an MDP:
initial state distribution p(s0)
transition distribution p(st+1jst;at)
reward function r(st;at)
policy(atjst) parameterized by 
Assume a fully observable environment, i.e. stcan be observed directly
Roger Grosse CSC321 Lecture 22: Q-Learning 4 / 21
Finite and Innite Horizon
Last time: nite horizon MDPs
Fixed number of steps Tper episode
Maximize expected return R=Ep()[r()]
Now: more convenient to assume innite horizon
We can't sum innitely many rewards, so we need to discount them:
$100 a year from now is worth less than $100 today
Discounted return
Gt=rt+
rt+1+
2rt+2+
Want to choose an action to maximize expected discounted return
The parameter 
 < 1 is called the discount factor
small 
= myopic
large 
= farsighted
Roger Grosse CSC321 Lecture 22: Q-Learning 5 / 21
Value Function
Value function V(s) of a state sunder policy : the expected
discounted return if we start in sand follow
V(s) =E[Gtjst=s]
=E"1X
i=0
irt+ijst=s#
Computing the value function is generally impractical, but we can try
to approximate (learn) it
The benet is credit assignment: see directly how an action aects
future returns rather than wait for rollouts
Roger Grosse CSC321 Lecture 22: Q-Learning 6 / 21
Value Function
Rewards: -1 per time step
Undiscounted ( 
= 1)
Actions: N, E, S, W
State: current location
Roger Grosse CSC321 Lecture 22: Q-Learning 7 / 21
Value Function
Roger Grosse CSC321 Lecture 22: Q-Learning 8 / 21
Action-Value Function
Can we use a value function to choose actions?
arg max
ar(st;a) +
Ep(st+1jst;at)[V(st+1)]
Problem: this requires taking the expectation with respect to the
environment's dynamics, which we don't have direct access to!
Instead learn an action-value function, or Q-function: expected
returns if you take action aand then follow your policy
Q(s;a) =E[Gtjst=s;at=a]
Relationship:
V(s) =X
a(ajs)Q(s;a)
Optimal action:
arg max
aQ(s;a)
Roger Grosse CSC321 Lecture 22: Q-Learning 9 / 21
Action-Value Function
Can we use a value function to choose actions?
arg max
ar(st;a) +
Ep(st+1jst;at)[V(st+1)]
Problem: this requires taking the expectation with respect to the
environment's dynamics, which we don't have direct access to!
Instead learn an action-value function, or Q-function: expected
returns if you take action aand then follow your policy
Q(s;a) =E[Gtjst=s;at=a]
Relationship:
V(s) =X
a(ajs)Q(s;a)
Optimal action:
arg max
aQ(s;a)
Roger Grosse CSC321 Lecture 22: Q-Learning 9 / 21
Bellman Equation
The Bellman Equation is a recursive formula for the action-value
function:
Q(s;a) =r(s;a) +
Ep(s0js;a)(a0js0)[Q(s0;a0)]
There are various Bellman equations, and most RL algorithms are
based on repeatedly applying one of them.
Roger Grosse CSC321 Lecture 22: Q-Learning 10 / 21
Optimal Bellman Equation
The optimal policy is the one that maximizes the expected
discounted return, and the optimal action-value function Qis the
action-value function for .
The Optimal Bellman Equation gives a recursive formula for Q:
Q(s;a) =r(s;a) +
Ep(s0js;a)
max
a0Q(st+1;a0)jst=s;at=a
This system of equations characterizes the optimal action-value
function. So maybe we can approximate Qby trying to solve the
optimal Bellman equation!
Roger Grosse CSC321 Lecture 22: Q-Learning 11 / 21
Q-Learning
LetQbe an action-value function which hopefully approximates Q.
The Bellman error is the update to our expected return when we
observe the next state s0.
r(st;at) +
max
aQ(st+1;a)
|{z}
inside Ein RHS of Bellman eqn Q(st;at)
The Bellman equation says the Bellman error is 0 in expectation
Q-learning is an algorithm that repeatedly adjusts Qto minimize the
Bellman error
Each time we sample consecutive states and actions ( st;at;st+1):
Q(st;at) Q(st;at) +h
r(st;at) +
max
aQ(st+1;a) Q(st;at)i
| {z }
Bellman error
Roger Grosse CSC321 Lecture 22: Q-Learning 12 / 21
Exploration-Exploitation Tradeo
Notice: Q-learning only learns about the states and actions it visits.
Exploration-exploitation tradeo: the agent should sometimes pick
suboptimal actions in order to visit new states and actions.
Simple solution: -greedy policy
With probability 1  , choose the optimal action according to Q
With probability , choose a random action
Believe it or not, -greedy is still used today!
Roger Grosse CSC321 Lecture 22: Q-Learning 13 / 21
Exploration-Exploitation Tradeo
You can't use an epsilon-greedy strategy with policy gradient because
it's an on-policy algorithm: the agent can only learn about the policy
it's actually following.
Q-learning is an o-policy algorithm: the agent can learn Qregardless
of whether it's actually following the optimal policy
Hence, Q-learning is typically done with an -greedy policy, or some
other policy that encourages exploration.
Roger Grosse CSC321 Lecture 22: Q-Learning 14 / 21
Q-Learning
Roger Grosse CSC321 Lecture 22: Q-Learning 15 / 21
Function Approximation
So far, we've been assuming a tabular representation of Q: one entry
for every state/action pair.
This is impractical to store for all but the simplest problems, and
doesn't share structure between related states.
Solution: approximate Qusing a parameterized function, e.g.
linear function approximation: Q(s;a) =w> (s;a)
compute Qwith a neural net
Update Qusing backprop:
t r(st;at) +
max
aQ(st+1;a)
 +(t Q(s;a))@Q
@
Roger Grosse CSC321 Lecture 22: Q-Learning 16 / 21
Function Approximation
Approximating Qwith a neural net is a decades-old idea, but
DeepMind got it to work really well on Atari games in 2013 (\deep
Q-learning")
They used a very small network by today's standards
Main technical innovation: store experience into a replay buer, and
perform Q-learning using stored experience
Gains sample eciency by separating environment interaction from
optimization | don't need new experience for every SGD update!
Roger Grosse CSC321 Lecture 22: Q-Learning 17 / 21
Atari
Mnih et al., Nature 2015. Human-level control through deep
reinforcement learning
Network was given raw pixels as observations
Same architecture shared between all games
Assume fully observable environment, even though that's not the case
After about a day of training on a particular game, often beat
\human-level" performance (number of points within 5 minutes of
play)
Did very well on reactive games, poorly on ones that require planning
(e.g. Montezuma's Revenge)
https://www.youtube.com/watch?v=V1eYniJ0Rnk
https://www.youtube.com/watch?v=4MlZncshy1Q
Roger Grosse CSC321 Lecture 22: Q-Learning 18 / 21
Wireheading
If rats have a lever that causes an electrode to stimulate certain
\reward centers" in their brain, they'll keep pressing the lever at the
expense of sleep, food, etc.
RL algorithms show this \wireheading" behavior if the reward
function isn't designed carefully
https://blog.openai.com/faulty-reward-functions/
Roger Grosse CSC321 Lecture 22: Q-Learning 19 / 21
Policy Gradient vs. Q-Learning
Policy gradient and Q-learning use two very dierent choices of
representation: policies and value functions
Advantage of both methods: don't need to model the environment
Pros/cons of policy gradient
Pro: unbiased estimate of gradient of expected return
Pro: can handle a large space of actions (since you only need to sample
one)
Con: high variance updates (implies poor sample eciency)
Con: doesn't do credit assignment
Pros/cons of Q-learning
Pro: lower variance updates, more sample ecient
Pro: does credit assignment
Con: biased updates since Q function is approximate (drinks its own
Kool-Aid)
Con: hard to handle many actions (since you need to take the max)
Roger Grosse CSC321 Lecture 22: Q-Learning 20 / 21
Actor-Critic (optional)
Actor-critic methods combine the best of both worlds
Fit both a policy network (the \actor") and a value network (the
\critic")
Repeatedly update the value network to estimate V
Unroll for only a few steps, then compute the REINFORCE policy
update using the expected returns estimated by the value network
The two networks adapt to each other, much like GAN training
Modern version: Asynchronous Advantage Actor-Critic (A3C)
Roger Grosse CSC321 Lecture 22: Q-Learning 21 / 21
Recurrent Neural NetworksInstructor: Yoav ArtziCS5740: Natural Language ProcessingSpring 2018
Adapted from Yoav Goldberg’s Book and slides by Sasha Rush
Overview•Finite state models•Recurrent neural networks (RNNs)•Training RNNs•RNN Models•Long short-term memory (LSTM)•Attention
Text Classification•Consider the example:–Goal: classify sentimentHow can you not see this movie?You should not see this movie.•Model: bag of words•How well will the classifier work?–Similar unigrams and bigrams•Generally: need to maintain a stateto capture distant influences
Finite State Machines•Simple, classical way of representing state•Current state: saves necessary past information•Example: email address parsing

Deterministic Finite State Machines•!–states•Σ–vocabulary •#$∈!–start state •&:!×Σ→!–transition function•What does it do?–Maps input *+,…,*.to states #+,…,#.–For all /∈1,…,1#2=&(#25+,*2)•Can we use it for POS tagging? Language modeling? 
Types of State Machines•Acceptor–Compute final state !"and make a decision based on it: #=%(!")•Transducers–Apply function #(=%(!()to produce output for each intermediate state•Encoders–Compute final state !", and use it in another model 
Recurrent Neural Networks•Motivation:–Neural network model, but with state–How can we borrow ideas from FSMs?•RNNs are FSMs …–…with a twist–No longer finite in the same sense
RNN•!=ℝ$%&'-hidden state space•Σ=ℝ$&)-input state space•*+∈!-initial state vector•-∶ℝ$&)×ℝ$%&'→ℝ$%&'-transition function•Simple definition of -:-12345*,7=tanh(7,*=+?)Elman (1990)* Notation: vectors and matrices are bold
RNN•Map from dense sequence to dense representation–!",…,!%→'",…,'%–For all (∈1,…,+',=.',/",!–.is parameterized, and parameters are shared between all steps–Example:'0=.'1,!0=⋯=.(...'4,!",!5,!1,!0)
RNNs•Hidden states !"can be used in different ways•Similar to finite state machines–Acceptor–Transducer–Encoder•Output function maps vectors to symbols: #:ℝ&'()→ℝ&+,-•For example: single layer + softmax#!"=softmax(!"7+9)
Graphical RepresentationRecursive RepresentationUnrolled Representation

Graphical Representation

Training•RNNs are trained with SGD and Backprop•Define loss over outputs–Depends on supervision and task•Backpropagation through time (BPTT)–Use unrolled representation–Run forward propagation–Run backward propagation–Update all weights•Weights are shared between time steps–Sum the contributions of each time step to the gradient•Inefficient–Batch helps, common but tricky to implement with variable-size models
RNN: Acceptor Architecture
•Only care about the output from the last hidden state•Train: supervised, loss on prediction•Example:–Text classification
Language Modeling•Input: !=#$,…,#'•Goal: compute ((!)•Bi-gram decomposition:(!=+,-$'((#,∣#,/$)•With RNNs, can do non-Markovian models:(!=+,-$'((#,∣#$,…,#,/$)
RNN: Transducer Architecture
•Predict output for every time step
Language Modeling•Input: !=#$,…,#'•Goal: compute ((!)•Model:(!=+,-$'((#,∣#$,…,#,/$)(#,#$,…,#,/$=01,=0(21,/$,3,)01,=softmax(;,<+>)•Predict next token ?@,as we go:?@,=argmax0(1,)
RNN: Transducer Architecture
•Predict output for every time step•Examples:–Language modeling–POS tagging–NER
RNN: Transducer Architecture
X=x1,...,xnsi=R(si 1,xi),i=1,...,nO(si) =softmax(siW+b)ˆyi= arg maxO(si)
<latexit sha1_base64="UmXA3eSeIF2bRpqSfxmvb9cYbqQ=">AAACyXicbVFdi9NAFJ3ErzV+bFcffblYXLZYSyLC+rKwKIjgg6vYbaEpYTKdtMNOJnHmZkkNefIf+uabP8VJtpVs1wuBM+eee+ZkbpxLYdD3fzvurdt37t7bu+89ePjo8X7v4Mm5yQrN+JhlMtPTmBouheJjFCj5NNecprHkk/jifdOfXHJtRKa+4Trn85QulUgEo2ipqPdnCidwCGFKcRUnVVlHwTBcZGiGHUpBGHrbs6kjASeH8PWow1TiVVAPuzZiMASrg61d6/G5OyMGjQ2EyEusTJZgSsv6mmCLJzW8/Ocd14MwBC9cUazWbZYmP9VLKyhh9wYv6vX9kd8W3ATBBvTJps6i3i8bmBUpV8gkNWYW+DnOK6pRMMlrLywMzym7oEs+s1DRlJt51W6ihheWWUCSafsphJbtTlQ0NWadxlbZxDS7vYb8X29WYPJ2XgmVF8gVu7ooKSRgBs1aYSE0ZyjXFlCmhc0KbEU1ZWiX3zxCsPvLN8H561Hgj4Ivb/qn7zbPsUeekefkiATkmJySj+SMjAlzPjjSKZxL95P73S3dH1dS19nMPCXXyv35F3rw2uQ=</latexit><latexit sha1_base64="UmXA3eSeIF2bRpqSfxmvb9cYbqQ=">AAACyXicbVFdi9NAFJ3ErzV+bFcffblYXLZYSyLC+rKwKIjgg6vYbaEpYTKdtMNOJnHmZkkNefIf+uabP8VJtpVs1wuBM+eee+ZkbpxLYdD3fzvurdt37t7bu+89ePjo8X7v4Mm5yQrN+JhlMtPTmBouheJjFCj5NNecprHkk/jifdOfXHJtRKa+4Trn85QulUgEo2ipqPdnCidwCGFKcRUnVVlHwTBcZGiGHUpBGHrbs6kjASeH8PWow1TiVVAPuzZiMASrg61d6/G5OyMGjQ2EyEusTJZgSsv6mmCLJzW8/Ocd14MwBC9cUazWbZYmP9VLKyhh9wYv6vX9kd8W3ATBBvTJps6i3i8bmBUpV8gkNWYW+DnOK6pRMMlrLywMzym7oEs+s1DRlJt51W6ihheWWUCSafsphJbtTlQ0NWadxlbZxDS7vYb8X29WYPJ2XgmVF8gVu7ooKSRgBs1aYSE0ZyjXFlCmhc0KbEU1ZWiX3zxCsPvLN8H561Hgj4Ivb/qn7zbPsUeekefkiATkmJySj+SMjAlzPjjSKZxL95P73S3dH1dS19nMPCXXyv35F3rw2uQ=</latexit><latexit sha1_base64="UmXA3eSeIF2bRpqSfxmvb9cYbqQ=">AAACyXicbVFdi9NAFJ3ErzV+bFcffblYXLZYSyLC+rKwKIjgg6vYbaEpYTKdtMNOJnHmZkkNefIf+uabP8VJtpVs1wuBM+eee+ZkbpxLYdD3fzvurdt37t7bu+89ePjo8X7v4Mm5yQrN+JhlMtPTmBouheJjFCj5NNecprHkk/jifdOfXHJtRKa+4Trn85QulUgEo2ipqPdnCidwCGFKcRUnVVlHwTBcZGiGHUpBGHrbs6kjASeH8PWow1TiVVAPuzZiMASrg61d6/G5OyMGjQ2EyEusTJZgSsv6mmCLJzW8/Ocd14MwBC9cUazWbZYmP9VLKyhh9wYv6vX9kd8W3ATBBvTJps6i3i8bmBUpV8gkNWYW+DnOK6pRMMlrLywMzym7oEs+s1DRlJt51W6ihheWWUCSafsphJbtTlQ0NWadxlbZxDS7vYb8X29WYPJ2XgmVF8gVu7ooKSRgBs1aYSE0ZyjXFlCmhc0KbEU1ZWiX3zxCsPvLN8H561Hgj4Ivb/qn7zbPsUeekefkiATkmJySj+SMjAlzPjjSKZxL95P73S3dH1dS19nMPCXXyv35F3rw2uQ=</latexit><latexit sha1_base64="UmXA3eSeIF2bRpqSfxmvb9cYbqQ=">AAACyXicbVFdi9NAFJ3ErzV+bFcffblYXLZYSyLC+rKwKIjgg6vYbaEpYTKdtMNOJnHmZkkNefIf+uabP8VJtpVs1wuBM+eee+ZkbpxLYdD3fzvurdt37t7bu+89ePjo8X7v4Mm5yQrN+JhlMtPTmBouheJjFCj5NNecprHkk/jifdOfXHJtRKa+4Trn85QulUgEo2ipqPdnCidwCGFKcRUnVVlHwTBcZGiGHUpBGHrbs6kjASeH8PWow1TiVVAPuzZiMASrg61d6/G5OyMGjQ2EyEusTJZgSsv6mmCLJzW8/Ocd14MwBC9cUazWbZYmP9VLKyhh9wYv6vX9kd8W3ATBBvTJps6i3i8bmBUpV8gkNWYW+DnOK6pRMMlrLywMzym7oEs+s1DRlJt51W6ihheWWUCSafsphJbtTlQ0NWadxlbZxDS7vYb8X29WYPJ2XgmVF8gVu7ooKSRgBs1aYSE0ZyjXFlCmhc0KbEU1ZWiX3zxCsPvLN8H561Hgj4Ivb/qn7zbPsUeekefkiATkmJySj+SMjAlzPjjSKZxL95P73S3dH1dS19nMPCXXyv35F3rw2uQ=</latexit>
RNN: Encoder Architecture
•Similar to acceptor•Difference: last state is used as input to another model and not for prediction!"#="#à%&="&•Example:–Sentence embedding
Bidirectional RNNs•RNN decisions are based on historical data only–How can we account for future input?•When is it relevant? Feasible?

Bidirectional RNNs•RNN decisions are based on historical data only–How can we account for future input?•When is it relevant? Feasible?•When all the input is available. Not for real-time input.•Probabilistic model, for example for language modeling:!"=$%&'(!(*%∣*',…,*%.',*%/',…,*()

Deep RNNs•Can also make RNNs deeper (vertically) to increase the model capacity

RNN: Generator•Special case of the transducer architecture•Generation conditioned on !"•Probabilistic model:#$%"='()*+#(-(∣-*,…,-(1*,%")

RNN: Generator
•Stop when generating the STOP token•During learning (usually): force predicting the annotated token and compute losssj=R(sj 1,ˆtj 1)O(sj) =softmax(sjW+b)ˆtj= arg maxO(sj)
<latexit sha1_base64="0x57+9/HSx/9GOoZV/ePeDGhBZ8=">AAACp3icbVHbattAEF2pt1S9OcljX5a6LTFtjFQKyUsgtC+lL06T+AJeY0brlb326sLuqNgI/Vo/om/9m65spTh2BxYOZ87MmZ0JMyUN+v4fx33w8NHjJwdPvWfPX7x81Tg86pk011x0eapSPQjBCCUT0UWJSgwyLSAOleiHi69Vvv9TaCPT5BZXmRjFME1kJDmgpcaNXywGnIVRYcrxnF68p9cnW0wxPw3Kj/SOYTPAAsuab1HGvM62fN6qOlCGYomFSSOMYVneE9zhfkk//Gsbli3GqLfnsp6HgZ7azJLuOlXu3rjR9Nv+Oug+CGrQJHVcjRu/2STleSwS5AqMGQZ+hqMCNEquROmx3IgM+AKmYmhhArEwo2K955K+s8yERqm2L0G6ZrcrCoiNWcWhVVajmt1cRf4vN8wxOh8VMslyFAnfGEW5opjS6mh0IrXgqFYWANfSzkr5DDRwtKetlhDsfnkf9D61A78d/PjcvPxSr+OAvCZvyAkJyBm5JN/IFekS7rx1vjs3zq3bcjtuzx1spK5T1xyTe+HCX31lz3o=</latexit><latexit sha1_base64="0x57+9/HSx/9GOoZV/ePeDGhBZ8=">AAACp3icbVHbattAEF2pt1S9OcljX5a6LTFtjFQKyUsgtC+lL06T+AJeY0brlb326sLuqNgI/Vo/om/9m65spTh2BxYOZ87MmZ0JMyUN+v4fx33w8NHjJwdPvWfPX7x81Tg86pk011x0eapSPQjBCCUT0UWJSgwyLSAOleiHi69Vvv9TaCPT5BZXmRjFME1kJDmgpcaNXywGnIVRYcrxnF68p9cnW0wxPw3Kj/SOYTPAAsuab1HGvM62fN6qOlCGYomFSSOMYVneE9zhfkk//Gsbli3GqLfnsp6HgZ7azJLuOlXu3rjR9Nv+Oug+CGrQJHVcjRu/2STleSwS5AqMGQZ+hqMCNEquROmx3IgM+AKmYmhhArEwo2K955K+s8yERqm2L0G6ZrcrCoiNWcWhVVajmt1cRf4vN8wxOh8VMslyFAnfGEW5opjS6mh0IrXgqFYWANfSzkr5DDRwtKetlhDsfnkf9D61A78d/PjcvPxSr+OAvCZvyAkJyBm5JN/IFekS7rx1vjs3zq3bcjtuzx1spK5T1xyTe+HCX31lz3o=</latexit><latexit sha1_base64="0x57+9/HSx/9GOoZV/ePeDGhBZ8=">AAACp3icbVHbattAEF2pt1S9OcljX5a6LTFtjFQKyUsgtC+lL06T+AJeY0brlb326sLuqNgI/Vo/om/9m65spTh2BxYOZ87MmZ0JMyUN+v4fx33w8NHjJwdPvWfPX7x81Tg86pk011x0eapSPQjBCCUT0UWJSgwyLSAOleiHi69Vvv9TaCPT5BZXmRjFME1kJDmgpcaNXywGnIVRYcrxnF68p9cnW0wxPw3Kj/SOYTPAAsuab1HGvM62fN6qOlCGYomFSSOMYVneE9zhfkk//Gsbli3GqLfnsp6HgZ7azJLuOlXu3rjR9Nv+Oug+CGrQJHVcjRu/2STleSwS5AqMGQZ+hqMCNEquROmx3IgM+AKmYmhhArEwo2K955K+s8yERqm2L0G6ZrcrCoiNWcWhVVajmt1cRf4vN8wxOh8VMslyFAnfGEW5opjS6mh0IrXgqFYWANfSzkr5DDRwtKetlhDsfnkf9D61A78d/PjcvPxSr+OAvCZvyAkJyBm5JN/IFekS7rx1vjs3zq3bcjtuzx1spK5T1xyTe+HCX31lz3o=</latexit><latexit sha1_base64="0x57+9/HSx/9GOoZV/ePeDGhBZ8=">AAACp3icbVHbattAEF2pt1S9OcljX5a6LTFtjFQKyUsgtC+lL06T+AJeY0brlb326sLuqNgI/Vo/om/9m65spTh2BxYOZ87MmZ0JMyUN+v4fx33w8NHjJwdPvWfPX7x81Tg86pk011x0eapSPQjBCCUT0UWJSgwyLSAOleiHi69Vvv9TaCPT5BZXmRjFME1kJDmgpcaNXywGnIVRYcrxnF68p9cnW0wxPw3Kj/SOYTPAAsuab1HGvM62fN6qOlCGYomFSSOMYVneE9zhfkk//Gsbli3GqLfnsp6HgZ7azJLuOlXu3rjR9Nv+Oug+CGrQJHVcjRu/2STleSwS5AqMGQZ+hqMCNEquROmx3IgM+AKmYmhhArEwo2K955K+s8yERqm2L0G6ZrcrCoiNWcWhVVajmt1cRf4vN8wxOh8VMslyFAnfGEW5opjS6mh0IrXgqFYWANfSzkr5DDRwtKetlhDsfnkf9D61A78d/PjcvPxSr+OAvCZvyAkJyBm5JN/IFekS7rx1vjs3zq3bcjtuzx1spK5T1xyTe+HCX31lz3o=</latexit>
Example: Caption Generation•Given: image !•Goal: generate caption•Set "#=CNN(!)•Model:)*!=+,-./)(0,∣0.,…,0,4.,!)
Examples from Karpathyand Fei-Fei2015

Sequence-to-Sequence
•Connect encoder and generator•Many alternatives:–Set generator !"#to encoder output !$%–Concatenate generator !"#with each step input during generation •Examples:–Machine translation–Chatbots–Dialog systems•Can also generate other sequences –not only natural language!
Sequence-to-Sequence
X=x1,...,xnsEi=RE(sEi 1,xi),i=1,...,nc=OE(sEn)sDj=R(sDj 1,[ˆtj 1;c])O(sDj) =softmax(sDjW+b)ˆtj= arg maxO(sDj)
<latexit sha1_base64="EHk2mbe5MGD73rmjF/0TXEIMQ/Q=">AAADWHicbVJNa9tAEF1ZSZO4H3HSYy9DTYNNXSOVQgIlENoYekta6thg2WK1XtnrSCuhHRUboT9Z6KH9K710pditPzqw8PbN7Huzw3hxIBRa1k+jYu7tPzo4PKo+fvL02XHt5PRORWnCeJdFQZT0Pap4ICTvosCA9+OE09ALeM+7/1jke994okQkv+Ii5sOQTqTwBaOoKffEkH24hDNwQopTz8/muWu3nHGEqrVGSXCc6uqucleMOnB5Bl/cTmONzcQbOx91WutiotkCoR1WohtKLC9UbjZV5KjT3LKbja5Luw2zWWF23YLBinSmFDPMl6n38M9lWAreNDYlm4UmOMjnmKnIx5DO862S1a2Xw+u/el7edByo7tjOCj2HJhOdmcOuW9FD1a3VrbZVBuwCewnqZBm3bu27HhxLQy6RBVSpgW3FOMxogoIFPK86qeIxZfd0wgcaShpyNczKxcjhlWbG4EeJPhKhZNdfZDRUahF6urJoVm3nCvJ/uUGK/sUwEzJOkUv2YOSnAWAExZbBWCScYbDQgLJE6F6BTWlCGepdLIZgb395F9y9bdtW2/78rn71YTmOQ/KCvCQNYpNzckU+kVvSJcz4Yfyu7FX2K79MYh6YRw+lFWP55jnZCPP0D+P+C7o=</latexit><latexit sha1_base64="EHk2mbe5MGD73rmjF/0TXEIMQ/Q=">AAADWHicbVJNa9tAEF1ZSZO4H3HSYy9DTYNNXSOVQgIlENoYekta6thg2WK1XtnrSCuhHRUboT9Z6KH9K710pditPzqw8PbN7Huzw3hxIBRa1k+jYu7tPzo4PKo+fvL02XHt5PRORWnCeJdFQZT0Pap4ICTvosCA9+OE09ALeM+7/1jke994okQkv+Ii5sOQTqTwBaOoKffEkH24hDNwQopTz8/muWu3nHGEqrVGSXCc6uqucleMOnB5Bl/cTmONzcQbOx91WutiotkCoR1WohtKLC9UbjZV5KjT3LKbja5Luw2zWWF23YLBinSmFDPMl6n38M9lWAreNDYlm4UmOMjnmKnIx5DO862S1a2Xw+u/el7edByo7tjOCj2HJhOdmcOuW9FD1a3VrbZVBuwCewnqZBm3bu27HhxLQy6RBVSpgW3FOMxogoIFPK86qeIxZfd0wgcaShpyNczKxcjhlWbG4EeJPhKhZNdfZDRUahF6urJoVm3nCvJ/uUGK/sUwEzJOkUv2YOSnAWAExZbBWCScYbDQgLJE6F6BTWlCGepdLIZgb395F9y9bdtW2/78rn71YTmOQ/KCvCQNYpNzckU+kVvSJcz4Yfyu7FX2K79MYh6YRw+lFWP55jnZCPP0D+P+C7o=</latexit><latexit sha1_base64="EHk2mbe5MGD73rmjF/0TXEIMQ/Q=">AAADWHicbVJNa9tAEF1ZSZO4H3HSYy9DTYNNXSOVQgIlENoYekta6thg2WK1XtnrSCuhHRUboT9Z6KH9K710pditPzqw8PbN7Huzw3hxIBRa1k+jYu7tPzo4PKo+fvL02XHt5PRORWnCeJdFQZT0Pap4ICTvosCA9+OE09ALeM+7/1jke994okQkv+Ii5sOQTqTwBaOoKffEkH24hDNwQopTz8/muWu3nHGEqrVGSXCc6uqucleMOnB5Bl/cTmONzcQbOx91WutiotkCoR1WohtKLC9UbjZV5KjT3LKbja5Luw2zWWF23YLBinSmFDPMl6n38M9lWAreNDYlm4UmOMjnmKnIx5DO862S1a2Xw+u/el7edByo7tjOCj2HJhOdmcOuW9FD1a3VrbZVBuwCewnqZBm3bu27HhxLQy6RBVSpgW3FOMxogoIFPK86qeIxZfd0wgcaShpyNczKxcjhlWbG4EeJPhKhZNdfZDRUahF6urJoVm3nCvJ/uUGK/sUwEzJOkUv2YOSnAWAExZbBWCScYbDQgLJE6F6BTWlCGepdLIZgb395F9y9bdtW2/78rn71YTmOQ/KCvCQNYpNzckU+kVvSJcz4Yfyu7FX2K79MYh6YRw+lFWP55jnZCPP0D+P+C7o=</latexit><latexit sha1_base64="EHk2mbe5MGD73rmjF/0TXEIMQ/Q=">AAADWHicbVJNa9tAEF1ZSZO4H3HSYy9DTYNNXSOVQgIlENoYekta6thg2WK1XtnrSCuhHRUboT9Z6KH9K710pditPzqw8PbN7Huzw3hxIBRa1k+jYu7tPzo4PKo+fvL02XHt5PRORWnCeJdFQZT0Pap4ICTvosCA9+OE09ALeM+7/1jke994okQkv+Ii5sOQTqTwBaOoKffEkH24hDNwQopTz8/muWu3nHGEqrVGSXCc6uqucleMOnB5Bl/cTmONzcQbOx91WutiotkCoR1WohtKLC9UbjZV5KjT3LKbja5Luw2zWWF23YLBinSmFDPMl6n38M9lWAreNDYlm4UmOMjnmKnIx5DO862S1a2Xw+u/el7edByo7tjOCj2HJhOdmcOuW9FD1a3VrbZVBuwCewnqZBm3bu27HhxLQy6RBVSpgW3FOMxogoIFPK86qeIxZfd0wgcaShpyNczKxcjhlWbG4EeJPhKhZNdfZDRUahF6urJoVm3nCvJ/uUGK/sUwEzJOkUv2YOSnAWAExZbBWCScYbDQgLJE6F6BTWlCGepdLIZgb395F9y9bdtW2/78rn71YTmOQ/KCvCQNYpNzckU+kVvSJcz4Yfyu7FX2K79MYh6YRw+lFWP55jnZCPP0D+P+C7o=</latexit>
Sequence-to-Sequence Training Graph

Long-range Interactions•Promise: Learn long-range interactions of language from data•Example:How can you not see this movie?You should not see this movie.•Sometimes: requires ”remembering” early state–Key signal here is at !", but gradient is at !#
Long-term Gradients•Gradient go through (many) multiplications•OK at end layers àclose to the loss•But: issue with early layers•For example, derivative of tanh%%&tanh&=1−tanh*&–Large activation àgradient disappears•In other activation functions, values can become larger and larger
Exploding Gradients•Common when there is not saturation in activation (e.g., ReLu) and we get exponential blowup•Result: reasonable short-term gradient, but bad long-term ones•Common heuristic:–Gradient clipping: bounding all gradients by maximum value

Vanishing Gradients•Occurs when multiplying small values–For example: when tanhsaturates•Mainly affects long-term gradients•Solving this is more complex
Long Short-term Memory (LSTM)
Hochreiterand Schmidhuber(1997)
LSTM vs. Elman RNN

LSTM
Image by Tim RocktäschelCell StateOutput
Input
 (·)
<latexit sha1_base64="dNFa4QbOT9ylps99TO0e4EmcH+U=">AAAB9HicbVBNS8NAEJ34WetX1aOXxSLUS0lE0GPRi8cK9gOaUDabTbt0dxN3N4US+ju8eFDEqz/Gm//GbZuDtj4YeLw3w8y8MOVMG9f9dtbWNza3tks75d29/YPDytFxWyeZIrRFEp6obog15UzSlmGG026qKBYhp51wdDfzO2OqNEvko5mkNBB4IFnMCDZWCnzNBgLXfBIl5qJfqbp1dw60SryCVKFAs1/58qOEZIJKQzjWuue5qQlyrAwjnE7LfqZpiskID2jPUokF1UE+P3qKzq0SoThRtqRBc/X3RI6F1hMR2k6BzVAvezPxP6+XmfgmyJlMM0MlWSyKM45MgmYJoIgpSgyfWIKJYvZWRIZYYWJsTmUbgrf88ippX9Y9t+49XFUbt0UcJTiFM6iBB9fQgHtoQgsIPMEzvMKbM3ZenHfnY9G65hQzJ/AHzucPQvORwA==</latexit><latexit sha1_base64="dNFa4QbOT9ylps99TO0e4EmcH+U=">AAAB9HicbVBNS8NAEJ34WetX1aOXxSLUS0lE0GPRi8cK9gOaUDabTbt0dxN3N4US+ju8eFDEqz/Gm//GbZuDtj4YeLw3w8y8MOVMG9f9dtbWNza3tks75d29/YPDytFxWyeZIrRFEp6obog15UzSlmGG026qKBYhp51wdDfzO2OqNEvko5mkNBB4IFnMCDZWCnzNBgLXfBIl5qJfqbp1dw60SryCVKFAs1/58qOEZIJKQzjWuue5qQlyrAwjnE7LfqZpiskID2jPUokF1UE+P3qKzq0SoThRtqRBc/X3RI6F1hMR2k6BzVAvezPxP6+XmfgmyJlMM0MlWSyKM45MgmYJoIgpSgyfWIKJYvZWRIZYYWJsTmUbgrf88ippX9Y9t+49XFUbt0UcJTiFM6iBB9fQgHtoQgsIPMEzvMKbM3ZenHfnY9G65hQzJ/AHzucPQvORwA==</latexit><latexit sha1_base64="dNFa4QbOT9ylps99TO0e4EmcH+U=">AAAB9HicbVBNS8NAEJ34WetX1aOXxSLUS0lE0GPRi8cK9gOaUDabTbt0dxN3N4US+ju8eFDEqz/Gm//GbZuDtj4YeLw3w8y8MOVMG9f9dtbWNza3tks75d29/YPDytFxWyeZIrRFEp6obog15UzSlmGG026qKBYhp51wdDfzO2OqNEvko5mkNBB4IFnMCDZWCnzNBgLXfBIl5qJfqbp1dw60SryCVKFAs1/58qOEZIJKQzjWuue5qQlyrAwjnE7LfqZpiskID2jPUokF1UE+P3qKzq0SoThRtqRBc/X3RI6F1hMR2k6BzVAvezPxP6+XmfgmyJlMM0MlWSyKM45MgmYJoIgpSgyfWIKJYvZWRIZYYWJsTmUbgrf88ippX9Y9t+49XFUbt0UcJTiFM6iBB9fQgHtoQgsIPMEzvMKbM3ZenHfnY9G65hQzJ/AHzucPQvORwA==</latexit><latexit sha1_base64="dNFa4QbOT9ylps99TO0e4EmcH+U=">AAAB9HicbVBNS8NAEJ34WetX1aOXxSLUS0lE0GPRi8cK9gOaUDabTbt0dxN3N4US+ju8eFDEqz/Gm//GbZuDtj4YeLw3w8y8MOVMG9f9dtbWNza3tks75d29/YPDytFxWyeZIrRFEp6obog15UzSlmGG026qKBYhp51wdDfzO2OqNEvko5mkNBB4IFnMCDZWCnzNBgLXfBIl5qJfqbp1dw60SryCVKFAs1/58qOEZIJKQzjWuue5qQlyrAwjnE7LfqZpiskID2jPUokF1UE+P3qKzq0SoThRtqRBc/X3RI6F1hMR2k6BzVAvezPxP6+XmfgmyJlMM0MlWSyKM45MgmYJoIgpSgyfWIKJYvZWRIZYYWJsTmUbgrf88ippX9Y9t+49XFUbt0UcJTiFM6iBB9fQgHtoQgsIPMEzvMKbM3ZenHfnY9G65hQzJ/AHzucPQvORwA==</latexit>
ft= (Wf[ht 1,xt]+bf)it= (Wi[ht 1,xt]+bf)ct=ft ct 1+it tanh(Wc[ht 1,xt]+bc)ot= (Wo[ht 1,xt]+bo)ht=ot tanh(ct)
<latexit sha1_base64="5rN75Omq38gFp31KxTutiBaTFOk=">AAADsnicpVJNa9tAEF1L/UjdLyc99rLUpCS0caQQSC6F0F56TKGOQy1FrNYra8lqV+yOQozQ/+u5t/6brhyryHZdCBkQzL558/RmmDgX3IDn/e447qPHT55uPes+f/Hy1eve9s6FUYWmbEiVUPoyJoYJLtkQOAh2mWtGsliwUXz9pa6Pbpg2XMnvMMtZmJGp5AmnBCwUbXd+BhmBNE7KpIoAv/+EA8OnGdlr4FF1lYybR1pFJRz41UfcILe2K8Qf/r5jS9/HQdBtAL5Rlj9EljaybfeBmijALcpctSXDWywgMm3boet2/uOGLrlRG4dU91JVS6rpyoxqk/t6GftRr+8NvHng9cRfJH20iPOo9yuYKFpkTAIVxJix7+UQlkQDp4JV3aAwLCf0mkzZ2KaSZMyE5fzkKrxrkQlOlLafBDxH2x0lyYyZZbFl1ibNaq0G/1UbF5CchiWXeQFM0rsfJYXAoHB9v3jCNaMgZjYhVHPrFdOUaELBXnnXLsFfHXk9uTga+N7A/3bcP/u8WMcWeoveoT3koxN0hr6iczRE1Dl0hs6VE7nH7g+XuPSO6nQWPW/QUrjiD5/sNGU=</latexit><latexit sha1_base64="5rN75Omq38gFp31KxTutiBaTFOk=">AAADsnicpVJNa9tAEF1L/UjdLyc99rLUpCS0caQQSC6F0F56TKGOQy1FrNYra8lqV+yOQozQ/+u5t/6brhyryHZdCBkQzL558/RmmDgX3IDn/e447qPHT55uPes+f/Hy1eve9s6FUYWmbEiVUPoyJoYJLtkQOAh2mWtGsliwUXz9pa6Pbpg2XMnvMMtZmJGp5AmnBCwUbXd+BhmBNE7KpIoAv/+EA8OnGdlr4FF1lYybR1pFJRz41UfcILe2K8Qf/r5jS9/HQdBtAL5Rlj9EljaybfeBmijALcpctSXDWywgMm3boet2/uOGLrlRG4dU91JVS6rpyoxqk/t6GftRr+8NvHng9cRfJH20iPOo9yuYKFpkTAIVxJix7+UQlkQDp4JV3aAwLCf0mkzZ2KaSZMyE5fzkKrxrkQlOlLafBDxH2x0lyYyZZbFl1ibNaq0G/1UbF5CchiWXeQFM0rsfJYXAoHB9v3jCNaMgZjYhVHPrFdOUaELBXnnXLsFfHXk9uTga+N7A/3bcP/u8WMcWeoveoT3koxN0hr6iczRE1Dl0hs6VE7nH7g+XuPSO6nQWPW/QUrjiD5/sNGU=</latexit><latexit sha1_base64="5rN75Omq38gFp31KxTutiBaTFOk=">AAADsnicpVJNa9tAEF1L/UjdLyc99rLUpCS0caQQSC6F0F56TKGOQy1FrNYra8lqV+yOQozQ/+u5t/6brhyryHZdCBkQzL558/RmmDgX3IDn/e447qPHT55uPes+f/Hy1eve9s6FUYWmbEiVUPoyJoYJLtkQOAh2mWtGsliwUXz9pa6Pbpg2XMnvMMtZmJGp5AmnBCwUbXd+BhmBNE7KpIoAv/+EA8OnGdlr4FF1lYybR1pFJRz41UfcILe2K8Qf/r5jS9/HQdBtAL5Rlj9EljaybfeBmijALcpctSXDWywgMm3boet2/uOGLrlRG4dU91JVS6rpyoxqk/t6GftRr+8NvHng9cRfJH20iPOo9yuYKFpkTAIVxJix7+UQlkQDp4JV3aAwLCf0mkzZ2KaSZMyE5fzkKrxrkQlOlLafBDxH2x0lyYyZZbFl1ibNaq0G/1UbF5CchiWXeQFM0rsfJYXAoHB9v3jCNaMgZjYhVHPrFdOUaELBXnnXLsFfHXk9uTga+N7A/3bcP/u8WMcWeoveoT3koxN0hr6iczRE1Dl0hs6VE7nH7g+XuPSO6nQWPW/QUrjiD5/sNGU=</latexit><latexit sha1_base64="5rN75Omq38gFp31KxTutiBaTFOk=">AAADsnicpVJNa9tAEF1L/UjdLyc99rLUpCS0caQQSC6F0F56TKGOQy1FrNYra8lqV+yOQozQ/+u5t/6brhyryHZdCBkQzL558/RmmDgX3IDn/e447qPHT55uPes+f/Hy1eve9s6FUYWmbEiVUPoyJoYJLtkQOAh2mWtGsliwUXz9pa6Pbpg2XMnvMMtZmJGp5AmnBCwUbXd+BhmBNE7KpIoAv/+EA8OnGdlr4FF1lYybR1pFJRz41UfcILe2K8Qf/r5jS9/HQdBtAL5Rlj9EljaybfeBmijALcpctSXDWywgMm3boet2/uOGLrlRG4dU91JVS6rpyoxqk/t6GftRr+8NvHng9cRfJH20iPOo9yuYKFpkTAIVxJix7+UQlkQDp4JV3aAwLCf0mkzZ2KaSZMyE5fzkKrxrkQlOlLafBDxH2x0lyYyZZbFl1ibNaq0G/1UbF5CchiWXeQFM0rsfJYXAoHB9v3jCNaMgZjYhVHPrFdOUaELBXnnXLsFfHXk9uTga+N7A/3bcP/u8WMcWeoveoT3koxN0hr6iczRE1Dl0hs6VE7nH7g+XuPSO6nQWPW/QUrjiD5/sNGU=</latexit>
Attention•In seq-to-seqmodels, a single vector is connects encoding and decoding–Worrying?–All the input string information must encoded into a fixed-length vector–The decoder must recover all this information from a fixed-length vector •Attention relaxes the assumption that single vector must be used to encode the input sentence regardless of length
Attention•Encode input sentence as a sequence of vectors•At each step: pick what vector to use•But: discrete choice is not differentiable–Make the choice soft
Attention $0/%*5*0/&%(&/&3"5*0/8*5)"55&/5*0/ 
s0s1y1y2y3y4y5s2s3s4predictpredictpredictpredictpredict
concatconcatconcatconcatconcatattendattendattendattendattendtheblackfox jumped </s>
c0c1c4c2c3RD, ODRD, ODRD, ODRD, ODRD, OD
BIEBIEBIEBIEBIEE[<s>]<s>E[a]aE[conditioning]conditioningE[sequence]sequenceE[</s>]</s>E[<s>]<s>E[the]theE[black]blackE[fox]foxE[jumped]jumped
'JHVSF4FRVFODFUPTFRVFODF 3// HFOFSBUPS XJUI BUUFOUJPOUIBUJT
UIFZSFQSFTFOUBXJOEPXGPDVTFEBSPVOEUIFJOQVUJUFNxiBOEOPUUIFJUFNJUTFMG4FDPOE
CZ IBWJOH B USBJOBCMF FODPEJOH DPNQPOFOU UIBU JT USBJOFE KPJOUMZ XJUI UIF EFDPEFS
 UIF FODPEFSBOEEFDPEFSFWPMWFUPHFUIFSBOEUIFOFUXPSLDBOMFBSOUPFODPEFSFMFWBOUQSPQFSUJFTPGUIFJOQVUUIBUBSFVTFGVMGPSEFDPEJOH
BOEUIBUNBZOPUCFQSFTFOUBUUIFTPVSDFTFRVFODFx1WnEJSFDUMZ'PSFYBNQMF
UIFCJ3//FODPEFSNBZMFBSOUPFODPEFUIFQPTJUJPOPGxiXJUIJOUIFTFRVFODF
BOEUIFEFDPEFSDPVMEVTFUIJTJOGPSNBUJPOUPBDDFTTUIFFMFNFOUTJOPSEFS
PSMFBSOUPQBZNPSFBUUFOUJPOUP FMFNFOUT JO UIF CFHJOOJOH PG UIF TFRVFODF UIFO UP FMFNFOUT BU JUT FOE
Attention
 $0/%*5*0/&%(&/&3"5*0/8*5)"55&/5*0/ 
s0s1y1y2y3y4y5s2s3s4predictpredictpredictpredictpredict
concatconcatconcatconcatconcatattendattendattendattendattendtheblackfox jumped </s>
c0c1c4c2c3RD, ODRD, ODRD, ODRD, ODRD, OD
BIEBIEBIEBIEBIEE[<s>]<s>E[a]aE[conditioning]conditioningE[sequence]sequenceE[</s>]</s>E[<s>]<s>E[the]theE[black]blackE[fox]foxE[jumped]jumped
'JHVSF4FRVFODFUPTFRVFODF 3// HFOFSBUPS XJUI BUUFOUJPOUIBUJT
UIFZSFQSFTFOUBXJOEPXGPDVTFEBSPVOEUIFJOQVUJUFNxiBOEOPUUIFJUFNJUTFMG4FDPOE
CZ IBWJOH B USBJOBCMF FODPEJOH DPNQPOFOU UIBU JT USBJOFE KPJOUMZ XJUI UIF EFDPEFS
 UIF FODPEFSBOEEFDPEFSFWPMWFUPHFUIFSBOEUIFOFUXPSLDBOMFBSOUPFODPEFSFMFWBOUQSPQFSUJFTPGUIFJOQVUUIBUBSFVTFGVMGPSEFDPEJOH
BOEUIBUNBZOPUCFQSFTFOUBUUIFTPVSDFTFRVFODFx1WnEJSFDUMZ'PSFYBNQMF
UIFCJ3//FODPEFSNBZMFBSOUPFODPEFUIFQPTJUJPOPGxiXJUIJOUIFTFRVFODF
BOEUIFEFDPEFSDPVMEVTFUIJTJOGPSNBUJPOUPBDDFTTUIFFMFNFOUTJOPSEFS
PSMFBSOUPQBZNPSFBUUFOUJPOUP FMFNFOUT JO UIF CFHJOOJOH PG UIF TFRVFODF UIFO UP FMFNFOUT BU JUT FOE
AttendX=x1,...,xnsEi=RE(sEi 1,xi),i=1,...,nci=OE(sEi)¯↵ji=sDj 1·ci↵j=softmax(¯↵j1,...,¯↵jn)cj=nXi=1↵jicisDj=R(sDj 1,[ˆtj 1;cj])O(sDj) =softmax(sDjW+b)ˆtj= arg maxO(sDj)
<latexit sha1_base64="F9Zv6vf8f3p2j6ETYkMQmeGPtrc=">AAAEJ3icdVNbi9NAFJ5NvKz11tVHXw4WlxZrSURQkMKiFnzbVexuoWnDZDppp5sbMxNpCfk3vvhXfBFURB/9J86kF5JWBwJnzvnO933nkPGSgAlpWb8PDPPK1WvXD2/Ubt66fedu/ejeuYhTTmifxEHMBx4WNGAR7UsmAzpIOMWhF9AL7/K1rl98pFywOPoglwkdhXgaMZ8RLFXKPTK6A+jCMTghljPPzxa5a7edSSxFu5SKwHFqm7vIXTbuQfcY3ru9ZimbsSd2Pu61y2Ss1QamFDakFSaie3LNdFplUvwtBYSa42GeOThIZjgfz12msWXFuVZ8Aw5R5FCiZWWZbf9qUkkXMhOxL0O8yJs7CrZyXxiFnULUqjrfsIk0VEN0lQ01WoHWPv9nRXmeK796d839Odow3HqeYZnJfF16CWXhUWHltFklbWnWvekqkM3tIofHW0YvbxWb3hWeF7vGfKoqC9hX0x5qbr1hdaziwH5gr4MGWp8zt/5NrZekIY0kCbAQQ9tK5CjDXDIS0LzmpIImmFziKR2qMMIhFaOs+M9zeKQyE/Bjrr5IQpEtd2Q4FGIZegqpzYrdmk7+qzZMpf9ilLEoSSWNyErITwOQMehHAxPGKZHBUgWYcKa8AplhjolUT0svwd4deT84f9qxrY797lnj5NV6HYfoAXqImshGz9EJeovOUB8R45Pxxfhu/DA/m1/Nn+avFdQ4WPfcR5Vj/vkL/01bXg==</latexit><latexit sha1_base64="F9Zv6vf8f3p2j6ETYkMQmeGPtrc=">AAAEJ3icdVNbi9NAFJ5NvKz11tVHXw4WlxZrSURQkMKiFnzbVexuoWnDZDppp5sbMxNpCfk3vvhXfBFURB/9J86kF5JWBwJnzvnO933nkPGSgAlpWb8PDPPK1WvXD2/Ubt66fedu/ejeuYhTTmifxEHMBx4WNGAR7UsmAzpIOMWhF9AL7/K1rl98pFywOPoglwkdhXgaMZ8RLFXKPTK6A+jCMTghljPPzxa5a7edSSxFu5SKwHFqm7vIXTbuQfcY3ru9ZimbsSd2Pu61y2Ss1QamFDakFSaie3LNdFplUvwtBYSa42GeOThIZjgfz12msWXFuVZ8Aw5R5FCiZWWZbf9qUkkXMhOxL0O8yJs7CrZyXxiFnULUqjrfsIk0VEN0lQ01WoHWPv9nRXmeK796d839Odow3HqeYZnJfF16CWXhUWHltFklbWnWvekqkM3tIofHW0YvbxWb3hWeF7vGfKoqC9hX0x5qbr1hdaziwH5gr4MGWp8zt/5NrZekIY0kCbAQQ9tK5CjDXDIS0LzmpIImmFziKR2qMMIhFaOs+M9zeKQyE/Bjrr5IQpEtd2Q4FGIZegqpzYrdmk7+qzZMpf9ilLEoSSWNyErITwOQMehHAxPGKZHBUgWYcKa8AplhjolUT0svwd4deT84f9qxrY797lnj5NV6HYfoAXqImshGz9EJeovOUB8R45Pxxfhu/DA/m1/Nn+avFdQ4WPfcR5Vj/vkL/01bXg==</latexit><latexit sha1_base64="F9Zv6vf8f3p2j6ETYkMQmeGPtrc=">AAAEJ3icdVNbi9NAFJ5NvKz11tVHXw4WlxZrSURQkMKiFnzbVexuoWnDZDppp5sbMxNpCfk3vvhXfBFURB/9J86kF5JWBwJnzvnO933nkPGSgAlpWb8PDPPK1WvXD2/Ubt66fedu/ejeuYhTTmifxEHMBx4WNGAR7UsmAzpIOMWhF9AL7/K1rl98pFywOPoglwkdhXgaMZ8RLFXKPTK6A+jCMTghljPPzxa5a7edSSxFu5SKwHFqm7vIXTbuQfcY3ru9ZimbsSd2Pu61y2Ss1QamFDakFSaie3LNdFplUvwtBYSa42GeOThIZjgfz12msWXFuVZ8Aw5R5FCiZWWZbf9qUkkXMhOxL0O8yJs7CrZyXxiFnULUqjrfsIk0VEN0lQ01WoHWPv9nRXmeK796d839Odow3HqeYZnJfF16CWXhUWHltFklbWnWvekqkM3tIofHW0YvbxWb3hWeF7vGfKoqC9hX0x5qbr1hdaziwH5gr4MGWp8zt/5NrZekIY0kCbAQQ9tK5CjDXDIS0LzmpIImmFziKR2qMMIhFaOs+M9zeKQyE/Bjrr5IQpEtd2Q4FGIZegqpzYrdmk7+qzZMpf9ilLEoSSWNyErITwOQMehHAxPGKZHBUgWYcKa8AplhjolUT0svwd4deT84f9qxrY797lnj5NV6HYfoAXqImshGz9EJeovOUB8R45Pxxfhu/DA/m1/Nn+avFdQ4WPfcR5Vj/vkL/01bXg==</latexit><latexit sha1_base64="F9Zv6vf8f3p2j6ETYkMQmeGPtrc=">AAAEJ3icdVNbi9NAFJ5NvKz11tVHXw4WlxZrSURQkMKiFnzbVexuoWnDZDppp5sbMxNpCfk3vvhXfBFURB/9J86kF5JWBwJnzvnO933nkPGSgAlpWb8PDPPK1WvXD2/Ubt66fedu/ejeuYhTTmifxEHMBx4WNGAR7UsmAzpIOMWhF9AL7/K1rl98pFywOPoglwkdhXgaMZ8RLFXKPTK6A+jCMTghljPPzxa5a7edSSxFu5SKwHFqm7vIXTbuQfcY3ru9ZimbsSd2Pu61y2Ss1QamFDakFSaie3LNdFplUvwtBYSa42GeOThIZjgfz12msWXFuVZ8Aw5R5FCiZWWZbf9qUkkXMhOxL0O8yJs7CrZyXxiFnULUqjrfsIk0VEN0lQ01WoHWPv9nRXmeK796d839Odow3HqeYZnJfF16CWXhUWHltFklbWnWvekqkM3tIofHW0YvbxWb3hWeF7vGfKoqC9hX0x5qbr1hdaziwH5gr4MGWp8zt/5NrZekIY0kCbAQQ9tK5CjDXDIS0LzmpIImmFziKR2qMMIhFaOs+M9zeKQyE/Bjrr5IQpEtd2Q4FGIZegqpzYrdmk7+qzZMpf9ilLEoSSWNyErITwOQMehHAxPGKZHBUgWYcKa8AplhjolUT0svwd4deT84f9qxrY797lnj5NV6HYfoAXqImshGz9EJeovOUB8R45Pxxfhu/DA/m1/Nn+avFdQ4WPfcR5Vj/vkL/01bXg==</latexit>
Attention•Many variants of attention function–Dot product (previous slide)–MLP–Bi-linear transformation•Various ways to combine context vector into decoder computation•See Luong et al. 2015
A History of AI
Public Perception
Zaid Harchaoui
Countdown to the Rise of an Artificial Intelligence?

Perception of AI
The Mecha David in “A. I. Artiﬁcial Intelligence” (Spielberg, 2001)
Perception of AI
Terminators after the rise of Skynet “Terminator III” (Mostow, 2003)
Cybernetics
Norbert Wiener, 1894-19641947
1950
CyberneticsNon-linear Systems

Machines in Pop Culture, 1940-1950
Electro, crime-ﬁghting robot (Marvel, 1940)
wiener’s visionWiener’s insights 1.problem-solving machines 2.machines making machines 3.acceleration of  progress in making machines
wiener’s visionWiener’s insights 1.problem-solving machines 2.machines making machines 3.acceleration of  progress in making machines
Solving problemswith machines
Mechanical Turk chess-playing automaton, circa 1770
Fathers of AI
Dartmouth Summer Research Project on Artiﬁcial Intelligence (1956)
The Logic Theorist

The Logic Theorist

General Problem SolverComputer program that solves any problem that can be expressed  with well-formed formulas (Horn Clauses) •Towers of  Hanoi •Euclidean Geometry
Solving problemswith machines
Stalin vs Truman

Solving problemswith machines
David Levy vs Chess 4.6, 1978

Solving problemswith machines
Boris chess-player, 1979

Solving problemswith machines
Boris chess-player, 1980s

Solving problemswith machinesFantasies •diplomacy •generic sequential decision-making •machine à gouverner 
Solving problemswith machinesProblems •general problems (1959) •chess-playing •automatic translation •computer vision 
wiener’s visionWiener’s insights 1.problem-solving machines 2.machines making machines 3.acceleration of  progress in making machines
machines  making machines
Self-reproducing machines

Self-Improving machines

Self-Improving machines
Jet-Jaguar defeating Gigan “Godzilla vs Megalon” (Fukuda,1972)

Self-Improving machines
Jet-Jaguar and Godzilla greetings,“Godzilla vs Megalon” (Fukuda,1972)

Machine Learning
LeNet neural network for digit recognition (LeCun, 1989)

wiener’s visionWiener’s insights 1.problem-solving machines 2.machines making machines 3.acceleration of progress
Data
Collecting data through social computing and crowdsourcing
AMT
Large Datasets
ImageNet Image Classiﬁcation Dataset

Acceleration of Progress
ImageNet Image Classiﬁcation Dataset

AI Scientistsas Rock-Stars
AI and Yann LeCun in Rolling Stone magazine

Acceleration of ProgressStrategy •gather the most talented people •give unprecedented amount of  resources •focus on particular challenging problems •anticipate the outcomes of the technology
Future of AISymposium
http://cds.nyu.edu/ai/
Challenges of AI progressPotential challenges •Scientiﬁc: safety , control •Economic: jobs, employment •Societal: personal assistants, education •Ethical: values and norms for robots
Discriminative Trainingpart 2
Machine TranslationLecture 12 Instructor: Chris Callison-Burch TAs: Mitchell Stern, Justin Chiu Website: mt-class.org/penn
The Noisy Channel-log p(g | e)-log p(e)
As a Linear Model-log p(g | e)-log p(e)~w
As a Linear Model-log p(g | e)-log p(e)~w
As a Linear Model-log p(g | e)-log p(e)~w
As a Linear Model-log p(g | e)-log p(e)~w
gImprovement 1:change      to ﬁnd better translations~w
As a Linear Model-log p(g | e)-log p(e)~w
As a Linear Model-log p(g | e)-log p(e)~w
As a Linear Model-log p(g | e)-log p(e)~w
As a Linear Model-log p(g | e)-log p(e)~wImprovement 2:Add dimensions to make points separable
Linear Models•Improve the modeling capacity of the noisy channel in two ways•Reorient the weight vector•Add new dimensions (new features)•Questions•What features?•How do we set the weights?e⇤= arg maxew>h(g,e)
h(g,e)w
Parameter Learning
12
13h1
h2
Hypothesis Space
Hypotheses
14h1
h2
Hypothesis Space
References
Preliminaries
15We assume a decoder that computes:he⇤,a⇤i= arg maxhe,aiw>h(g,e,a)And K-best lists of, that is:{ e⇤i,a⇤i⇥}i=Ki=1= argith- maxhe,aiw>h(g,e,a)Standard, efﬁcient algorithms exist for this.
Cost-Sensitive Training•Assume we have a cost function that gives a score for how good/bad a translation is•Optimize the weight vector by making reference to this function•We will talk about two ways to do this16 (ˆe,E)⇥ [0,1]
MERT•Minimum Error Rate Training•Directly optimize for an automatic evaluation metric instead of likelihood•Maximize the BLEU score on a held out development set •Iteratively update the parameters by re-scoring n-best lists and comparing the highest scoring translation to the reference17

MERT•Even with 10-15 features it’s not possible to exhaustively search the space of possible feature values•We need a good heuristic method to search the space•Another problem: the initial parameters might be so bad that the original n-best list is not a good sample of the translations18
Iterative parameter tuning

Powell Search•Explore a high-dimensional space by ﬁnding a better point along one line in the space•Simplest form:  Vary one parameter at a time•If the optimal value is better than the current value, then change it and move to the next parameter•Iterate until there are no single parameter updates that increase the score
Powell Search•Problem: searching for the best value for a single parameter is still daunting•Parameters are real-valued #s, so they have an inﬁnite number of possible values•Key insight of MERT: only a small number of threshold values will change the 1-best translation•Only 1-best translations change BLEU
Finding the threshold points for 1 sentenceGiven weight vector    , any hypothesis          will have a (scalar) scorewhe,aim=w>h(g,e,a)m=(w+ v)>h(g,e,a)=w>h(g,e,a)|{z}b+ v>h(g,e,a)|{z}a=a +bwnew=w+ vmLinear function in 2D!Now pick a search vector v, and considerhow the score of this hypothesis will change:
MERT
23m 
MERT
24Recall our k-best set{ e⇤i,a⇤i⇥}Ki=1m 
MERT
25Recall our k-best set{ e⇤i,a⇤i⇥}Ki=1m 
MERT
26m 
MERT
27mhe⇤162,a⇤162ihe⇤28,a⇤28ihe⇤73,a⇤73i 
MERT
28m he⇤162,a⇤162ihe⇤28,a⇤28ihe⇤73,a⇤73i
MERT
29m he⇤162,a⇤162ihe⇤28,a⇤28ihe⇤73,a⇤73i
 errors
MERT
30m he⇤162,a⇤162ihe⇤28,a⇤28ihe⇤73,a⇤73i
 errors
MERT
31m  errors
MERT
32m  errors 
33 errorswnew= ⇤v+wLet ⇤
The effect on BLEU varying one parameter

The effect on BLEU varying one parameter

MERT•Minimum error rate training •Can maximize or minimize!•In practice “errors” are sufﬁcient statistics for evaluation metrics (e.g., BLEU,  AMBER, TER, etc)•Downside: MERT can only be used to optimize a small handful of features36
Training as Classiﬁcation•Pairwise Ranking Optimization•Reduce training problem to binary classiﬁcation with a linear model •Algorithm •For i=1 to N •Pick random pair of hypotheses (A,B) from K-best list•Use cost function to determine if is A or B better•Create ith training instance•Train binary linear classiﬁer37

38h1
h2~w
#2
#1K-Best List Example
#3
#4
#5
#6
#7
#8
#9
#10
39h1
h2#2#1K-Best List Example#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2~w
40h1
h2#2#1#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2h1h2
41h1
h2#2#1#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2h1h2Worse!
42h1
h2#2#1#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2h1h2Worse!
43h1
h2#2#1#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2h1h2
44h1
h2#2#1#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2h1h2Better!
45h1
h2#2#1#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2h1h2Better!
46h1
h2#2#1#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2h1h2Worse!
47h1
h2#2#1#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2h1h2Better!
48h1
h2#2#1#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2h1h2
49h1h2Fit a linear model
50h1h2Fit a linear model~w
51h1
h2#2#1K-Best List Example#3#4#5#6#7#8#9#100.8 <1.00.6 <0.80.4 <0.60.2 <0.40.0 <0.2~w
Summary•Evaluation metrics•Figure out how well we’re doing•Figure out if a feature helps•Train your system•What’s a great way to improve translation?•Improve evaluation!52
Reading•Read chapter 9 from the textbook•HW4 will be a discriminative re-ranking project

Announcements•HW3 has been released.  It is due a week from Thursday.•Upcoming: •T erm project (25% of your ﬁnal grade) and the   language research project (10%)•These are group projects (2-6 students), where the work scales to the group size•Speciﬁcations will be posted soon
•Problem description – similar to the descriptions on the HW assignments•Data collection – used to train a model, and  evaluate its performance•Objective function – score submissions on a leaderboard•Default system – An implementation of the simplest possible solution•Baseline system  –An implementation of a published baselineT erm project
•Gather monolingual and bilingual data for the language•Investigate where it is spoken, and what other languages its speakers are exposed to•Collect information about the syntax and morphology of the language•Describe its writing system•Create your own NLP tools for the language (# will vary by team size)Language Research
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  CS224D: Deep Learning for Natural Language Processing Andrew	
  Maas	
  Spring	
  2016	
  	
  	
  Neural	
  Networks	
  in	
  Speech	
  Recogni5on	
  	
  

Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Outline	
  • Speech	
  recogni?on	
  systems	
  overview	
  • HMM-­‐DNN	
  (Hybrid)	
  acous?c	
  modeling	
  • What’s	
  diﬀerent	
  about	
  modern	
  HMM-­‐DNNs?	
  • HMM-­‐free	
  RNN	
  recogni?on	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  
Cat	
  Clothes	
  Climbing	
  
Noise	
  Reduc?on	
  Transcrip?on	
  Are	
  there	
  any	
  good	
  robot	
  movies	
  I	
  can	
  rent	
  tonight?	
  Understanding	
  What	
  ac?on?	
  Is	
  the	
  user	
  annoyed?	
  Ask	
  for	
  clariﬁca?on?	
  Deep	
  Neural	
  Network	
  

Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Conversa?onal	
  Speech	
  Data	
  
300	
  hours	
  
4,870	
  speakers	
  
but	
  it	
  was	
  really	
  nice	
  to	
  get	
  back	
  with	
  a	
  telephone	
  and	
  the	
  city	
  and	
  everything	
  and	
  you	
  know	
  yeah	
  well	
  (i-­‐)	
  the	
  only	
  way	
  i	
  could	
  bear	
  it	
  was	
  to	
  (pass)	
  (some)	
  to	
  be	
  asleep	
  i	
  was	
  like	
  well	
  it	
  is	
  not	
  gonna	
  (be-­‐)	
  get	
  over	
  un?l	
  you	
  know	
  (w-­‐)	
  (w-­‐)	
  yeah	
  it	
  (re-­‐)	
  really	
  i	
  (th-­‐)	
  i	
  think	
  that	
  is	
  what	
  ruined	
  it	
  for	
  us	
  Switchboard	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Outline	
  • Speech	
  recogni?on	
  systems	
  overview	
  • HMM-­‐DNN	
  (Hybrid)	
  acous5c	
  modeling	
  • What’s	
  diﬀerent	
  about	
  modern	
  HMM-­‐DNNs?	
  • HMM-­‐free	
  RNN	
  recogni?on	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Acous?c	
  Modeling	
  with	
  GMMs	
  Samson	
  S	
  –	
  AE	
  –	
  M	
  –	
  S	
  –AH	
  –	
  N	
  942	
  –	
  6	
  –	
  37	
  –	
  8006	
  –	
  4422	
  …	
  Transcrip5on:	
  Pronuncia5on:	
  Sub-­‐phones	
  :	
  	
  Hidden	
  Markov	
  Model	
  (HMM):	
  	
  	
  Acous5c	
  Model:	
  	
  	
  Audio	
  Input:	
  Features	
  942	
  
Features	
  942	
  
Features	
  6	
  
!!!"!#!$%$#"!%%&$%&#%&"%&!%&'%&(%&)%&*
!!!"!#!$%$#"!%%&$%&#%&"%&!%&'%&(%&)%&*
!!!"!#!$%$#"!%%&$%&#%&"%&!%&'%&(%&)%&*GMM	
  models:	
  P(x|s)	
  x:	
  input	
  features	
  s:	
  HMM	
  state	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  DNN	
  Hybrid	
  Acous?c	
  Models	
  Samson	
  S	
  –	
  AE	
  –	
  M	
  –	
  S	
  –AH	
  –	
  N	
  942	
  –	
  6	
  –	
  37	
  –	
  8006	
  –	
  4422	
  …	
  Transcrip5on:	
  Pronuncia5on:	
  Sub-­‐phones	
  :	
  	
  Hidden	
  Markov	
  Model	
  (HMM):	
  	
  	
  	
  	
  Acous5c	
  Model:	
  	
  	
  	
  	
  	
  Audio	
  Input:	
  Features	
  (x1)	
  P(s|x1)	
  942	
  
Features	
  (x2)	
  P(s|x2)	
  942	
  
Features	
  (x3)	
  P(s|x3)	
  6	
  Use	
  a	
  DNN	
  to	
  approximate:	
  P(s|x)	
  	
  Apply	
  Bayes’	
  Rule:	
  P(x|s)	
  =	
  P(s|x)	
  *	
  P(x)	
  /	
  P(s)	
  	
  	
  	
  DNN	
  *	
  Constant	
  /	
  State	
  prior	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  
Not	
  Really	
  a	
  New	
  Idea	
  
Renals,	
  Morgan,	
  Bourland,	
  Cohen,	
  &	
  Franco.	
  1994.	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Modern	
  Systems	
  use	
  DNNs	
  and	
  Senones	
  
Dahl,	
  Yu,	
  Deng	
  &	
  Acero.	
  2011.	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Hybrid	
  Systems	
  now	
  Dominate	
  ASR	
  
Hinton	
  et	
  al.	
  2012.	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  What’s	
  Diﬀerent	
  in	
  Modern	
  DNNs?	
  • Fast	
  computers	
  =	
  run	
  many	
  experiments	
  • Deeper	
  nets	
  improve	
  on	
  shallow	
  nets	
  • Architecture	
  choices	
  (easiest	
  is	
  replacing	
  sigmoid)	
  • Pre-­‐training	
  ma#ers	
  very	
  li#le.	
  Ini?ally	
  we	
  thought	
  this	
  was	
  the	
  new	
  trick	
  that	
  made	
  things	
  work	
  • Many	
  more	
  parameters	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Depth	
  Magers	
  (Somewhat)	
  
Yu,	
  Seltzer,	
  Li,	
  Huang,	
  Seide.	
  2013.	
  Warning!	
  Depth	
  can	
  also	
  act	
  as	
  a	
  regularizer	
  because	
  it	
  makes	
  op?miza?on	
  more	
  diﬃcult.	
  This	
  is	
  why	
  you	
  will	
  some?mes	
  see	
  very	
  deep	
  networks	
  perform	
  well	
  on	
  TIMIT	
  or	
  other	
  small	
  tasks.	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Replacing	
  Sigmoid	
  Hidden	
  Units	
  
(Glorot	
  &	
  Bengio.	
  2011)	
  -­‐1	
  0	
  1	
  2	
  
-­‐4	
  0	
  4	
  TanH	
  ReL	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Comparing	
  Nonlineari?es	
  
0	
  5	
  10	
  15	
  20	
  25	
  
GMM	
  2	
  Layer	
  3	
  Lyaer	
  4	
  Layer	
  MSR	
  9	
  Layer	
  IBM	
  7	
  Layer	
  MMI	
  Switchboard	
  WER	
  TanH	
  ReL	
  
(Maas,	
  Qi,	
  Xie,	
  Hannun,	
  Lengerich,	
  Jurafsky,	
  &	
  Ng.	
  In	
  Submission)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Scaling	
  up	
  NN	
  acous?c	
  models	
  in	
  1999	
  
(Ellis	
  &	
  Morgan.	
  1999)	
  0.7M	
  total	
  NN	
  parameters	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Adding	
  More	
  Parameters	
  15	
  Years	
  Ago	
  Size	
  ma#ers:	
  An	
  empirical	
  study	
  of	
  neural	
  network	
  training	
  for	
  LVCSR.	
  Ellis	
  &	
  Morgan.	
  ICASSP.	
  1999.	
  	
  	
  Hybrid	
  NN.	
  1	
  hidden	
  layer.	
  54	
  HMM	
  states.	
  	
  74hr	
  broadcast	
  news	
  task	
  	
  “…improvements	
  are	
  almost	
  always	
  obtained	
  by	
  increasing	
  either	
  or	
  both	
  of	
  the	
  amount	
  of	
  training	
  data	
  or	
  the	
  number	
  of	
  network	
  parameters	
  …	
  We	
  are	
  now	
  planning	
  to	
  train	
  an	
  8000	
  hidden	
  unit	
  net	
  on	
  150	
  hours	
  of	
  data	
  …	
  this	
  training	
  will	
  require	
  over	
  three	
  weeks	
  of	
  computa?on.”	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Adding	
  More	
  Parameters	
  Now	
  	
  • Comparing	
  total	
  number	
  of	
  parameters	
  (in	
  millions)	
  of	
  previous	
  work	
  versus	
  our	
  new	
  experiments	
  
0	
  50	
  100	
  150	
  200	
  250	
  300	
  350	
  400	
  450	
  Total	
  DNN	
  parameters	
  (M)	
  
(Maas,	
  Qi,	
  Xie,	
  Hannun,	
  Lengerich,	
  Jurafsky,	
  &	
  Ng.	
  In	
  Submission)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Combining	
  Speech	
  Corpora	
  
300	
  hours	
  
2,000	
  hours	
  
4,870	
  speakers	
  
23,394	
  speakers	
  Switchboard	
  Fisher	
  
Combined	
  corpus	
  baseline	
  system	
  now	
  available	
  in	
  Kaldi	
  	
  (Maas,	
  Qi,	
  Xie,	
  Hannun,	
  Lengerich,	
  Jurafsky,	
  &	
  Ng.	
  In	
  Submission)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Scaling	
  Total	
  Parameters	
  
0	
  2	
  4	
  6	
  8	
  10	
  12	
  14	
  16	
  18	
  20	
  22	
  24	
  26	
  28	
  30	
  32	
  34	
  36	
  38	
  40	
  
GMM	
  36M	
  100M	
  200M	
  400M	
  20	
  25	
  30	
  35	
  40	
  45	
  50	
  
RT-­‐03	
  Word	
  Error	
  Rate	
  Model	
  Size	
  Frame	
  Error	
  Rate	
  	
  
(Maas,	
  Qi,	
  Xie,	
  Hannun,	
  Lengerich,	
  Jurafsky,	
  &	
  Ng.	
  In	
  Submission)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Scaling	
  Total	
  Parameters	
  
20	
  25	
  30	
  35	
  40	
  45	
  
GMM	
  36M	
  100M	
  200M	
  400M	
  20	
  25	
  30	
  35	
  40	
  45	
  50	
  
RT-­‐03	
  Word	
  Error	
  Rate	
  Model	
  Size	
  Frame	
  Error	
  Rate	
  	
  Frame	
  Error	
  Rate	
  Word	
  Error	
  Rate	
  
(Maas,	
  Qi,	
  Xie,	
  Hannun,	
  Lengerich,	
  Jurafsky,	
  &	
  Ng.	
  In	
  Submission)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Outline	
  • Speech	
  recogni?on	
  systems	
  overview	
  • HMM-­‐DNN	
  (Hybrid)	
  acous?c	
  modeling	
  • What’s	
  diﬀerent	
  about	
  modern	
  HMM-­‐DNNs?	
  • HMM-­‐free	
  RNN	
  recogni5on	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  HMM-­‐DNN	
  Speech	
  Recogni?on	
  Samson	
  S	
  –	
  AE	
  –	
  M	
  –	
  S	
  –AH	
  –	
  N	
  942	
  –	
  6	
  –	
  37	
  –	
  8006	
  –	
  4422	
  …	
  Transcrip5on:	
  Pronuncia5on:	
  Sub-­‐phones	
  :	
  	
  Hidden	
  Markov	
  Model	
  (HMM):	
  	
  	
  	
  	
  Acous5c	
  Model:	
  	
  	
  	
  	
  	
  Audio	
  Input:	
  Features	
  (x1)	
  P(s|x1)	
  942	
  
Features	
  (x2)	
  P(s|x2)	
  942	
  
Features	
  (x3)	
  P(s|x3)	
  6	
  Use	
  a	
  DNN	
  to	
  approximate:	
  P(s|x)	
  	
  Apply	
  Bayes’	
  Rule:	
  P(x|s)	
  =	
  P(s|x)	
  *	
  P(x)	
  /	
  P(s)	
  	
  	
  	
  DNN	
  *	
  Constant	
  /	
  State	
  prior	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  HMM-­‐Free	
  Recogni?on	
  Samson	
  S	
  –	
  AE	
  –	
  M	
  –	
  S	
  –AH	
  –	
  N	
  942	
  –	
  6	
  –	
  37	
  –	
  8006	
  –	
  4422	
  …	
  Transcrip5on:	
  Pronuncia5on:	
  Sub-­‐phones	
  :	
  	
  Hidden	
  Markov	
  Model	
  (HMM):	
  	
  	
  	
  	
  Acous5c	
  Model:	
  	
  	
  	
  	
  	
  Audio	
  Input:	
  Features	
  (x1)	
  P(s|x1)	
  942	
  
Features	
  (x2)	
  P(s|x2)	
  942	
  
Features	
  (x3)	
  P(s|x3)	
  6	
  
(Graves	
  &	
  Jaitly.	
  2014)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  HMM-­‐Free	
  Recogni?on	
  Samson	
  Transcrip5on:	
  	
  Characters:	
  	
  Collapsing	
  func5on:	
  	
  	
  	
  Acous5c	
  Model:	
  	
  	
  	
  	
  	
  Audio	
  Input:	
  Features	
  (x1)	
  P(a|x1)	
  
Features	
  (x2)	
  P(a|x2)	
  
Features	
  (x3)	
  P(a|x3)	
  Use	
  a	
  DNN	
  to	
  approximate:	
  P(a|x)	
  	
  The	
  distribu?on	
  over	
  characters	
  
(Graves	
  &	
  Jaitly.	
  2014)	
  S	
  S	
  _	
  SAMSON	
  	
  SS___AA_M_S___O___NNNN	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  CTC	
  Objec?ve	
  Func?on	
  Labels	
  at	
  each	
  ?me	
  index	
  are	
  condi?onally	
  independent	
  (like	
  HMMs)	
  	
  	
  Sum	
  over	
  all	
  ?me-­‐level	
  labelings	
  consistent	
  with	
  the	
  output	
  label.	
  	
  Output	
  label:	
  AB	
  Time-­‐level	
  labelings:	
  AB,	
  _AB,	
  A_B,	
  …	
  _A_B_	
  	
  Final	
  objec?ve	
  maximizes	
  probability	
  of	
  true	
  labels:	
  
(Graves	
  &	
  Jaitly.	
  2014)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Collapsing	
  Example	
  
(Hannun,	
  Maas,	
  Jurafsky,	
  &	
  Ng.	
  2014)	
  Per-­‐frame	
  argmax:	
  ____________________________________________________________________________________________________yy__ee_________g_	
  	
  ____________________________________________a_____	
  	
  _rr__e________hh__________b___ii_______lll__i_____g______aa______g_______iio__n___	
  	
  ___cc_____rrr_u_____________________	
  	
  ________ii___ss	
  	
  ______________o__________nn_____________hhh_a___________________nnddd	
  	
  ________________i__n___	
  __thh_e_____	
  	
  __________________________________________bb_uuii_______lllldd____ii____nng_____	
  	
  ___________________________________l___o___o_g__g___ii____nng______	
  	
  ____b___rr_ii________ck__s__________________________________________p___ll__a________ssg_________eerr__	
  	
  ______a___nnd_	
  	
  ___b___lll_uu____ee__pp___r___i________nnss_	
  	
  ________________f______oou____________rrr________	
  _____________f_____oo__rrr__g_y____	
  	
  _____t____www_oo__________	
  	
  	
  ____nn___ew___________________	
  	
  ______________________________________________________b___e_______t__________i____n___	
  	
  ____e________pp_____aa___rr___g____mm_ee___nnntss	
  	
  	
  _____________________________________________________________________________________________________________________________________	
  	
  	
  APer	
  collapsing:	
  yet	
  a	
  rehbilita?on	
  cru	
  is	
  onhand	
  in	
  the	
  building	
  loogging	
  bricks	
  plaster	
  and	
  blueprins	
  four	
  forty	
  two	
  new	
  be?n	
  epartments	
  	
  Reference:	
  yet	
  a	
  rehabilita?on	
  crew	
  is	
  on	
  hand	
  in	
  the	
  building	
  lugging	
  bricks	
  plaster	
  and	
  blueprints	
  for	
  forty	
  two	
  new	
  bedroom	
  apartments	
  	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Recurrence	
  Magers!	
  
Features	
  (x1)	
  P(a|x1)	
  
Features	
  (x2)	
  P(a|x2)	
  
Features	
  (x3)	
  P(a|x3)	
  S	
  S	
  _	
  Architecture	
  CER	
  DNN	
  22	
  +	
  recurrence	
  13	
  +	
  bi-­‐direc?onal	
  recurrence	
  10	
  
(Hannun,	
  Maas,	
  Jurafsky,	
  &	
  Ng.	
  2014)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Decoding	
  with	
  a	
  Language	
  Model	
  
Character	
  Probabili5es	
  Language	
  Model	
  p(“yeah”	
  |	
  “oh”)	
  __oo_h__y_e_aa_h	
  Lexicon	
  [a,	
  …,	
  zebra]	
  
(Hannun,	
  Maas,	
  Jurafsky,	
  &	
  Ng.	
  2014)	
  0	
  4	
  8	
  12	
  None	
  Lexicon	
  Bigram	
  Character	
  Error	
  Rate	
  
0	
  10	
  20	
  30	
  40	
  None	
  Lexicon	
  Bigram	
  Word	
  Error	
  Rate	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  
Character	
  Probabili5es	
  Language	
  Model	
  p(“yeah”	
  |	
  “oh”)	
  __oo_h__y_e_aa_h	
  Lexicon	
  [a,	
  …,	
  zebra]	
  syriza	
  abo-­‐-­‐	
  schmidhuber	
  bae	
  Out	
  of	
  Vocabulary	
  Words	
  sof-­‐-­‐	
  
Character	
  Probabili5es	
  Character	
  Language	
  Model	
  p(h	
  |	
  o,h,	
  ,y,e,a,)	
  __oo_h__y_e_aa_h	
  Rethinking	
  Decoding	
  
(Maas*,	
  Xie*,	
  Jurafsky,	
  &	
  Ng.	
  2015)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Lexicon-­‐Free	
  &	
  HMM-­‐Free	
  on	
  Switchboard	
  
0	
  5	
  10	
  15	
  20	
  25	
  30	
  35	
  40	
  
HMM-­‐GMM	
  CTC	
  No	
  LM	
  CTC	
  +	
  7-­‐gram	
  CTC	
  +	
  NN	
  LM	
  HMM-­‐DNN	
  (Maas*,	
  Xie*,	
  Jurafsky,	
  &	
  Ng.	
  2015)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Transcribing	
  Out	
  of	
  Vocabulary	
  Words	
  Truth:	
  yeah	
  i	
  went	
  into	
  the	
  i	
  do	
  not	
  know	
  what	
  you	
  think	
  of	
  ﬁdelity	
  but	
  HMM-­‐GMM:	
  yeah	
  when	
  the	
  i	
  don’t	
  know	
  what	
  you	
  think	
  of	
  ﬁdel	
  it	
  even	
  them	
  CTC-­‐CLM:	
  yeah	
  i	
  went	
  to	
  i	
  don’t	
  know	
  what	
  you	
  think	
  of	
  ﬁdelity	
  but	
  um	
  Truth:	
  no	
  no	
  speaking	
  of	
  weather	
  do	
  you	
  carry	
  a	
  al?meter	
  slash	
  barometer	
  HMM-­‐GMM:	
  no	
  i’m	
  not	
  all	
  being	
  the	
  weather	
  do	
  you	
  uh	
  carry	
  a	
  uh	
  helped	
  emiUers	
  last	
  brahms	
  her	
  CTC-­‐CLM:	
  no	
  no	
  bea?ng	
  of	
  whether	
  do	
  you	
  uh	
  carry	
  a	
  uh	
  a	
  5me	
  or	
  less	
  barometer	
  Truth:	
  i	
  would	
  ima-­‐	
  well	
  yeah	
  it	
  is	
  i	
  know	
  you	
  are	
  able	
  to	
  stay	
  home	
  with	
  them	
  HMM-­‐GMM:	
  i	
  would	
  amount	
  well	
  yeah	
  it	
  is	
  i	
  know	
  um	
  you’re	
  able	
  to	
  stay	
  home	
  with	
  them	
  CTC-­‐CLM:	
  i	
  would	
  ima-­‐	
  well	
  yeah	
  it	
  is	
  i	
  know	
  uh	
  you’re	
  able	
  to	
  stay	
  home	
  with	
  them	
  (Maas*,	
  Xie*,	
  Jurafsky,	
  &	
  Ng.	
  2015)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Comparing	
  Alignments	
  
(HMM	
  slide	
  from	
  Dan	
  Ellis)	
  
HMM-­‐GMM	
  phone	
  probabili?es	
  CTC	
  character	
  probabili?es	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Learning	
  Phonemes	
  and	
  Timing	
  
(Maas*,	
  Xie*,	
  Jurafsky,	
  &	
  Ng.	
  2015)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Learning	
  Phonemes	
  and	
  Timing	
  
(Maas*,	
  Xie*,	
  Jurafsky,	
  &	
  Ng.	
  2015)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Pushing	
  Performance	
  with	
  HMM-­‐Free	
  	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  CTC	
  now	
  powers	
  Google	
  search	
  ASR	
  • Context-­‐dependent	
  states	
  rather	
  than	
  characters	
  • Uni-­‐direc?onal	
  LSTM	
  for	
  faster	
  streaming	
  • CTC	
  +	
  sequence	
  discrimina?ve	
  loss	
  
hgp://googleresearch.blogspot.com/2015/09/google-­‐voice-­‐search-­‐faster-­‐and-­‐more.html	
  (Sak,	
  Senior,	
  Rao,	
  &	
  Beaufays.	
  2015)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  
Deep	
  Speech	
  2:	
  Scaling	
  up	
  CTC	
  • Eﬃcient	
  GPU	
  training	
  • Some	
  recurrent	
  architecture	
  variants	
  • Data	
  augmenta?on	
  • Works	
  on	
  both	
  English	
  and	
  Mandarin	
  
(Amodei	
  	
  et	
  al.	
  2015)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Listen,	
  agend,	
  and	
  spell	
  
(Chan,	
  Jaitly,	
  Le,	
  &	
  Vinyals.	
  2015)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Listen,	
  agend,	
  and	
  spell	
  
(Chan,	
  Jaitly,	
  Le,	
  &	
  Vinyals.	
  2015)	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  Conclusion	
  • HMM-­‐DNN	
  systems	
  are	
  now	
  the	
  default,	
  state-­‐of-­‐the-­‐art	
  for	
  speech	
  recogni?on	
  • We	
  roughly	
  understand	
  why	
  HMM-­‐DNNs	
  work	
  but	
  older,	
  shallow	
  hybrid	
  models	
  didn’t	
  work	
  as	
  well	
  • HMM-­‐Free	
  approaches	
  are	
  rapidly	
  improving	
  and	
  making	
  their	
  way	
  to	
  produc?on	
  systems	
  • It’s	
  a	
  very	
  exciEng	
  Eme	
  for	
  speech	
  recogniEon	
  
Andrew	
  Maas.	
  Stanford	
  CS224D.	
  2016	
  End	
  • More	
  on	
  spoken	
  language	
  understanding:	
  – cs224s.stanford.edu	
  • Open	
  source	
  speech	
  recogni?on	
  toolkit	
  (Kaldi):	
  – Kaldi.sf.net	
  • Mul?ple	
  open	
  source	
  implementa?ons	
  of	
  CTC	
  available	
  
Introduc)on	to	Informa)on	Retrieval	
		
		Introduc*on	to	
Informa(on	Retrieval	CS276	Informa*on	Retrieval	and	Web	Search	Chris	Manning	and	Pandu	Nayak	Evalua*on	
Introduc)on	to	Informa)on	Retrieval	
		
		
Situa*on	§ Thanks	to	your	stellar	performance	in	CS276,	you	quickly	rise	to	VP	of	Search	at	internet	retail	giant	nozama.com.	Your	boss	brings	in	her	nephew	Sergey, 	who	claims	to	have	built	a	beLer	search	engine	for	nozama.	Do	you	§ Laugh	derisively	and	send	him	to	rival	Tramlaw	Labs?	§ Counsel	Sergey	to	go	to	Stanford	and	take	CS276?	§ Try	a	few	queries	on	his	engine	and	say	“Not	bad”?	§ …	?	2	
Introduc)on	to	Informa)on	Retrieval	
		
		
3	What	could	you	ask	Sergey?	§ How	fast	does	it	index?	§ Number	of	documents/hour	§ Incremental	indexing	–	nozama	adds	10K	products/day	§ How	fast	does	it	search?	§ Latency	and	CPU	needs	for	nozama’s	5	million	products	§ Does	it	recommend	related	products?	§ This	is	all	good,	but	it	says	nothing	about	the	quality	of	Sergey’s	search	§ You	want	nozama’s	users	to	be	happy	with	the	search	experience	Sec. 8.6 
Introduc)on	to	Informa)on	Retrieval	
		
		
How	do	you	tell	if	users	are	happy?	§ Search	returns	products	relevant	to	users	§ How	do	you	assess	this	at	scale?	§ Search	results	get	clicked	a	lot	§ Misleading	*tles/summaries	can	cause	users	to	click	§ Users	buy	a^er	using	the	search	engine	§ Or,	users	spend	a	lot	of	$	a^er	using	the	search	engine	§ Repeat	visitors/buyers	§ Do	users	leave	soon	a^er	searching?	§ Do	they	come	back	within	a	week/month/…	?	4	
Introduc)on	to	Informa)on	Retrieval	
		
		
5	Happiness:	elusive	to	measure	§ Most	common	proxy:	relevance	of	search	results	§ But	how	do	you	measure	relevance?	§ Pioneered	by	Cyril	Cleverdon	in	the	Cranﬁeld	Experiments	Sec. 8.1 

Introduc)on	to	Informa)on	Retrieval	
		
		
6	Measuring	relevance	§ Three	elements:	1. A	benchmark	document	collec*on		2. A	benchmark	suite	of	queries	3. An	assessment	of	either	Relevant	or	Nonrelevant	for	each	query	and	each	document	Sec. 8.1 
Introduc)on	to	Informa)on	Retrieval	
		
		
So	you	want	to	measure	the	quality	of	a	new	search	algorithm	§ Benchmark	documents	–	nozama’s	products	§ Benchmark	query	suite	–	more	on	this	§ Judgments	of	document	relevance	for	each	query	
7	
5 million nozama.com products 50000 sample  queries 
Relevance	judgement	
Introduc)on	to	Informa)on	Retrieval	
		
		
Relevance	judgments	§ Binary	(relevant	vs.	non-relevant)	in	the	simplest	case,	more	nuanced	(0,	1,	2,	3	…)	in	others	§ What	are	some	issues	already?	§ 5	million	*mes	50K	takes	us	into	the	range	of	a	quarter	trillion	judgments	§ If	each	judgment	took	a	human	2.5	seconds,	we’d	s*ll	need	1011	seconds,	or	nearly	$300	million	if	you	pay	people	$10	per	hour	to	assess	§ 10K	new	products	per	day	8	
Introduc)on	to	Informa)on	Retrieval	
		
		
Crowd	source	relevance	judgments?	§ Present	query-document	pairs	to	low-cost	labor	on	online	crowd-sourcing	plalorms	§ Hope	that	this	is	cheaper	than	hiring	qualiﬁed	assessors	§ Lots	of	literature	on	using	crowd-sourcing	for	such	tasks	§ Main	takeaway	–	you	get	some	signal,	but	the	variance	in	the	resul*ng	judgments	is	very	high	
9	
Introduc)on	to	Informa)on	Retrieval	
		
		
10	Evalua*ng	an	IR	system	§ Note:	user	need	is	translated	into	a	query	§ Relevance	is	assessed	rela*ve	to	the	user	need,	not	the	query	§ E.g.,	Informa*on	need:	My	swimming	pool	bo;om	is	becoming	black	and	needs	to	be	cleaned.	§ Query:	pool	cleaner	§ Assess	whether	the	doc	addresses	the	underlying	need,	not	whether	it	has	these	words	Sec. 8.1 
Introduc)on	to	Informa)on	Retrieval	
		
		
11	What	else?	§ S*ll	need	test	queries	§ Must	be	germane	to	docs	available	§ Must	be	representa*ve	of	actual	user	needs	§ Random	query	terms	from	the	documents	generally	not	a	good	idea	§ Sample	from	query	logs	if	available	§ Classically	(non-Web)	§ Low	query	rates	–	not	enough	query	logs	§ Experts	hand-cra^	“user	needs”	Sec. 8.5 
Introduc)on	to	Informa)on	Retrieval	
		
		
12	Some	public	test	Collec*ons	
Sec. 8.5 
Typical	TREC	
Introduc)on	to	Informa)on	Retrieval	
		
		
Now	we	have	the	basics	of	a	benchmark	§ Let’s	review	some	evalua*on	measures	§ Precision	§ Recall	§ NDCG	§ …		
13	
Introduc)on	to	Informa)on	Retrieval	
		
		
14	Unranked	retrieval	evalua*on:	Precision	and	Recall	–	recap	from	IIR	8/video	§ Binary	assessments	Precision:	frac*on	of	retrieved	docs	that	are	relevant	=	P(relevant|retrieved)	Recall:	frac*on	of	relevant	docs	that	are	retrieved		=	P(retrieved|relevant)			§ Precision	P	=	tp/(tp	+	fp)	§ Recall						R	=	tp/(tp	+	fn)	Relevant Nonrelevant Retrieved tp fp Not Retrieved fn tn Sec. 8.3 
Introduc)on	to	Informa)on	Retrieval	
		
		Rank-Based Measures § Binary relevance § Precision@K (P@K) § Mean Average Precision (MAP) § Mean Reciprocal Rank (MRR) § Multiple levels of relevance § Normalized Discounted Cumulative Gain (NDCG) 
Introduc)on	to	Informa)on	Retrieval	
		
		Precision@K § Set a rank threshold K § Compute % relevant in top K § Ignores documents ranked lower than K § Ex:                   § Prec@3 of 2/3  § Prec@4 of 2/4 § Prec@5 of 3/5 § In similar fashion we have Recall@K 

Introduc)on	to	Informa)on	Retrieval	
		
		
17	A	precision-recall	curve	
0.00.20.40.60.81.0
0.00.20.40.60.81.0RecallPrecisionSec. 8.4 Lots more detail on this in the Coursera video 
Introduc)on	to	Informa)on	Retrieval	
		
		Mean Average Precision § Consider rank position of each relevant doc § K1, K2, … KR § Compute Precision@K for each K1, K2, … KR § Average precision = average of P@K § Ex:                    has AvgPrec of § MAP is Average Precision across multiple queries/rankings 
76.053321131≈⎟⎠⎞⎜⎝⎛++⋅
Introduc)on	to	Informa)on	Retrieval	
		
		
Average Precision 

Introduc)on	to	Informa)on	Retrieval	
		
		
MAP 

Introduc)on	to	Informa)on	Retrieval	
		
		
Mean	average	precision	§ If a relevant document never gets retrieved, we assume the precision corresponding to that relevant doc to be zero  § MAP is macro-averaging: each query counts equally § Now perhaps most commonly used measure in research papers § Good for web search? § MAP assumes user is interested in finding many relevant documents for each query § MAP requires many relevance judgments in text collection 
Introduc)on	to	Informa)on	Retrieval	
		
		BEYOND	BINARY	RELEVANCE	
22	
Introduc)on	to	Informa)on	Retrieval	
		
		
fair	fair	Good	
Introduc)on	to	Informa)on	Retrieval	
		
		
Discounted Cumulative Gain § Popular measure for evaluating web search and related tasks § Two assumptions: § Highly relevant documents are more useful than marginally relevant documents § the lower the ranked position of a relevant document, the less useful it is for the user, since it is less likely to be examined 
Introduc)on	to	Informa)on	Retrieval	
		
		
Discounted Cumulative Gain § Uses graded relevance as a measure of  usefulness, or gain, from examining a document § Gain is accumulated starting at the top of the ranking and may be reduced, or discounted, at lower ranks § Typical discount is 1/log (rank) § With base 2, the discount at rank 4 is 1/2, and at rank 8 it is 1/3 
Introduc)on	to	Informa)on	Retrieval	
		
		
26  Summarize a Ranking: DCG § What if relevance judgments are in a scale of [0,r]?  r>2 § Cumulative Gain (CG) at rank n § Let the ratings of the n documents be r1, r2, …rn (in ranked order) § CG = r1+r2+…rn § Discounted Cumulative Gain (DCG) at rank n § DCG = r1 + r2/log22 + r3/log23 + … rn/log2n § We may use any base for the logarithm 
Introduc)on	to	Informa)on	Retrieval	
		
		
Discounted Cumulative Gain § DCG is the total gain accumulated at a particular rank p: § Alternative formulation: § used by some web search companies § emphasis on retrieving highly relevant documents 

Introduc)on	to	Informa)on	Retrieval	
		
		
DCG Example § 10 ranked documents judged on 0-3 relevance scale:  3, 2, 3, 0, 0, 1, 2, 2, 3, 0 § discounted gain:  3, 2/1, 3/1.59, 0, 0, 1/2.59, 2/2.81, 2/3, 3/3.17, 0  = 3, 2, 1.89, 0, 0, 0.39, 0.71, 0.67, 0.95, 0 § DCG: 3, 5, 6.89, 6.89, 6.89, 7.28, 7.99, 8.66, 9.61, 9.61  
Introduc)on	to	Informa)on	Retrieval	
		
		
29  Summarize a Ranking: NDCG § Normalized Discounted Cumulative Gain (NDCG) at rank n § Normalize DCG at rank n by the DCG value at rank n of the ideal ranking § The ideal ranking would first return the documents with the highest relevance level, then the next highest relevance level, etc § Normalization useful for contrasting queries with varying numbers of relevant results  § NDCG is now quite popular in evaluating Web search 
Introduc)on	to	Informa)on	Retrieval	
		
		
NDCG - Example i	Ground	Truth	Ranking	Func*on1	Ranking	Func*on2	Document	Order	ri	Document	Order	ri	Document	Order	ri	1	d4	2	d3	2	d3	2	2	d3	2	d4	2	d2	1	3	d2	1	d2	1	d4	2	4	d1	0	d1	0	d1	0	NDCGGT=1.00	NDCGRF1=1.00	NDCGRF2=0.9203	6309.44log03log12log22222=⎟⎟⎠⎞⎜⎜⎝⎛+++=GTDCG6309.44log03log12log222221=⎟⎟⎠⎞⎜⎜⎝⎛+++=RFDCG2619.44log03log22log122222=⎟⎟⎠⎞⎜⎜⎝⎛+++=RFDCG6309.4==GTDCGMaxDCG4 documents: d1, d2, d3, d4 
Introduc)on	to	Informa)on	Retrieval	
		
		
31		What	if	the	results	are	not	in	a	list?	§ Suppose	there’s	only	one	Relevant	Document	§ Scenarios:		§ known-item	search	§ naviga*onal	queries	§ looking	for	a	fact	§ Search	dura*on	~	Rank	of	the	answer		§ measures	a	user’s	eﬀort		
Introduc)on	to	Informa)on	Retrieval	
		
		Mean Reciprocal Rank § Consider rank position, K, of first relevant doc § Could be – only clicked doc § Reciprocal Rank score =  § MRR is the mean RR across multiple queries    K1
Introduc)on	to	Informa)on	Retrieval	
		
		
Human	judgments	are	§ Expensive	§ Inconsistent	§ Between	raters	§ Over	*me	§ Decay	in	value	as	documents/query	mix	evolves	§ Not	always	representa*ve	of	“real	users”	§ Ra*ng	vis-à-vis	query,	vs	underlying	need	§ So	–	what	alterna*ves	do	we	have?	33		
Introduc)on	to	Informa)on	Retrieval	
		
		USING	USER	CLICKS	
34	
Introduc)on	to	Informa)on	Retrieval	
		
		
What	do	clicks	tell	us?	
35	#	of	clicks	received	
Strong position bias, so absolute click rates unreliable 
Introduc)on	to	Informa)on	Retrieval	
		
		
Rela*ve	vs	absolute	ra*ngs	
36	
Hard to conclude Result1 > Result3 Probably can conclude Result3 > Result2 User’s click sequence 
Introduc)on	to	Informa)on	Retrieval	
		
		
Pairwise	rela*ve	ra*ngs	§ Pairs	of	the	form:	DocA	beLer	than	DocB	for	a	query	§ Doesn’t	mean	that	DocA	relevant	to	query	§ Now,	rather	than	assess	a	rank-ordering	wrt	per-doc	relevance	assessments	§ Assess	in	terms	of	conformance	with	historical	pairwise	preferences	recorded	from	user	clicks	
37	
Introduc)on	to	Informa)on	Retrieval	
		
		
A/B	tes*ng	at	web	search	engines	§ Purpose:	Test	a	single	innova*on		§ Prerequisite:	You	have	a	large	search	engine	up	and	running.	§ Have	most	users	use	old	system	§ Divert	a	small	propor*on	of	traﬃc	(e.g.,	1%)	to	an	experiment	to	evaluate	an	innova*on	§ Full	page	experiment	§ Interleaved	experiment	38	Sec. 8.6.3 
Introduc)on	to	Informa)on	Retrieval	
		
		
Comparing	two	rankings	via	clicks	(Joachims	2002)	
39	
Kernel	machines	
SVM-light		
Lucent	SVM	demo	
Royal	Holl.	SVM	
SVM	so^ware	
SVM	tutorial	
Kernel	machines	
SVMs	
Intro	to	SVMs	
Archives	of	SVM	
SVM-light	
SVM	so^ware	Query: [support vector machines] Ranking A Ranking B 
Introduc)on	to	Informa)on	Retrieval	
		
		
Interleave	the	two	rankings	
40	
Kernel	machines	
SVM-light		
Lucent	SVM	demo	
Royal	Holl.	SVM	
Kernel	machines	
SVMs	
Intro	to	SVMs	
Archives	of	SVM	
SVM-light	This interleaving starts with B … 
Introduc)on	to	Informa)on	Retrieval	
		
		
Remove	duplicate	results	
41	
Kernel	machines	
SVM-light		
Lucent	SVM	demo	
Royal	Holl.	SVM	
Kernel	machines	
SVMs	
Intro	to	SVMs	
Archives	of	SVM	
SVM-light	… 
Introduc)on	to	Informa)on	Retrieval	
		
		
Count	user	clicks	
42	
Kernel	machines	
SVM-light		
Lucent	SVM	demo	
Royal	Holl.	SVM	
Kernel	machines	
SVMs	
Intro	to	SVMs	
Archives	of	SVM	
SVM-light	… 
Clicks Ranking A: 3 Ranking B: 1 A, B A A 
Introduc)on	to	Informa)on	Retrieval	
		
		
Interleaved	ranking		§ Present	interleaved	ranking	to	users	§ Start	randomly	with	ranking	A	or	ranking	B	to	evens	out	presenta*on	bias	§ Count	clicks	on	results	from	A	versus	results	from	B		§ BeLer	ranking	will	(on	average)	get	more	clicks	
43	
Introduc)on	to	Informa)on	Retrieval	
		
		
Facts/en**es	(what	happens	to	clicks?)	
44	

Introduc)on	to	Informa)on	Retrieval	
		
		
Comparing	two	rankings	to	a	baseline	ranking	§ Given	a	set	of	pairwise	preferences	P	§ We	want	to	measure	two	rankings	A	and	B	§ Deﬁne	a	proximity	measure	between	A	and	P	§ And	likewise,	between	B	and	P	§ Want	to	declare	the	ranking	with	beLer	proximity	to	be	the	winner	§ Proximity	measure	should	reward	agreements	with	P	and	penalize	disagreements	45	
Introduc)on	to	Informa)on	Retrieval	
		
		
Kendall	tau	distance	§ Let	X	be	the	number	of	agreements	between	a	ranking	(say	A)	and	P	§ Let	Y	be	the	number	of	disagreements	§ Then	the	Kendall	tau	distance	between	A	and	P	is		(X-Y)/(X+Y)	§ Say	P	=	{(1,2),	(1,3),	(1,4),	(2,3),	(2,4),	(3,4))}	and	A=(1,3,2,4)	§ Then	X=5,	Y=1	…	§ (What	are	the	minimum	and	maximum	possible	values	of	the	Kendall	tau	distance?)	46	
Introduc)on	to	Informa)on	Retrieval	
		
		
Recap	§ Benchmarks	consist	of	§ Document	collec*on	§ Query	set	§ Assessment	methodology	§ Assessment	methodology	can	use	raters,	user	clicks,	or	a	combina*on	§ These	get	quan*zed	into	a	goodness	measure	–	Precision/NDCG	etc.	§ Diﬀerent	engines/algorithms	compared	on	a	benchmark	together	with	a	goodness	measure	47	
1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  Introduc*on	
  to	
  
Informa(on	
  Retrieval	
  CS276:	
  Informa*on	
  Retrieval	
  and	
  Web	
  Search	
  Pandu	
  Nayak	
  and	
  Prabhakar	
  Raghavan	
  Lecture	
  4:	
  Index	
  Construc*on	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Plan	
  § Last	
  lecture:	
  § Dic*onary	
  data	
  structures	
  § Tolerant	
  retrieval	
  § Wildcards	
  § Spell	
  correc*on	
  § Soundex	
  § This	
  *me:	
  § Index	
  construc*on	
  a-hu hy-m n-z 
mo on among $m mace abandon amortize madden among 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Index	
  construc*on	
  § How	
  do	
  we	
  construct	
  an	
  index?	
  § What	
  strategies	
  can	
  we	
  use	
  with	
  limited	
  main	
  memory?	
  Ch. 4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Hardware	
  basics	
  § Many	
  design	
  decisions	
  in	
  informa*on	
  retrieval	
  are	
  based	
  on	
  the	
  characteris*cs	
  of	
  hardware	
  § We	
  begin	
  by	
  reviewing	
  hardware	
  basics	
  Sec. 4.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Hardware	
  basics	
  § Access	
  to	
  data	
  in	
  memory	
  is	
  much	
  faster	
  than	
  access	
  to	
  data	
  on	
  disk.	
  § Disk	
  seeks:	
  No	
  data	
  is	
  transferred	
  from	
  disk	
  while	
  the	
  disk	
  head	
  is	
  being	
  posi*oned.	
  § Therefore:	
  Transferring	
  one	
  large	
  chunk	
  of	
  data	
  from	
  disk	
  to	
  memory	
  is	
  faster	
  than	
  transferring	
  many	
  small	
  chunks.	
  § Disk	
  I/O	
  is	
  block-­‐based:	
  Reading	
  and	
  wri*ng	
  of	
  en*re	
  blocks	
  (as	
  opposed	
  to	
  smaller	
  chunks).	
  § Block	
  sizes:	
  8KB	
  to	
  256	
  KB.	
  Sec. 4.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Hardware	
  basics	
  § Servers	
  used	
  in	
  IR	
  systems	
  now	
  typically	
  have	
  several	
  GB	
  of	
  main	
  memory,	
  some*mes	
  tens	
  of	
  GB.	
  	
  § Available	
  disk	
  space	
  is	
  several	
  (2–3)	
  orders	
  of	
  magnitude	
  larger.	
  § Fault	
  tolerance	
  is	
  very	
  expensive:	
  It`s	
  much	
  cheaper	
  to	
  use	
  many	
  regular	
  machines	
  rather	
  than	
  one	
  fault	
  tolerant	
  machine.	
  Sec. 4.1 
2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Hardware	
  assump*ons	
  for	
  this	
  lecture	
  § symbol	
  	
  sta(s(c	
  	
  	
  	
  	
  	
  value	
  § s	
  	
  average	
  seek	
  *me	
  	
  	
  	
  5	
  ms	
  =	
  5	
  x	
  10−3	
  s	
  § b	
  	
  	
  transfer	
  *me	
  per	
  byte	
  	
  0.02	
  μs	
  =	
  2	
  x	
  10−8	
  s	
  § 	
  	
  	
  	
  	
  	
  	
  	
  processor`s	
  clock	
  rate	
  	
  109	
  s−1	
  § p	
  	
  low-­‐level	
  opera*on	
  	
  	
  0.01	
  μs	
  =	
  10−8	
  s	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  (e.g.,	
  compare	
  &	
  swap	
  a	
  word)	
  § 	
  	
  	
  	
  	
  	
  	
  	
  size	
  of	
  main	
  memory	
  	
  	
  several	
  GB	
  § 	
  	
  	
  	
  	
  	
  	
  	
  size	
  of	
  disk	
  space	
  	
  	
  	
  1	
  TB	
  or	
  more	
  Sec. 4.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
RCV1:	
  Our	
  collec*on	
  for	
  this	
  lecture	
  § Shakespeare`s	
  collected	
  works	
  deﬁnitely	
  aren`t	
  large	
  enough	
  for	
  demonstra*ng	
  many	
  of	
  the	
  points	
  in	
  this	
  course.	
  § The	
  collec*on	
  we`ll	
  use	
  isn`t	
  really	
  large	
  enough	
  either,	
  but	
  it`s	
  publicly	
  available	
  and	
  is	
  at	
  least	
  a	
  more	
  plausible	
  example.	
  § As	
  an	
  example	
  for	
  applying	
  scalable	
  index	
  construc*on	
  algorithms,	
  we	
  will	
  use	
  the	
  Reuters	
  RCV1	
  collec*on.	
  § This	
  is	
  one	
  year	
  of	
  Reuters	
  newswire	
  (part	
  of	
  1995	
  and	
  1996)	
  Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
A	
  Reuters	
  RCV1	
  document	
  
Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Reuters	
  RCV1	
  sta*s*cs	
  § symbol	
  sta(s(c	
  	
  	
  	
  	
  	
  	
  value	
  § N	
  	
  	
  	
  documents	
  	
  	
  	
  	
  	
  800,000	
  § L	
  	
  	
  	
  avg.	
  #	
  tokens	
  per	
  doc	
  	
  	
  200	
  § M	
  	
  	
  terms	
  (=	
  word	
  types)	
  	
  	
  400,000	
  § 	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  avg.	
  #	
  bytes	
  per	
  token	
  	
  6	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  (incl.	
  spaces/punct.)	
  § 	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  avg.	
  #	
  bytes	
  per	
  token	
  	
  4.5	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  (without	
  spaces/punct.)	
  § 	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  avg.	
  #	
  bytes	
  per	
  term	
  	
  7.5	
  § 	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  non-­‐posi*onal	
  pos*ngs	
  100,000,000	
  4.5 bytes per word token vs. 7.5 bytes per word type: why? Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
§ Documents	
  are	
  parsed	
  to	
  extract	
  words	
  and	
  these	
  are	
  saved	
  with	
  the	
  Document	
  ID.	
  I did enact Julius Caesar I was killed  i' the Capitol;  Brutus killed me. Doc 1 So let it be with Caesar. The noble Brutus hath told you Caesar was ambitious Doc 2 
Recall	
  IIR	
  1	
  index	
  construc*on	
  TermDoc #I1did1enact 1julius1caesar1I1was1killed1i'1the1capitol1brutus1killed1me1so2let2it2be2with2caesar2the2noble 2brutus2hath 2told 2you2caesar 2was2ambitious2Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
TermDoc #I1did1enact 1julius1caesar1I1was1killed1i'1the1capitol1brutus1killed1me1so2let2it2be2with2caesar2the2noble 2brutus2hath 2told 2you2caesar 2was2ambitious2TermDoc #ambitious2be2brutus1brutus 2capitol1caesar1caesar2caesar2did1enact1hath1I1I 1i'1it2julius1killed1killed1let2me1noble2so2the1the 2told2you2was1was2with2	
  Key	
  step	
  § Ager	
  all	
  documents	
  have	
  been	
  parsed,	
  the	
  inverted	
  ﬁle	
  is	
  sorted	
  by	
  terms.	
  	
  We focus on this sort step. We have 100M items to sort. Sec. 4.2 
3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Scaling	
  index	
  construc*on	
  § In-­‐memory	
  index	
  construc*on	
  does	
  not	
  scale	
  § Can`t	
  stuﬀ	
  en*re	
  collec*on	
  into	
  memory,	
  sort,	
  then	
  write	
  back	
  § How	
  can	
  we	
  construct	
  an	
  index	
  for	
  very	
  large	
  collec*ons?	
  § Taking	
  into	
  account	
  the	
  hardware	
  constraints	
  we	
  just	
  learned	
  about	
  .	
  .	
  .	
  § Memory,	
  disk,	
  speed,	
  etc.	
  Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Sort-­‐based	
  index	
  construc*on	
  § As	
  we	
  build	
  the	
  index,	
  we	
  parse	
  docs	
  one	
  at	
  a	
  *me.	
  § While	
  building	
  the	
  index,	
  we	
  cannot	
  easily	
  exploit	
  compression	
  tricks	
  	
  	
  (you	
  can,	
  but	
  much	
  more	
  complex)	
  § The	
  ﬁnal	
  pos*ngs	
  for	
  any	
  term	
  are	
  incomplete	
  un*l	
  the	
  end.	
  § At	
  12	
  bytes	
  per	
  non-­‐posi*onal	
  pos*ngs	
  entry	
  (term,	
  doc,	
  freq),	
  demands	
  a	
  lot	
  of	
  space	
  for	
  large	
  collec*ons.	
  § T	
  =	
  100,000,000	
  in	
  the	
  case	
  of	
  RCV1	
  § So	
  …	
  we	
  can	
  do	
  this	
  in	
  memory	
  in	
  2009,	
  but	
  typical	
  collec*ons	
  are	
  much	
  larger.	
  	
  E.g.,	
  the	
  New	
  York	
  Times	
  provides	
  an	
  index	
  of	
  >150	
  years	
  of	
  newswire	
  § Thus:	
  We	
  need	
  to	
  store	
  intermediate	
  results	
  on	
  disk.	
  Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Sort	
  using	
  disk	
  as	
  lmemoryz?	
  § Can	
  we	
  use	
  the	
  same	
  index	
  construc*on	
  algorithm	
  for	
  larger	
  collec*ons,	
  but	
  by	
  using	
  disk	
  instead	
  of	
  memory?	
  § No:	
  Sor*ng	
  T	
  =	
  100,000,000	
  records	
  on	
  disk	
  is	
  too	
  slow	
  –	
  too	
  many	
  disk	
  seeks.	
  § We	
  need	
  an	
  external	
  sor*ng	
  algorithm.	
  Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Bomleneck	
  § Parse	
  and	
  build	
  pos*ngs	
  entries	
  one	
  doc	
  at	
  a	
  *me	
  § Now	
  sort	
  pos*ngs	
  entries	
  by	
  term	
  (then	
  by	
  doc	
  within	
  each	
  term)	
  § Doing	
  this	
  with	
  random	
  disk	
  seeks	
  would	
  be	
  too	
  slow	
  –	
  must	
  sort	
  T=100M	
  records	
  If every comparison took 2 disk seeks, and N items could be sorted with N log2N comparisons, how long would this take? Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
BSBI:	
  Blocked	
  sort-­‐based	
  Indexing	
  (Sor*ng	
  with	
  fewer	
  disk	
  seeks)	
  § 12-­‐byte	
  (4+4+4)	
  records	
  (term,	
  doc,	
  freq).	
  § These	
  are	
  generated	
  as	
  we	
  parse	
  docs.	
  § Must	
  now	
  sort	
  100M	
  such	
  12-­‐byte	
  records	
  by	
  term.	
  § Deﬁne	
  a	
  Block	
  ~	
  10M	
  such	
  records	
  § Can	
  easily	
  ﬁt	
  a	
  couple	
  into	
  memory.	
  § Will	
  have	
  10	
  such	
  blocks	
  to	
  start	
  with.	
  § Basic	
  idea	
  of	
  algorithm:	
  § Accumulate	
  pos*ngs	
  for	
  each	
  block,	
  sort,	
  write	
  to	
  disk.	
  § Then	
  merge	
  the	
  blocks	
  into	
  one	
  long	
  sorted	
  order.	
  Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Sec. 4.2 
4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Sor*ng	
  10	
  blocks	
  of	
  10M	
  records	
  
§ First,	
  read	
  each	
  block	
  and	
  sort	
  within:	
  	
  	
  § Quicksort	
  takes	
  2N	
  ln	
  N	
  expected	
  steps	
  § In	
  our	
  case	
  2	
  x	
  (10M	
  ln	
  10M)	
  steps	
  § Exercise:	
  es)mate	
  total	
  )me	
  to	
  read	
  each	
  block	
  from	
  disk	
  and	
  and	
  quicksort	
  it.	
  § 10	
  *mes	
  this	
  es*mate	
  –	
  gives	
  us	
  10	
  sorted	
  runs	
  of	
  10M	
  records	
  each.	
  § Done	
  straighqorwardly,	
  need	
  2	
  copies	
  of	
  data	
  on	
  disk	
  § But	
  can	
  op*mize	
  this	
  Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
How	
  to	
  merge	
  the	
  sorted	
  runs?	
  § Can	
  do	
  binary	
  merges,	
  with	
  a	
  merge	
  tree	
  of	
  log210	
  =	
  4	
  layers.	
  § During	
  each	
  layer,	
  read	
  into	
  memory	
  runs	
  in	
  blocks	
  of	
  10M,	
  merge,	
  write	
  back.	
  
Disk 1 3 4 2 2 1 4 3 Runs being merged. Merged run. Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
How	
  to	
  merge	
  the	
  sorted	
  runs?	
  § But	
  it	
  is	
  more	
  eﬃcient	
  to	
  do	
  a	
  mul*-­‐way	
  merge,	
  where	
  you	
  are	
  reading	
  from	
  all	
  blocks	
  simultaneously	
  § Providing	
  you	
  read	
  decent-­‐sized	
  chunks	
  of	
  each	
  block	
  into	
  memory	
  and	
  then	
  write	
  out	
  a	
  decent-­‐sized	
  output	
  chunk,	
  then	
  you`re	
  not	
  killed	
  by	
  disk	
  seeks	
  Sec. 4.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Remaining	
  problem	
  with	
  sort-­‐based	
  algorithm	
  § Our	
  assump*on	
  was:	
  we	
  can	
  keep	
  the	
  dic*onary	
  in	
  memory.	
  § We	
  need	
  the	
  dic*onary	
  (which	
  grows	
  dynamically)	
  in	
  order	
  to	
  implement	
  a	
  term	
  to	
  termID	
  mapping.	
  § Actually,	
  we	
  could	
  work	
  with	
  term,docID	
  pos*ngs	
  instead	
  of	
  termID,docID	
  pos*ngs	
  .	
  .	
  .	
  § .	
  .	
  .	
  but	
  then	
  intermediate	
  ﬁles	
  become	
  very	
  large.	
  (We	
  would	
  end	
  up	
  with	
  a	
  scalable,	
  but	
  very	
  slow	
  index	
  construc*on	
  method.)	
  Sec. 4.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
SPIMI:	
  	
  Single-­‐pass	
  in-­‐memory	
  indexing	
  § Key	
  idea	
  1:	
  Generate	
  separate	
  dic*onaries	
  for	
  each	
  block	
  –	
  no	
  need	
  to	
  maintain	
  term-­‐termID	
  mapping	
  across	
  blocks.	
  § Key	
  idea	
  2:	
  Don`t	
  sort.	
  Accumulate	
  pos*ngs	
  in	
  pos*ngs	
  lists	
  as	
  they	
  occur.	
  § With	
  these	
  two	
  ideas	
  we	
  can	
  generate	
  a	
  complete	
  inverted	
  index	
  for	
  each	
  block.	
  § These	
  separate	
  indexes	
  can	
  then	
  be	
  merged	
  into	
  one	
  big	
  index.	
  Sec. 4.3 
5 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
	
  SPIMI-­‐Invert	
  
§ Merging	
  of	
  blocks	
  is	
  analogous	
  to	
  BSBI.	
  
Sec. 4.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
SPIMI:	
  Compression	
  § Compression	
  makes	
  SPIMI	
  even	
  more	
  eﬃcient.	
  § Compression	
  of	
  terms	
  § Compression	
  of	
  pos*ngs	
  § See	
  next	
  lecture	
  Sec. 4.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Distributed	
  indexing	
  § For	
  web-­‐scale	
  indexing	
  (don`t	
  try	
  this	
  at	
  home!):	
  must	
  use	
  a	
  distributed	
  compu*ng	
  cluster	
  § Individual	
  machines	
  are	
  fault-­‐prone	
  § Can	
  unpredictably	
  slow	
  down	
  or	
  fail	
  § How	
  do	
  we	
  exploit	
  such	
  a	
  pool	
  of	
  machines?	
  Sec. 4.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Web	
  search	
  engine	
  data	
  centers	
  § Web	
  search	
  data	
  centers	
  (Google,	
  Bing,	
  Baidu)	
  mainly	
  contain	
  commodity	
  machines.	
  § Data	
  centers	
  are	
  distributed	
  around	
  the	
  world.	
  § Es*mate:	
  Google	
  ~1	
  million	
  servers,	
  3	
  million	
  processors/cores	
  (Gartner	
  2007)	
  Sec. 4.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Massive	
  data	
  centers	
  § If	
  in	
  a	
  non-­‐fault-­‐tolerant	
  system	
  with	
  1000	
  nodes,	
  each	
  node	
  has	
  99.9%	
  up*me,	
  what	
  is	
  the	
  up*me	
  of	
  the	
  system?	
  § Answer:	
  63%	
  § Exercise:	
  Calculate	
  the	
  number	
  of	
  servers	
  failing	
  per	
  minute	
  for	
  an	
  installa*on	
  of	
  1	
  million	
  servers.	
  Sec. 4.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Distributed	
  indexing	
  § Maintain	
  a	
  master	
  machine	
  direc*ng	
  the	
  indexing	
  job	
  –	
  considered	
  lsafez.	
  § Break	
  up	
  indexing	
  into	
  sets	
  of	
  (parallel)	
  tasks.	
  § Master	
  machine	
  assigns	
  each	
  task	
  to	
  an	
  idle	
  machine	
  from	
  a	
  pool.	
  Sec. 4.4 
6 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Parallel	
  tasks	
  § We	
  will	
  use	
  two	
  sets	
  of	
  parallel	
  tasks	
  § Parsers	
  § Inverters	
  § Break	
  the	
  input	
  document	
  collec*on	
  into	
  splits	
  § Each	
  split	
  is	
  a	
  subset	
  of	
  documents	
  (corresponding	
  to	
  blocks	
  in	
  BSBI/SPIMI)	
  Sec. 4.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Parsers	
  § Master	
  assigns	
  a	
  split	
  to	
  an	
  idle	
  parser	
  machine	
  § Parser	
  reads	
  a	
  document	
  at	
  a	
  *me	
  and	
  emits	
  (term,	
  doc)	
  pairs	
  § Parser	
  writes	
  pairs	
  into	
  j	
  par**ons	
  § Each	
  par**on	
  is	
  for	
  a	
  range	
  of	
  terms`	
  ﬁrst	
  lemers	
  § (e.g.,	
  a-­‐f,	
  g-­‐p,	
  q-­‐z)	
  –	
  here	
  j	
  =	
  3.	
  § Now	
  to	
  complete	
  the	
  index	
  inversion	
  Sec. 4.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Inverters	
  § An	
  inverter	
  collects	
  all	
  (term,doc)	
  pairs	
  (=	
  pos*ngs)	
  for	
  one	
  term-­‐par**on.	
  § Sorts	
  and	
  writes	
  to	
  pos*ngs	
  lists	
  Sec. 4.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Data	
  ﬂow	
  
splits Parser Parser Parser Master a-f g-p q-z a-f g-p q-z a-f g-p q-z Inverter Inverter Inverter Postings a-f g-p q-z assign assign 
Map phase Segment files Reduce phase Sec. 4.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
MapReduce	
  § The	
  index	
  construc*on	
  algorithm	
  we	
  just	
  described	
  is	
  an	
  instance	
  of	
  MapReduce.	
  § MapReduce	
  (Dean	
  and	
  Ghemawat	
  2004)	
  is	
  a	
  robust	
  and	
  conceptually	
  simple	
  framework	
  for	
  distributed	
  compu*ng	
  …	
  § …	
  without	
  having	
  to	
  write	
  code	
  for	
  the	
  distribu*on	
  part.	
  § They	
  describe	
  the	
  Google	
  indexing	
  system	
  (ca.	
  2002)	
  as	
  consis*ng	
  of	
  a	
  number	
  of	
  phases,	
  each	
  implemented	
  in	
  MapReduce.	
  Sec. 4.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
MapReduce	
  § Index	
  construc*on	
  was	
  just	
  one	
  phase.	
  § Another	
  phase:	
  transforming	
  a	
  term-­‐par**oned	
  index	
  into	
  a	
  document-­‐par**oned	
  index.	
  § Term-­‐par))oned:	
  one	
  machine	
  handles	
  a	
  subrange	
  of	
  terms	
  § Document-­‐par))oned:	
  one	
  machine	
  handles	
  a	
  subrange	
  of	
  documents	
  § As	
  we`ll	
  discuss	
  in	
  the	
  web	
  part	
  of	
  the	
  course,	
  most	
  search	
  engines	
  use	
  a	
  document-­‐par**oned	
  index	
  …	
  bemer	
  load	
  balancing,	
  etc.	
  Sec. 4.4 
7 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Schema	
  for	
  index	
  construc*on	
  in	
  MapReduce	
  § Schema	
  of	
  map	
  and	
  reduce	
  func(ons	
  § map:	
  input	
  →	
  list(k,	
  v)	
  	
  	
  	
  	
  reduce:	
  (k,list(v))	
  →	
  output	
  § Instan(a(on	
  of	
  the	
  schema	
  for	
  index	
  construc(on	
  § map:	
  collec*on	
  →	
  list(termID,	
  docID)	
  § reduce:	
  (<termID1,	
  list(docID)>,	
  <termID2,	
  list(docID)>,	
  …)	
  →	
  (pos*ngs	
  list1,	
  pos*ngs	
  list2,	
  …)	
  Sec. 4.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Example	
  for	
  index	
  construc(on	
  § Map:	
  § d1	
  :	
  C	
  came,	
  C	
  c`ed.	
  	
  § d2	
  :	
  C	
  died.	
  →	
  § <C,d1>,	
  <came,d1>,	
  <C,d1>,	
  <c`ed,	
  d1>,	
  <C,	
  d2>,	
  <died,d2>	
  § Reduce:	
  § (<C,(d1,d2,d1)>,	
  <died,(d2)>,	
  <came,(d1)>,	
  <c`ed,(d1)>)	
  	
  →	
  	
  (<C,(d1:2,d2:1)>,	
  <died,(d2:1)>,	
  <came,(d1:1)>,	
  <c`ed,(d1:1)>)	
  38	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Dynamic	
  indexing	
  § Up	
  to	
  now,	
  we	
  have	
  assumed	
  that	
  collec*ons	
  are	
  sta*c.	
  § They	
  rarely	
  are:	
  	
  § Documents	
  come	
  in	
  over	
  *me	
  and	
  need	
  to	
  be	
  inserted.	
  § Documents	
  are	
  deleted	
  and	
  modiﬁed.	
  § This	
  means	
  that	
  the	
  dic*onary	
  and	
  pos*ngs	
  lists	
  have	
  to	
  be	
  modiﬁed:	
  § Pos*ngs	
  updates	
  for	
  terms	
  already	
  in	
  dic*onary	
  § New	
  terms	
  added	
  to	
  dic*onary	
  Sec. 4.5 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Simplest	
  approach	
  § Maintain	
  lbigz	
  main	
  index	
  § New	
  docs	
  go	
  into	
  lsmallz	
  auxiliary	
  index	
  § Search	
  across	
  both,	
  merge	
  results	
  § Dele*ons	
  § Invalida*on	
  bit-­‐vector	
  for	
  deleted	
  docs	
  § Filter	
  docs	
  output	
  on	
  a	
  search	
  result	
  by	
  this	
  invalida*on	
  bit-­‐vector	
  § Periodically,	
  re-­‐index	
  into	
  one	
  main	
  index	
  Sec. 4.5 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Issues	
  with	
  main	
  and	
  auxiliary	
  indexes	
  § Problem	
  of	
  frequent	
  merges	
  –	
  you	
  touch	
  stuﬀ	
  a	
  lot	
  § Poor	
  performance	
  during	
  merge	
  § Actually:	
  § Merging	
  of	
  the	
  auxiliary	
  index	
  into	
  the	
  main	
  index	
  is	
  eﬃcient	
  if	
  we	
  keep	
  a	
  separate	
  ﬁle	
  for	
  each	
  pos*ngs	
  list.	
  § Merge	
  is	
  the	
  same	
  as	
  a	
  simple	
  append.	
  § But	
  then	
  we	
  would	
  need	
  a	
  lot	
  of	
  ﬁles	
  –	
  ineﬃcient	
  for	
  OS.	
  § Assump*on	
  for	
  the	
  rest	
  of	
  the	
  lecture:	
  The	
  index	
  is	
  one	
  big	
  ﬁle.	
  § In	
  reality:	
  Use	
  a	
  scheme	
  somewhere	
  in	
  between	
  (e.g.,	
  split	
  very	
  large	
  pos*ngs	
  lists,	
  collect	
  pos*ngs	
  lists	
  of	
  length	
  1	
  in	
  one	
  ﬁle	
  etc.)	
  Sec. 4.5 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Logarithmic	
  merge	
  § Maintain	
  a	
  series	
  of	
  indexes,	
  each	
  twice	
  as	
  large	
  as	
  the	
  previous	
  one	
  § At	
  any	
  *me,	
  some	
  of	
  these	
  powers	
  of	
  2	
  are	
  instan*ated	
  § Keep	
  smallest	
  (Z0)	
  in	
  memory	
  § Larger	
  ones	
  (I0,	
  I1,	
  …)	
  on	
  disk	
  § If	
  Z0	
  gets	
  too	
  big	
  (>	
  n),	
  write	
  to	
  disk	
  as	
  I0	
  § or	
  merge	
  with	
  I0	
  (if	
  I0	
  already	
  exists)	
  as	
  Z1	
  § Either	
  write	
  merge	
  Z1	
  to	
  disk	
  as	
  I1	
  (if	
  no	
  I1)	
  § Or	
  merge	
  with	
  I1	
  to	
  form	
  Z2	
  Sec. 4.5 
8 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Sec. 4.5 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Logarithmic	
  merge	
  § Auxiliary	
  and	
  main	
  index:	
  index	
  construc*on	
  *me	
  is	
  O(T2)	
  as	
  each	
  pos*ng	
  is	
  touched	
  in	
  each	
  merge.	
  § Logarithmic	
  merge:	
  Each	
  pos*ng	
  is	
  merged	
  O(log	
  T)	
  *mes,	
  so	
  complexity	
  is	
  O(T	
  log	
  T)	
  § So	
  logarithmic	
  merge	
  is	
  much	
  more	
  eﬃcient	
  for	
  index	
  construc*on	
  § But	
  query	
  processing	
  now	
  requires	
  the	
  merging	
  of	
  O(log	
  T)	
  indexes	
  § Whereas	
  it	
  is	
  O(1)	
  if	
  you	
  just	
  have	
  a	
  main	
  and	
  auxiliary	
  index	
  Sec. 4.5 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Further	
  issues	
  with	
  mul*ple	
  indexes	
  § Collec*on-­‐wide	
  sta*s*cs	
  are	
  hard	
  to	
  maintain	
  § E.g.,	
  when	
  we	
  spoke	
  of	
  spell-­‐correc*on:	
  which	
  of	
  several	
  corrected	
  alterna*ves	
  do	
  we	
  present	
  to	
  the	
  user?	
  § We	
  said,	
  pick	
  the	
  one	
  with	
  the	
  most	
  hits	
  § How	
  do	
  we	
  maintain	
  the	
  top	
  ones	
  with	
  mul*ple	
  indexes	
  and	
  invalida*on	
  bit	
  vectors?	
  § One	
  possibility:	
  ignore	
  everything	
  but	
  the	
  main	
  index	
  for	
  such	
  ordering	
  § Will	
  see	
  more	
  such	
  sta*s*cs	
  used	
  in	
  results	
  ranking	
  Sec. 4.5 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Dynamic	
  indexing	
  at	
  search	
  engines	
  § All	
  the	
  large	
  search	
  engines	
  now	
  do	
  dynamic	
  indexing	
  § Their	
  indices	
  have	
  frequent	
  incremental	
  changes	
  § News	
  items,	
  blogs,	
  new	
  topical	
  web	
  pages	
  § Sarah	
  Palin,	
  …	
  § But	
  (some*mes/typically)	
  they	
  also	
  periodically	
  reconstruct	
  the	
  index	
  from	
  scratch	
  § Query	
  processing	
  is	
  then	
  switched	
  to	
  the	
  new	
  index,	
  and	
  the	
  old	
  index	
  is	
  deleted	
  Sec. 4.5 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Sec. 4.5 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Other	
  sorts	
  of	
  indexes	
  § Posi*onal	
  indexes	
  § Same	
  sort	
  of	
  sor*ng	
  problem	
  …	
  just	
  larger	
  § Building	
  character	
  n-­‐gram	
  indexes:	
  § As	
  text	
  is	
  parsed,	
  enumerate	
  n-­‐grams.	
  § For	
  each	
  n-­‐gram,	
  need	
  pointers	
  to	
  all	
  dic*onary	
  terms	
  containing	
  it	
  –	
  the	
  lpos*ngsz.	
  § Note	
  that	
  the	
  same	
  lpos*ngs	
  entryz	
  will	
  arise	
  repeatedly	
  in	
  parsing	
  the	
  docs	
  –	
  need	
  eﬃcient	
  hashing	
  to	
  keep	
  track	
  of	
  this.	
  § E.g.,	
  that	
  the	
  trigram	
  uou	
  occurs	
  in	
  the	
  term	
  deciduous	
  will	
  be	
  discovered	
  on	
  each	
  text	
  occurrence	
  of	
  deciduous	
  § Only	
  need	
  to	
  process	
  each	
  term	
  once	
  Why? Sec. 4.5 
9 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Resources	
  for	
  today`s	
  lecture	
  § Chapter	
  4	
  of	
  IIR	
  § MG	
  Chapter	
  5	
  § Original	
  publica*on	
  on	
  MapReduce:	
  Dean	
  and	
  Ghemawat	
  (2004)	
  § Original	
  publica*on	
  on	
  SPIMI:	
  Heinz	
  and	
  Zobel	
  (2003)	
  Ch. 4 
5/21/17
1
Introduction	to	Information	Retrieval
Introduction	to
Information	RetrievalCS276:	Information	Retrieval	and	Web	SearchChristopher	Manning	and	Pandu	NayakLecture	13:	Distributed	Word	Representations	for	Information	Retrieval
Introduction	to	Information	Retrieval
How	can	we	more	robustly	match	a	user’s	search	intent?We	want	to	understand	the	query,	not	just	do	String	equals()§If	user	searches	for	[Dell	notebook	battery	size],	we	would	like	to	match	documents	discussing	“Dell	laptop	battery	capacity”§If	user	searches	for	[Seattle	motel],	we	would	like	to	match	documents	containing	“Seattle	hotel”A	naïve	information	retrieval	system	does	nothing	to	helpSimple	facilities	that	we	have	already	discussed	do	a	bit	to	help§Spelling	correction§Stemming	/	case	foldingBut	we’d	like	to	better	understandwhen	query/document	matchSec. 9.2.2
Introduction	to	Information	Retrieval
How	can	we	more	robustly	match	a	user’s	search	intent?§Use	of	anchor	text	may	solve	this	by	providing	human	authored	synonyms,	but	not	for	new	or	less	popular	web	pages,	or	non-hyperlinked	collections§Relevance	feedback	could	allow	us	to	capture	this	if	we	get	near	enough	to	matching	documents	with	these	words§We	can	also	fix	this	with	information	on	word	similarities:§A	manual	thesaurusof	synonyms§A	measure	of	word	similarity§Calculated	from	a	big	document	collection§Calculated	by	query	log	mining	(common	on	the	web)Sec. 9.2.2
Introduction	to	Information	Retrieval
Example	of	manual	thesaurus	
Sec. 9.2.2
Introduction	to	Information	Retrieval
Search	log	query	expansion§Context-free	query	expansion	ends	up	problematic§[light	hair]	≈	[fair	hair]										At	least	in	U.K./Australia?	≈	blonde§So	expand	[light]	⇒[light	fair]§But	[outdoor	light	price]	≠	[outdoor	fair	price]§You	can	learn	query	context-specific	rewritings	from	search	logs	by	attempting	to	identify	the	same	user	making	a	second	attempt	at	the	same	user	need§[Hinton	word	vector]§[Hinton	word	embedding]§In	this	context,	[vector]	≈	[embedding]§But	not	when	talking	about	a	disease	vector	or	C++!
Introduction	to	Information	Retrieval
Automatic	Thesaurus	Generation§Attempt	to	generate	a	thesaurus	automatically	by	analyzing	a	collection	of	documents§Fundamental	notion:	similarity	between	two	words§Definition	1:	Two	words	are	similar	if	they	co-occur	with	similar	words.§Definition	2:	Two	words	are	similar	if	they	occur	in	a	given	grammatical	relation	with	the	same	words.§You	can	harvest,	peel,	eat,	prepare,	etc.	apples	and	pears,	so	apples	and	pears	must	be	similar.§Co-occurrence	based	is	more	robust,	grammatical	relations	are	more	accurate.Why?Sec. 9.2.3
5/21/17
2
Introduction	to	Information	Retrieval
Simple	Co-occurrence	Thesaurus§Simplest	way	to	compute	one	is	based	on	term-term	similarities	in	C	=	AAT	where	Ais	term-document	matrix.§wi,j=	(normalized)	weight	for	(ti,dj)
§For	each	ti,	pick	terms	with	high	values	in	CtidjNMWhat does Ccontain if Ais a term-doc incidence (0/1) matrix?Sec. 9.2.3A
Introduction	to	Information	Retrieval
Automatic	thesaurus	generation	example	…sort	of	worksWordNearest	neighborsabsolutelyabsurd,	whatsoever,	totally,	exactly,	nothingbottomeddip,	copper,	drops,	topped,	slide,	trimmedcaptivatingshimmer,	stunningly,	superbly,plucky,	wittydoghousedog,	porch,	crawling,	beside,	downstairsmakeuprepellent,	lotion,	glossy,	sunscreen,	skin,	gelmediatingreconciliation,negotiate,	cease,	conciliationkeepinghoping,	bring,	wiping,	could,some,	wouldlithographsdrawings,Picasso,	Dali,	sculptures,	Gauguinpathogenstoxins,	bacteria,	organisms,bacterial,	parasitessensesgrasp,	psyche,truly,	clumsy,	naïve,	innate	But data is  too sparse in this form 100,000 words = 1010entries in C. 
Introduction	to	Information	Retrieval
How	can	we	represent	term	relations?§With	the	standard	symbolic	encoding	of	terms,	each	term	is	a	dimension§Different	terms	have	no	inherent	similarity§motel [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]Thotel  [0 0 0 0 0 0 0 3 0 0 0 0 0 0 0] = 0§If	query	on	hoteland	document	has	motel,	then	our	query	and	document	vectors	are	orthogonalSec. 9.2.2
Introduction	to	Information	Retrieval
Can	you	directly	learn	term	relations?§Basic	IR	is	scoring	on	qTd§No	treatment	of	synonyms;	no	machine	learning§Can	we	learn	parameters	Wto	rank	via	qTWd?
§Problem	is	again	sparsity–Wis	huge	>	1010
Introduction	to	Information	Retrieval
Is	there	a	better	way?§Idea:§Can	we	learn	a	dense	low-dimensional	representation	of	a	word	in	ℝdsuch	that	dot	productsuTvexpress	word	similarity?§We	could	still	if	we	want	to	include	a	“translation”	matrix	between	vocabularies	(e.g.,	cross-language):	uTWv§But	now	Wis	small!§Supervised	Semantic	Indexing	(Baiet	al.	Journal	of	Information	Retrieval	2009)	shows	successful	use	of	learning	W	for	information	retrieval§But	we’ll	develop	direct	similarity	in	this	class
Introduction	to	Information	Retrieval
Distributional	similarity	based	representations§You	can	get	a	lot	of	value	by	representing	a	word	by	means	of	its	neighbors§“You	shall	know	a	word	by	the	company	it	keeps”§(J.	R.	Firth	1957:	11)§One	of	the	most	successful	ideas	of	modern	statistical	NLPgovernment debt problems turning into banking crises as has happened insaying that Europe needs unified banking regulation to replace the hodgepodgeëThese	words	will	represent	banking	ì12
5/21/17
3
Introduction	to	Information	Retrieval
Solution:	Low	dimensional	vectors§The	number	of	topics	that	people	talk	about	is	small	(in	some	sense)§Clothes,	movies,	politics,	…•Idea:	store	“most”	of	the	important	information	in	a	fixed,	small	number	of	dimensions:	a	dense	vector•Usually	25	–1000	dimensions•How	to	reduce	the	dimensionality?•Go	from	big,	sparse	co-occurrence	count	vector	to	low	dimensional	“word	embedding”	13
Introduction	to	Information	Retrieval
Traditional	Way:Latent	Semantic	Indexing/Analysis§Use	Singular	Value	Decomposition	(SVD)	–kind	of	like	Principal	Components	Analysis	(PCA)	for	an	arbitrary	rectangular	matrix	–or	just	random	projection	to	find	a	low-dimensional	basis	or	orthogonal	vectors§Theory	is	that	similarity	is	preserved	as	much	as	possible§You	can	actually	gain	in	IR	(slightly)	by	doing	LSA,		as	“noise”	of	term	variation	gets	replaced	by	semantic	“concepts”§Popular	in	the	1990s	[Deerwesteret	al.	1990,	etc.]§Results	were	always	somewhat	iffy	(…	it	worked	sometimes)§Hard	to	implement	efficiently	in	an	IR	system	(dense	vectors!)§Discussed	in	IIRchapter	18,	but	not	discussed	further	here§And	not	on	the	exam	(!!!)Sec. 18.2
Introduction	to	Information	Retrieval
“NEURAL	EMBEDDINGS”
Introduction	to	Information	Retrieval
Word	meaning	is	defined	in	terms	of	vectors§We	will	build	a	dense	vector	for	each	word	type,	chosen	so	that	it	is	good	at	predicting	other	words	appearing	in	its	context…those	other	words	also	being	represented	by	vectors	…it	all	gets	a	bit	recursivelinguistics=0.2860.792−0.177−0.1070.109−0.5420.3490.271
Introduction	to	Information	Retrieval
Neural	word	embeddings	-visualization
17
Introduction	to	Information	Retrieval
Basic	idea	of	learning	neural	network	word	embeddingsWe	define	a	model	that	aims	to	predict	between	a	center	word	wtand	context	words	in	terms	of	word	vectorsp(context|wt)	=	…which	has	a	loss	function,	e.g.,J=	1	−	p(w−t	|wt)	We	look	at	many	positions	t	in	a	big	language	corpusWe	keep	adjusting	the	vector	representations	of	words	to	minimize	this	loss
5/21/17
4
Introduction	to	Information	Retrieval
Idea:	Directly	learn	low-dimensional	word	vectors	based	on	ability	to	predict•Old	idea.	Relevant	for	this	lecture	&	deep	learning:•Learning	representations	by	back-propagating	errors.	(Rumelhartet	al.,	1986)•A	neural	probabilistic	language	model	(Bengioet	al.,	2003)		•NLP	(almost)	from	Scratch	(Collobert&	Weston,	2008)•A	recent,	even	simpler	and	faster	model:	word2vec	(Mikolovet	al.	2013)	àintro	now•The	GloVe	model	from	Stanford	(Pennington,	Socher,	and	Manning	2014)	connects	back	to	matrix	factorization•Initial	models	were	quite	non-linear	and	slow;	recent	work	has	used	fast,	bilinear	models19
Introduction	to	Information	Retrieval
Word2vec	is	a	family	of	algorithms[Mikolovet	al.	2013]Predict	between	every	word	and	its	context	words!Two	algorithms1.Skip-grams	(SG)Predict	context	words	given	target	(position	independent)2.Continuous	Bag	of	Words	(CBOW)Predict	target	word	from	bag-of-words	contextTwo	(moderately	efficient)	training	methods1.Hierarchical	softmax2.Negative	samplingNaïve	softmax
Introduction	to	Information	Retrieval
Skip-gram	prediction
Introduction	to	Information	Retrieval
Details	of	word2vecFor	each	word	t=	1	…T,	predict	surrounding	words	in	a	window	of	“radius”	mof	every	word.Objective	function:	Maximize	the	probability	of	any	context	word	given	the	current	center	word:Where	θrepresents	all	variables	we	will	optimize
Introduction	to	Information	Retrieval
Details	of	Word2VecPredict	surrounding	words	in	a	window	of	radius	mof	every	wordFor																						the	simplest	first	formulation	is	where	ois	the	outside	(or	output)	word	index,	cis	the	center	word	index,	vcand	uoare	“center”	and	“outside”	vectors	of	indices	cand	oSoftmaxusing	word	cto	obtain	probability	of	word	otraining time. The basic Skip-gram formulation deﬁnesp(wt+j|wt)using the softmax function:p(wO|wI)=exp/parenleftBigv′wO⊤vwI/parenrightBig
/summationtextWw=1exp/parenleftBigv′w⊤vwI/parenrightBig(2)wherevwandv′ware the “input” and “output” vector representations ofw,a n dWis the num-ber of words in the vocabulary. This formulation is impractical because the cost of computing∇logp(wO|wI)is proportional toW,w h i c hi so f t e nl a r g e(105–107terms).2.1 Hierarchical SoftmaxAc o m p u t a t i o n a l l ye f ﬁ c i e n ta p p r o x i m a t i o no ft h ef u l ls o f t max is the hierarchical softmax. In thecontext of neural network language models, it was ﬁrst introduced by Morin and Bengio [12]. Themain advantage is that instead of evaluatingWoutput nodes in the neural network to obtain theprobability distribution, it is needed to evaluate only aboutlog2(W)nodes.The hierarchical softmax uses a binary tree representationof the output layer with theWwords asits leaves and, for each node, explicitly represents the relative probabilities of its child nodes. Thesedeﬁne a random walk that assigns probabilities to words.More precisely, each wordwcan be reached by an appropriate path from the root of the tree.L e tn(w, j)be thej-th node on the path from the root tow,a n dl e tL(w)be the length of this path, son(w,1) = rootandn(w, L(w)) =w.I n a d d i t i o n , f o r a n y i n n e r n o d en,l e tch(n)be an arbitraryﬁxed child ofnand let[[x]]be 1 ifxis true and -1 otherwise. Then the hierarchical softmax deﬁnesp(wO|wI)as follows:p(w|wI)=L(w)−1/productdisplayj=1σ/parenleftBig[[n(w, j+1 )=c h (n(w, j))] ]·v′n(w,j)⊤vwI/parenrightBig(3)whereσ(x)=1/(1 + exp(−x)).I tc a nb ev e r i ﬁ e dt h a t/summationtextWw=1p(w|wI)=1.T h i si m p l i e st h a tt h ecost of computinglogp(wO|wI)and∇logp(wO|wI)is proportional toL(wO),w h i c ho na v e r a g eis no greater thanlogW.A l s o , u n l i k e t h e s t a n d a r d s o f t m a x f o r m u l a t i o n o f t h e S k i p -gram whichassigns two representationsvwandv′wto each wordw,t h eh i e r a r c h i c a ls o f t m a xf o r m u l a t i o nh a sone representationvwfor each wordwand one representationv′nfor every inner nodenof thebinary tree.The structure of the tree used by the hierarchical softmax hasac o n s i d e r a b l ee f f e c to nt h ep e r f o r -mance. Mnih and Hinton explored a number of methods for constructing the tree structure and theeffect on both the training time and the resulting model accuracy [10]. In our work we use a binaryHuffman tree, as it assigns short codes to the frequent wordswhich results in fast training. It hasbeen observed before that grouping words together by their frequency works well as a very simplespeedup technique for the neural network based language models [5, 8].2.2 Negative SamplingAn alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in-troduced by Gutmann and Hyvarinen [4] and applied to languagem o d e l i n gb yM n i ha n dT e h[ 1 1 ] .NCE posits that a good model should be able to differentiate data from noise by means of logisticregression. This is similar to hinge loss used by Collobert and Weston [2] who trained the modelsby ranking the data above noise.While NCE can be shown to approximately maximize the log probability of the softmax, the Skip-gram model is only concerned with learning high-quality vector representations, so we are free tosimplify NCE as long as the vector representations retain their quality. We deﬁne Negative sampling(NEG) by the objectivelogσ(v′wO⊤vwI)+k/summationdisplayi=1Ewi∼Pn(w)/bracketleftBiglogσ(−v′wi⊤vwI)/bracketrightBig(4)3
Introduction	to	Information	Retrieval
Softmax	function:	Standard	mapfrom	ℝVto	a	probability	distribution
Exponentiate	tomake	positive
Normalize	togive	probability
5/21/17
5
Introduction	to	Information	Retrieval
Skip	gram	model	structure
Introduction	to	Information	Retrieval
To	learn	good	word	vectors:Compute	allvector	gradients!§We	often	define	the	set	of	allparameters	in	a	model	in	terms	of	one	long	vector	§In	our	case	with	d-dimensional	vectorsandVmany	words:§We	then	optimizethese	parameters
Note: Every word has two vectors! Makes it simpler!
Introduction	to	Information	Retrieval
Intuition	of	how	to	minimize	loss	for	a	simple	function	over	two	parametersWe	start	at	a	random	point	and	walk	in	the	steepest	direction,	which	is	given	by	the	derivative	of	the	function
Contour	lines	show	points	of	equal	value	of	objective	function
Introduction	to	Information	Retrieval
Descending	by	using	derivatives
We	will	minimize	a	cost	function	bygradient	descentTrivial	example:	(from	Wikipedia)Find	a	local	minimum	of	the	function	f(x)	=	x4−3x3+2,	with	derivative	f'(x)	=	4x3−9x2
Subtracting	a	fraction	of	the	gradient	moves	you	towards	the	minimum!
Introduction	to	Information	Retrieval
Vanilla	Gradient	Descent	Code
Introduction	to	Information	Retrieval
Stochastic	Gradient	Descent§But	Corpus	may	have	40B	tokens	and	windows§You	would	wait	a	very	long	time	before	making	a	single	update!§Verybad	idea	for	pretty	much	all	neural	nets!§Instead:	We	will	update	parameters	after	each	window	t	àStochastic	gradient	descent	(SGD)

5/21/17
6
Introduction	to	Information	Retrieval
Working	out	how	to	optimize	a	neural	network	is	really	all	the	chain	rule!Chain	rule!	If	y=	f(u)	and	u=	g(x),	i.e.	y	=	f(g(x)),	then:Simple	example:	
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
36

5/21/17
7
Introduction	to	Information	Retrieval
Linear	Relationships	in	word2vecThese	representations	are	very	good	at	encoding	similarityand	dimensions	of	similarity!§Analogies	testing	dimensions	of	similarity	can	be	solved	quite	well	just	by	doing	vector	subtraction	in	the	embedding	spaceSyntactically§xapple−	xapples≈	xcar−	xcars≈xfamily−	xfamilies§Similarly	for	verb	and	adjective	morphological	formsSemantically	(Semeval2012	task	2)§xshirt−	xclothing≈xchair−	xfurniture§xking−	xman≈	xqueen−	xwoman37
Introduction	to	Information	Retrieval
kingmanwomanTest for linear relationships, examined by Mikolovet al.a:b :: c:?
manwoman[ 0.20 0.20 ][ 0.60 0.30 ]king[ 0.30 0.70 ][ 0.70 0.80 ]−++queenqueenman:woman :: king:?a:b :: c:?
Word	Analogies
Introduction	to	Information	Retrieval
GloVeVisualizations
39
http://nlp.stanford.edu/projects/glove/
Introduction	to	Information	Retrieval
Glove	Visualizations:	Company	-CEO
40
Introduction	to	Information	Retrieval
Glove	Visualizations:	Superlatives
5/21/1741
Introduction	to	Information	Retrieval
Application	to	Information	RetrievalApplication	is	just	beginning	–there’s	little	to	go	on§Google’s	RankBrain–almost	nothing	is	publicly	known§Bloomberg	article	by	Jack	Clark	(Oct	26,	2015):	http://www.bloomberg.com/news/articles/2015-10-26/google-turning-its-lucrative-web-search-over-to-ai-machines§A	result	reranking	system§Even	though	more	of	the	value	is	in	the	tail?§New	SIGIR	Neu-IR	workshop	series	(2016	and	2017)

5/21/17
8
Introduction	to	Information	Retrieval
2011														2013										2015											2017speech							vision						NLP										IRFinal	Thoughtsfrom	Chris	Manning	SIGIR	2016	keynote
You	arehere
Introduction	to	Information	Retrieval
An	application	to	information	retrievalNalisnick,	Mitra,	Craswell&	Caruana.	2016.	Improving	Document	Ranking	with	Dual	Word	Embeddings.	WWW	2016	Companion.	http://research.microsoft.com/pubs/260867/pp1291-Nalisnick.pdfMitra,	Nalisnick,	Craswell&	Caruana.	2016.	A	Dual	Embedding	Space	Model	for	Document	Ranking.	arXiv:1602.01137[cs.IR]Builds	on	BM25	model	idea	of	“aboutness”§Not	just	term	repetition	indicating	aboutness§Relationship	between	query	terms	and	all	terms	in	the	document	indicates	aboutness	(BM25	uses	only	query	terms)Makes	clever	argument	for	different	use	of	word	and	context	vectors	in	word2vec’s	CBOW/SGNS	or	GloVe
Introduction	to	Information	Retrieval
Modeling	document	aboutness:	Results	from	a	search	for	Albuquerque
d1d2
Introduction	to	Information	Retrieval
Using	2	word	embeddings
word2vec model with 1 word of contextFocuswordContextwordWINEmbeddingsfor focuswordsWOUTEmbeddingsfor contextwordsWe can gain by using thesetwo embeddings differently
Introduction	to	Information	Retrieval
Using	2	word	embeddings
Introduction	to	Information	Retrieval
Dual	Embedding	Space	Model	(DESM)§Simple	model§A	document	is	represented	by	the	centroid	of	its	word	vectors§Query-document	similarity	is	average	over	query	words	of	cosine	similarity

5/21/17
9
Introduction	to	Information	Retrieval
Dual	Embedding	Space	Model	(DESM)§What	works	best	is	to	use	the	OUT	vectors	for	the	document	and	the	IN	vectors	for	the	query§This	way	similarity	measures	aboutness–words	that	appear	with	this	word	–which	is	more	useful	in	this	context	than	(distributional)	semantic	similarity
Introduction	to	Information	Retrieval
Experiments§Train	word2vec	from	either§600	million	Bing	queries§342	million	web	document	sentences§Test	on	7,741	randomly	sampled	Bing	queries§5	level	eval(Perfect,	Excellent,	Good,	Fair,	Bad)§Two	approaches1.Use	DESM	model	to	reranktop	results	from	BM252.Use	DESM	alone	or	a	mixture	model	of	it	and	BM25
Introduction	to	Information	Retrieval
Results	–reranking	k-best	list	
Pretty	decent	gains	–e.g.,	2%	for	NDCG@3Gains	are	bigger	for	model	trained	on	queries	than	docs
Introduction	to	Information	Retrieval
Results	–whole	ranking	system
Introduction	to	Information	Retrieval
A	possible	explanation
IN-OUT	has	some	ability	to	prefer	Relevant	to	close-by	(judged)	non-relevant,	but	it’s	scores	induce	too	much	noise	vs.	BM25	to	be	usable	alone
Introduction	to	Information	Retrieval
DESM	conclusions§DESM	is	a	weak	ranker	but	effective	at	finding	subtler	similarities/aboutness§It	is	effective	at,	but	only	at,	ranking	at	least	somewhat	relevant	documents§For	example,	DESM	can	confuse	Oxford	and	Cambridge§Bing	rarely	makes	the	Oxford-Cambridge	mistake
5/21/17
10
Introduction	to	Information	Retrieval
Global	vs.	local	embedding	[Diaz	2016]
Introduction	to	Information	Retrieval
Global	vs.	local	embedding	[Diaz	2016]
Train w2v on documents from first round of retrievalFine-grained word sense disambiguation
Introduction	to	Information	Retrieval
Ad-hoc	retrieval	using	local	and	distributed	representation	[Mitraet	al.	2017]§Argues	both	“lexical”	and	“semantic”	matching	is	important	for	document	ranking§Duet	model	is	a	linear	combination	of	two	DNNs	using	local	and	distributed	representations	of	query/	document	as	inputs,	and	jointly	trained	on	labelled	data
Introduction	to	Information	Retrieval
Summary:	Embed	all	the	things!Word	embeddings	are	the	hot	new	technology	(again!)Lots	of	applications	wherever	knowing	word	context	or	similarity	helps	prediction:§Synonym	handling	in	search§Document	aboutness§Ad	serving§Language	models:	from	spelling	correction	to	email	response§Machine	translation§Sentiment	analysis§…
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Thesaurus-based	query	expansion§For	each	term	tin	a	query,	expand	the	query	with	synonyms	and	related	words	of	tfrom	the	thesaurus§feline	→	feline	cat§May	weight	added	terms	less	than	original	query	terms.§Generally	increases	recall§Widely	used	in	many	science/engineering	fields§May	significantly	decrease	precision,	particularly	with	ambiguous	terms.§“interest	rate”	®“interest	rate	fascinate	evaluate”§There	is	a	high	cost	of	manually	producing	a	thesaurus§And	for	updating	it	for	scientific	changesSec. 9.2.2
5/21/17
11
Introduction	to	Information	Retrieval
Automatic	Thesaurus	Generation	IssuesnQuality of associations is usually a problemnSparsitynTerm ambiguity may introduce irrelevant statistically correlated terms.n“planet earth facts” ®“planet earth soil ground facts”nSince terms are highly correlated anyway, expansion may not retrieve many additional documents.Sec. 9.2.3
C1010entries100,000100,000
Introduction	to	Information	Retrieval
COALS	model	(count-modified	LSA)[Rohde,	Gonnerman&	Plaut,	ms.,	2005]
Introduction	to	Information	Retrieval
Count	based	vs.	direct	prediction
63
LSA, HAL (Lund & Burgess), COALS (Rohde et al), Hellinger-PCA (Lebret& Collobert)•Fast training•Efficient usage of statistics•Primarily used to capture word similarity•Disproportionate importance given to small counts
•NNLM, HLBL, RNN, word2vec Skip-gram/CBOW, (Bengio et al; Collobert& Weston; Huang et al; Mnih& Hinton; Mikolovet al; Mnih& Kavukcuoglu)•Scales with corpus size•Inefficient usage of statistics•Can capture complex patterns beyond word similarity •Generate improved performance on other tasks
Introduction	to	Information	Retrieval
Ratios	of	co-occurrence	probabilities	can	encode	meaning	componentsCrucial	insight:	x=	solidx=	water			largex=	gassmall
x=	random			smalllarge
smalllargelargesmall~1~1
largesmallEncoding	meaning	in	vector	differences[Pennington,	Socher,	and	Manning,	EMNLP	2014]
Introduction	to	Information	Retrieval
Ratios	of	co-occurrence	probabilities	can	encode	meaning	componentsCrucial	insight:	x=	solidx=	water			1.9	x	10-4x=	gas
x=	fashion
2.2	x	10-51.360.96
Encoding	meaning	in	vector	differences[Pennington,	Socher,	and	Manning,	EMNLP	2014]
8.97.8	x	10-42.2	x	10-33.0	x	10-31.7	x	10-51.8	x	10-56.6	x	10-58.5	x	10-2
Introduction	to	Information	Retrieval
GloVe:	A	new	model	for	learning	word	representations[Pennington,	Socher,	and	Manning,	EMNLP	2014]

5/21/17
12
Introduction	to	Information	Retrieval
Nearest	words	tofrog:1.	frogs2.	toad3.	litoria4.	leptodactylidae5.	rana6.	lizard7.	eleutherodactylusWord	similarities
litorialeptodactylidae
rana
eleutherodactylus
http://nlp.stanford.edu/projects/glove/
Introduction	to	Information	Retrieval
ModelDimensionsCorpus	sizePerformance(Syn+	Sem)CBOW(Mikolovet	al.	2013b)3001.6	billion36.1GloVe(this	work)3001.6	billion70.3CBOW	(M	et	al.	2013b,	by	us)3006	billion65.7GloVe(this	work)3006	billion71.7CBOW(Mikolovet	al.	2013b)10006	billion63.7GloVe(this	work)30042	billion75.0Word	analogy	task			[Mikolov,	Yih&	Zweig	2013a]
Approximate Inference:Randomized MethodsOctober 15, 2015
Topics•Hard Inference –Local search & hill climbing –Stochastic hill climbing / Simulated Annealing •Soft Inference –Monte-Carlo approximations –Markov-Chain Monte Carlo methods •Gibbs sampling •Metropolis Hastings sampling –Importance Sampling
Local Search•Start with a candidate solution •Until (time > limit) or no changes possible: –Apply a local change to generate a new candidate solutions –Pick the one with the highest score (“steepest ascent”) •A neighborhood function maps a search state (+ optionally, algorithm state) to a set of neighboring states –Assumption: computing the score (cf. unnormalized probability) of the new state is inexpensive
Hill Climbing
timeflieslikeanarrow
NNNNVBDTNN
Hill Climbing
timeflieslikeanarrow
NNNNVBDTNNNNVBVBDDT NNSP
Hill Climbing
timeflieslikeanarrow
NNNNVBDTNNNNVBVBDDT NNSP
Hill Climbing
timeflieslikeanarrow
NNNNSVBDTNNNNVBVBDDT NNSP
Hill Climbing
timeflieslikeanarrow
NNNNSVBDTNNNNVBVBDDT NNSP
Hill Climbing
timeflieslikeanarrow
NNNNSPDTNN…
Hill Climbing: Sequence Labeling•Start with greedy assignment – O(n|L|) •While stop criterion not met –For each label position (n of them) •Consider changing to any label, including no change •When should we stop?
Fixed number of iterations•Let’s say we run the previous algorithm for |L| iterations –The runtime is O(n|L|2) –The Viterbi runtime for a bigram model is O(n|L|2) •Here’s where it gets interesting: –Now imagine we were using a k-gram modelViterbi runtime: O(n|L|k) –We could get arbitrarily better speedup!
Local Search•Pros –This is an “any time” algorithm: stop any time and you will have a solution •Cons –There is no guarantee that we found a good solution –Local optima: to get to a good solution, you have to go through a bad scoring solution –Plateau: you get caught on a plateau and you can either go down or “stay the same”
In Pictures
Plateau
Local Optima: Random Restarts•Start from lots of different places •Look at the score of the best solution •Pros –Easy to parallelize –Easy to implement •Cons –Lots of computational work •Interesting paper:Zhang et al. (2014) Greed is Good if Randomized: New Inference for DependencyParsing. Proc. EMNLP.
Local Optima: Take Bigger Steps•We can use any neighborhood function! •Why not use a bigger neighborhood function? –E.g., consider two words at once
Local Search
timeflieslikeanarrow
NNNNVBDTNN
Local Search
timeflieslikeanarrow
NNNNVBDTNNNNVBVBDDT NNSP
NNVBVBDDT NNSP
Local Search
timeflieslikeanarrow
NNVBVBDTNNNNVBVBDDT NNSP
NNVBVBDDT NNSP
Neighborhood Sizes•In general: neighborhood size is exponential in the number of variables you are considering changing •But, sometimes you can use dynamic programming (or other combinatorial algorithms) to search exponential spaces in polytime –Consider a sequence labeling problem where you have a bigram Markov model + some global features –Example: NER with constraints that say that all phrases should have the same label across a document
Stochastic Hill Climbing•In general, there is no neighborhood function that will give you correct and efficient local search –Hill climbing may still be good enough! –“Some of my best friends are hill climbing algorithms!” (EM) •Another variation –Replace the arg max with a stochastic decision: pick low-scoring decisions with some probability
Simulated Annealing•View configurations as having an “energy”•Pick change in state by sampling•Start with a high “temperature” (model specific) •Gradually cool down to T=0 •Important: keep track of best scoring x so far!
In Pictures

In Pictures

Simulated Annealing•We don’t have to compute the partition function, just differences in energy •In general: –Better solutions for slower annealing schedules –For probabilistic models, T=1 corresponds to Gibbs sampling (more in a few slides), provided certain conditions are met on the neighborhood function
Whither Soft Inference?•As we discussed, hard inference isn’t the only game in town •We can use local search to approximate soft inference as well –Posterior distributions –Expected values of functions under distributions •This brings us to the family of Monte Carlo techniques
Monte Carlo Approximations•Monte Carlo techniques let you –Approximately represent a distribution p(x) [x can be discrete, continuous, or mixed] using a collection of N samples from p(x) –Approximate marginal probabilities of x using samples from a joint distribution p(x,y) –Approximate expected values of f(x) using samples from p(x)
Monte Carlo approximation of a Gaussian distribution:
Monte Carlo approximation of a ??? distribution:
Monte Carlo Questions•How do we generate samples from the target distribution? –Direct (or “perfect”) sampling –Markov-Chain MC methods (Gibbs, Metropolis-Hastings) •How good are the approximations?
Monte Carlo Approximations“Samples”
Point mass at X(i)

Monte Carlo Expectations
Monte Carlo estimator of

Monte Carlo Expectations•Nice properties –Estimator is unbiased –Estimator is consistent –Approximation error decreases at a rate ofO(1/N), independent of the dimension of X •Problems –We don’t generally know how to sample from p –When we do, the sampling scheme would be linear in dim(X)
Direct Sampling from p•Sampling from p is generally hard –We may need to compute some very hard marginal quantities •Claim. For every Viterbi/Inside-Outside algorithm there is a sampling algorithm that you get with the same “start up” cost –There is a question about this in the HW… •But we want to use MC approximations when we can’t run Inside-Outside!
Gibbs Sampling•Markov chain Monte Carlo (MCMC) method –Build a Markov model •The states represent samples from p •Transitions = Neighborhoods from local search! •Transition probabilities constructed such that the MM’s stationary distribution is p –MCMC samples are correlated •Taking every m samples can make samples more independent (How big should m be?)
Gibbs Sampling•Gibbs sampling relies on the fact that sampling from p(a|b,c,d,e,f) is easier than sampling from p(a,b,c,d,e,f) •Algorithm –We want N samples from –The ith sample is –Start with some  x(0) –For each sample i=1,…,N •For each variable j=1,…,m –Sample 
The Beauty Part: No More Partitions
Requirements•There must be a positive probability path between any two states •Process must satisfy detailed balance–Ie, this is a reversible Markov process –Important: This does not mean that you have to be able to reverse what happened at time (t) at time (t+1). Why?
Ensuring Detailed Balance•Option 1: Visit all variables in a deterministic order that is independent of their current settings •Option 2: Visit variables uniformly at random, independently of their current settings •Option 3: Unfortunately, both of the above may not be feasible –Other orders are possible, but you have to prove that detailed balance obtains. This can be a pain.
Glossary•Mixing time –How long until a Markov chain approaches the stationary distribution? •Collapsed sampling –Marginalize some variables during sampling –Obviously: marginalize variables you don’t care about! •Block sampling –Resample a block of random variables –This is exactly equivalent to the “large neighborhoods” idea – goal: reduce mixing time
Gibbs Sampling•How do we sample trees? •How do we sample segmentations? •Key idea: sampling representation –Encode your random structure as a set of random variables –Important: these will not (necessarily) be the same as your model
Sampling Representationsᇿਹ:ࢶᥴই֜୏ṛᨶᰁ࿆Ԇኞၚտ
Sampling Representationsᇿਹ:ࢶᥴই֜୏ṛᨶᰁ࿆Ԇኞၚտᇿਹ:ࢶᥴই֜୏ṛᨶᰁ࿆Ԇኞၚտ
BCBBCCBBBBCBCBBB
Sampling Representationsᇿਹ:ࢶᥴই֜୏ṛᨶᰁ࿆Ԇኞၚտᇿਹ:ࢶᥴই֜୏ṛᨶᰁ࿆Ԇኞၚտ
BCBBBBCCCBBCBCCB
Sampling Representationsᇿਹ:ࢶᥴই֜୏ṛᨶᰁ࿆Ԇኞၚտᇿ ਹ : ࢶ ᥴ ই ֜ ୏ ṛ ᨶ ᰁ ࿆ Ԇ ኞ ၚ տ

Sampling Representationsᇿਹ:ࢶᥴই֜୏ṛᨶᰁ࿆Ԇኞၚտᇿ ਹ : ࢶ ᥴ ই ֜ ୏ ṛ ᨶ ᰁ ࿆ Ԇ ኞ ၚ տ

Sampling Representationsᇿਹ:ࢶᥴই֜୏ṛᨶᰁ࿆Ԇኞၚտᇿ ਹ : ࢶ ᥴ ই ֜ ୏ ṛ ᨶ ᰁ ࿆ Ԇ ኞ ၚ տ

Sampling Representations•Requirements –Define reasonably sized neighborhoods –Model score changes should be easy to compute •Standard tricks –Binary variables that indicate breaks –Random variables that indicate span lengths –Categorical random variables that indicate break,type •Many papers just written on sampling representations for structured problems!
How Things Go Wrong•Three common failure modes –Mixing time is awful –Sampling density is intractable/incomputable –Variance of estimates (e.g., of expectations) is too high •This is why MCMC methods are still an active area of research •We consider two (potential) solutions that rely on proposal distributions
Using Proposal Distributions•Idea: sample from a distribution that “looks like” the distribution you want to sample from, i.e.                              or –Common trade off: good approximation of p vs. easy to sample from •Then perform some kind of correction using p (or, usually, p*C) –Metropolis-Hastings: possibly reject sample –Importance sampling: reweight sample
What Proposal Distribution?•Specifics depend on your problem –Sample from a bigram HMM’s posterior distribution as a proposal for a k-gram HMM –Sample from a Gaussian as a proposal for some other continuous density –Sample from an unconditional distribution as a proposal for a conditional distribution •In general: good proposal distributions have heavier tails
Metropolis Hastings Sampling•Very simple strategy for incorporating a proposal distribution •Can be used to propose full ensemble of variables, a single variable, or anything in between •Standard uses –Sampling continuous variables (e.g., sample from Gaussian and accept into non-Gaussian distribution) –Sample sequence or tree from PCFG/HMM and accept into model with non-local factors
Metropolis Hastings Sampling•The MH algorithm works as follows •For each block of variables you are resampling –Sample –Accept this sample with probability –If accepted, update x –Otherwise x remains the same
Metropolis Hastings Sampling•Note: with an unconditional proposal •Also note: you only need to be able to sample from p and q and evaluate them up to a fixed factor (e.g., partition)
Metropolis-Hastings•Pros –A paper cited 18,000 times can’t be wrong! –Hand-crafted proposal distributions give you the ability to improve performance •Cons –Keep track of your rejections –Variance of computed quantities can be exceedingly high
Importance Sampling•MH samples can be highly correlated -> high variance of MC estimates of expectations •Importance sampling is a technique for reducing variance (albeit by increasing bias) •Intuition –Rather than rejecting bad samples, down-weight them appropriately •Benefits –Lower variance –Biased, but still consistent –Estimate of Z
Importance Sampling•Given                            and importance dist. •We define the unnormalized weight function •We can now write
Importance Sampling•Given                            and importance dist. •We define the unnormalized weight function •We can now write
Importance Sampling•Given                            and importance dist. •We define the unnormalized weight function •We can now write
Importance Sampling•Given                            and importance dist. •We define the unnormalized weight function •We can now write
Importance SamplingNotice that this has the form of an expected valueof w(x) under q:We can replace this with a Monte Carlo estimate
Importance SamplingNotice that this has the form of an expected valueof w(x) under q:We can replace this with a Monte Carlo estimate
Importance SamplingThis lets us derive the following approximation: Intuitively, we have reweighted each samplex(i) from q(x) with an importance weight
Importance SamplingThis lets us derive the following approximation: Intuitively, we have reweighted each samplex(i) from q(x) with an importance weight
Importance SamplingIS Expectations are defined straightforwardly as 
Importance Sampling•You can show –That the IS estimator is biased –That the IS estimator is consistent –That the IS estimator obeys a central limit theorem with asymptotic variance –That the IS estimator is more efficient than rejection sampling
Particle Filtering•Particle filtering is a special kind of importance sampling –It creates proposal distributions by conditioning only on the past and current observations –Each “particle” is a single sample that is built up progressively across time •This looks a lot like beam search except you sample a single decision at each time step and then discard anything else –As time progresses, you figure out that some particles have a bad importance weight and others are good •Key idea: throw out low-weight particles and duplicate high weight particles
Summary•Monte Carlo techniques are a huge field of research –This is a survey of the important ones that are used in structured prediction •We will return to these methods when we talk about Bayesian unsupervised learning
General Artificial Intelligence Ed Grefenstette How much linguistics is 
needed for NLP? 
etg@google.com 
Based on work with:  Karl Moritz Hermann, Phil Blunsom, Tim Rocktäschel, 
Tomá š Kočiský , Lasse Espeholt, Will Kay, and Mustafa Suleyman 
General Artificial Intelligence An Identity Crisis in NLP? 

General Artificial Intelligence 1.Sequence-to-Sequence Modelling with RNNs 
2.Transduction with Unbounded Neural Memory 
3.Machine Reading with Attention 
4.Recognising Entailment with Attention Today's Topics 
Some Preliminaries: RNNs 
●Recurrent hidden layer 
outputs distribution over 
next symbol 
●Connects "back to itself" 
●Conceptually: hidden 
layer models history of 
the sequence. 

Some Preliminaries: RNNs 
●RNNs fit variable width 
problems well 
●Unfold to feedforward 
nets with shared weights 
●Can capture long range 
dependencies 
●Hard to train (exploding / 
vanishing gradients) 

Some Preliminaries: LSTM RNNs 
Network state determines 
when information is read 
in/out of cell, and when cell 
is emptied. 

Some Preliminaries: Deep RNNs 
●RNNs can be layered: 
output of lower layers is 
input to higher layers 
●Different interpretations: 
higher-order patterns, 
memory 
●Generally needed for 
harder problems 

General Artificial Intelligence Conditional Generation 

General Artificial Intelligence Conditional Generation 

General Artificial Intelligence Many NLP (and other!) tasks are castable as transduction problems. E.g.: 
Translation:  English to French transduction 
Parsing:  String to tree transduction 
Computation:  Input data to output data transduction Transduction and RNNs 
General Artificial Intelligence Generally, goal is to transform some source sequence 
into some target sequence Transduction and RNNs 

General Artificial Intelligence Approach: 
1.Model P(ti+1|t1...tn; S) with an RNN 
2.Read in source sequences 
3.Generate target sequences (greedily, beam search, etc). Transduction and RNNs 
General Artificial Intelligence ●Concatenate source and target sequences into joint sequences: 
s1 s2 ... sm ||| t1 t2 ... tn
●Train a single RNN over joint sequences 
●Ignore RNN output until separator symbol (e.g. "|||") 
●Jointly learn to compose source and generate target sequences Encoder-Decoder Model 
General Artificial Intelligence 
Deep LSTMs for Translation 
(Sutskever et al.  NIPS 2014) 
General Artificial Intelligence Task (Zaremba and Sutskever, 2014): 
●Read simple python scripts character-by-character 
●Output numerical result character-by-character. Learning to Execute 

General Artificial Intelligence 
The Transduction Bottleneck 
General Artificial Intelligence 1.Sequence-to-Sequence Modelling with RNNs 
2.Transduction with Unbounded Neural Memory 
3.Machine Reading with Attention 
4.Recognising Entailment with Attention Today's Topics 
General Artificial Intelligence We introduce memory modules that act like Stacks/Queues/DeQues: 
●Memory "size" grows/shrinks dynamically 
●Continuous push/pop not affected by number of objects stored 
●Can capture unboundedly long range dependencies*
●Propagates gradient flawlessly*
Solution: Unbounded Neural Memory 
(* if operated correctly: see paper's appendix) 
General Artificial Intelligence Example: A Continuous Stack 

General Artificial Intelligence Example: A Continuous Stack 

General Artificial Intelligence Controlling a Neural Stack 

General Artificial Intelligence Copy 
a1a2a3...an → a1a2a3...an
Reversal 
a1a2a3...an → an...a3a2a1
Bigram Flipping 
a1a2a3a4...an-1an → a2a1a4a3...anan-1Synthetic Transduction Tasks 
General Artificial Intelligence Subject-Verb-Object to Subject-Object-Verb Reordering 
si1 vi28 oi5 oi7 si15 rpi si19 vi16 oi10 oi24 → so1 oo5 oo7 so15 rpo so19 vo16 oo10 oo24 vo28 
Genderless to Gendered Grammar 
we11 the en19 and the em17 → wg11 das gn19 und der gm17 Synthetic ITG Transduction Tasks 
General Artificial Intelligence ●Coarse-grained accuracy 
Proportion of entirely correctly predicted sequences in test set 
●Fine-grained accuracy 
Average proportion of sequence correctly predicted before first error Coarse- and Fine-Grained Accuracy 
General Artificial Intelligence Results 
Experiment Stack Queue DeQue Deep LSTM 
Copy Poor Solved Solved Poor 
Reversal Solved Poor Solved Poor 
Bigram Flip Converges Best Results Best Results Converges 
SVO-SOV Solved Solved Solved Converges 
Conjugation Converges Solved Solved Converges 
Every Neural Stack/Queue/DeQue that solves a problem preserves the solution for 
longer sequences (tested up to 2x length of training sequences). 
General Artificial Intelligence Rapid Convergence 

General Artificial Intelligence 1.Sequence-to-Sequence Modelling with RNNs 
2.Transduction with Unbounded Neural Memory 
3.Machine Reading with Attention 
4.Recognising Entailment with Attention Today's Topics 
General Artificial Intelligence 1.Read text 
2.Synthesise its information 
3.Reason on basis of that information 
4.Answer questions based on steps 1–3 
We want to build models that can read text and 
answer questions based on them! 
Natural Language Understanding 
For the other three steps we first need to solve the data bottleneck So far we are very good at step 1! 
General Artificial Intelligence Data (I) – Microsoft MCTest Corpus 
James the Turtle was always getting in trouble. Sometimes he’d reach into the freezer and 
empty out all the food. Other times he’d sled on the deck and get a splinter. His aunt Jane 
tried as hard as she could to keep him out of trouble, but he was sneaky and got into lots 
of trouble behind her back. One day, James thought he would go into town and see what 
kind of trouble he could get into. He went to the grocery store and pulled all the pudding 
off the shelves and ate two jars. Then he walked to the fast food restaurant and ordered 15 
bags of fries. He didn’t pay, and instead headed home. … 
Where did James go after he went to the grocery store? 
1. his deck 
2. his freezer 
3. a fast food restaurant 
4. his room 
General Artificial Intelligence Data (II) – Facebook Synthetic Data 
John picked up the apple. 
John went to the office. 
John went to the kitchen. 
John dropped the apple. 
Query:  Where was the apple before the kitchen? 
Answer:  office 
General Artificial Intelligence A new source for Reading Comprehension data 
The CNN and Daily Mail websites provide paraphrase 
summary sentences for each full news story. 
Hundreds of thousands of documents 
Millions of context-query pairs 
Hundreds of entities 
General Artificial Intelligence Large-scale Supervised Reading Comprehension 
The BBC producer allegedly struck by Jeremy Clarkson will not press charges against the 
“Top Gear” host, his lawyer said Friday. Clarkson, who hosted one of the most-watched 
television shows in the world, was dropped by the BBC Wednesday after an internal 
investigation by the British broadcaster found he had subjected producer Oisin Tymon “to 
an unprovoked physical and verbal attack.” … 
Cloze-style question: 
Query: Producer X will not press charges against Jeremy Clarkson, his lawyer says. 
Answer: Oisin Tymon 
General Artificial Intelligence One catch: Avoid the Language Model trap 
From the Daily Mail: 
●The hi-tech bra that helps you beat breast X
●Could Saccharin help beat X ?
●Can fish oils help fight prostate X ?
Any n-gram language model train on the Daily 
Mail would correctly predict ( X = cancer) 
General Artificial Intelligence Anonymisation and permutation 
Carefully designed problem to avoid shortcuts such as QA by LM: 
⇛ We only solve this task if we solve it in the most general way possible: 
(CNN) New Zealand are on course 
for a first ever World Cup title after a 
thrilling semifinal victory over South 
Africa, secured off the penultimate 
ball of the match. 
Chasing an adjusted target of 298 in 
just 43 overs after a rain interrupted 
the match at Eden Park, Grant Elliott 
hit a six right at the death to confirm 
victory and send the Auckland crowd 
into raptures. It is the first time they 
have ever reached a world cup final. 
Question: 
_____ reach cricket Word Cup 
final? 
Answer: 
New Zealand (ent23 ) ent7 are on course for a first 
ever ent15 title after a thrilling 
semifinal victory over ent34 , secured 
off the penultimate ball of the match. 
Chasing an adjusted target of 298 in 
just 43 overs after a rain interrupted 
the match at ent12 , ent17  hit a six 
right at the death to confirm victory 
and send the ent83 crowd into 
raptures. It is the first time they have 
ever reached a ent15  final. 
Question: 
_____ reach ent3 ent15 final? 
Answer: 
ent7The easy way ... … our way 
General Artificial Intelligence www.github.com/deepmind/rc-data 
or follow " Further Details " link under the paper's entry on 
www.deepmind.com/publications Get the data now! 
General Artificial Intelligence Baseline Model Results 

General Artificial Intelligence Neural Machine Reading 
We estimate the probability of 
word type a from document d 
answering query q:
where W(a) indexes row a of W 
and g(d,q) embeds of a 
document and query pair. The Deep LSTM Reader 

General Artificial Intelligence Achtung! 
We can improve on this using 
an attention model over a 
bidirectional LSTM 
● Separate encodings for 
query and context tokens 
● Attend over context 
token encodings 
● Predict based on joint 
weighted attention and 
query representation The Attentive Reader 

General Artificial Intelligence Impatience can be a virtue 
We developed a nice iterative 
extension to the Attentive 
Reader as follows 
● Read query word by word 
● Attend over document at 
each step through query 
● Iteratively combine 
attention distribution 
● Predict answer with 
increased accuracy The Impatient Reader 

General Artificial Intelligence Impatience is a virtue - Results 

General Artificial Intelligence The Attentive Reader - Correct Example 
Correct prediction ( ent49 ) - Requires anaphora resolution 
General Artificial Intelligence The Attentive Reader - Failed Prediction 
Correct entity ent2, predicted ent24  - Geographic ambiguity 


General Artificial Intelligence 1.Sequence-to-Sequence Modelling with RNNs 
2.Transduction with Unbounded Neural Memory 
3.Machine Reading with Attention 
4.Recognising Entailment with Attention Today's Topics 
General Artificial Intelligence Recognizing Textual Entailment (RTE) 
A wedding party is taking pictures 
-There is a funeral Contradiction 
-They are outside Neutral 
-Someone got married Entailment 
A man is crowd surfing at a concert 
-The man is at a football game Contradiction 
-The man is drunk Neutral 
-The man is at a concert Entailment 
46
Project on RTE while working with SICK corpus 
(Marelli et al., SemEval 2014) The last 1.5 months of Tim's internship, with the 
SNLI corpus (Bowman et al., EMNLP 2015) 
10k sentence pairs, partly synthetic 570k sentence pairs from Mechanical Turkers 
EMNLP 2015 “best data set or resource” award!  
47Stanford Natural Language Inference Corpus 
General Artificial Intelligence Model 
48
General Artificial Intelligence Attention ( Bahdanau et al., 2014; Mnih et al., 2014 )
49
Word Matching 
50
Spotting Contradictions 
51
Fuzzy Attention 
52
Word-by-Word Attention ( Hermann et al. 2015 )
53
Word Matching and Synonyms 
54
Words and Phrases 
55
Girl + Boy = Kids 
56
Reordering 
57
Snow is outside 
58
It can get confused 
59
General Artificial Intelligence 60
Results 
General Artificial Intelligence Thanks for listening! 
Learning to Transduce with Unbounded Memory  (NIPS 2015) 
Grefenstette et al.  2015, arXiv:1506.02516 [cs.NE] 
Teaching Machines to Read and Comprehend (NIPS 2015) 
Hermann et al.  2015, arXiv:1506.03340 [cs.CL] 
Reasoning about Entailment with Neural Attention  (upcoming) 
Rocktäschel et al.  2015, arXiv:1509.06664 [cs.CL] 
joinus@deepmind.com 
Learning	
  theory	
  Lecture	
  9	
  David	
  Sontag	
  New	
  York	
  University	
  Slides adapted from Carlos Guestrin & Luke Zettlemoyer 
1. Generaliza:on	
  of	
  ﬁnite	
  hypothesis	
  spaces	
  2. VC-­‐dimension	
  • Will	
  show	
  that	
  linear	
  classiﬁers	
  need	
  to	
  see	
  approximately	
  d	
  training	
  points,	
  where	
  d	
  is	
  the	
  dimension	
  of	
  the	
  feature	
  vectors	
  • Explains	
  the	
  good	
  performance	
  we	
  obtained	
  using	
  perceptron!!!!	
  (we	
  had	
  a	
  few	
  thousand	
  features)	
  3. Margin	
  based	
  generaliza:on	
  • Applies	
  to	
  inﬁnite	
  dimensional	
  feature	
  vectors	
  (e.g.,	
  Gaussian	
  kernel)	
  
Number of training examples Test error (percentage misclassified) Perceptron algorithm on spam classification Roadmap	
  of	
  next	
  lectures	
  
[Figure from Cynthia Rudin] 
How	
  big	
  should	
  your	
  valida:on	
  set	
  be?	
  • In	
  PS1,	
  you	
  tried	
  many	
  conﬁgura:ons	
  of	
  your	
  algorithms	
  (avg	
  vs.	
  regular	
  perceptron,	
  max	
  #	
  of	
  itera:ons)	
  and	
  chose	
  the	
  one	
  that	
  had	
  smallest	
  valida:on	
  error	
  • Suppose	
  in	
  total	
  you	
  tested	
  |H|=40	
  diﬀerent	
  classiﬁers	
  on	
  the	
  valida:on	
  set	
  of	
  m	
  held-­‐out	
  e-­‐mails	
  • The	
  best	
  classiﬁer	
  obtains	
  98%	
  accuracy	
  on	
  these	
  m	
  e-­‐mails!!!	
  • But,	
  what	
  is	
  the	
  true	
  classiﬁca:on	
  accuracy?	
  • How	
  large	
  does	
  m	
  need	
  to	
  be	
  so	
  that	
  we	
  can	
  guarantee	
  that	
  the	
  best	
  conﬁgura:on	
  (measured	
  on	
  validate)	
  is	
  truly	
  good?	
  
A	
  simple	
  se_ng…	
  	
  • Classiﬁca:on	
  – m	
  data	
  points	
  – Finite	
  number	
  of	
  possible	
  hypothesis	
  (e.g.,	
  40	
  spam	
  classiﬁers)	
  • A	
  learner	
  ﬁnds	
  a	
  hypothesis	
  h	
  that	
  is	
  consistent	
  with	
  training	
  data	
  – Gets	
  zero	
  error	
  in	
  training:	
  	
  errortrain(h)	
  =	
  0	
  – I.e.,	
  assume	
  for	
  now	
  that	
  one	
  of	
  the	
  classiﬁers	
  gets	
  100%	
  accuracy	
  on	
  the	
  m	
  e-­‐mails	
  (we’ll	
  handle	
  the	
  98%	
  case	
  aderward)	
  • What	
  is	
  the	
  probability	
  that	
  h	
  has	
  more	
  than	
  ε	
  true	
  error?	
  – errortrue(h)	
  ≥	
  ε	

HHc ⊆H consistent with data 
Intro	
  to	
  probability:	
  outcomes	
  • An	
  outcome	
  space	
  speciﬁes	
  the	
  possible	
  outcomes	
  that	
  we	
  would	
  like	
  to	
  reason	
  about,	
  e.g.	
  Ω = { } Coin toss 
, Ω = { } Die toss , 
, , , , • We	
  specify	
  a	
  probability	
  p(x)	
  for	
  each	
  outcome	
  x	
  such	
  that	
  Xx2⌦p(x)=1p(x) 0,
E.g.,  p( ) = .6 
p( ) = .4 
Intro	
  to	
  probability:	
  events	
  • An	
  event	
  is	
  a	
  subset	
  of	
  the	
  outcome	
  space,	
  e.g.	
  O = { } Odd die tosses , 
, E = { } Even die tosses 
, , • The	
  probability	
  of	
  an	
  event	
  is	
  given	
  by	
  the	
  sum	
  of	
  the	
  probabili:es	
  of	
  the	
  outcomes	
  it	
  contains,	
  p(E)=Xx2Ep(x)E.g.,  p(E) =  p(         ) + p(        ) + p(        ) 
= 1/2,  if fair die 
Intro	
  to	
  probability:	
  union	
  bound	
  • P(A	
  or	
  B	
  or	
  C	
  or	
  D	
  or	
  …)	
  ≤ P(A) + P(B) + P(C) + P(D) + … 
Q: When is this a tight bound? A: For disjoint events (i.e., non-overlapping circles) p(A[B)=p(A)+p(B) p(A\B)p(A)+p(B)D
B
A
C

Intro	
  to	
  probability:	
  independence	
  • Two	
  events	
  A	
  and	
  B	
  are	
  independent	
  if	
  p(A\B)=p(A)p(B)B
A
Are these events independent? No! p(A\B)=0p(A)p(B)=✓16◆2
Intro	
  to	
  probability:	
  independence	
  • Two	
  events	
  A	
  and	
  B	
  are	
  independent	
  if	
  p(A\B)=p(A)p(B)• Suppose	
  our	
  outcome	
  space	
  had	
  two	
  diﬀerent	
  die:	
  Ω = { } 2 die tosses , 
, 
, … , 
62 = 36 outcomes and the probability of each outcome is defined as 
p( ) = a1 b1 
p( ) = a1 b2 
a1	
  a2	
  a3	
  a4	
  a5	
  a6	
  .1	
  .12	
  .18	
  .2	
  .1	
  .3	
  b1	
  b2	
  b3	
  b4	
  b5	
  b6	
  .19	
  .11	
  .1	
  .22	
  .18	
  .2	
  … 6Xi=1ai=16Xj=1bj=1Analogy:	
  outcome	
  space	
  deﬁnes	
  all	
  possible	
  sequences	
  of	
  e-­‐mails	
  in	
  training	
  set	
  
Intro	
  to	
  probability:	
  independence	
  • Two	
  events	
  A	
  and	
  B	
  are	
  independent	
  if	
  • Are	
  these	
  events	
  independent?	
  p(A\B)=p(A)p(B)A 
B 
p(A) = 
p( ) p(B) = p( ) 
p(A)p(B)=✓16◆2
    p( ) ) p( 
Yes! p(A\B)=0p( ) 
=6Xj=1a1bj=a16Xj=1bj=a1=b2Analogy:	
  asking	
  about	
  ﬁrst	
  e-­‐mail	
  in	
  training	
  set	
  Analogy:	
  asking	
  about	
  second	
  e-­‐mail	
  in	
  training	
  set	
  
Intro	
  to	
  probability:	
  discrete	
  random	
  variables	
  Discrete random variablesOften each outcome corresponds to a setting of variousattributes(e.g., “age”, “gender”, “hasPneumonia”, “hasDiabetes”)Arandom variableXis a mappingX:⌦!DDis some set (e.g., the integers)Induces a partition of all outcomes⌦For somex2D,w es a yp(X=x)=p({!2⌦:X(!)=x})“probability that variableXassumes statex”Notation: Val(X)=s e tDof all values assumed by X(will interchangeably call these the “values” or “states” of variableX)p(X) is a distribution:Px2Val(X)p(X=x)=1David Sontag (NYU)Graphical ModelsLecture 1, January 31, 2013 20 / 44Ω = { } 2 die tosses , 
, 
, … , 

Intro	
  to	
  probability:	
  discrete	
  random	
  variables	
  Discrete random variablesOften each outcome corresponds to a setting of variousattributes(e.g., “age”, “gender”, “hasPneumonia”, “hasDiabetes”)Arandom variableXis a mappingX:⌦!DDis some set (e.g., the integers)Induces a partition of all outcomes⌦For somex2D,w es a yp(X=x)=p({!2⌦:X(!)=x})“probability that variableXassumes statex”Notation: Val(X)=s e tDof all values assumed by X(will interchangeably call these the “values” or “states” of variableX)p(X) is a distribution:Px2Val(X)p(X=x)=1David Sontag (NYU)Graphical ModelsLecture 1, January 31, 2013 20 / 44
Ω = { } 2 die tosses , 
, 
, … , 
• E.g.	
  X1	
  may	
  refer	
  to	
  the	
  value	
  of	
  the	
  ﬁrst	
  dice,	
  and	
  X2	
  to	
  the	
  value	
  of	
  the	
  second	
  dice	
  • We	
  call	
  two	
  random	
  variables	
  X	
  and	
  Y	
  iden+cally	
  distributed	
  if	
  Val(X)	
  =	
  Val(Y)	
  and	
  p(X=s)	
  =	
  p(Y=s)	
  for	
  all	
  s	
  in	
  Val(X)	
  
p( ) = a1 b1 
p( ) = a1 b2 
a1	
  a2	
  a3	
  a4	
  a5	
  a6	
  .1	
  .12	
  .18	
  .2	
  .1	
  .3	
  b1	
  b2	
  b3	
  b4	
  b5	
  b6	
  .19	
  .11	
  .1	
  .22	
  .18	
  .2	
  … 6Xi=1ai=16Xj=1bj=1X1	
  and	
  X2	
  NOT	
  iden:cally	
  distributed 
Intro	
  to	
  probability:	
  discrete	
  random	
  variables	
  Discrete random variablesOften each outcome corresponds to a setting of variousattributes(e.g., “age”, “gender”, “hasPneumonia”, “hasDiabetes”)Arandom variableXis a mappingX:⌦!DDis some set (e.g., the integers)Induces a partition of all outcomes⌦For somex2D,w es a yp(X=x)=p({!2⌦:X(!)=x})“probability that variableXassumes statex”Notation: Val(X)=s e tDof all values assumed by X(will interchangeably call these the “values” or “states” of variableX)p(X) is a distribution:Px2Val(X)p(X=x)=1David Sontag (NYU)Graphical ModelsLecture 1, January 31, 2013 20 / 44
Ω = { } 2 die tosses , 
, 
, … , 
• E.g.	
  X1	
  may	
  refer	
  to	
  the	
  value	
  of	
  the	
  ﬁrst	
  dice,	
  and	
  X2	
  to	
  the	
  value	
  of	
  the	
  second	
  dice	
  • We	
  call	
  two	
  random	
  variables	
  X	
  and	
  Y	
  iden+cally	
  distributed	
  if	
  Val(X)	
  =	
  Val(Y)	
  and	
  p(X=s)	
  =	
  p(Y=s)	
  for	
  all	
  s	
  in	
  Val(X)	
  
p( ) = a1 a1 
p( ) = a1 a2 
a1	
  a2	
  a3	
  a4	
  a5	
  a6	
  .1	
  .12	
  .18	
  .2	
  .1	
  .3	
  … 6Xi=1ai=1X1	
  and	
  X2	
  iden:cally	
  distributed 
• X=x	
  is	
  simply	
  an	
  event,	
  so	
  can	
  apply	
  union	
  bound,	
  etc.	
  • Two	
  random	
  variables	
  X	
  and	
  Y	
  are	
  independent	
  if:	
  • The	
  expectaEon	
  of	
  X	
  is	
  deﬁned	
  as:	
  • If	
  X	
  is	
  binary	
  valued,	
  i.e.	
  x	
  is	
  either	
  0	
  or	
  1,	
  then:	
  • Linearity	
  of	
  expecta:ons:	
  p(X=x, Y=y)=p(X=x)p(Y=y)8x2Val(X),y2Val(Y)E[X]=Xx2Val(X)p(X=x)xIntro	
  to	
  probability:	
  discrete	
  random	
  variables	
  
E[X]=p(X= 0)·0+p(X= 1)·1=p(X= 1)
Joint	
  probability.	
  Formally,	
  given	
  by	
  the	
  event	
  X=x\Y=y
EhnXi=1Xii=nXi=1E[Xi]
A	
  simple	
  se_ng…	
  	
  • Classiﬁca:on	
  – m	
  data	
  points	
  – Finite	
  number	
  of	
  possible	
  hypothesis	
  (e.g.,	
  40	
  spam	
  classiﬁers)	
  • A	
  learner	
  ﬁnds	
  a	
  hypothesis	
  h	
  that	
  is	
  consistent	
  with	
  training	
  data	
  – Gets	
  zero	
  error	
  in	
  training:	
  	
  errortrain(h)	
  =	
  0	
  – I.e.,	
  assume	
  for	
  now	
  that	
  one	
  of	
  the	
  classiﬁers	
  gets	
  100%	
  accuracy	
  on	
  the	
  m	
  e-­‐mails	
  (we’ll	
  handle	
  the	
  98%	
  case	
  aderward)	
  • What	
  is	
  the	
  probability	
  h	
  correctly	
  classiﬁes	
  all	
  m	
  data	
  points	
  given	
  that	
  h	
  has	
  more	
  than	
  ε	
  true	
  error?	
  – errortrue(h)	
  ≥	
  ε	

HHc ⊆H consistent with data 
• The	
  probability	
  of	
  a	
  hypothesis	
  h	
  incorrectly	
  classifying:	
  • Let	
  	
  	
  	
  	
  	
  	
  	
  be	
  a	
  random	
  variable	
  that	
  takes	
  two	
  values:	
  1	
  if	
  h	
  correctly	
  classiﬁes	
  ith	
  	
  data	
  point,	
  and	
  0	
  otherwise	
  • The	
  Zh	
  variables	
  are	
  independent	
  and	
  idenEcally	
  distributed	
  (i.i.d.)	
  with	
  • What	
  is	
  the	
  probability	
  that	
  h	
  classiﬁes	
  m	
  data	
  points	
  correctly?	
  ZhiX(~x,y)ˆp(~x, y)1[h(~x)6=y]How	
  likely	
  is	
  a	
  single	
  hypothesis	
  to	
  get	
  	
  m	
  data	
  points	
  right?	
  ✏h=Pr(Zhi= 0) =X(~x,y)ˆp(~x, y)1[h(~x)6=y]=✏hPr(h gets m iid data points right) =( 1 ✏h)me ✏hm
Are	
  we	
  done?	
  • Says	
  “with	
  probability	
  >	
  1-­‐e-­‐εm,	
  if	
  h	
  gets	
  m	
  data	
  points	
  correct,	
  then	
  it	
  is	
  close	
  to	
  perfect	
  (will	
  have	
  error	
  ≤	
  ε)”	
  • This	
  only	
  considers	
  one	
  hypothesis!	
  • Suppose	
  1	
  billion	
  classiﬁers	
  were	
  tried,	
  and	
  each	
  was	
  a	
  random	
  func:on	
  • For	
  m	
  small	
  enough,	
  one	
  of	
  the	
  func:ons	
  will	
  classify	
  all	
  points	
  correctly	
  –	
  but	
  all	
  have	
  very	
  large	
  true	
  error	
  Pr(h gets m iid data points right | errortrue(h) ≥ ε) ≤ e-εm 
How	
  likely	
  is	
  learner	
  to	
  pick	
  a	
  bad	
  hypothesis?	
  Suppose	
  there	
  are	
  |Hc|	
  hypotheses	
  consistent	
  with	
  the	
  training	
  data	
  – How	
  likely	
  is	
  learner	
  to	
  pick	
  a	
  bad	
  one,	
  i.e.	
  with	
  true	
  error	
  ≥	
  ε?	
  – We	
  need	
  a	
  bound	
  that	
  holds	
  for	
  all	
  of	
  them!	
  Pr(h gets m iid data points right | errortrue(h) ≥ ε) ≤ e-εm P(errortrue(h1) ≥ ε OR	
  errortrue(h2) ≥ ε OR … OR errortrue(h|Hc|) ≥ ε)  ≤ ∑kP(errortrue(hk) ≥ ε)  ! Union bound ≤ ∑k(1-ε)m  ! bound on individual hks  ≤ |H|(1-ε)m  ! |Hc| ≤ |H|   ≤ |H| e-mε  ! (1-ε) ≤ e-ε	
  for	
  0≤ε≤1  
Extra	
  analysis	
  

Generaliza:on	
  error	
  of	
  ﬁnite	
  hypothesis	
  spaces	
  [Haussler	
  ’88]	
  	
  Theorem:	
  Hypothesis	
  space	
  H	
  ﬁnite,	
  dataset	
  D	
  with	
  m	
  i.i.d.	
  samples,	
  0	
  <	
  ε	
  <	
  1	
  :	
  for	
  any	
  learned	
  hypothesis	
  h	
  that	
  is	
  consistent	
  on	
  the	
  training	
  data:	
  
We just proved the following result: 
Using	
  a	
  PAC	
  bound	
  	
  Typically,	
  2	
  use	
  cases:	
  – 1:	
  Pick	
  ε	
  and	
  δ,	
  compute	
  m	
  – 2:	
  Pick	
  m	
  and	
  δ,	
  compute	
  ε	

Argument:	
  Since	
  for	
  all	
  h	
  we	
  know	
  that	
  …	
  with	
  probability	
  1-­‐δ	
  the	
  following	
  holds…	
  (either	
  case	
  1	
  or	
  case	
  2)	
  ⌅(x)=⇤⌥⌥⌥⌥⌥⌥⌥⌥⌥⌥⌥⇧x(1)...x(n)x(1)x(2)x(1)x(3)...ex(1)...⌅           ⌃⇧L⇧w=w  j jyjxj⌅(u).⌅(v)= u1u2⇥. v1v2⇥=u1v1+u2v2=u.v⌅(u).⌅(v)=⇤⌥⌥⇧u21u1u2u2u1u22⌅  ⌃.⇤⌥⌥⇧v21v1v2v2v1v22⌅  ⌃=u21v21+2u1v1u2v2+u22v22=(u1v1+u2v2)2=(u.v)2⌅(u).⌅(v)=(u.v)dP(errortrue(h)⇥⇤)⇥|H|e m ⇥⇥
7⌅(x)=⇧           ⌥x(1)...x(n)x(1)x(2)x(1)x(3)...ex(1)...⌃⌦⌦⌦⌦⌦⌦⌦⌦⌦⌦⌦ ⇧L⇧w=w ↵j jyjxj⌅(u).⌅(v)=⇤u1u2⌅.⇤v1v2⌅=u1v1+u2v2=u.v⌅(u).⌅(v)=⇧  ⌥u21u1u2u2u1u22⌃⌦⌦ .⇧  ⌥v21v1v2v2v1v22⌃⌦⌦ =u21v21+2u1v1u2v2+u22v22=(u1v1+u2v2)2=(u.v)2⌅(u).⌅(v)=(u.v)dP(errortrue(h)⇥⇤)⇥|H|e m ⇥⇥ln |H|e m ⇥⇥ln⇥
7⌅(x)=⇧           ⌥x(1)...x(n)x(1)x(2)x(1)x(3)...ex(1)...⌃⌦⌦⌦⌦⌦⌦⌦⌦⌦⌦⌦ ⇧L⇧w=w ↵j jyjxj⌅(u).⌅(v)=⇤u1u2⌅.⇤v1v2⌅=u1v1+u2v2=u.v⌅(u).⌅(v)=⇧  ⌥u21u1u2u2u1u22⌃⌦⌦ .⇧  ⌥v21v1v2v2v1v22⌃⌦⌦ =u21v21+2u1v1u2v2+u22v22=(u1v1+u2v2)2=(u.v)2⌅(u).⌅(v)=(u.v)dP(errortrue(h)⇥⇤)⇥|H|e m ⇥⇥ln |H|e m ⇥⇥ln⇥ln|H| m⇤⇥ln⇥
7⌅(x)=⇧           ⌥x(1)...x(n)x(1)x(2)x(1)x(3)...ex(1)...⌃⌦⌦⌦⌦⌦⌦⌦⌦⌦⌦⌦ ⇧L⇧w=w ↵j jyjxj⌅(u).⌅(v)=⇤u1u2⌅.⇤v1v2⌅=u1v1+u2v2=u.v⌅(u).⌅(v)=⇧  ⌥u21u1u2u2u1u22⌃⌦⌦ .⇧  ⌥v21v1v2v2v1v22⌃⌦⌦ =u21v21+2u1v1u2v2+u22v22=(u1v1+u2v2)2=(u.v)2⌅(u).⌅(v)=(u.v)dP(errortrue(h)⇥⇤)⇥|H|e m⇥⇥⇥ln |H|e m⇥⇥⇥ln⇥ln|H| m⇤⇥ln⇥m⇤ln|H|+ ln1 ⇤7
Case 1 ⌅(x)=⇧           ⌥x(1)...x(n)x(1)x(2)x(1)x(3)...ex(1)...⌃⌦⌦⌦⌦⌦⌦⌦⌦⌦⌦⌦ ⇧L⇧w=w ↵j jyjxj⌅(u).⌅(v)=⇤u1u2⌅.⇤v1v2⌅=u1v1+u2v2=u.v⌅(u).⌅(v)=⇧  ⌥u21u1u2u2u1u22⌃⌦⌦ .⇧  ⌥v21v1v2v2v1v22⌃⌦⌦ =u21v21+2u1v1u2v2+u22v22=(u1v1+u2v2)2=(u.v)2⌅(u).⌅(v)=(u.v)dP(errortrue(h)⇥⇤)⇥|H|e m⇥⇥⇥ln |H|e m⇥⇥⇥ln⇥ln|H| m⇤⇥ln⇥m⇤ln|H|+ ln1 ⇤⇤⇤ln|H|+ ln1 m7
Case 2 
Log	
  dependence	
  on	
  |H|,	
  OK	
  if	
  exponen:al	
  size	
  (but	
  not	
  doubly)	
  
ε	
  shrinks	
  at	
  rate	
  O(1/m)	
  ε has	
  stronger	
  inﬂuence	
  than δ  
Says: we are willing to tolerate a δ probability of having ≥ ε error  p(errortrue(h) ✏)|H|e m✏✏= =.01,|H|= 40Needm 830
Limita:ons	
  of	
  Haussler	
  ‘88	
  bound	
  • There	
  may	
  be	
  no	
  consistent	
  hypothesis	
  h	
  (where	
  errortrain(h)=0)	
  • Size	
  of	
  hypothesis	
  space	
  – What	
  if	
  |H|	
  is	
  really	
  big?	
  – What	
  if	
  it	
  is	
  con:nuous?	
  • First	
  Goal:	
  Can	
  we	
  get	
  a	
  bound	
  for	
  a	
  learner	
  with	
  errortrain(h)	
  in	
  the	
  data	
  set?	
  	
  
Ques:on:	
  What’s	
  the	
  expected	
  error	
  of	
  a	
  hypothesis?	
  • The	
  probability	
  of	
  a	
  hypothesis	
  incorrectly	
  classifying:	
  • Let’s	
  now	
  let	
  	
  	
  	
  	
  	
  	
  be	
  a	
  random	
  variable	
  that	
  takes	
  two	
  values,	
  1	
  if	
  h	
  correctly	
  classiﬁes	
  ith	
  	
  data	
  point,	
  and	
  0	
  otherwise	
  • The	
  Z	
  variables	
  are	
  independent	
  and	
  idenEcally	
  distributed	
  (i.i.d.)	
  with	
  • Es:ma:ng	
  the	
  true	
  error	
  probability	
  is	
  like	
  es:ma:ng	
  the	
  parameter	
  of	
  a	
  coin!	
  • Chernoﬀ	
  bound:	
  for	
  m	
  i.i.d.	
  coin	
  ﬂips,	
  X1,…,Xm,	
  where	
  Xi	
  ∈	
  {0,1}.	
  For	
  0<ε<1:	
  
Zhi
p(Xi= 1) =✓E[1mmXi=1Xi]=1mmXi=1E[Xi]=✓
Observed	
  frac:on	
  of	
  points	
  incorrectly	
  classiﬁed	
  (by	
  linearity	
  of	
  expecta:on)	
  
True	
  error	
  probability	
  X(~x,y)ˆp(~x, y)1[h(~x)6=y]Pr(Zhi= 0) =X(~x,y)ˆp(~x, y)1[h(~x)6=y]
Generaliza:on	
  bound	
  for	
  |H|	
  hypothesis	
  Theorem:	
  Hypothesis	
  space	
  H	
  ﬁnite,	
  dataset	
  D	
  with	
  m	
  i.i.d.	
  samples,	
  0	
  <	
  ε	
  <	
  1	
  :	
  for	
  any	
  learned	
  hypothesis	
  h:	
  Why? Same reasoning as before. Use the Union bound over individual Chernoff bounds  Pr(errortrue(h) errorD(h)>✏)|H|e 2m✏2
PAC	
  bound	
  and	
  Bias-­‐Variance	
  tradeoﬀ	
  	
  for all h, with probability at least 1-δ:	

“bias” 
“variance” 
• For	
  large	
  |H|	
  – low	
  bias	
  (assuming	
  we	
  can	
  ﬁnd	
  a	
  good	
  h)	
  – high	
  variance	
  (because	
  bound	
  is	
  looser)	
  • For	
  small	
  |H|	
  – high	
  bias	
  (is	
  there	
  a	
  good	
  h?)	
  – low	
  variance	
  (:ghter	
  bound)	
  errortrue(h)errorD(h)+sln|H|+l n1 2m
Support vector machines (SVMs) Lecture 3 David Sontag New York University Slides adapted from Luke Zettlemoyer, Vibhav Gogate, and Carlos Guestrin 
Geometry of linear separators (see blackboard) 
A plane can be specified as the set of all points given by: 
Barber, Section A.1.1-4 
Vector from origin to a point in the plane 
Two non-parallel directions in the plane 
Alternatively, it can be specified as: 
Normal vector (we will call this w) 
Only need to specify this dot product, a scalar (we will call this the offset, b) 
Linear Separators ! If training data is linearly separable, perceptron is guaranteed to find some linear separator ! Which of these is optimal?  
! SVMs (Vapnik, 1990’s) choose the linear separator with the largest margin 
• Good according to intuition, theory, practice • SVM became famous when, using images as input, it gave accuracy comparable to neural-network with hand-designed features in a handwriting recognition task Support Vector Machine (SVM) 
V. Vapnik Robust to outliers! 
1. Use optimization to find solution (i.e. a hyperplane) with few errors 2. Seek large margin separator to improve generalization 3. Use kernel trick to make large feature spaces computationally efficient Support vector machines: 3 key ideas 
w.x + b = +1 w.x + b = -1 w.x + b = 0 Finding a perfect classifier (when one exists)  using linear programming  for yt = +1, and for yt = -1, For every data point (xt, yt), enforce the constraint 
Equivalently, we want to satisfy all of the linear constraints 
This linear program can be efficiently solved using algorithms such as simplex, interior point, or ellipsoid 
Finding a perfect classifier (when one exists)  using linear programming  
Example of 2-dimensional linear programming  (feasibility) problem: For SVMs, each data point gives one inequality: 
What happens if the data set is not linearly separable? 
Weight space 
• Try to find weights that violate as few constraints as possible? • Formalize this using the 0-1 loss: • Unfortunately, minimizing 0-1 loss is NP-hard in the worst-case – Non-starter. We need another approach. 
#(mistakes) Minimizing number of errors (0-1 loss)  
where 

Key idea #1: Allow for slack  
For each data point: • If functional margin ≥ 1, don’t care • If functional margin < 1, pay linear penalty 
w.x + b = +1 w.x + b = -1 w.x + b = 0 
ξ2 
ξ1 
ξ3 
ξ4 Σj ξj - ξj ξj≥0 “slack variables” 
We now have a linear program again, and can efficiently find its optimum , ξ 
Key idea #1: Allow for slack  
w.x + b = +1 w.x + b = -1 w.x + b = 0 Σj ξj - ξj ξj≥0 “slack variables” 
, ξ What is the optimal value ξj* as a function of w* and b*? If  
then ξj =  0 
If  then ξj =  
Sometimes written as 
ξ2 
ξ1 
ξ3 
ξ4 
Equivalent hinge loss formulation  
Σj ξj - ξj ξj≥0 
Substituting into the objective, we get: , ξ 
This is empirical risk minimization, using the hinge loss 
The hinge loss is defined as  

Hinge loss vs. 0/1 loss  
1 0 1 Hinge loss upper bounds 0/1 loss! It is the tightest convex upper bound on the 0/1 loss  
Hinge loss: 
0-1 Loss: 

Key idea #2: seek large margin 
Decoding,*Con,nued*September*5,*2013*
Lecture*Outline*! Viterbi*algorithm*! Decoding*more*generally*3. Five*views*! MPE/MAP*inference*in*a*graphical*model*
2.**Polytopes*
“Parts”*• Assume*that*feature*func,on*g*breaks*down*into*local*parts.**• Each*part*has*an*alphabet*of*possible*values.*– Decoding*is*choosing*values*for*all*parts,*with*consistency*constraints.*– (In*the*graphical*models*view,*a*part*is*a*clique.)*g(x,y)=#parts(x) i=1f( i(x,y))
Example*• One*part*per*word,*each*is*in*{B,*I,*O}*• No*features*look*at*mul,ple*parts*– Fast*inference*– Not*very*expressive*

Example*• One*part*per*bigram,*each*is*in*{BB,*BI,*BO,*IB,*II,*IO,*OB,*OO}*• Features*and*constraints*can*look*at*pairs**– Slower*inference*– A*bit*more*expressive*

Geometric*View*• Let*zi,π*be*1*if*part*i*takes*value*π*and*0*otherwise.*• z*is*a*vector*in*{0,*1}N**– N%=*total*number*of*localized*part*values*– Each$z*is*a*vertex*of*the*unit*cube*

Score*is*Linear*in*z$arg maxyw⇥g(x,y) = arg maxyw⇥#parts(x)⇤i=1f( i(x,y))= arg maxyw⇥#parts(x)⇤i=1⇤  Values( i)f( )1{ i(x,y)= }= arg maxz Zxw⇥#parts(x)⇤i=1⇤  Values( i)f( )zi, = arg maxz Zxw⇥Fxz= arg maxz Zx w⇥Fx⇥z
not*really*equal;*need*to*transform*back*to*get*y$
Polyhedra*• Not*all*ver,ces*of*the*NZdimensional*unit*cube*sa,sfy*the*constraints.*– E.g.,*cant*have*z1,BI*=*1 and*z2,BI*=*1*• Some,mes*we*can*write*down*a*small*(polynomial*number)*of*linear*constraints*on*z.*• Result:**linear*objec,ve,*linear*constraints,*integer*constraints*…*


Integer*Linear*Programming*• Very*easy*to*add*new*constraints*and*nonZlocal*features.*• Many*decoding*problems*have*been*mapped*to*ILP*(sequence*labeling,*parsing,*…),*but*its*not%always*trivial.**• NPZhard*in*general.*– But*there*are*packages*that*o`en*work*well*in*prac,ce*(e.g.,*CPLEX)*– Specialized*algorithms*in*some*cases*– LP*relaxa,on*for*approximate*solu,ons*

Remark*• Graphical*models*assumed*a*probabilis,c*interpreta,on*– Though*they*are*not*always*learned*using*a*probabilis,c*interpreta,on!*• The*polytope*view*is*agnos,c*about*how*you*interpret*the*weights.*– It*only*says*that*the*decoding*problem*is*an*ILP.*
3.**Weighted*Parsing*
Grammars*• Grammars*are*o`en*associated*with*natural*language*parsing,*but*they*are*extremely*powerful*for*imposing*constraints.*• We*can*add*weights*to*them.*– HMMs*are*a*kind*of*weighted*regular*grammar*(closely*connected*to*WFSAs)*– PCFGs*are*a*kind*of*weighted*CFG*– Many,*many*more.*• Weighted*parsing:**ﬁnd*the*maximumZweighted*deriva,on*for*a*string*x.**
Decoding*as*Weighted*Parsing*• Every*valid*y*is*a*gramma,cal*deriva,on*(parse)*for*x.*– HMM:**sequence*of*gramma,cal*states*is*one*allowed*by*the*transi,on*table.*• Augment*parsing*algorithms*with*weights*and*ﬁnd*the*best*parse.**
The*Viterbi*algorithm*is*an*instance*of*recogni,on*by*a*weighted*grammar!*
BIO*Tagging*as*a*CFG*
• Weighted*(or*probabilis,c)*CKY*is*a*dynamic*programming*algorithm*very*similar*in*structure*to*classical*CKY.*

4.**Paths*and*Hyperpaths*
Best*Path*• General*idea:**take*x*and*build*a*graph.*• Score*of*a*path*factors*into*the*edges.*• Decoding*is*ﬁnding*the*best%path.**arg maxyw⇥g(x,y) = arg maxyw⇥ e Edgesf(e)1{eis crossed byy’s path}
The*Viterbi*algorithm*is*an*instance*of*ﬁnding*a*best*path!*
“Lajce”*View*of*Viterbi*

A*Generic*Best*Path*Algorithm*• Input:**directed*graph*G*=*(V,*E),*cost*:*E*→*,*start*vertex*v0*• Output:**d*:*V*→**(shortest*path*func,on)*and*back*pointers*b*:*V*→*V**for*all*v**V*\*{v0},*d(v)*:=*∞*and*b(v)*:=**set*d(v0)*=*0*while*d*has*not*converged:**pick*an*arbitrary*edge*(u,*v)**if*d(u)*+*cost(u,*v)*<*d(v):***d(v)*:=*d(u)*+*cost(u,*v)****b(v)*:=*u***
Ordering*Updates*• Naïve*ways*of*choosing*edges*will*lead*to*cyclic*upda,ng*and*gross*ineﬃciency!*• Before*considering*various*ways*of*doing*it,*let's*consider*how*the*Viterbi*algorithm*is*essen,ally*solving*the*same*problem.*
Viterbi*Algorithm**(In*the*Style*of*A*Best*Path*Algorithm)*• Input:***– directed*graph*G*=*(V,*E)*where**each*vertex*v*=*(q,*t),*q**Q**{},*t**{Z1,*0,*1,*…,*n}**and*each*edge*(u,*v)*=*((q,*t),*(q',*t*+*1))*– cost*:*E*→*,*deﬁned*by**cost((q,*t),*(q',*t*+*1))*=*–*log*γ(q'*|*q)*–*log*η(st+1*|*q)*–*log*(1*Z*ξ(q))*cost((q,*n*Z*1),*(q',*n))*=*–*log*γ(q'*|*q)*–*log*η(st+1*|*q)*–*log*ξ(q')*cost((,*Z1),*(q,*0))*=*–*log*π(q)*– ﬁxed*start*vertex*v0*=*(,*Z1)*• Output:**d*:*V*→**(shortest*path*func,on)*and*back*pointers*b*:*V*→*V**for*all*v**V*\*{v0},*d(v)*:=*∞*and*b(v)*:=**set*d(v0)*=*0*perform*a*topological*sort*on*V*while*d*has*not*converged:**for*each*v*in*topZsort*order:**pick*an*arbitrary*edge*(u,*v)****for*each*(u,*v)**E:*****if*d(u)*+*cost(u,*v)*<*d(v):****d(v)*:=*d(u)*+*cost(u,*v)*****b(v)*:=*u*%//%d(v)%and%b(v)%are%now%known%

The*Viterbi*Trick*• From*a*“best*path”*perspec,ve,*Viterbi*is:*– deﬁning*the*ver,ces*and*edges*to*have*special*structure*(state/,me*step)*– assigning*costs*based*on*HMM*weights*and*the*speciﬁc*input*string*s1*…*sn*– ordering*the*edges*cleverly*to*make*things*eﬃcient*• Note*also:**Viterbi's*graph*has*no*cycles.*
Another*Variant:*Forward*Upda,ng*• A`er*topological*sort,*can*also*choose*all*edges*going*out%of*current*node.*for*all*v**V*\*{v0},*d(v)*:=*∞*and*b(v)*:=**set*d(v0)*=*0*perform*a*topological*sort*on*V*for*each*u*in*topZsort*order:**for*each*(u,*v)**E:*****if*d(u)*+*cost(u,*v)*<*d(v):****d(v)*:=*d(u)*+*cost(u,*v)*****b(v)*:=*u*%*
Memoized*Recursion*• Input:**directed*graph*G*=*(V,*E),*cost*:*E*→*,*start*vertex*v0,*target*vertex*vt*• Output:**d*:*V*→**(shortest*path*func,on)*and*back*pointers*b*:*V*→*V***for*all*v**V*\*{v0},*d(v)*:=**and*b(v)*:=**set*d(v0)*=*0*memoize(vt)**memoize(v):*%//%guaranteed%to%return%best8cost%path%score%for%v%*if*d(v)*=*:***d(v)*:=*∞***for*each*(u,*v)**E:******if*memoize(u)*+*cost(u,*v)*<*d(v):*****d(v)*:=*d(u)*+*cost(u,*v)******b(v)*:=*u**return*d(v)*
A*Generic*Best*Path*Algorithm*• Input:**directed*graph*G*=*(V,*E),*cost*:*E*→*,*start*vertex*v0*• Output:**d*:*V*→**(shortest*path*func,on)*and*back*pointers*b*:*V*→*V**for*all*v**V*\*{v0},*d(v)*:=*∞*and*b(v)*:=**set*d(v0)*=*0*while*d*has*not*converged:**pick*an*arbitrary*edge*(u,*v)**if*d(u)*+*cost(u,*v)*<*d(v):***d(v)*:=*d(u)*+*cost(u,*v)****b(v)*:=*u***
Dijkstra's*Algorithm*• Input:**directed*graph*G*=*(V,*E),*cost*:*E*→*≥0*(important!),*start*vertex*v0*• Output:**d*:*V*→**(shortest*path*func,on)*and*back*pointers*b*:*V*→*V**for*all*v**V*\*{v0},*d(v)*:=*∞*and*b(v)*:=**set*d(v0)*=*0*Q*:=*priority*queue*on*V*ordered*by*d*(lower*cost*=*higher*priority)*while*d*has*not*converged:**while*Q*is*not*empty:**pick*an*arbitrary*edge*(u,*v)**u*:=*extractZmin(Q)**for*each*(u,*v)**E:***if*d(u)*+*cost(u,*v)*<*d(v):****d(v)*:=*d(u)*+*cost(u,*v)*****b(v)*:=*u****update*v's*priority*in*Q**

A**Algorithm*• Input:**directed*graph*G*=*(V,*E),*cost*:*E*→*≥0,*start*vertex*v0,*target*vertex*vt,*heuris,c*h*:*V*→*≥0*such*that*h(v)*≤*bestZcost(v,*vt)*• Output:**d*:*V*→**(shortest*path*func,on)*and*back*pointers*b*:*V*→*V**for*all*v**V*\*{v0},*d(v)*:=*∞*and*b(v)*:=**set*d(v0)*=*0*Q*:=*priority*queue*on*V*ordered*by*d*+*h*(lower*cost*=*higher*priority)*while*Q*is*not*empty:**u*:=*extractZmin(Q)**for*each*(u,*v)**E:***if*d(u)*+*cost(u,*v)*<*d(v):****d(v)*:=*d(u)*+*cost(u,*v)*****b(v)*:=*u****update*v's*priority*in*Q**
Minimum*Cost*Hyperpath*• General*idea:**take*x*and*build*a*hypergraph.*• Score*of*a*hyperpath*factors*into*the*hyperedges.*• Decoding*is*ﬁnding*the*best*hyperpath.*• This*connec,on*was*elucidated*by*Klein*and*Manning*(2002).**
Parsing*as*a*Hypergraph*

Parsing*as*a*Hypergraph*
cf.*Dean*for*democracy*
Parsing*as*a*Hypergraph*
Forced*to*work*on*his*thesis,*sunshine*streaming*in*the*window,*Mike*experienced*a*…*
Parsing*as*a*Hypergraph*
Forced*to*work*on*his*thesis,*sunshine*streaming*in*the*window,*Mike*began*to*…*
Why*Hypergraphs?*• Useful,*compact*encoding*of*the*hypothesis*space.*– Build*hypothesis*space*using*local*features,*maybe*do*some*ﬁltering.*– Pass*it*oﬀ*to*another*module*for*more*ﬁneZgrained*scoring*with*richer*or*more*expensive*features.*
5.**Weighted*Logic*Programming*
Logic*Programming*• Start*with*a*set*of*axioms*and*a*set*of*inference*rules.*• The*goal*is*to*prove*a*speciﬁc*theorem,*goal.*• Many*approaches,*but*we*assume*a*deduc=ve*approach.*– Start*with*axioms,*itera,vely*produce*more*theorems.*


Weighted*Logic*Programming*• Twist:**axioms*have*weights.*• Want*the*proof*of*goal with*the*best*score:*• Note*that*axioms*can*be*used*more*than*once*in*a*proof*(y).*arg maxyw⇥g(x,y) = arg maxyw⇥ a Axiomsf(a)freq(a;y)
Whence*WLP?*• Shieber,*Schabes,*and*Pereira*(1995):**many*parsing*algorithms*can*be*understood*in*the*same*deduc,ve*logic*framework.*• Goodman*(1999):**add*weights*in*a*semiring,*get*many*useful*NLP*algorithms.*• Eisner,*Goldlust,*and*Smith*(2004,*2005):**semiringZgeneric*algorithms,*Dyna.*
Dynamic*Programming*• Most*views*(excep,on*is*polytopes)*can*be*understood*as*DP*algorithms.*– The*lowZlevel*procedures%we*use*are*o`en*DP.*– Even*DP*is*too*highZlevel*to*know*the*best*way*to*implement.**• Break*a*problem*into*slightly*smaller*problems*with*op)mal$substructure.*– Best*path*to*v*depends*on*best*paths*to*all*u*such*that*(u,v)**E.*• Overlapping$subproblems:**each*subproblem*gets*used*repeatedly,*and*there*arent*too*many*of*them.$
Dynamic*Programming*• Three*main*strategies*for*DP:*– Viterbi,*Levenshtein*edit*distance,*CKY:**predeﬁned,*clever*ordering.*– Memoiza,on*– Agenda*(Dijkstras*algorithm,*A*)*• Things*to*remember*in*general:*– The*hypergraph*may*too*big*to*represent*explicitly;*exhaus,ve*calcula,on*may*be*too*expensive.*– The*hypergraph*may*or*may*not*have*proper,es*that*make*“clever”*orderings*possible.*– DP*does*not%imply*polynomial*,me*and*space!**Most*common*approxima,ons*when*the*desired*state*space*is*too*big:**beam*search,*cube*pruning,*agendas*with*early*stopping,*...*
Summary*• Decoding*is*the*general*problem*of*choosing*a*complex*structure.*– Linguis,c*analysis,*machine*transla,on,*speech*recogni,on,*…*– Sta,s,cal*models*are*usually*involved*(not*necessarily*probabilis,c).*• No*perfect*general*view,*but*much*can*be*gained*through*a*combina,on*of*views.*• First*ques,on:**can*I*solve*it*exactly*with*DP?*
Structured	
  Predic+on	
  for	
  Language	
  and	
  Other	
  Discrete	
  Data	
  (10-­‐710	
  and	
  11-­‐763)	
  Introductory	
  Lecture	
  
A	
  LiAle	
  Bit	
  of	
  History	
  1935:	
  	
  Zipf’s	
  law	
  1940s	
  &	
  1950s:	
  	
  empiricism:	
  	
  Shannon,	
  Weaver,	
  Harris,	
  Yngve	
  	
  
George	
  Kingsley	
  Zipf,	
  1935	
  • Heavy	
  tail	
  in	
  word	
  distribu+ons	
  • (Incomes,	
  too;	
  accurately	
  predicted	
  revolu+on	
  in	
  Indonesia)	
  
p(w) 1rank(w)
Claude	
  Shannon,	
  1948	
  • Father	
  of	
  informa+on	
  theory	
  • Entropy:	
  	
  a	
  mathema+cal	
  measure	
  of	
  uncertainty	
  • Informa+on	
  can	
  be	
  encoded	
  digitally;	
  ques+ons	
  include	
  how	
  to	
  encode	
  informa+on	
  eﬃciently	
  and	
  reliably.	
  • Huge	
  impact	
  on	
  speech	
  recogni+on	
  (and	
  space	
  explora+on	
  and	
  digital	
  media	
  inven+on	
  and	
  …)	
  

Warren	
  Weaver,	
  1949	
  • “One	
  naturally	
  wonders	
  if	
  the	
  problem	
  of	
  transla+on	
  could	
  conceivably	
  be	
  treated	
  as	
  a	
  problem	
  in	
  cryptography.	
  When	
  I	
  look	
  at	
  an	
  ar+cle	
  in	
  Russian,	
  I	
  say:	
  'This	
  is	
  really	
  wriAen	
  in	
  English,	
  but	
  it	
  has	
  been	
  coded	
  in	
  some	
  strange	
  symbols.	
  I	
  will	
  now	
  proceed	
  to	
  decode.”	
  

Zellig	
  Harris,	
  1940s	
  and	
  forward	
  • Centrality	
  of	
  data	
  for	
  linguis+c	
  analysis	
  • Transforma+ons	
  (a	
  step	
  toward	
  computa+onal	
  models	
  of	
  language)	
  • Heavy	
  use	
  of	
  mathema+cs	
  in	
  linguis+cs	
  

Victor	
  Yngve,	
  1958	
  • Early	
  computa+onal	
  linguist	
  • Showed	
  “depth	
  limit”	
  of	
  human	
  sentence	
  processing	
  -­‐	
  restricted	
  led	
  branching	
  (but	
  not	
  right)	
  • Theme:	
  	
  what	
  are	
  the	
  real	
  observables	
  in	
  language	
  study?	
  	
  Sound	
  waves!	
  • Early	
  programming	
  language,	
  COMIT,	
  for	
  linguists	
  (inﬂuenced	
  SNOBOL)	
  • Random	
  sentence	
  genera+on	
  (in	
  the	
  1950s)	
  
A	
  LiAle	
  Bit	
  of	
  History	
  1935:	
  	
  Zipf’s	
  law	
  1940s	
  &	
  1950s:	
  	
  empiricism:	
  	
  Shannon,	
  Weaver,	
  Harris,	
  Yngve	
  	
  1960-­‐1985:	
  	
  ra+onalism/representa+ons/formalisms/syntax/unapplied	
  AI	
  – 1962:	
  	
  ACL	
  (then	
  MTACL)	
  begins	
  – 1964-­‐6:	
  	
  ALPAC	
  report,	
  MT	
  winter,	
  Bar-­‐Hillel	
  leaves	
  the	
  ﬁeld	
  1980:	
  	
  ICML	
  begins	
  ~1985:	
  	
  sta+s+cal	
  and	
  informa+on	
  theore+c	
  methods	
  catch	
  hold	
  again	
  in	
  NLP,	
  in	
  part	
  due	
  to	
  their	
  success	
  in	
  ASR	
  – This	
  has	
  con+nued	
  unabated	
  for	
  25+	
  years,	
  with	
  help	
  from	
  Moore’s	
  Law-­‐type	
  phenomena	
  1986:	
  	
  LTI	
  founded	
  (then	
  called	
  “CMT”)	
  1993:	
  “Very	
  Large	
  Corpora”	
  workshops	
  start	
  at	
  ACL	
  1996:	
  	
  EMNLP	
  conference	
  starts	
  ~1997:	
  	
  Laﬀerty	
  and	
  Rosenfeld	
  start	
  teaching	
  “Language	
  and	
  Sta+s+cs”	
  at	
  CMU	
  1998-­‐early	
  2000s:	
  	
  Internet	
  boom,	
  commercial	
  language	
  technologies	
  becoming	
  viable	
  	
  ~2003:	
  	
  MLD	
  founded	
  (then	
  called	
  “CALD”)	
  2004:	
  	
  Cohen	
  starts	
  teaching	
  “Informa+on	
  Extrac+on”	
  2006:	
  	
  Smith	
  starts	
  teaching	
  “Language	
  and	
  Sta+s+cs	
  2”	
  2011:	
  	
  Cohen	
  and	
  Smith	
  start	
  teaching	
  “Structured	
  Predic+on” 	
  
What	
  is	
  Structured	
  Predic+on?	
  Having	
  observed	
  some	
  informa+on	
  (input)	
  …	
  • Binary	
  classiﬁca+on:	
  	
  predict	
  a	
  coin	
  toss	
  (given	
  some	
  informa+on)	
  • Mul+-­‐class:	
  	
  predict	
  which	
  side	
  of	
  a	
  die	
  (given	
  some	
  informa+on)	
  • Structured	
  predic+on:	
  	
  choose	
  among	
  a	
  very	
  large	
  number	
  of	
  complex	
  outcomes.	
  – Large	
  means	
  “exponen+al	
  in	
  the	
  size	
  of	
  the	
  input.”	
  
E.g.,	
  (Part	
  of	
  Speech)	
  Tagging	
  Bill	
  directed	
  plays	
  about	
  English	
  kings	
  proper	
  noun,	
  noun,	
  verb	
  
noun,	
  verb	
  
proper	
  noun,	
  	
  plural	
  proper	
  noun,	
  adjec+ve	
  
noun,	
  verb	
  
adjec+ve,	
  verb	
  
preposi+on,	
  par+cle	
  

E.g.,	
  Segmenta+on	
  into	
  Words	
  第二阶段的奥运会体育比赛㛭ථ༩࢒Ԟձ䇖闭ນࣜ㛭ථత预订工作已经结束,现在进ೖ㛭ථ෼഑阶段。在此期间,我们ෆ࠶઀ड৽త㛭ථ预订申请。	
  
E.g.,	
  Segmenta+on	
  within	
  Words	
  uygarlaştramadıklarımızdanmışsınızcasına	
  	
  “(behaving)	
  as	
  if	
  you	
  are	
  among	
  those	
  whom	
  we	
  could	
  not	
  civilize”	
  
E.g.,	
  Segmenta+on	
  and	
  Tagging	
  Britain	
  sent	
  warships	
  across	
  the	
  English	
  Channel	
  Monday	
  to	
  rescue	
  Britons	
  stranded	
  by	
  Eyjavallajökull	
  's	
  volcanic	
  ash	
  cloud	
  geopoli+cal	
  en+ty	
  
geographic	
  feature	
  
+me	
  
cultural/ethnic	
  group	
  
geographic	
  feature	
  

E.g.,	
  Trees	
  Britain	
  sent	
  warships	
  across	
  the	
  English	
  Channel	
  	
  	
  Monday	
  to	
  rescue	
  Britons	
  stranded	
  by	
  	
  	
  Eyjavallajökull	
  's	
  volcanic	
  ash	
  cloud	
  

E.g.,	
  Predicate-­‐Argument	
  Structures	
  
sender	
  sent	
  thing/rescuer	
  place	
  sent	
  3me	
  rescued	
  thing/	
  stranded	
  thing	
  
stranding	
  thing	
  Britain	
  sent	
  warships	
  across	
  the	
  English	
  Channel	
  	
  	
  Monday	
  to	
  rescue	
  Britons	
  stranded	
  by	
  	
  	
  Eyjavallajökull	
  's	
  volcanic	
  ash	
  cloud	
  
E.g.,	
  Alignments	
  Mr	
  President	
  ,	
  Noah's	
  ark	
  was	
  ﬁlled	
  not	
  with	
  	
  Noahs	
  Arche	
  war	
  nicht	
  voller	
  	
  produc+on	
  factors	
  ,	
  but	
  with	
  living	
  creatures	
  .	
  	
  	
  Produk+onsfaktoren	
  ,	
  sondern	
  Geschöpfe	
  .	
  

Gene	
  Finding	
  and	
  Analysis 	
  
Slide due to E. Xing 
Phylogene+c	
  Rela+onships 	
  
Time 
Image	
  Segmenta+on 	
  
from	
  Nowozin	
  and	
  Lampert	
  (2010)	
  

Implica+ons	
  of	
  “Going	
  Structured”	
  • All	
  aspects	
  of	
  training	
  and	
  tes+ng	
  become	
  more	
  complex:	
  – Designing	
  a	
  model	
  – Predic+on	
  algorithms	
  (once	
  you	
  have	
  a	
  model)	
  – Learning	
  your	
  model	
  from	
  data	
  – Measuring	
  “error”	
  of	
  a	
  predic+on	
  • Machine	
  learning	
  helps	
  with	
  “mental	
  hygiene”!	
  – Principles	
  that	
  will	
  help	
  you	
  explain	
  and	
  understand	
  your	
  methods	
  – Generic	
  op+miza+on	
  algorithms	
  – Formal	
  guarantees	
  (some+mes)	
  – Baselines	
  when	
  you’re	
  tackling	
  a	
  new	
  problem	
  
The	
  Structured	
  Predic+on	
  Way 	
  1. Formally	
  deﬁne	
  the	
  inputs	
  and	
  outputs.	
  	
  2. Iden+fy	
  a	
  scoring	
  func+on	
  over	
  input-­‐output	
  pairs,	
  and	
  an	
  algorithm	
  that	
  can	
  ﬁnd	
  the	
  maximum-­‐scoring	
  output	
  given	
  an	
  input.	
  3. Determine	
  what	
  data	
  can	
  be	
  used	
  to	
  learn	
  to	
  predict	
  outputs	
  from	
  inputs,	
  and	
  apply	
  a	
  learning	
  algorithm	
  to	
  tune	
  the	
  parameters	
  of	
  the	
  scoring	
  func+on.	
  4. Evaluate	
  the	
  model	
  on	
  an	
  objec+ve	
  criterion	
  measured	
  on	
  unseen	
  test	
  data.	
  	
  
Topics	
  • Inference	
  (ch.	
  2,	
  5)	
  • Learning	
  from	
  Complete	
  Data	
  (ch.	
  3)	
  • Learning	
  from	
  Incomplete	
  Data	
  (ch.	
  4)	
  
Format	
  of	
  the	
  Course	
  • About	
  ﬁve	
  assignments	
  (12	
  points	
  each)	
  • Survey	
  paper	
  – 20	
  points	
  spread	
  over	
  the	
  term	
  – 20	
  points	
  for	
  the	
  ﬁnal	
  paper	
  • No	
  exams	
  Email	
  list:	
  hAps://mailman.srv.cs.cmu.edu/mailman/lis+nfo/11763-­‐announce	
  	
  
The	
  Book 	
  • Linguis3c	
  Structure	
  Predic3on	
  • Available	
  in	
  electronic	
  form	
  (free	
  at	
  CMU)	
  and	
  print	
  form.	
  

Algos.	
  SPFLODD	
  parsing	
  inference	
  formal	
  rep’ns.	
  learning	
  some	
  overlap!	
  
L&S	
  SPFLODD	
  es+ma+on	
  learning	
  sequences,	
  a	
  bit	
  on	
  trees	
  general	
  discrete	
  structures	
  some	
  overlap!	
  SPFLODD	
  and	
  Other	
  Classes	
  
SPFLODD	
  
Machine	
  Learning	
  
prerequisite	
  
Language	
  and	
  Sta+s+cs	
  
Language	
  and	
  Sta+s+cs	
  2	
  
Probabilis+c	
  Graphical	
  Models	
  
PGM	
  SPFLODD	
  theory	
  applica+on	
  rela+onal	
  data	
  structural	
  data	
  some	
  overlap!	
  
Algorithms	
  for	
  NLP	
  
Informa+on	
  Extrac+on	
  

Homework	
  for	
  Thursday	
  • Read	
  LSP,	
  preface	
  and	
  chapter	
  1.	
  
Introduction to
Information Retrieval 
Introducing Information Retrieval 
and Web Search
Information Retrieval 
•Information Retrieval (IR) is finding material 
(usually documents) of an unstructured nature 
(usually text) that satisfies an information need 
from within large collections (usually stored on 
computers).
–These days we frequently think first of web search , 
but there are many other cases: 
•E-mail search 
•Searching your laptop 
•Corporate knowledge bases 
•Legal information retrieval 
2
Unstructured (text) vs. structured (database) 
data in the mid -nineties 
3
050 100 150 200 250 
Data volume Market Cap Unstructured 
Structured 
Unstructured (text) vs. structured (database) 
data today 
4
050 100 150 200 250 
Data volume Market Cap Unstructured 
Structured 
Basic assumptions of Information Retrieval 
•Collection : Aset of documents 
–Assume it is a static collection for the moment 
•Goal : Retrieve documents with information 
that is relevant to the user’s information need
and helps the user complete a task 
5Sec. 1.1 
how trap mice alive The classic search model 
Collection User task 
Info need 
Query 
Results Search 
engine 
Query 
refinement Get rid of mice in a politically correct way 
Info about removing mice without killing them Misconception? 
Misformulation? 
Searc 
h
How good are the retrieved docs?
/square4Precision : Fraction of retrieved docs that are 
relevant to the user’s information need
/square4Recall : Fraction of relevant docs in collection 
that are retrieved
/square4More precise definitions and measurements to follow later 
7Sec. 1.1 
Introduction to
Information Retrieval 
Term -document incidence matrices 
Unstructured data in 1620
•Which plays of Shakespeare contain the words Brutus AND Caesar but NOT Calpurnia ?
•One could grep all of Shakespeare’s plays for 
Brutus and Caesar, then strip out lines containing 
Calpurnia ?
•Why is that not the answer? 
–Slow (for large corpora) 
–NOT Calpurnia is non-trivial 
–Other operations (e.g., find the word Romans near 
countrymen ) not feasible 
–Ranked retrieval (best documents to return) 
•Later lectures 
9Sec. 1.1 
Term -document incidence matrices
Antony and Cleopatra Julius Caesar The Tempest Hamlet Othello Macbeth 
Antony 1 1 0 0 0 1
Brutus 1 1 0 1 0 0
Caesar 1 1 0 1 1 1
Calpurnia 0 1 0 0 0 0
Cleopatra 1 0 0 0 0 0
mercy 1 0 1 1 1 1
worser 1 0 1 1 1 0
1 if play contains 
word , 0 otherwise Brutus AND Caesar BUT NOT 
Calpurnia Sec. 1.1 
Incidence vectors
•So we have a 0/1 vector for each term. 
•To answer query: take the vectors for Brutus, 
Caesar and Calpurnia (complemented) /barb4right
bitwise AND .
–110100 AND
–110111 AND
–101111 = 
–100100 
11 Sec. 1.1 
Antony and Cleopatra Julius Caesar The Tempest Hamlet Othe llo Macbeth 
Antony 1 1 0 0 0 1
Brutus 1 1 0 1 0 0
Caesar 1 1 0 1 1 1
Calpurnia 0 1 0 0 0 0
Cleopatra 1 0 0 0 0 0
mercy 1 0 1 1 1 1
worser 1 0 1 1 1 0
Answers to query
• Antony and Cleopatra, Act III, Scene ii 
Agrippa [Aside to DOMITIUS ENOBARBUS]: Why, Enobarbus,
When Antony found Julius Caesar dead, 
He cried almost to roaring; and he wept When at Philippi he found Brutus slain. 
• Hamlet, Act III, Scene ii 
Lord Polonius: I did enact Julius Caesar I was killed i’ the 
Capitol; Brutus killed me. 
12 Sec. 1.1 

Bigger collections
•Consider N = 1 million documents, each with 
about 1000 words. 
•Avg 6 bytes/word including spaces/punctuation 
–6GB of data in the documents. 
•Say there are M = 500K 
distinct terms among 
these. 
13 Sec. 1.1 
Can’t build the matrix
•500K x 1M matrix has half-a-trillion 0’s and 1’s. 
•But it has no more than one billion 1’s.
–matrix is extremely sparse.
•What’s a better representation? 
–We only record the 1 positions. 
14 Why? Sec. 1.1 
Introduction to
Information Retrieval 
The Inverted Index 
The key data structure underlying 
modern IR
Inverted index
•For each term t, we must store a list of all 
documents that contain t.
–Identify each doc by a docID , a document serial 
number 
•Can we used fixed -size arrays for this? 
16 What happens if the word Caesar 
is added to document 14? Sec. 1.2 
Brutus 
Calpurnia Caesar 1 2 4 5 6 16 57132 1 2 4 11 31 45173 
231 174 
54101 
Inverted index
•We need variable-size postings lists 
–On disk, a continuous run of postings is normal and best 
–In memory, can use linked lists or variable length arrays 
•Some tradeoffs in size/ease of insertion 
17 Dictionary Postings 
Sorted by docID (more later on why). Posting Sec. 1.2 
Brutus 
Calpurnia Caesar 1 2 4 5 6 16 57132 1 2 4 11 31 45173 
231 174 
54101 
Tokenizer 
Token stream Friends Romans Countrymen Inverted index construction
Linguistic modules 
Modified tokens friend roman countryman 
Indexer 
Inverted index friend 
roman 
countryman 24
2
13 16 1Documents to be indexed Friends, Romans, countrymen. Sec. 1.2 
Initial stages of text processing
•Tokenization 
–Cut character sequence into word tokens 
•Deal with “John’s” , a state-of-the-art solution 
•Normalization 
–Map text and query term to same form
•You want U.S.A. and USA to match 
•Stemming 
–We may wish different forms of a root to match 
•authorize ,authorization 
•Stop words 
–We may omit very common words (or not) 
•the, a, to, of 
Indexer steps: Token sequence
•Sequence of (Modified token, Document ID) pairs. 
I did enact Julius 
Caesar I was killed 
i’ the Capitol; 
Brutus killed me. Doc 1 
So let it be with 
Caesar. The noble 
Brutus hath told you 
Caesar was ambitious Doc 2 
Sec. 1.2 
Indexer steps: Sort 
•Sort by terms 
– And then docID 
Core indexing step Sec. 1.2 
Indexer steps: Dictionary & Postings 
•Multiple term entries in a single document are merged. 
•Split into Dictionary and Postings 
•Doc. frequency information is added. 
Why frequency? 
Will discuss later. Sec. 1.2 

Where do we pay in storage?
23 Pointers Terms 
and 
counts IR system implementation 
•How do we index efficiently? 
•How much storage do we need? Sec. 1.2 
Lists of 
docIDs 
Introduction to
Information Retrieval 
Query processing with an inverted index 
The index we just built 
•How do we process a query? 
–Later - what kinds of queries can we process? 
25 Our focus Sec. 1.3 
Query processing: AND
•Consider processing the query: 
Brutus AND Caesar 
–Locate Brutus in the Dictionary; 
•Retrieve its postings. 
–Locate Caesar in the Dictionary; 
•Retrieve its postings. 
–“Merge” the two postings (intersect the document sets): 
26 128 
34 2 4 816 32 64 
1 2 35813 21 Brutus Brutus Brutus Brutus 
Caesar Caesar Caesar Caesar Sec. 1.3 
The merge
•Walk through the two postings simultaneously, in time linear in the total number of postings entries 
27 34 128 2 4 816 32 64 
1 2 35813 21 Brutus Brutus Brutus Brutus 
Caesar Caesar Caesar Caesar 
If the list lengths are xand y, the merge takes O( x+y )
operations. 
Crucial: postings sorted by docID. Sec. 1.3 
Intersecting two postings lists 
(a “merge” algorithm) 
28 

Introduction to
Information Retrieval 
The Boolean Retrieval Model 
& Extended Boolean Models 
Boolean queries: Exact match
•The Boolean retrieval model is being able to ask a 
query that is a Boolean expression: 
–Boolean Queries are queries using AND, OR and NOT 
to join query terms 
•Views each document as a set of words 
•Is precise: document matches condition or not. 
–Perhaps the simplest model to build an IR system on
•Primary commercial retrieval tool for 3 decades. 
•Many search systems you still use are Boolean: 
–Email, library catalog, Mac OS X Spotlight 
30 Sec. 1.3 
Example: WestLaw http://www.westlaw.com /
• Largest commercial (paying subscribers) 
legal search service (started 1975; ranking added 1992; new federated search added 2010)
• Tens of terabytes of data; ~700,000 users • Majority of users still use boolean queries 
• Example query:
–What is the statute of limitations in cases 
involving the federal tort claims act? 
–LIMIT! /3 STATUTE ACTION /S FEDERAL /2 
TORT /3 CLAIM
• /3 = within 3 words, /S = in same sentence 
31 Sec. 1.4 
Example: WestLaw   http://www.westlaw.com/ 
•Another example query: 
–Requirements for disabled people to be able to access a workplace 
–disabl! /p access! /s work-site work-place (employment /3 place 
•Note that SPACE is disjunction, not conjunction! 
•Long, precise queries; proximity operators; incrementally developed; not like web search 
•Many professional searchers still like Boolean search 
–You know exactly what you are getting 
•But that doesn’t mean it actually works better…. Sec. 1.4 
Boolean queries: 
More general merges 
•Exercise : Adapt the merge for the queries: 
Brutus AND NOT Caesar 
Brutus OR NOT Caesar 
•Can we still run through the merge in time O( x+y )?   What can we achieve? 
33 Sec. 1.3 
Merging
What about an arbitrary Boolean formula? (Brutus OR Caesar) AND NOT 
(Antony OR Cleopatra) 
•Can we always merge in “linear” time? 
–Linear in what? 
•Can we do better? 
34 Sec. 1.3 
Query optimization
•What is the best order for query processing? 
•Consider a query that is an AND of nterms. 
•For each of the nterms, get its postings, 
then AND them together. 
Brutus 
Caesar 
Calpurnia 1 2 3 5 8 16 21 34 2 4 8 16 32 64128 
13 16 
Query: Brutus AND Calpurnia AND Caesar 
35 Sec. 1.3 
Query optimization example
•Process in order of increasing freq :
–start with smallest set, then keep cutting further .
36 This is why we kept 
document freq. in dictionary 
Execute the query as ( Calpurnia AND Brutus) AND Caesar .Sec. 1.3 
Brutus 
Caesar 
Calpurnia 1 2 3 5 8 16 21 34 2 4 8 16 32 64128 
13 16 
More general optimization
•e.g., (madding OR crowd ) AND ( ignoble OR 
strife )
•Get doc. freq.’s for all terms. 
•Estimate the size of each ORby the sum of its 
doc. freq.’s (conservative). 
•Process in increasing order of ORsizes. 
37 Sec. 1.3 
Exercise
•Recommend a query processing order for 
•Which two terms should we process first?  Term Freq  
  eyes 213312   kaleidoscope 87009   marmalade 107913   skies 271658   tangerine 46653   trees 316812 
38 (tangerine OR trees) AND
(marmalade OR skies) AND
(kaleidoscope OR eyes) 
Query processing exercises
•Exercise : If the query is friends AND romans AND 
(NOT countrymen ), how could we use the freq of 
countrymen ?
•Exercise : Extend the merge to an arbitrary 
Boolean query.  Can we always guarantee execution in time linear in the total postings size ? 
•Hint : Begin with the case of a Boolean formula 
query: in this, each query term appears only once in the query. 
39 
Exercise
•Try the search feature at 
http://www.rhymezone.com/shakespeare/ 
•Write down five search features you think it could do better 
40 
Introduction to
Information Retrieval 
Phrase queries and positional indexes 
Phrase queries
•We want to be able to answer queries such as “stanford university” – as a phrase 
•Thus the sentence “I went to university at 
Stanford” is not a match. 
–The concept of phrase queries has proven easily understood by users; one of the few “advanced search” ideas that works 
–Many more queries are implicit phrase queries 
•For this, it no longer suffices to store only 
<term : docs > entries Sec. 2.4 
A first attempt: Biword indexes
•Index every consecutive pair of terms in the text as a phrase 
•For example the text “Friends, Romans, Countrymen” would generate the biwords 
–friends romans 
–romans countrymen 
•Each of these biwords is now a dictionary term
•Two-word phrase query-processing is now immediate. Sec. 2.4.1 
Longer phrase queries
•Longer phrases can be processed by breaking them down 
•stanford university palo alto can be broken into 
the Boolean query on biwords:
stanford university AND university palo AND palo 
alto 
Without the docs, we cannot verify that the docs 
matching the above Boolean query do contain the phrase. 
Can have false positives! Sec. 2.4.1 
Issues for biword indexes
•False positives, as noted before 
•Index blowup due to bigger dictionary 
–Infeasible for more than biwords, big even for them
•Biword indexes are not the standard solution (for all biwords) but can be part of a compound strategy Sec. 2.4.1 
Solution 2: Positional indexes
•In the postings, store, for each term the 
position(s) in which tokens of it appear: 
<term , number of docs containing term ;
doc1 : position1, position2 … ; 
doc2 : position1, position2 … ; 
etc.> Sec. 2.4.2 
Positional index example
•For phrase queries, we use a merge algorithm recursively at the document level 
•But we now need to deal with more than just equality <be : 993427; 
1: 7, 18, 33, 72, 86, 231; 
2: 3, 149; 
4: 17, 191, 291, 430, 434; 
5: 363, 367, …> Which of docs 1,2,4,5 
could contain “ to be 
or not to be ”? Sec. 2.4.2 
Processing a phrase query
•Extract inverted index entries for each distinct term: to, be, or, not. 
•Merge their doc:position lists to enumerate all 
positions with “ to be or not to be ”. 
–to : 
•2:1,17,74,222,551; 
4:8,16,190,429,433; 7:13,23,191; ... 
–be :  
•1:17,19; 4:17,191,291,430,434; 5:14,19,101; ... 
•Same general method for proximity searches Sec. 2.4.2 
Proximity queries
•LIMIT! /3 STATUTE /3 FEDERAL /2 TORT 
–Again, here, / kmeans “within kwords of”. 
•Clearly, positional indexes can be used for such queries; biword indexes cannot. 
•Exercise: Adapt the linear merge of postings to handle proximity queries.  Can you make it work for any value of k?
–This is a little tricky to do correctly and efficientl y 
–See Figure 2.12 of IIR Sec. 2.4.2 
Positional index size
•A positional index expands postings storage substantially 
–Even though indices can be compressed 
•Nevertheless, a positional index is now standardly used because of the power and usefulness of phrase and proximity queries … whether used explicitly or implicitly in a ranking retrieval system. Sec. 2.4.2 
Positional index size
•Need an entry for each occurrence, not just once per document 
•Index size depends on average document size 
–Average web page has <1000 terms 
–SEC filings, books, even some epic poems … easily 100,000 terms 
•Consider a term with frequency 0.1%Why? 
100 1 100,000 1 1 1000 Positional postings Postings Document size Sec. 2.4.2 
Rules of thumb
•A positional index is 2–4 as large as a non -
positional index 
•Positional index size 35–50% of volume of original text 
–Caveat: all of this holds for “English-like” languages Sec. 2.4.2 
Combination schemes
•These two approaches can be profitably combined 
–For particular phrases ( “Michael Jackson”, “Britney 
Spears” ) it is inefficient to keep on merging positional 
postings lists 
•Even more so for phrases like “The Who” 
•Williams et al. (2004) evaluate a more sophisticated mixed indexing scheme 
–A typical web query mixture was executed in ¼ of th e 
time of using just a positional index 
–It required 26% more space than having a positional  
index alone Sec. 2.4.3 
Introduction to
Information Retrieval 
Structured vs. Unstructured Data 
IR vs. databases: 
Structured vs unstructured data 
•Structured data tends to refer to information in “tables” 
55 Employee Manager Salary 
Smith Jones 50000 
Chang Smith 60000 50000 Ivy Smith 
Typically allows numerical range and exact match (for text) queries, e.g., 
Salary < 60000 AND Manager = Smith .
Unstructured data
•Typically refers to free text 
•Allows 
–Keyword queries including operators 
–More sophisticated “concept” queries e.g., 
•find all web pages dealing with drug abuse 
•Classic model for searching text documents 
56 
Semi-structured data
•In fact almost no data is “unstructured” 
•E.g., this slide has distinctly identified zones su ch 
as the Title and Bullets 
•… to say nothing of linguistic structure 
•Facilitates “semi-structured” search such as 
–Title contains data AND Bullets contain search 
•Or even 
–Title is about Object Oriented Programming AND 
Author something like stro*rup 
–where * is the wild-card operator 
57 
4/4/2013 
1Introduction to 
Information Retrieval 
Introducing Information Retrieval 
and Web Search Information Retrieval 
•Information Retrieval (IR) is finding material 
(usually documents) of an unstructured nature 
(usually text) that satisfies an information need 
from within large collections (usually stored on 
computers).
–These days we frequently think first of web search , 
but there are many other cases: 
•E-mail search 
•Searching your laptop 
•Corporate knowledge bases 
•Legal information retrieval 
2
Unstructured (text) vs. structured (database) 
data in the mid-nineties 
3
050 100 150 200 250 
Data volume Market Cap Unstructured 
Structured Unstructured (text) vs. structured (database) 
data today 
4
050 100 150 200 250 
Data volume Market Cap Unstructured 
Structured 
Basic assumptions of Information Retrieval 
•Collection : A set of documents 
–Assume it is a static collection for the moment 
•Goal : Retrieve documents with information 
that is relevant to the user’s information need 
and helps the user complete a task 
5Sec. 1.1 
how trap mice alive The classic search model 
Collection User task 
Info need 
Query 
Results Search 
engine 
Query 
refinement Get rid of mice in a 
politically correct way 
Info about removing mice 
without killing them Misconception? 
Misformulation? 
Searc 
h
4/4/2013 
2How good are the retrieved docs? 
/square4Precision : Fraction of retrieved docs that are 
relevant to the user’s information need 
/square4Recall : Fraction of relevant docs in collection 
that are retrieved 
/square4More precise definitions and measurements to 
follow later 
7Sec. 1.1 
Introduction to 
Information Retrieval 
Term-document incidence matrices 
Unstructured data in 1620 
•Which plays of Shakespeare contain the words 
Brutus AND Caesar but NOT Calpurnia ?
•One could grep all of Shakespeare’s plays for 
Brutus and Caesar, then strip out lines containing 
Calpurnia ?
•Why is that not the answer? 
–Slow (for large corpora) 
–NOT Calpurnia is non-trivial 
–Other operations (e.g., find the word Romans near 
countrymen ) not feasible 
–Ranked retrieval (best documents to return) 
•Later lectures 
9Sec. 1.1 
Term-document incidence matrices 
Antony and Cleopatra Julius Caesar The Tempest Hamlet Othello Macbeth 
Antony 1 1 0 0 0 1
Brutus 1 1 0 1 0 0
Caesar 1 1 0 1 1 1
Calpurnia 0 1 0 0 0 0
Cleopatra 1 0 0 0 0 0
mercy 1 0 1 1 1 1
worser 1 0 1 1 1 0
1 if play contains 
word , 0 otherwise Brutus AND Caesar BUT NOT 
Calpurnia Sec. 1.1 
Incidence vectors 
•So we have a 0/1 vector for each term. 
•To answer query: take the vectors for Brutus, 
Caesar and Calpurnia (complemented) /barb4right
bitwise AND .
–110100 AND 
–110111 AND 
–101111 = 
–100100 
11 Sec. 1.1 
Antony and Cleopatra Julius Caesar The T empest Hamlet Othello Macbeth 
Antony 1 1 0 0 0 1
Brutus 1 1 0 1 0 0
Caesar 1 1 0 1 1 1
Calpurnia 0 1 0 0 0 0
Cleopatra 1 0 0 0 0 0
mercy 1 0 1 1 1 1
worser 1 0 1 1 1 0Answers to query 
• Antony and Cleopatra, Act III, Scene ii 
Agrippa [Aside to DOMITIUS ENOBARBUS]: Why, Enobarbus,
When Antony found Julius Caesar dead, 
He cried almost to roaring; and he wept 
When at Philippi he found Brutus slain. 
• Hamlet, Act III, Scene ii 
Lord Polonius: I did enact Julius Caesar I was killed i’ the 
Capitol; Brutus killed me. 
12 Sec. 1.1 

4/4/2013 
3Bigger collections 
•Consider N = 1 million documents, each with 
about 1000 words. 
•Avg 6 bytes/word including 
spaces/punctuation 
–6GB of data in the documents. 
•Say there are M = 500K distinct terms among 
these. 
13 Sec. 1.1 
Can’t build the matrix 
•500K x 1M matrix has half-a-trillion 0’s and 1’s. 
•But it has no more than one billion 1’s.
–matrix is extremely sparse.
•What’s a better representation? 
–We only record the 1 positions. 
14 Why? Sec. 1.1 
Introduction to 
Information Retrieval 
The Inverted Index 
The key data structure underlying 
modern IR Inverted index 
•For each term t, we must store a list of all 
documents that contain t.
–Identify each doc by a docID , a document serial 
number 
•Can we used fixed-size arrays for this? 
16 What happens if the word Caesar 
is added to document 14? Sec. 1.2 
Brutus 
Calpurnia Caesar 1 2 4 5 6 16 57132 1 2 4 11 31 45173 
231 174 
54101 
Inverted index 
•We need variable-size postings lists 
–On disk, a continuous run of postings is normal 
and best 
–In memory, can use linked lists or variable length 
arrays 
•Some tradeoffs in size/ease of insertion 
17 Dictionary Postings 
Sorted by docID (more later on why). 
Posting Sec. 1.2 
Brutus 
Calpurnia Caesar 1 2 4 5 6 16 57132 1 2 4 11 31 45173 
231 174 
54101 Tokenizer 
Token stream Friends Romans Countrymen Inverted index construction 
Linguistic modules 
Modified tokens friend roman countryman 
Indexer 
Inverted index friend 
roman 
countryman 24
2
13 16 1Documents to 
be indexed Friends, Romans, countrymen. Sec. 1.2 
4/4/2013 
4Initial stages of text processing 
•Tokenization 
–Cut character sequence into word tokens 
•Deal with “John’s” , a state-of-the-art solution 
•Normalization 
–Map text and query term to same form 
•You want U.S.A. and USA to match 
•Stemming 
–We may wish different forms of a root to match 
•authorize ,authorization 
•Stop words 
–We may omit very common words (or not) 
•the, a, to, of Indexer steps: Token sequence 
•Sequence of (Modified token, Document ID) pairs. 
I did enact Julius 
Caesar I was killed 
i’ the Capitol; 
Brutus killed me. Doc 1 
So let it be with 
Caesar. The noble 
Brutus hath told you 
Caesar was ambitious Doc 2 
Sec. 1.2 
Indexer steps: Sort 
•Sort by terms 
– And then docID 
Core indexing step Sec. 1.2 
Indexer steps: Dictionary & Postings 
•Multiple term entries 
in a single document 
are merged. 
•Split into Dictionary 
and Postings 
•Doc. frequency 
information is added. 
Why frequency? 
Will discuss later. Sec. 1.2 
Where do we pay in storage? 
23 Pointers Terms 
and 
counts IR system 
implementation 
•How do we 
index efficiently? 
•How much 
storage do we 
need? Sec. 1.2 
Lists of 
docIDs Introduction to 
Information Retrieval 
Query processing with an inverted index 
4/4/2013 
5The index we just built 
•How do we process a query? 
–Later - what kinds of queries can we process? 
25 Our focus Sec. 1.3 
Query processing: AND 
•Consider processing the query: 
Brutus AND Caesar 
–Locate Brutus in the Dictionary; 
•Retrieve its postings. 
–Locate Caesar in the Dictionary; 
•Retrieve its postings. 
–“Merge” the two postings (intersect the document 
sets): 
26 128 
34 2 4 816 32 64 
1 2 35813 21 Brutus Brutus Brutus Brutus 
Caesar Caesar Caesar Caesar Sec. 1.3 
The merge 
•Walk through the two postings 
simultaneously, in time linear in the total 
number of postings entries 
27 34 128 2 4 816 32 64 
1 2 35813 21 Brutus Brutus Brutus Brutus 
Caesar Caesar Caesar Caesar 
If the list lengths are xand y, the merge takes O( x+y )
operations. 
Crucial: postings sorted by docID. Sec. 1.3 
Intersecting two postings lists 
(a “merge” algorithm) 
28 
Introduction to 
Information Retrieval 
The Boolean Retrieval Model 
& Extended Boolean Models Boolean queries: Exact match 
•The Boolean retrieval model is being able to ask a 
query that is a Boolean expression: 
–Boolean Queries are queries using AND, OR and NOT 
to join query terms 
•Views each document as a set of words 
•Is precise: document matches condition or not. 
–Perhaps the simplest model to build an IR system on 
•Primary commercial retrieval tool for 3 decades. 
•Many search systems you still use are Boolean: 
–Email, library catalog, Mac OS X Spotlight 
30 Sec. 1.3 
4/4/2013 
6Example: WestLaw http://www.westlaw.com/
• Largest commercial (paying subscribers) 
legal search service (started 1975; ranking 
added 1992; new federated search added 
2010)
• Tens of terabytes of data; ~700,000 users 
• Majority of users still use booleanqueries 
• Example query:
–What is the statute of limitations in cases 
involving the federal tort claims act? 
–LIMIT! /3 STATUTE ACTION /S FEDERAL /2 
TORT /3 CLAIM 
• /3 = within 3 words, /S = in same sentence 
31 Sec. 1.4 
Example: WestLaw   http://www.westlaw.com/ 
•Another example query: 
–Requirements for disabled people to be able to 
access a workplace 
–disabl! /p access! /s work-site work-place 
(employment /3 place 
•Note that SPACE is disjunction, not conjunction! 
•Long, precise queries; proximity operators; 
incrementally developed; not like web search 
•Many professional searchers still like Boolean 
search 
–You know exactly what you are getting 
•But that doesn’t mean it actually works better…. Sec. 1.4 
Boolean queries: 
More general merges 
•Exercise : Adapt the merge for the queries: 
Brutus AND NOT Caesar 
Brutus OR NOT Caesar 
•Can we still run through the merge in time 
O( x+y )?   What can we achieve? 
33 Sec. 1.3 
Merging 
What about an arbitrary Boolean formula? 
(Brutus OR Caesar) AND NOT 
(Antony OR Cleopatra) 
•Can we always merge in “linear” time? 
–Linear in what? 
•Can we do better? 
34 Sec. 1.3 
Query optimization 
•What is the best order for query 
processing? 
•Consider a query that is an AND of nterms. 
•For each of the nterms, get its postings, 
then AND them together. Brutus 
Caesar 
Calpurnia 1 2 3 5 8 16 21 34 2 4 8 16 32 64128 
13 16 
Query: Brutus AND Calpurnia AND Caesar 
35 Sec. 1.3 
Query optimization example 
•Process in order of increasing freq:
–start with smallest set, then keep cutting further .
36 This is why we kept 
document freq. in dictionary 
Execute the query as ( Calpurnia AND Brutus) AND Caesar .Sec. 1.3 
Brutus 
Caesar 
Calpurnia 1 2 3 5 8 16 21 34 2 4 8 16 32 64128 
13 16 
4/4/2013 
7More general optimization 
•e.g., (madding OR crowd ) AND ( ignoble OR 
strife )
•Get doc. freq.’s for all terms. 
•Estimate the size of each OR by the sum of its 
doc. freq.’s (conservative). 
•Process in increasing order of OR sizes. 
37 Sec. 1.3 
Exercise 
•Recommend a query 
processing order for 
•Which two terms should we 
process first?  Term Freq  
  eyes 213312 
  kaleidoscope 87009 
  marmalade 107913 
  skies 271658 
  tangerine 46653 
  trees 316812 
38 (tangerine OR trees) AND 
(marmalade OR skies) AND 
(kaleidoscope OR eyes) 
Query processing exercises 
•Exercise : If the query is friends AND romans AND 
(NOT countrymen ), how could we use the freq of 
countrymen ?
•Exercise : Extend the merge to an arbitrary 
Boolean query.  Can we always guarantee 
execution in time linear in the total postings size? 
•Hint : Begin with the case of a Boolean formula 
query: in this, each query term appears only once 
in the query. 
39 Exercise 
•Try the search feature at 
http://www.rhymezone.com/shakespeare/ 
•Write down five search features you think it 
could do better 
40 
Introduction to 
Information Retrieval 
Phrase queries and positional indexes Phrase queries 
•We want to be able to answer queries such as 
“stanford university” – as a phrase 
•Thus the sentence “I went to university at 
Stanford” is not a match. 
–The concept of phrase queries has proven easily 
understood by users; one of the few “advanced 
search” ideas that works 
–Many more queries are implicit phrase queries 
•For this, it no longer suffices to store only 
<term : docs > entries Sec. 2.4 
4/4/2013 
8A first attempt: Biword indexes 
•Index every consecutive pair of terms in the text 
as a phrase 
•For example the text “Friends, Romans, 
Countrymen” would generate the biwords 
–friends romans 
–romans countrymen 
•Each of these biwords is now a dictionary term 
•Two-word phrase query-processing is now 
immediate. Sec. 2.4.1 
Longer phrase queries 
•Longer phrases can be processed by breaking 
them down 
•stanford university palo alto can be broken into 
the Boolean query on biwords:
stanford university AND university palo AND palo 
alto 
Without the docs, we cannot verify that the docs 
matching the above Boolean query do contain 
the phrase. 
Can have false positives! Sec. 2.4.1 
Issues for biword indexes 
•False positives, as noted before 
•Index blowup due to bigger dictionary 
–Infeasible for more than biwords, big even for 
them 
•Biword indexes are not the standard solution 
(for all biwords) but can be part of a 
compound strategy Sec. 2.4.1 
Solution 2: Positional indexes 
•In the postings, store, for each term the 
position(s) in which tokens of it appear: 
<term , number of docs containing term ;
doc1 : position1, position2 … ; 
doc2 : position1, position2 … ; 
etc.> Sec. 2.4.2 
Positional index example 
•For phrase queries, we use a merge 
algorithm recursively at the document level 
•But we now need to deal with more than 
just equality <be : 993427; 
1: 7, 18, 33, 72, 86, 231; 
2: 3, 149; 
4: 17, 191, 291, 430, 434; 
5: 363, 367, …> Which of docs 1,2,4,5 
could contain “ to be 
or not to be ”? Sec. 2.4.2 
Processing a phrase query 
•Extract inverted index entries for each distinct 
term: to, be, or, not. 
•Merge their doc:position lists to enumerate all 
positions with “ to be or not to be ”. 
–to : 
•2:1,17,74,222,551; 4:8,16,190,429,433; 7:13,23,191; ... 
–be :  
•1:17,19; 4:17,191,291,430,434; 5:14,19,101; ... 
•Same general method for proximity searches Sec. 2.4.2 
4/4/2013 
9Proximity queries 
•LIMIT! /3 STATUTE /3 FEDERAL /2 TORT 
–Again, here, / kmeans “within kwords of”. 
•Clearly, positional indexes can be used for 
such queries; biword indexes cannot. 
•Exercise: Adapt the linear merge of postings to 
handle proximity queries.  Can you make it 
work for any value of k?
–This is a little tricky to do correctly and efficiently 
–See Figure 2.12 of IIR Sec. 2.4.2 
Positional index size 
•A positional index expands postings storage 
substantially 
–Even though indices can be compressed 
•Nevertheless, a positional index is now 
standardly used because of the power and 
usefulness of phrase and proximity queries … 
whether used explicitly or implicitly in a 
ranking retrieval system. Sec. 2.4.2 
Positional index size 
•Need an entry for each occurrence, not just 
once per document 
•Index size depends on average document size 
–Average web page has <1000 terms 
–SEC filings, books, even some epic poems … easily 
100,000 terms 
•Consider a term with frequency 0.1% Why? 
100 1 100,000 1 1 1000 Positional postings Postings Document size Sec. 2.4.2 
Rules of thumb 
•A positional index is 2–4 as large as a non-
positional index 
•Positional index size 35–50% of volume of 
original text 
–Caveat: all of this holds for “English-like” 
languages Sec. 2.4.2 
Combination schemes 
•These two approaches can be profitably 
combined 
–For particular phrases ( “Michael Jackson”, “Britney 
Spears” ) it is inefficient to keep on merging positional 
postings lists 
•Even more so for phrases like “The Who” 
•Williams et al. (2004) evaluate a more 
sophisticated mixed indexing scheme 
–A typical web query mixture was executed in ¼ of the 
time of using just a positional index 
–It required 26% more space than having a positional 
index alone Sec. 2.4.3 
Introduction to 
Information Retrieval 
Structured vs. Unstructured Data 
4/4/2013 
10 IR vs. databases: 
Structured vs unstructured data 
•Structured data tends to refer to information 
in “tables” 
55 Employee Manager Salary 
Smith Jones 50000 
Chang Smith 60000 
50000 Ivy Smith 
Typically allows numerical range and exact match 
(for text) queries, e.g., 
Salary < 60000 AND Manager = Smith .Unstructured data 
•Typically refers to free text 
•Allows 
–Keyword queries including operators 
–More sophisticated “concept” queries e.g., 
•find all web pages dealing with drug abuse 
•Classic model for searching text documents 
56 
Semi-structured data 
•In fact almost no data is “unstructured” 
•E.g., this slide has distinctly identified zones such 
as the Title and Bullets 
•… to say nothing of linguistic structure 
•Facilitates “semi-structured” search such as 
–Title contains data AND Bullets contain search 
•Or even 
–Title is about Object Oriented Programming AND 
Author something like stro*rup 
–where * is the wild-card operator 
57 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  Introduc*on	
  to	
  
Informa(on	
  Retrieval	
  CS276	
  Informa*on	
  Retrieval	
  and	
  Web	
  Search	
  Chris	
  Manning	
  and	
  Pandu	
  Nayak	
  Link	
  analysis	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Today’s	
  lecture	
  –	
  hypertext	
  and	
  links	
  § We	
  look	
  beyond	
  the	
  content	
  of	
  documents	
  § We	
  begin	
  to	
  look	
  at	
  the	
  hyperlinks	
  between	
  them	
  § Address	
  ques*ons	
  like	
  § Do	
  the	
  links	
  represent	
  a	
  conferral	
  of	
  authority	
  to	
  some	
  pages?	
  Is	
  this	
  useful	
  for	
  ranking?	
  § How	
  likely	
  is	
  it	
  that	
  a	
  page	
  pointed	
  to	
  by	
  the	
  CERN	
  home	
  page	
  is	
  about	
  high	
  energy	
  physics	
  § Big	
  applica*on	
  areas	
  § The	
  Web	
  § Email	
  § Social	
  networks	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Links	
  are	
  everywhere	
  § Powerful	
  sources	
  of	
  authen*city	
  and	
  authority	
  § Mail	
  spam	
  –	
  which	
  email	
  accounts	
  are	
  spammers?	
  § Host	
  quality	
  –	
  which	
  hosts	
  are	
  “bad”?	
  § Phone	
  call	
  logs	
  § The	
  Good,	
  The	
  Bad	
  and	
  The	
  Unknown	
  
3	
  ? ? ? ? Good Bad 

Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Example	
  1:	
  Good/Bad/Unknown	
  § The	
  Good,	
  The	
  Bad	
  and	
  The	
  Unknown	
  § Good	
  nodes	
  won’t	
  point	
  to	
  Bad	
  nodes	
  § All	
  other	
  combina*ons	
  plausible	
  
4	
  ? ? ? ? Good Bad 

Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Simple	
  itera*ve	
  logic	
  § Good	
  nodes	
  won’t	
  point	
  to	
  Bad	
  nodes	
  § If	
  you	
  point	
  to	
  a	
  Bad	
  node,	
  you’re	
  Bad	
  § If	
  a	
  Good	
  node	
  points	
  to	
  you,	
  you’re	
  Good	
  
5	
  ? ? ? ? Good Bad 

Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Simple	
  itera*ve	
  logic	
  § Good	
  nodes	
  won’t	
  point	
  to	
  Bad	
  nodes	
  § If	
  you	
  point	
  to	
  a	
  Bad	
  node,	
  you’re	
  Bad	
  § If	
  a	
  Good	
  node	
  points	
  to	
  you,	
  you’re	
  Good	
  
6	
  ? ? Good Bad 

Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Simple	
  itera*ve	
  logic	
  § Good	
  nodes	
  won’t	
  point	
  to	
  Bad	
  nodes	
  § If	
  you	
  point	
  to	
  a	
  Bad	
  node,	
  you’re	
  Bad	
  § If	
  a	
  Good	
  node	
  points	
  to	
  you,	
  you’re	
  Good	
  
7	
  Good Bad 
Sometimes need probabilistic analogs – e.g., mail spam 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Example	
  2:	
  In-­‐links	
  to	
  pages	
  –	
  unusual	
  paZerns	
  J	
  
8	
  
Spammers	
  viola*ng	
  power	
  laws!	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Many	
  other	
  examples	
  of	
  link	
  analysis	
  § Social	
  networks	
  are	
  a	
  rich	
  source	
  of	
  grouping	
  behavior	
  § E.g.,	
  Shoppers’	
  aﬃnity	
  –	
  Goel+Goldstein	
  2010	
  § Consumers	
  whose	
  friends	
  spend	
  a	
  lot,	
  spend	
  a	
  lot	
  themselves	
  § hZp://www.cs.cornell.edu/home/kleinber/networks-­‐book/	
  
9	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Our	
  primary	
  interest	
  in	
  this	
  course	
  § Link	
  analysis	
  for	
  most	
  IR	
  func*onality	
  thus	
  far	
  based	
  purely	
  on	
  text	
  § Scoring	
  and	
  ranking	
  § Link-­‐based	
  clustering	
  –	
  topical	
  structure	
  from	
  links	
  § Links	
  as	
  features	
  in	
  classiﬁca*on	
  –	
  documents	
  that	
  link	
  to	
  one	
  another	
  are	
  likely	
  to	
  be	
  on	
  the	
  same	
  subject	
  § Crawling	
  § Based	
  on	
  the	
  links	
  seen,	
  where	
  do	
  we	
  crawl	
  next?	
  
10	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
The	
  Web	
  as	
  a	
  Directed	
  Graph	
  
Hypothesis	
  1:	
  A	
  hyperlink	
  between	
  pages	
  denotes	
  	
  	
  	
  	
  	
  	
  	
  a	
  conferral	
  of	
  authority	
  (quality	
  signal)	
  Hypothesis	
  2:	
  The	
  text	
  in	
  the	
  anchor	
  of	
  the	
  hyperlink	
  on	
  page	
  A	
  describes	
  the	
  target	
  page	
  B	
  Page A hyperlink Page B Anchor Sec. 21.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Assump*on	
  1:	
  reputed	
  sites	
  
12	
  

Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Assump*on	
  2:	
  annota*on	
  of	
  target	
  
13	
  

Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Anchor	
  Text	
  	
  WWW	
  Worm	
  -­‐	
  McBryan	
  [Mcbr94]	
  	
  § For	
  ibm	
  how	
  to	
  dis*nguish	
  between:	
  § IBM’s	
  home	
  page	
  (mostly	
  graphical)	
  § IBM’s	
  copyright	
  page	
  (high	
  term	
  freq.	
  for	
  ‘ibm’)	
  § Rival’s	
  spam	
  page	
  (arbitrarily	
  high	
  term	
  freq.)	
  www.ibm.com “ibm”  “ibm.com” “IBM home page” A million pieces of anchor text with “ibm” send a strong signal Sec. 21.1.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Indexing	
  anchor	
  text	
  § When	
  indexing	
  a	
  document	
  D,	
  include	
  (with	
  some	
  weight)	
  anchor	
  text	
  from	
  links	
  poin*ng	
  to	
  D.	
  www.ibm.com Armonk, NY -based computer giant IBM announced today Joe’s computer hardware links Sun HP IBM Big Blue today announced record profits for the quarter Sec. 21.1.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Indexing	
  anchor	
  text	
  § Can	
  some*mes	
  have	
  unexpected	
  eﬀects,	
  e.g.,	
  spam,	
  miserable	
  failure	
  § Can	
  score	
  anchor	
  text	
  with	
  weight	
  depending	
  on	
  the	
  authority	
  of	
  the	
  anchor	
  page’s	
  website	
  § E.g.,	
  if	
  we	
  were	
  to	
  assume	
  that	
  content	
  from	
  cnn.com	
  or	
  yahoo.com	
  is	
  authorita*ve,	
  then	
  trust	
  (more)	
  the	
  anchor	
  text	
  from	
  them	
  § Increase	
  the	
  weight	
  of	
  oﬀ-­‐site	
  anchors	
  (non-­‐nepo*s*c	
  scoring)	
  Sec. 21.1.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Gekng	
  at	
  all	
  that	
  link	
  informa*on	
  Inexpensively	
  Connec*vity	
  servers	
  
17	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Connec*vity	
  Server	
  § Support	
  for	
  fast	
  queries	
  on	
  the	
  web	
  graph	
  § Which	
  URLs	
  point	
  to	
  a	
  given	
  URL?	
  § Which	
  URLs	
  does	
  a	
  given	
  URL	
  point	
  to?	
  Stores	
  mappings	
  in	
  memory	
  from	
  § URL	
  to	
  outlinks,	
  URL	
  to	
  inlinks	
  § Applica*ons	
  § Link	
  analysis	
  § Web	
  graph	
  analysis	
  § Connec*vity,	
  crawl	
  op*miza*on	
  § Crawl	
  control	
  Sec. 20.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Boldi	
  and	
  Vigna	
  2004	
  § hZp://www2004.org/proceedings/docs/1p595.pdf	
  § Webgraph	
  –	
  set	
  of	
  algorithms	
  and	
  a	
  java	
  implementa*on	
  § Fundamental	
  goal	
  –	
  maintain	
  node	
  adjacency	
  lists	
  in	
  memory	
  § For	
  this,	
  compressing	
  the	
  adjacency	
  lists	
  is	
  the	
  cri*cal	
  component	
  Sec. 20.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Adjacency	
  lists	
  § The	
  set	
  of	
  neighbors	
  of	
  a	
  node	
  § Assume	
  each	
  URL	
  represented	
  by	
  an	
  integer	
  § E.g.,	
  for	
  a	
  4	
  billion	
  page	
  web,	
  need	
  32	
  bits	
  per	
  node	
  § Naively,	
  this	
  demands	
  64	
  bits	
  to	
  represent	
  each	
  hyperlink	
  § Boldi/Vigna	
  get	
  down	
  to	
  an	
  average	
  of	
  ~3	
  bits/link	
  § Further	
  work	
  achieves	
  2	
  bits/link	
  Sec. 20.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Adjaceny	
  list	
  compression	
  § Proper*es	
  exploited	
  in	
  compression:	
  § Similarity	
  (between	
  lists)	
  § Locality	
  (many	
  links	
  from	
  a	
  page	
  go	
  to	
  “nearby”	
  pages)	
  § Use	
  gap	
  encodings	
  in	
  sorted	
  lists	
  § Distribu*on	
  of	
  gap	
  values	
  Sec. 20.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Main	
  ideas	
  of	
  Boldi/Vigna	
  § Consider	
  lexicographically	
  ordered	
  list	
  of	
  all	
  URLs,	
  e.g.,	
  	
  § www.stanford.edu/alchemy	
  § www.stanford.edu/biology	
  § www.stanford.edu/biology/plant	
  § www.stanford.edu/biology/plant/copyright	
  § www.stanford.edu/biology/plant/people	
  § www.stanford.edu/chemistry	
  Sec. 20.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Why 7? Boldi/Vigna	
  § Each	
  of	
  these	
  URLs	
  has	
  an	
  adjacency	
  list	
  § Main	
  idea:	
  due	
  to	
  templates,	
  the	
  adjacency	
  list	
  of	
  a	
  node	
  is	
  similar	
  to	
  one	
  of	
  the	
  7	
  preceding	
  URLs	
  in	
  the	
  lexicographic	
  ordering	
  § Express	
  adjacency	
  list	
  in	
  terms	
  of	
  one	
  of	
  these	
  § E.g.,	
  consider	
  these	
  adjacency	
  lists	
  § 1,	
  2,	
  4,	
  8,	
  16,	
  32,	
  64	
  § 1,	
  4,	
  9,	
  16,	
  25,	
  36,	
  49,	
  64	
  § 1,	
  2,	
  3,	
  5,	
  8,	
  13,	
  21,	
  34,	
  55,	
  89,	
  144	
  § 1,	
  4,	
  8,	
  16,	
  25,	
  36,	
  49,	
  64	
  Encode as (-2), remove 9, add 8 Sec. 20.4 

Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Gap	
  encodings	
  § Given	
  a	
  sorted	
  list	
  of	
  integers	
  x,	
  y,	
  z,	
  …,	
  represent	
  by	
  x,	
  y-­‐x,	
  z-­‐y,	
  …	
  	
  § Compress	
  each	
  integer	
  using	
  a	
  code	
  §  γ code	
  -­‐	
  Number	
  of	
  bits	
  =	
  1	
  +	
  2	
  ⎣lg	
  x⎦ 	

§ δ code:	
  …	
  § Informa*on	
  theore*c	
  bound:	
  1	
  +	
  ⎣lg	
  x⎦ bits	
  	
  § ζ code:	
  Works	
  well	
  for	
  integers	
  from	
  a	
  power	
  law	
  Boldi	
  Vigna	
  DCC	
  2004	
  Sec. 20.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Main	
  advantages	
  of	
  BV	
  § Depends	
  only	
  on	
  locality	
  in	
  a	
  canonical	
  ordering	
  § Lexicographic	
  ordering	
  works	
  well	
  for	
  the	
  web	
  § Adjacency	
  queries	
  can	
  be	
  answered	
  very	
  eﬃciently	
  § To	
  fetch	
  out-­‐neighbors,	
  trace	
  back	
  the	
  chain	
  of	
  prototypes	
  § This	
  chain	
  is	
  typically	
  short	
  in	
  prac*ce	
  (since	
  similarity 	
  is	
  mostly	
  intra-­‐host)	
  § Can	
  also	
  explicitly	
  limit	
  the	
  length	
  of	
  the	
  chain	
  during	
  encoding	
  § Easy	
  to	
  implement	
  one-­‐pass	
  algorithm	
  Sec. 20.4 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  Link	
  analysis:	
  Pagerank	
  
26	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Cita*on	
  Analysis	
  § Cita*on	
  frequency	
  § Bibliographic	
  coupling	
  frequency	
  § Ar*cles	
  that	
  co-­‐cite	
  the	
  same	
  ar*cles	
  are	
  related	
  	
  § Cita*on	
  indexing	
  § Who	
  is	
  this	
  author	
  cited	
  by?	
  (Garﬁeld	
  1972)	
  § Pagerank	
  preview:	
  Pinsker	
  and	
  Narin	
  ’60s	
  § Asked:	
  which	
  journals	
  are	
  authorita*ve?	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
The	
  web	
  isn’t	
  scholarly	
  cita*on	
  § Millions	
  of	
  par*cipants,	
  each	
  with	
  self	
  interests	
  § Spamming	
  is	
  widespread	
  § Once	
  search	
  engines	
  began	
  to	
  use	
  links	
  for	
  ranking	
  (roughly	
  1998),	
  link	
  spam	
  grew	
  § You	
  can	
  join	
  a	
  link	
  farm	
  –	
  a	
  group	
  of	
  websites	
  that	
  heavily	
  link	
  to	
  one	
  another	
  
28	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Pagerank	
  scoring	
  § Imagine	
  a	
  user	
  doing	
  a	
  random	
  walk	
  on	
  web	
  pages:	
  § Start	
  at	
  a	
  random	
  page	
  § At	
  each	
  step,	
  go	
  out	
  of	
  the	
  	
  current	
  page	
  along	
  one	
  of	
  	
  the	
  links	
  on	
  that	
  page,	
  equiprobably	
  § “In	
  the	
  long	
  run”	
  each	
  page	
  has	
  a	
  long-­‐term	
  visit	
  rate	
  -­‐	
  use	
  this	
  as	
  the	
  page’s	
  score.	
  	
  1/3 1/3 1/3 Sec. 21.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Not	
  quite	
  enough	
  § The	
  web	
  is	
  full	
  of	
  dead-­‐ends.	
  § Random	
  walk	
  can	
  get	
  stuck	
  in	
  dead-­‐ends.	
  § Makes	
  no	
  sense	
  to	
  talk	
  about	
  long-­‐term	
  visit	
  rates.	
  ?? Sec. 21.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Telepor*ng	
  § At	
  a	
  dead	
  end,	
  jump	
  to	
  a	
  random	
  web	
  page.	
  § At	
  any	
  non-­‐dead	
  end,	
  with	
  probability	
  10%,	
  jump	
  to	
  a	
  random	
  web	
  page.	
  § With	
  remaining	
  probability	
  (90%),	
  go	
  out	
  on	
  a	
  random	
  link.	
  § 10%	
  -­‐	
  a	
  parameter.	
  Sec. 21.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Result	
  of	
  telepor*ng	
  § Now	
  cannot	
  get	
  stuck	
  locally.	
  § There	
  is	
  a	
  long-­‐term	
  rate	
  at	
  which	
  any	
  page	
  is	
  visited	
  (not	
  obvious,	
  will	
  show	
  this).	
  § How	
  do	
  we	
  compute	
  this	
  visit	
  rate?	
  Sec. 21.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Markov	
  chains	
  § A	
  Markov	
  chain	
  consists	
  of	
  n	
  states,	
  plus	
  an	
  n×n	
  transi*on	
  probability	
  matrix	
  P.	
  § At	
  each	
  step,	
  we	
  are	
  in	
  one	
  of	
  the	
  states.	
  § For	
  1	
  ≤	
  i,j	
  ≤	
  n,	
  the	
  matrix	
  entry	
  Pij	
  tells	
  us	
  the	
  probability	
  of	
  j	
  being	
  the	
  next	
  state,	
  given	
  we	
  are	
  currently	
  in	
  state	
  i.	
  	
  i j Pij Pii>0 is OK. Sec. 21.2.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
.11=∑=ijnjPMarkov	
  chains	
  § Clearly,	
  for	
  all	
  i,	
  § 	
  Markov	
  chains	
  are	
  abstrac*ons	
  of	
  random	
  walks.	
  § Exercise:	
  represent	
  the	
  telepor*ng	
  random	
  walk	
  from	
  3	
  slides	
  ago	
  as	
  a	
  Markov	
  chain,	
  for	
  this	
  case:	
  	
  Sec. 21.2.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Ergodic	
  Markov	
  chains	
  § For	
  any	
  ergodic	
  Markov	
  chain,	
  there	
  is	
  a	
  unique	
  long-­‐term	
  visit	
  rate	
  for	
  each	
  state.	
  § Steady-­‐state	
  probability	
  distribu)on.	
  § Over	
  a	
  long	
  *me-­‐period,	
  we	
  visit	
  each	
  state	
  in	
  propor*on	
  to	
  this	
  rate.	
  § It	
  doesn’t	
  maZer	
  where	
  we	
  start.	
  Sec. 21.2.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Probability	
  vectors	
  § A	
  probability	
  (row)	
  vector	
  x	
  =	
  (x1,	
  …	
  xn)	
  tells	
  us	
  where	
  the	
  walk	
  is	
  at	
  any	
  point.	
  § E.g.,	
  (000…1…000)	
  means	
  we’re	
  in	
  state	
  i.	
  i n 1 More generally, the vector x = (x1, … xn) means the walk is in state i with probability xi.  .11=∑=niixSec. 21.2.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Change	
  in	
  probability	
  vector	
  § If	
  the	
  probability	
  vector	
  is	
  	
  x	
  =	
  (x1,	
  …	
  xn)	
  at	
  this	
  step,	
  what	
  is	
  it	
  at	
  the	
  next	
  step?	
  § Recall	
  that	
  row	
  i	
  of	
  the	
  transi*on	
  prob.	
  Matrix	
  P	
  tells	
  us	
  where	
  we	
  go	
  next	
  from	
  state	
  i.	
  § So	
  from	
  x,	
  our	
  next	
  state	
  is	
  distributed	
  as	
  xP	
  § The	
  one	
  aser	
  that	
  is	
  xP2,	
  then	
  xP3,	
  etc.	
  § (Where)	
  Does	
  this	
  converge?	
  Sec. 21.2.1 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
How	
  do	
  we	
  compute	
  this	
  vector?	
  § Let	
  a	
  =	
  (a1,	
  …	
  an)	
  denote	
  the	
  row	
  vector	
  of	
  steady-­‐state	
  probabili*es.	
  § If	
  our	
  current	
  posi*on	
  is	
  described	
  by	
  a,	
  then	
  the	
  next	
  step	
  is	
  distributed	
  as	
  aP.	
  § But	
  a	
  is	
  the	
  steady	
  state,	
  so	
  a=aP.	
  § Solving	
  this	
  matrix	
  equa*on	
  gives	
  us	
  a.	
  § So	
  a	
  is	
  the	
  (les)	
  eigenvector	
  for	
  P.	
  § (Corresponds	
  to	
  the	
  “principal”	
  eigenvector	
  of	
  P	
  with	
  the	
  largest	
  eigenvalue.)	
  § Transi*on	
  probability	
  matrices	
  always	
  have	
  largest	
  eigenvalue	
  1.	
  Sec. 21.2.2 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  Link	
  analysis:	
  HITS	
  
39	
  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Hyperlink-­‐Induced	
  Topic	
  Search	
  (HITS)	
  § In	
  response	
  to	
  a	
  query,	
  instead	
  of	
  an	
  ordered	
  list	
  of	
  pages	
  each	
  mee*ng	
  the	
  query,	
  ﬁnd	
  two	
  sets	
  of	
  inter-­‐related	
  pages:	
  § Hub	
  pages	
  are	
  good	
  lists	
  of	
  links	
  on	
  a	
  subject.	
  § e.g.,	
  “Bob’s	
  list	
  of	
  cancer-­‐related	
  links.”	
  § Authority	
  pages	
  occur	
  recurrently	
  on	
  good	
  hubs	
  for	
  the	
  subject.	
  § Best	
  suited	
  for	
  “broad	
  topic”	
  queries	
  rather	
  than	
  for	
  page-­‐ﬁnding	
  queries.	
  § Gets	
  at	
  a	
  broader	
  slice	
  of	
  common	
  opinion.	
  Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Hubs	
  and	
  Authori*es	
  § Thus,	
  a	
  good	
  hub	
  page	
  for	
  a	
  topic	
  points	
  to	
  many	
  authorita*ve	
  pages	
  for	
  that	
  topic.	
  § A	
  good	
  authority	
  page	
  for	
  a	
  topic	
  is	
  pointed	
  to	
  by	
  many	
  good	
  hubs	
  for	
  that	
  topic.	
  § Circular	
  deﬁni*on	
  -­‐	
  will	
  turn	
  this	
  into	
  an	
  itera*ve	
  computa*on.	
  Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
The	
  hope	
                                                      AT&T         Alice                                                        ITIM  Bob                                   O2 Mobile telecom companies Hubs Authorities Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
High-­‐level	
  scheme	
  § Extract	
  from	
  the	
  web	
  a	
  base	
  set	
  of	
  pages	
  that	
  could	
  be	
  good	
  hubs	
  or	
  authori*es.	
  § From	
  these,	
  iden*fy	
  a	
  small	
  set	
  of	
  top	
  hub	
  and	
  authority	
  pages;	
  → itera*ve	
  algorithm.	
  Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Base	
  set	
  § Given	
  text	
  query	
  (say	
  browser),	
  use	
  a	
  text	
  index	
  to	
  get	
  all	
  pages	
  containing	
  browser.	
  § Call	
  this	
  the	
  root	
  set	
  of	
  pages.	
  	
  § Add	
  in	
  any	
  page	
  that	
  either	
  § points	
  to	
  a	
  page	
  in	
  the	
  root	
  set,	
  or	
  § is	
  pointed	
  to	
  by	
  a	
  page	
  in	
  the	
  root	
  set.	
  § Call	
  this	
  the	
  base	
  set.	
  Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Visualiza*on	
  Root set Base set Sec. 21.3 
Get in-links (and out-links) from a connectivity server  
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Dis*lling	
  hubs	
  and	
  authori*es	
  § Compute,	
  for	
  each	
  page	
  x	
  in	
  the	
  base	
  set,	
  a	
  hub	
  score	
  h(x)	
  and	
  an	
  authority	
  score	
  a(x).	
  § Ini*alize:	
  for	
  all	
  x,	
  h(x)←1;	
  a(x)	
  ←1;	
  § Itera*vely	
  update	
  all	
  h(x),	
  a(x);	
  § Aser	
  itera*ons	
  § output	
  pages	
  with	
  highest	
  h()	
  scores	
  as	
  top	
  hubs	
  § 	
  highest	
  a()	
  scores	
  as	
  top	
  authori*es.	
  Key Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Itera*ve	
  update	
  § Repeat	
  the	
  following	
  updates,	
  for	
  all	
  x:	
  ∑←yxyaxh!)()(∑←xyyhxa!)()(x x Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Scaling	
  § To	
  prevent	
  the	
  h()	
  and	
  a()	
  values	
  from	
  gekng	
  too	
  big,	
  can	
  scale	
  down	
  aser	
  each	
  itera*on.	
  § Scaling	
  factor	
  doesn’t	
  really	
  maZer:	
  § we	
  only	
  care	
  about	
  the	
  rela)ve	
  values	
  of	
  the	
  scores.	
  Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
How	
  many	
  itera*ons?	
  § Claim:	
  rela*ve	
  values	
  of	
  scores	
  will	
  converge	
  aser	
  a	
  few	
  itera*ons:	
  § in	
  fact,	
  suitably	
  scaled,	
  h()	
  and	
  a()	
  scores	
  seZle	
  into	
  a	
  steady	
  state!	
  § proof	
  of	
  this	
  comes	
  later.	
  § In	
  prac*ce,	
  ~5	
  itera*ons	
  get	
  you	
  close	
  to	
  stability.	
  Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Proof	
  of	
  convergence	
  § n×n	
  adjacency	
  matrix	
  A:	
  § each	
  of	
  the	
  n	
  pages	
  in	
  the	
  base	
  set	
  has	
  a	
  row	
  and	
  column	
  in	
  the	
  matrix.	
  § Entry	
  Aij	
  =	
  1	
  if	
  page	
  i	
  links	
  to	
  page	
  j,	
  else	
  =	
  0.	
  1 2 3  1      2      3 1 2 3 0      1      0  1      1      1  1      0      0 Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Hub/authority	
  vectors	
  § View	
  the	
  hub	
  scores	
  h()	
  and	
  the	
  authority	
  scores	
  a()	
  as	
  vectors	
  with	
  n	
  components.	
  § Recall	
  the	
  itera*ve	
  updates	
  ∑←yxyaxh!)()(∑←xyyhxa!)()(Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Rewrite	
  in	
  matrix	
  form	
  § h=Aa.	
  § a=Ath.	
  Recall At is the transpose of A.  Substituting, h=AAth and a=AtAa. Thus, h is an eigenvector of AAt and a is an eigenvector of AtA. Further, our algorithm is a particular, known algorithm for computing eigenvectors: the power iteration method. Guaranteed to converge. Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Issues	
  § Topic	
  Dris	
  § Oﬀ-­‐topic	
  pages	
  can	
  cause	
  oﬀ-­‐topic	
  “authori*es”	
  to	
  be	
  returned	
  § E.g.,	
  the	
  neighborhood	
  graph	
  can	
  be	
  about	
  a	
  “super	
  topic”	
  § Mutually	
  Reinforcing	
  Aﬃliates	
  § 	
  Aﬃliated	
  pages/sites	
  can	
  boost	
  each	
  others’	
  scores	
  	
  § Linkage	
  between	
  aﬃliated	
  pages	
  is	
  not	
  a	
  useful	
  signal	
  Sec. 21.3 
Introduc)on	
  to	
  Informa)on	
  Retrieval	
  
	
  	
  
	
  	
  
Resources	
  § IIR	
  Chap	
  21	
  § hZp://www2004.org/proceedings/docs/1p309.pdf	
  § hZp://www2004.org/proceedings/docs/1p595.pdf	
  § hZp://www2003.org/cdrom/papers/refereed/p270/kamvar-­‐270-­‐xhtml/index.html	
  § hZp://www2003.org/cdrom/papers/refereed/p641/xhtml/p641-­‐mccurley.html	
  § The	
  WebGraph	
  framework	
  I:	
  Compression	
  techniques	
  (Boldi	
  et	
  al.	
  2004)	
  
Neural	Machine	Transla/on	Thang	Luong	Lecture	@	CS224D	Spring	2016	(Special	thanks	to	Chris	Manning	for	feedback!)				

7	billion	people,	7000	languages	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	2	
A	universal	translator	
(The	Babel	Fish	from	“the	Hitchhiker's	Guide	to	the	Galaxy”)	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	3	
Machine	vs.	Human	Transla/on	
“Nevertheless,	within	the	discipline	of	machine	learning,	a	specializa8on	called	deep	learning	has	been	developed.	Its	dis8nguishing	feature	is	that	it	is	inspired	by	neurobiology.	Deep	learning	deals	with	computa8onal	elements	which	allow	a	network	of	ar8ﬁcial	neurons	to	learn	a	model	of	the	human	brain.”	
Faithful	transla/on	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	4	• GrammaHcally	incorrect.	
Machine	vs.	Human	Transla/on	
“Nevertheless,	within	the	discipline	of	machine	learning,	a	specializa8on	called	deep	learning	has	been	developed.	Its	dis8nguishing	feature	is	that	it	is	inspired	by	neurobiology.	Deep	learning	deals	with	computa8onal	elements	which	allow	a	network	of	ar8ﬁcial	neurons	to	learn	a	model	of	the	human	brain.”	
Faithful	transla/on	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	5	• Bad	word	choices.	
Machine	vs.	Human	Transla/on	
“Nevertheless,	within	the	discipline	of	machine	learning,	a	specializa8on	called	deep	learning	has	been	developed.	Its	dis8nguishing	feature	is	that	it	is	inspired	by	neurobiology.	Deep	learning	deals	with	computa8onal	elements	which	allow	a	network	of	ar8ﬁcial	neurons	to	learn	a	model	of	the	human	brain.”	
Faithful	transla/on	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	6	• Bad	sentence	structures.	
Machine	vs.	Human	Transla/on	
“However,	in	machine	learning	a	specializa8on	called	deep	learning	has	emerged.	It	can	be	recognized	by	its	dis8nc8ve	neurobiological	inﬂuence.	Deep	learning	is	centered	around	networks	of	ar8ﬁcial	neurons	which	can	learn	models	of	the	human	brain.”	
Fluent	transla/on	A	big	gap!	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	7	
How	has	MT	evolved?	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	8	
Phrase-based	MT	She	loves	Elle	aime	cute	les	chats	cats	mignons	(Brown	et	al.,	1993;	Koehn	et	al.,	2003;	Och	&	Ney,	2004)	• Break	sentences	into	chunks.	• Transla8on	model:	look	up	phrase	translaHons.	• Language	model:	He	phrases	together.	Translate	locally		LM	uses	only	target	words		5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	9	
Joint	Neural	Language	Model	• CondiHoned	on	source	words	(Devlin	et	al.,	2014)	• SHll	translate	locally.	…	allés	pour	une	promenade	le	long	de	la	We	went	for	a	stroll	along	the	South	Bank	
rive	
walk	
along	
bank	(river)	MT	systems	become	more	complex!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	10	
Neural	Machine	Transla/on	to	the	rescue!	
Let’s	ﬁnd	out!	• Sequence-to-sequence:	translate	globally.	• End-to-end:	simple	&	generalizable.	(Sutskever	et	al.,	2014;	Cho	et	al.,	2014)	
NMT	
I	am	a	student	Je	suis	étudiant	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	11	
Outline	• Basic	NMT	– RNN	Recap.	– Encoder-Decoder.	– Training.	– TesHng.	• Advanced	NMT	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	12	
Recurrent	Neural	Networks	(RNNs)	
I	am	a	student	input:	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	13	(Picture	adapted	from	Andrej	Karparthy)	
Recurrent	Neural	Networks	(RNNs)	
I	am	a	student	input:	RNNs	to	represent	sequences!	
ht-1	ht	xt	(Picture	adapted	from	Andrej	Karparthy)	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	14	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
I
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	15	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
I
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	16	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
I
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	17	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
I
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	18	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
I
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	19	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
IEncoder	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	20	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
IEncoder	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	21	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	Boundary	marker	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
IEncoder	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	22	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
I		Encoder	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	23	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	
Neural	Machine	Transla/on	(NMT)	
am a student_Je suis étudiantJesuis étudiant_
IEncoder	Decoder	• Recurrent	Neural	Networks:	– Model	P(target	|	source)	directly.		– Can	be	trained	end-to-end.	
Boundary	marker	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	24	
Word	Embeddings	
am a student_Je suis étudiantJesuis étudiant_
I• One	for	each	language:	can	learn	from	scratch.	Source	embeddings	Target	embeddings	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	25	
am a student_Je suis étudiantJesuis étudiant_
IRecurrent	Connec/ons	IniHal		states	
• Olen	set	to	0.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	26	
am a student_Je suis étudiantJesuis étudiant_
IRecurrent	Connec/ons	Encoder	1st	layer	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	27	• Diﬀerent:	{1st	layer,	2nd	layer}	x	{encoder,	decoder}.	
am a student_Je suis étudiantJesuis étudiant_
IRecurrent	Connec/ons	Encoder	2nd	layer	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	28	• Diﬀerent:	{1st	layer,	2nd	layer}	x	{encoder,	decoder}.	
am a student_Je suis étudiantJesuis étudiant_
IRecurrent	Connec/ons	
• Diﬀerent:	{1st	layer,	2nd	layer}	x	{encoder,	decoder}.	Decoder	1st	layer	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	29	
am a student_Je suis étudiantJesuis étudiant_
IRecurrent	Connec/ons	Decoder	2nd	layer	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	30	• Diﬀerent:	{1st	layer,	2nd	layer}	x	{encoder,	decoder}.	
Recurrent	Units	• Vanilla:		• LSTM:		
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	31	
RNN	
LSTM	
C’mon,	it’s	been	around	for	20	years!	Vanishing	gradient	problem!		
SoTmax:	vectors	↦	categories	
		
am a student_Je suis étudiantJesuis étudiant_
Iam a student_Je suis étudiantJesuis étudiant_
ISolmax	parameters	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	32	am a student_Je suis étudiantJesuis étudiant_
I		Target	hidden	state	
|V|	
SoTmax:	vectors	↦	categories	
		
am a student_Je suis étudiantJesuis étudiant_
Iam a student_Je suis étudiantJesuis étudiant_
I5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	33	am a student_Je suis étudiantJesuis étudiant_
I
|V|	• Hidden	states	↦	scores		↦	probabiliHes.	Scores	=					
		Probs	P(Je|	…)	exp	&	normalize	
Training	Loss	
• Maximize	P(target	|	source)		am a student_Je suis étudiantIJesuis étudiant_
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	34	
Training	Loss	
am a student_Je suis étudiant-log P(Je)
I-log P(suis)			
• Sum	of	all	individual	losses	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	35	
Training	Loss	
am a student_Je suis étudiant-log P(Je)
I-log P(suis)								
• Sum	of	all	individual	losses	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	36	
Training	Loss	
• Sum	of	all	individual	losses					
am a student_Je suis étudiantI-log P(étudiant)
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	37	
Training	Loss	
• Sum	of	all	individual	losses	am a student_Je suis étudiantI-log P(_)				
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	38	
Backpropaga/on	Through	Time	
am a student_Je suis étudiantI-log P(_)				
Init	to	0	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	39	
am a student_Je suis étudiantI-log P(étudiant)				Backpropaga/on	Through	Time	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	40	
am a student_Je suis étudiantI-log P(étudiant)				Backpropaga/on	Through	Time	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	41	
am a student_Je suis étudiantI-log P(suis)				Backpropaga/on	Through	Time	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	42	
am a student_Je suis étudiantI-log P(suis)				Backpropaga/on	Through	Time	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	43	
				
am a student_Je suis étudiantIBackpropaga/on	Through	Time	
RNN	gradients	are	accumulated.		5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	44	
Training	vs.	Tes/ng	• Training	– Correct	translaHons	are	available.		• Tes8ng	– Only	source	sentences	are	given.	am a student_Je suis étudiantJesuis étudiant_
I
am a student_Je suis étudiantJesuis étudiant_
I
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	45	
Tes/ng	
• Feed	the	most	likely	word	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	46	
Tes/ng	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	47	• Feed	the	most	likely	word	
Tes/ng	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	48	• Feed	the	most	likely	word	
Tes/ng	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	49	• Feed	the	most	likely	word	
Tes/ng	
Simple	beam-search	decoders!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	50	
37	33.3	34.8	36.5	
31	32	33	34	35	36	37	38	BLEU	SOTA	SMT	(Durrani+,	2014)	Avg	SMT	(Schwenk,	2014)	NMT	(Sutskever+,	2014)	SMT	+	NMT	rescore	(Sutskever+,	2014)	English-French	WMT’14	results	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	51	
English-French	WMT’14	results	
37	33.3	34.8	36.5	
31	32	33	34	35	36	37	38	BLEU	SOTA	SMT	(Durrani+,	2014)	Avg	SMT	(Schwenk,	2014)	NMT	(Sutskever+,	2014)	SMT	+	NMT	rescore	(Sutskever+,	2014)	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	52	
2	decades	of	research		
1-2	years	of	research	
Encoder-decoder	Variants	Encoder	Decoder	(Sutskever	et	al.,	2014)	My	NMT	models	Deep	LSTM	Deep	LSTM	(Cho	et	al.,	2014)	(Bahdanau	et	al.,	2015)	(Jean	et	al.,	2015)	(BidirecHonal)	GRU	GRU	(Kalchbrenner	&	Blunsom,	2013)	CNN	(Inverse	CNN)	RNN	Next,	advanced	NMT!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	53	
Deep	fried	baby	Meat	muscle	stupid	bean	sprouts	
Break	/me:	when	MT	fails	…	
Sale	of	chicken	murder	Go	back	toward	your	behind	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	54	
Limita/ons	• #1:	the	vocabulary	size	problem	– Goal:	extend	the	vocabulary	coverage.	• #2:	the	sentence	length	problem	– Goal:	translate	long	sentences	bever.	• #3:	the	language	complexity	problem	– Goal:	handle	more	language	variaHons.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	55	
Advancing	NMT	• #1:	the	vocabulary	size	problem	– Sol:	“copy”	mechanism.	• #2:	the	sentence	length	problem	– Sol:	avenHon	mechanism.	• #3:	the	language	complexity	problem	– Sol:	character-level	translaHon.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	56	The			<unk>			porHco			in			<unk>		Le			<unk>			<unk>			de				<unk>	ecotax	Pont-de-Buis	por8que	écotaxe	Pont-de-Buis	
am a student_Je suis étudiantJesuis étudiant_
I

CV	vs.	NLP	
Computer	Vision	
cat	1K	categories	1M	categories	
NLP	
mat	The	cat	sat	on	a		5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	57	
#1	The	Vocabulary	Size	Problem	• Word	generaHon	problem	– Vocabs	are	modest:	50K.	– Simple	solmax:	GPU	friendliness.	am a student_Je suis étudiantJesuis étudiant_
IThe			<unk>			porHco			in			<unk>	Le			<unk>			<unk>			de				<unk>	The			ecotax			porHco			in			Pont-de-Buis	Le			porHque			écotaxe	de				Pont-de-Buis	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	58	
• Propose	“copy”	mechanisms	for	<unk>.	• Simple	&	eﬀecHve	– Treat	any	NMT	as	a	black	box.	– Annotate	training	data.	– Post-process	translaHons.	Thang	Luong*,	Ilya	Sutskever*,	Quoc	Le*,	Oriol	Vinyals,	and	Wojciech	Zaremba.	Addressing	the	Rare	Word	Problem	in	Neural	Machine	Transla>on.	ACL	2015.	SOTA	for	English-French	translaHon.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	59	

Our	approach	–	training	annota>on	• Add	relaHve	posiHons.	“AvenHon”	for	rare	words	The			ecotax			porHco			in			Pont-de-Buis		Le			porHque			écotaxe	de				Pont-de-Buis	• Learn	alignments.	The			<unk>			porHco			in			<unk>		Le			unk1			unk-1			de				unk0	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	60	
Our	approach	–	post-process	Test	sentence	
		Transla8on	The			<unk>			porHco			in			<unk>	Le			porHque			unk-1			de				unk0	ecotax	Pont-de-Buis	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	61	
Our	approach	–	post-process	Test	sentence	
		Transla8on	The			<unk>			porHco			in			<unk>	Le			porHque			unk-1			de				unk0	ecotax	Pont-de-Buis	
		Post-edit	Transla8on	Dic/onary	translaHon	Le			porHque			écotaxe			de				Pont-de-Buis		5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	62	
Our	approach	–	post-process	Test	sentence	
		Transla8on	The			<unk>			porHco			in			<unk>	Le			porHque			unk-1			de				unk0	ecotax	Pont-de-Buis	
		Le			porHque			écotaxe			de				Pont-de-Buis		Iden/ty	copy	Post-edit	Transla8on	Orthogonal	to	large-vocab	techniques	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	63	
Eﬀects	of	Transla/ng	Rare	Words	
25	30	35	40	BLEU	Sentences	ordered	by	average	frequency	rank	Durrani	et	al.	(37.0)	Sutskever	et	al.	(34.8)	This	work	(37.5)	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	64	First	SOTA	NMT	system!	
Sample	transla/ons	
• Predict	well	long-distance	alignments.	– Correct:	cataract	vs.	cataracte.		source	An	addiHonal	2600	operaHons	including	orthopedic	and	cataract	surgery	will	help	clear	a	backlog	.	human	2600	opéraHons	supplémentaires	,	notamment	dans	le	domaine	de	la	chirurgie	orthopédique	et	de	la	cataracte	,	aideront	à	ravraper	le	retard	.	trans	En	outre	,	unk1	opéraHons	supplémentaires	,	dont	la	chirurgie	unk5	et	la	unk6	,	permevront	de	résorber	l'	arriéré	.	trans+unk	En	outre	,	2600	opéraHons	supplémentaires	,	dont	la	chirurgie	orthopédiques	et	la	cataracte	,	permevront	de	résorber	l'	arriéré	.	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	65	
Sample	transla/ons	
• Translate	well	long	sentences.	– Correct:	JPMorgan	vs.	JPMorgan.		source	This	trader	,	Richard	Usher	,	lel	RBS	in	2010	and	is	understand	to	have	be	given	leave	from	his	current	posiHon	as	European	head	of	forex	spot	trading	at	JPMorgan	.	human	Ce	trader	,	Richard	Usher	,	a	quivé	RBS	en	2010	et	aurait	été	mis	suspendu	de	son	poste	de	responsable	européen	du	trading	au	comptant	pour	les	devises	chez	JPMorgan	.	trans	Ce	unk0	,	Richard	unk0	,	a	quivé	unk1	en	2010	et	a	compris	qu'	il	est	autorisé	à	quiver	son	poste	actuel	en	tant	que	leader	européen	du	marché	des	points	de	vente	au	unk5	.	trans+unk	Ce	négociateur	,	Richard	Usher	,	a	quivé	RBS	en	2010	et	a	compris	qu'	il	est	autorisé	à	quiver	son	poste	actuel	en	tant	que	leader	européen	du	marché	des	points	de	vente	au	JPMorgan	.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	66	
Sample	transla/ons	
• Incorrect	alignment	predicHon:	was	–	était	vs.	abandonnait.	source	But	concerns	have	grown	aler	Mr	Mazanga	was	quoted	as	saying	Renamo	was	abandoning	the	1992	peace	accord	.	human	Mais	l'	inquiétude	a	grandi	après	que	M.	Mazanga	a	déclaré	que	la	Renamo	abandonnait	l'	accord	de	paix	de	1992	.	trans	Mais	les	inquiétudes	se	sont	accrues	après	que	M.	unkpos3	a	déclaré	que	la	unk3	unk3	l'	accord	de	paix	de	1992	.	trans+unk	Mais	les	inquiétudes	se	sont	accrues	après	que	M.	Mazanga	a	déclaré	que	la	Renamo	était	l'	accord	de	paix	de	1992	.	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	67	
Advancing	NMT	• #1:	the	vocabulary	size	problem	– Sol:	“copy”	mechanism.	• #2:	the	sentence	length	problem	– Sol:	avenHon	mechanism.	• #3:	the	language	complexity	problem	– Sol:	character-level	translaHon.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	68	The			<unk>			porHco			in			<unk>		Le			<unk>			<unk>			de				<unk>	ecotax	Pont-de-Buis	por8que	écotaxe	Pont-de-Buis	
am a student_Je suis étudiantJesuis étudiant_
I

#2	The	Sentence	Length	Problem	
Problem:	sentence	meaning	is	represented	by	a	ﬁxed-dimensional	vector.	am a student_Je suis étudiantJesuis étudiant_
I
• TranslaHon	quality	degrades	with	long	sentences.	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	69	
You	can't	cram	the	meaning	of	a	whole	%&!$#	sentence	into	a	single	$&!#*	vector!		
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	70	
(Adapted	from	KyungHuyn	Cho’	talk)	
Amen/on	Mechanism	
• SoluHon:	random	access	memory	– Retrieve	as	needed.	am a student_Je suis étudiantJesuis étudiant_
IPool	of		source		states	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	71	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	72	
Dzmitry	Bahdanau,	KyungHuyn	Cho,	and	Yoshua	Bengio.	Neural	Machine	Transla>on	by	Jointly	Learning	to	Translate	and	Align.	ICLR	2015.	
With	avenHon	
Without	avenHon	

Amen/on	Mechanism	
am a student_Jesuis
IAttention LayerContext vector?	A	simpliﬁed	version	of	(Bahdanau	et	al.,	2015)	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	73	
• Compare	target	and	source	hidden	states.	Amen/on	Mechanism	–	Scoring	
am a student_Jesuis
IAttention LayerContext vector?	3	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	74	
• Compare	target	and	source	hidden	states.	Amen/on	Mechanism	–	Scoring	
am a student_Jesuis
IAttention LayerContext vector?	5	3	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	75	
• Compare	target	and	source	hidden	states.	Amen/on	Mechanism	–	Scoring	
am a student_Jesuis
IAttention LayerContext vector?	1	3	5	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	76	
• Compare	target	and	source	hidden	states.	Amen/on	Mechanism	–	Scoring	
am a student_Jesuis
IAttention LayerContext vector?	1	3	5	1	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	77	
• Convert	into	alignment	weights.	Amen/on	Mechanism	–	Normaliza>on	
am a student_Jesuis
IAttention LayerContext vector?	0.1	0.3	0.5	0.1	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	78	
am a student_Jesuis
IContext vector
• Build	context	vector:	weighted	average.	Amen/on	Mechanism	–	Context	vector	?	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	79	
am a student_Jesuis
IContext vector
• Compute	the	next	hidden	state.	Amen/on	Mechanism	–	Hidden	state	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	80	
am a student_Jesuis
IContext vector
• Predict	the	next	word.	Amen/on	Mechanism	–	Predict	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	81	
• Examine	various	avenHon	mechanisms:	
Thang	Luong,	Hieu	Pham,	and	Chris	Manning.	Eﬀec>ve	Approaches	to	AGen>on-based	Neural	Machine	Transla>on.	EMNLP	2015.	Global:	all	source	states.	Local:	subset	of	source	states.	SOTA	for	English-German	translaHon.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	82	

Translate	Long	Sentences	
1020304050607010152025
Sent LengthsBLEU     
  ours, no attn (BLEU 13.9)ours, local−p attn (BLEU 20.9)ours, best system (BLEU 23.0)WMT’14 best (BLEU 20.7)Jeans et al., 2015 (BLEU 21.6)No	AvenHon	
AvenHon	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	83	
Sample	English-German	transla/ons	
• Translate	names	correctly.	source	Orlando	Bloom	and	Miranda	Kerr	sHll	love	each	other		human	Orlando	Bloom	und	Miranda	Kerr	lieben	sich	noch	immer	best	Orlando	Bloom	und	Miranda	Kerr	lieben	einander	noch	immer	.		base	Orlando	Bloom	und	Lucas	Miranda	lieben	einander	noch	immer	.		
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	84	
Sample	English-German	transla/ons	
• Translate	a	doubly-negated	phrase	correctly	• Fail	to	translate	“passenger	experience”.	source	We	ʹ	re	pleased	the	FAA	recognizes	that	an	enjoyable	passenger	experience	is	not	incompa>ble	with	safety	and	security	,	said	Roger	Dow	,	CEO	of	the	U.S.	Travel	AssociaHon	.		human	Wir	freuen	uns	,	dass	die	FAA	erkennt	,	dass	ein	angenehmes	Passagiererlebnis	nicht	im	Wider-	spruch	zur	Sicherheit	steht	,	sagte	Roger	Dow	,	CEO	der	U.S.	Travel	AssociaHon	.		best	Wir	freuen	uns	,	dass	die	FAA	anerkennt	,	dass	ein	angenehmes	ist	nicht	mit	Sicherheit	und	Sicherheit	unvereinbar	ist	,	sagte	Roger	Dow	,	CEO	der	US	-	die	.		base	Wir	freuen	uns	u	̈ber	die	<unk>	,	dass	ein	<unk>	<unk>	mit	Sicherheit	nicht	vereinbar	ist	mit	Sicherheit	und	Sicherheit	,	sagte	Roger	Cameron	,	CEO	der	US	-	<unk>	.		5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	85	
source	We	ʹ	re	pleased	the	FAA	recognizes	that	an	enjoyable	passenger	experience	is	not	incompa>ble	with	safety	and	security	,	said	Roger	Dow	,	CEO	of	the	U.S.	Travel	AssociaHon	.		human	Wir	freuen	uns	,	dass	die	FAA	erkennt	,	dass	ein	angenehmes	Passagiererlebnis	nicht	im	Wider-	spruch	zur	Sicherheit	steht	,	sagte	Roger	Dow	,	CEO	der	U.S.	Travel	AssociaHon	.		best	Wir	freuen	uns	,	dass	die	FAA	anerkennt	,	dass	ein	angenehmes	ist	nicht	mit	Sicherheit	und	Sicherheit	unvereinbar	ist	,	sagte	Roger	Dow	,	CEO	der	US	-	die	.		base	Wir	freuen	uns	u	̈ber	die	<unk>	,	dass	ein	<unk>	<unk>	mit	Sicherheit	nicht	vereinbar	ist	mit	Sicherheit	und	Sicherheit	,	sagte	Roger	Cameron	,	CEO	der	US	-	<unk>	.		Sample	English-German	transla/ons	
• Translate	a	doubly-negated	phrase	correctly	• Fail	to	translate	“passenger	experience”.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	86	
TED	talk,	English-German	
30.85	26.18	26.02	24.96	22.51	20.08	0	5	10	15	20	25	30	35	BLEU	
16.16	21.84	22.67	23.42	28.18	
0	5	10	15	20	25	30	
Stanford	Edinburgh	Karlsruhe	Heidelberg	PJAIT	HUMAN	TER	
26%	
Thang	Luong	and	Chris	Manning.	Stanford	Neural	Machine	Transla>on	Systems	for	Spoken	Language	Domain.	IWSLT	2015.	
Winning	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	87	

Advancing	NMT	• #1:	the	vocabulary	size	problem	– Sol:	“copy”	mechanism.	• #2:	the	sentence	length	problem	– Sol:	avenHon	mechanism.	• #3:	the	language	complexity	problem	– Sol:	character-level	translaHon.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	88	The			<unk>			porHco			in			<unk>		Le			<unk>			<unk>			de				<unk>	ecotax	Pont-de-Buis	por8que	écotaxe	Pont-de-Buis	
am a student_Je suis étudiantJesuis étudiant_
I

#3	The	rare	word	problem	• “Copying”	mechanisms	are	not	suﬃcient.	– Diﬀerent	alphabets:	Christopher	↦	Kryštof	– MulH-word	alignment:	Solar	system	↦	Sonnensystem	• Need	to	handle	large,	open	vocabulary	– Rich	morphology:	nejneobhospodařovávatelnějšímu											(“to	the	worst	farmable	one”)	– Informal	spelling:	goooooood	morning	!!!!!	Be	able	to	generate	at	the	character	level.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	89	
Recent	character-level	NMT	• UnsaHsfactory	performance	– (Wang	Ling,	Isabel	Trancoso,	Chris	Dyer,	Alan	Black,	arXiv	2015)	• Incomplete	soluHon	– Decoder	only	(Junyoung	Chung,	Kyunghyun	Cho,	Yoshua	Bengio.	arXiv	2016).	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	90	
• A	best-of-both-worlds	architecture:	– Translate	mostly	at	the	word	level	– Only	go	the	character	level	when	needed.	• AddiHonal	+2.1	↦	+11.4	BLEU	improvement.	Thang	Luong	and	Chris	Manning.	Achieving	Open	Vocabulary	Neural	Machine	Transla>on	with	Hybrid	Word-Character	Models.	In	submission,	ACL	2016.	SOTA	for	English-Czech	translaHon.	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	91	

Hybrid	NMT	Word-level	(4	layers)	End-to-end	training	8-stacking	LSTM	layers.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	92	
Source		Representa/ons	
• On-the-ﬂy	embeddings.	– Zero-init,	batch	computaHon.		5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	93	
Target	Genera/on	Init	with	word	hidden	states.	
Purposely,	use	<unk>	to	make	decoding	easier.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	94	
English-Czech	WMT’15	Results	
Systems	BLEU	Winning	entry	(Bojar	&	Tamchyna,	2015)	18.8	Exis8ng	word-level	NMT	(Jean	et	al.,	2015)	Single	model	15.7	Ensemble	4	models	18.3	
Large	vocab	+	unk	replace	
30x	data	3	systems	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	95	
English-Czech	WMT’15	Results	
Systems	BLEU	Winning	entry	(Bojar	&	Tamchyna,	2015)	18.8	Exis8ng	word-level	NMT	(Jean	et	al.,	2015)	Single	model	15.7	Ensemble	4	models	18.3	Our	character-based	NMT	Single	model	(600-step	backprop)	15.9	
Large	vocab	+	unk	replace	
30x	data	3	systems	• Purely	character-based:	slow	but	promising!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	96	
English-Czech	WMT’15	Results	
Systems	BLEU	Winning	entry	(Bojar	&	Tamchyna,	2015)	18.8	Exis8ng	word-level	NMT	(Jean	et	al.,	2015)	Single	model	15.7	Ensemble	4	models	18.3	Our	character-based	NMT	Single	model	(600-step	backprop)	15.9	Our	hybrid	NMT	Single	model	19.6	
Large	vocab	+	unk	replace	
30x	data	3	systems	
	New	SOTA!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	97	
English-Czech	WMT’15	Results	
Systems	BLEU	Winning	entry	(Bojar	&	Tamchyna,	2015)	18.8	Exis8ng	word-level	NMT	(Jean	et	al.,	2015)	Single	model	15.7	Ensemble	4	models	18.3	Our	character-based	NMT	Single	model	(600-step	backprop)	15.9	Our	hybrid	NMT	Single	model	19.6	Ensemble	4	models	20.7	
Large	vocab	+	unk	replace	
30x	data	3	systems	
Bemer	SOTA!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	98	
Eﬀects	of	Vocabulary	Sizes	
0	2	4	6	8	10	12	14	16	18	20	
1K	10K	20K	50K	BLEU	Vocabulary	Size	Word	Word	+	unk	replace	Hybrid	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	99	
Eﬀects	of	Vocabulary	Sizes	
0	2	4	6	8	10	12	14	16	18	20	
1K	10K	20K	50K	BLEU	Vocabulary	Size	Word	Word	+	unk	replace	Hybrid	
AddiHonal	gains	of	+2.1	↦	+11.4	BLEU	
+11.4	
+4.5	
+3.5	
+2.1	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	100	
Eﬀects	of	Vocabulary	Sizes	
0	2	4	6	8	10	12	14	16	18	20	
1K	10K	20K	50K	BLEU	Vocabulary	Size	Word	Word	+	unk	replace	Hybrid	
Small-vocab	hybrid	=	Large-vocab	word	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	101	
Rare	Word	Embeddings	
00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91acceptableacknowledgementadmissionadmitadmittanceadmittingadvance
antagonistchoosechoosesconnectdecidedevelopdevelopmentsevidentlyexplicitfoundergovernanceimmobileimmoveableimpossibleinsensitiveinsufficiency
linkmanagementnecessary
nominatednoticeableobviousperceptiblepossible
practicesatisfactory
sponsorunacceptableunaffecteduncomfortableunsatisfactoryunsuitableantagonizecofounderscompanionshipsdisrespectfulheartlesslyheartlessness
illiberalimpossibilitiesinabilitieslovelessnarrowïmindednarrowïmindednessnonconscious
regretfulspiritlessunattainableness
unconcernuncontroversial
unfeatheredunfledgedungracefulunrealizable
unsighteduntrustworthywholeheartedness
• Word	&	character-based	embeddings.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	102	
Rare	Word	Embeddings	
00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91acceptableacknowledgementadmissionadmitadmittanceadmittingadvance
antagonistchoosechoosesconnectdecidedevelopdevelopmentsevidentlyexplicitfoundergovernanceimmobileimmoveableimpossibleinsensitiveinsufficiency
linkmanagementnecessary
nominatednoticeableobviousperceptiblepossible
practicesatisfactory
sponsorunacceptableunaffecteduncomfortableunsatisfactoryunsuitableantagonizecofounderscompanionshipsdisrespectfulheartlesslyheartlessness
illiberalimpossibilitiesinabilitieslovelessnarrowïmindednarrowïmindednessnonconscious
regretfulspiritlessunattainableness
unconcernuncontroversial
unfeatheredunfledgedungracefulunrealizable
unsighteduntrustworthywholeheartedness
• Word	&	character-based	embeddings.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	103	
Rare	Word	Embeddings	
00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91acceptableacknowledgementadmissionadmitadmittanceadmittingadvance
antagonistchoosechoosesconnectdecidedevelopdevelopmentsevidentlyexplicitfoundergovernanceimmobileimmoveableimpossibleinsensitiveinsufficiency
linkmanagementnecessary
nominatednoticeableobviousperceptiblepossible
practicesatisfactory
sponsorunacceptableunaffecteduncomfortableunsatisfactoryunsuitableantagonizecofounderscompanionshipsdisrespectfulheartlesslyheartlessness
illiberalimpossibilitiesinabilitieslovelessnarrowïmindednarrowïmindednessnonconscious
regretfulspiritlessunattainableness
unconcernuncontroversial
unfeatheredunfledgedungracefulunrealizable
unsighteduntrustworthywholeheartedness
• Word	&	character-based	embeddings.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	104	
Sample	English-Czech	transla/ons	source	The	author	Stephen	Jay	Gould	died	20	years	aler	diagnosis	.	human	Autor	Stephen	Jay	Gould	zemřel	20	let	po	diagnóze	.	char	Autor	Stepher	Stepher	zemřel	20	let	po	diagnóze	.	word	Autor	Stephen	Jay	<unk>	zemřel	20	let	po	<unk>	.	Autor	Stephen	Jay	Gould	zemřel	20	let	po	po	.	hybrid	Autor	Stephen	Jay	<unk>	zemřel	20	let	po	<unk>	.	Autor	Stephen	Jay	Gould	zemřel	20	let	po	diagnóze	.	
Perfect	translaHon!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	105	
Sample	English-Czech	transla/ons	
• Char-based:	wrong	name	translaHon.	• Char-based:	wrong	name	translaHon.	source	The	author	Stephen	Jay	Gould	died	20	years	aler	diagnosis	.	human	Autor	Stephen	Jay	Gould	zemřel	20	let	po	diagnóze	.	char	Autor	Stepher	Stepher	zemřel	20	let	po	diagnóze	.	word	Autor	Stephen	Jay	<unk>	zemřel	20	let	po	<unk>	.	Autor	Stephen	Jay	Gould	zemřel	20	let	po	po	.	hybrid	Autor	Stephen	Jay	<unk>	zemřel	20	let	po	<unk>	.	Autor	Stephen	Jay	Gould	zemřel	20	let	po	diagnóze	.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	106	
Sample	English-Czech	transla/ons	
• Word-based:	incorrect	alignment	• Char-based:	wrong	name	translaHon.	source	The	author	Stephen	Jay	Gould	died	20	years	aler	diagnosis	.	human	Autor	Stephen	Jay	Gould	zemřel	20	let	po	diagnóze	.	char	Autor	Stepher	Stepher	zemřel	20	let	po	diagnóze	.	word	Autor	Stephen	Jay	<unk>	zemřel	20	let	po	<unk>	.	Autor	Stephen	Jay	Gould	zemřel	20	let	po	po	.	hybrid	Autor	Stephen	Jay	<unk>	zemřel	20	let	po	<unk>	.	Autor	Stephen	Jay	Gould	zemřel	20	let	po	diagnóze	.	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	107	
Sample	English-Czech	transla/ons	
• Char-based	&	hybrid:	correct	translaHon	of	diagnóze.	• Char-based:	wrong	name	translaHon.	source	The	author	Stephen	Jay	Gould	died	20	years	aler	diagnosis	.	human	Autor	Stephen	Jay	Gould	zemřel	20	let	po	diagnóze	.	char	Autor	Stepher	Stepher	zemřel	20	let	po	diagnóze	.	word	Autor	Stephen	Jay	<unk>	zemřel	20	let	po	<unk>	.	Autor	Stephen	Jay	Gould	zemřel	20	let	po	po	.	hybrid	Autor	Stephen	Jay	<unk>	zemřel	20	let	po	<unk>	.	Autor	Stephen	Jay	Gould	zemřel	20	let	po	diagnóze	.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	108	
Sample	English-Czech	transla/ons	source	As	the	Reverend	Mar>n	Luther	King	Jr.	said	ﬁTy	years	ago	:	human	Jak	před	padesá/	lety	řekl	reverend	Mar/n	Luther	King	Jr	.	:	char	Jako	reverend	Mar/n	Luther	král	říkal	před	padesá>	lety	:	word	Jak	řekl	reverend	MarHn	<unk>	King	<unk>	před	padesáH	lety	:	Jak	řekl	reverend	Mar/n	Luther	King	řekl	před	padesá>	lety	:	hybrid	Jak	řekl	reverend	MarHn	<unk>	King	<unk>	před	padesáH	lety	:	Jak	před	padesá/	lety	řekl	reverend	Mar/n	Luther	King	Jr.	:	
Correct	reordering!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	109	
source	As	the	Reverend	Mar>n	Luther	King	Jr.	said	ﬁTy	years	ago	:	human	Jak	před	padesá/	lety	řekl	reverend	Mar/n	Luther	King	Jr	.	:	char	Jako	reverend	Mar/n	Luther	král	říkal	před	padesá>	lety	:	word	Jak	řekl	reverend	MarHn	<unk>	King	<unk>	před	padesáH	lety	:	Jak	řekl	reverend	Mar/n	Luther	King	řekl	před	padesá>	lety	:	hybrid	Jak	řekl	reverend	MarHn	<unk>	King	<unk>	před	padesáH	lety	:	Jak	před	padesá/	lety	řekl	reverend	Mar/n	Luther	King	Jr.	:	Sample	English-Czech	transla/ons	
• Char-based:	“král”	means	“king”.	• Char-based:	wrong	name	translaHon.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	110	
Sample	English-Czech	transla/ons	source	Her	11-year-old	daughter	,	Shani	Bart	,	said	it	felt	a	livle	bit	weird	human	Její	jedenác/letá	dcera	Shani	Bartová	prozradila	,	že	je	to	trochu	zvláštní	char	Její	jedenác/letá	dcera	,	Shani	Bartová	,	říkala	,	že	cí	trochu	divně	word	Její	<unk>	dcera	<unk>	<unk>	řekla	,	že	je	to	trochu	divné	Její	11-year-old	dcera	Shani	,	řekla	,	že	je	to	trochu	divné	hybrid	Její	<unk>	dcera	,	<unk>	<unk>	,	řekla	,	že	je	to	<unk>	<unk>	Její	jedenác/letá	dcera	,	Graham	Bart	,	řekla	,	že	cí	trochu	divný	
Generate	complex	words!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	111	
Sample	English-Czech	transla/ons	source	Her	11-year-old	daughter	,	Shani	Bart	,	said	it	felt	a	livle	bit	weird	human	Její	jedenác/letá	dcera	Shani	Bartová	prozradila	,	že	je	to	trochu	zvláštní	char	Její	jedenác/letá	dcera	,	Shani	Bartová	,	říkala	,	že	cí	trochu	divně	word	Její	<unk>	dcera	<unk>	<unk>	řekla	,	že	je	to	trochu	divné	Její	11-year-old	dcera	Shani	,	řekla	,	že	je	to	trochu	divné	hybrid	Její	<unk>	dcera	,	<unk>	<unk>	,	řekla	,	že	je	to	<unk>	<unk>	Její	jedenác/letá	dcera	,	Graham	Bart	,	řekla	,	že	cí	trochu	divný	• Word-based:	idenHty	copy	fails.	• Char-based:	wrong	name	translaHon.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	112	
Sample	English-Czech	transla/ons	source	Her	11-year-old	daughter	,	Shani	Bart	,	said	it	felt	a	livle	bit	weird	human	Její	jedenác/letá	dcera	Shani	Bartová	prozradila	,	že	je	to	trochu	zvláštní	char	Její	jedenác/letá	dcera	,	Shani	Bartová	,	říkala	,	že	cí	trochu	divně	word	Její	<unk>	dcera	<unk>	<unk>	řekla	,	že	je	to	trochu	divné	Její	11-year-old	dcera	Shani	,	řekla	,	že	je	to	trochu	divné	hybrid	Její	<unk>	dcera	,	<unk>	<unk>	,	řekla	,	že	je	to	<unk>	<unk>	Její	jedenác/letá	dcera	,	Graham	Bart	,	řekla	,	že	cí	trochu	divný	• Hybrid:	translate	names	incorrectly.	• Char-based:	wrong	name	translaHon.	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	113	
We	have	advanced	NMT	• #1:	the	vocabulary	size	problem	– Sol:	“copy”	mechanism.	– SOTA	English-French	• #2:	the	sentence	length	problem	– Sol:	avenHon	mechanism.	– SOTA	English-German	• #3:	the	language	complexity	problem	– Sol:	character-level	translaHon.	– SOTA	English-Czech	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	114	The			<unk>			porHco			in			<unk>		Le			<unk>			<unk>			de				<unk>	ecotax	Pont-de-Buis	por8que	écotaxe	Pont-de-Buis	
am a student_Je suis étudiantJesuis étudiant_
I

NMT	&	beyond	• Unsupervised	learning	for	NMT	– UHlize	monolingual	data.	• Long-context	NMT	– TranslaHng	an	arHcle	/	a	book.	– Smarter	avenHon,	longer	sequences.	• MulH-modal	Language	Understanding	System	– MulH-lingual	translaHon	+	speech	recogniHon	+	more	– MulH-task	learning	Thank	you!	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	115	
I’m	done.		But	if	you	are	curious,	read	on!	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	116	
• Can	we	uHlize	all	sequence-to-sequence	data?						• Can	we	compress	NMT	for	mobile	devices?	#4	For	the	future	of	NMT	
Machine	translaHon	ConsHtuent	parsing	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	117	
Our	work	• MulH-task	learning:	– Machine	translaHon			–	ConsHtuent	parsing	– Image	capHon	generaHon		–	Unsupervised	learning	• TranslaHon	improvement:	up	to	+1.5	BLEU.	• State-of-the-art	in	consHtuent	parsing.	
Thang	Luong,	Quoc	Le,	Ilya	Sutskever,	Oriol	Vinyals,	and	Lukasz	Kaiser.		Mul>-task	sequence	to	sequence	learning.	ICLR	2016.	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	118	
Many-to-one:		shared	decoder	English (unsupervised)Image (captioning)EnglishGerman (translation)
16	16.5	17	17.5	18	18.5	19	
German-English	Transla/on	(BLEU)	Big	(transla>on)	+	Medium	(cap>on)	(Luong	et	al.,	2015)	Single	+	capHon	(0.1x)	+	capHon	(0.05x)	+	capHon	(0.01x)	
+0.7	BLEU	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	119	
13	13.5	14	14.5	15	15.5	16	
English-German	Transla/on	(BLEU)	Big	(transla>on)	+	Small	(PTB	parsing)	(Luong	et	al.,	2015)	Single	+	parsing	(1x)	+	parsing	(0.1x)	+	parsing	(0.01x)	
+1.5	BLEU	
English (unsupervised)German (translation)Tags (parsing)EnglishOne-to-many:		shared	encoder	
Mixing	raHo	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	120	
Our	work	
Abigail	See*,	Thang	Luong*,	and	Chris	Manning.	Compression	of	Neural	Machine	Transla>on	Models	via	Pruning.	In	submission.	
• Compress	NMT	via	pruning	&	retraining:	
01020304050607080900510152025
percentage prunedBLEU scoreprunedpruned and retrainedFigure 1: Performance of pruned models, immediately after pruning and afterretraining.
1Original	model	
Prune	smallest	weights	
Prune	+	retrain	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	121	
Our	work	
• Compress	NMT	via	pruning	&	retraining:	
01020304050607080900510152025
percentage prunedBLEU scoreprunedpruned and retrainedFigure 1: Performance of pruned models, immediately after pruning and afterretraining.
1Original	model	
Prune	smallest	weights	
Prune	+	retrain	
Prune	80%	without	loss	of	performance.	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	122	
NMT	Redundancy	–	Embeddings	• Frequent	words	have	larger	weights	– white:	large.	– black:	small.	target embedding weightssource embedding weightsleast common wordmost common wordsource layer 1 weights
recurrentfeed-forwardinput gateforget gateoutput gateinput
source layer 2 weightssource layer 3 weightssource layer 4 weights
target layer 1 weightstarget layer 2 weightstarget layer 3 weightstarget layer 4 weights
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	123	
target embedding weightssource embedding weightsleast common wordmost common wordsource layer 1 weights
recurrentfeed-forwardinput gateforget gateoutput gateinput
source layer 2 weightssource layer 3 weightssource layer 4 weights
target layer 1 weightstarget layer 2 weightstarget layer 3 weightstarget layer 4 weights
NMT	Redundancy	–	LSTM	
Layer	1	Layer	2	Layer	3	Layer	4	
Encoder	
Decoder	
Input	gate	Forget	gate	Output	gate	Input	signal	
Feed-forward	Recurrent	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	124	
target embedding weightssource embedding weightsleast common wordmost common wordsource layer 1 weights
recurrentfeed-forwardinput gateforget gateoutput gateinput
source layer 2 weightssource layer 3 weightssource layer 4 weights
target layer 1 weightstarget layer 2 weightstarget layer 3 weightstarget layer 4 weights
NMT	Redundancy	–	LSTM	
Layer	1	Layer	2	Layer	3	Layer	4	
Encoder	
Decoder	
Input	gate	Forget	gate	Output	gate	Input	signal	
Feed-forward	Recurrent	Input	signal	is	important!	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	125	
target embedding weightssource embedding weightsleast common wordmost common wordsource layer 1 weights
recurrentfeed-forwardinput gateforget gateoutput gateinput
source layer 2 weightssource layer 3 weightssource layer 4 weights
target layer 1 weightstarget layer 2 weightstarget layer 3 weightstarget layer 4 weights
NMT	Redundancy	–	LSTM	
Layer	1	Layer	2	Layer	3	Layer	4	
Encoder	
Decoder	
Input	gate	Forget	gate	Output	gate	Input	signal	
Feed-forward	Recurrent	Forget	gate:	small	–	layer	1	large	–	layer	4	5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	126	
Future	Challenges	She	saw	an	elephant	in	her	dress.	The	elephant	must	have	a	good	sense	of	fashion!	
Needs	to	understand		common	sense	&	larger	context.	She	saw	an	elephant	in	her	dress.	
5/19/16	Thang	Luong	-	Neural	Machine	TranslaHon	127	Thank	you!	
Machine TranslationCIS 526Instructor: Chris Callison-BurchTAs: Mitchell Stern, Justin Chiu
Course web sitemt-class.org/penn
Adam LopezEdinburgh
Matt PostJHUCourse materials developed with
Chris DyerCMU
Textbook

الدن بن اسامہ میں کہ ہے کہنا کا چینی ڈک صدر امریکی نائب ہوں۔ چاہتا دیکھنا مردہ یا زندہ کوAmerican Vice President Dick Cheney has said that he wants to see Osama bin Laden dead or alive.
The Tower of BabelPieter Brueghel the Elder (1563)
ENIAC (1946)
When I look at an article in Russian, I say: “This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.”Warren Weaver (1949)
Star TrekHitchhiker’s Guide to the Galaxy
Because we want to provide everyone with access to all the world's information, including information written in every language, one of the exciting projects at Google Research is machine translation... Now you can see the results for yourself. We recently launched an online version of our system for Arabic-English and English-Arabic. Try it out!
Statistical Machine Translation Live4/28/2006Franz Och



Statistical Machine TranslationDevelop a statistical  model of translation that can be learned from data and used to predict the correct English translation of new Chinese sentences.
linguistics
algorithms
machine learning
formal 
language 
information
theoryStatistical Machine Translation
In-class exercise
Synchronous	
  Context	
  Free	
  Grammar
16UrduEnglishS →NP①  VP②NP①  VP②VP→PP① VP② VP② PP① VP→V① AUX② AUX② V① PP →NP①  P②P②   NP①NP →hamd ansaryHamid AnsariNP →na}b sdrVice PresidentV →namzdnominatedP →kylyeforAUX →taawas
NP❶
Hamid AnsariNP❶NP❷
Vice PresidentNP❷forP❸P❸
nominatedV❹V❹hamd ansaryna}b sdrkylyenamzdtaa
wasAUX❺AUX❺
NP❶
Hamid AnsariNP❶NP❷
Vice PresidentNP❷forP❸P❸
nominatedV❹V❹hamd ansaryna}b sdrkylyenamzdtaa
wasAUX❺AUX❺PP❻
PP❻
NP❶
Hamid AnsariNP❶NP❷
Vice PresidentNP❷forP❸P❸
nominatedV❹V❹hamd ansaryna}b sdrkylyenamzdtaa
wasAUX❺AUX❺PP❻
PP❻VP❼
VP❼
NP❶
Hamid AnsariNP❶NP❷P❸V❹hamd ansaryna}b sdrkylyenamzdtaaAUX❺PP❻
Vice PresidentNP❷forP❸PP❻VP❼
nominatedV❹wasAUX❺VP❼VP❽
VP❽
NP❶
Hamid AnsariNP❶NP❷P❸V❹hamd ansaryna}b sdrkylyenamzdtaaAUX❺PP❻
Vice PresidentNP❷forP❸PP❻VP❼
nominatedV❹wasAUX❺VP❼VP❽
VP❽S❾S❾
Transla7on	
  improvements
22'first nuclear experiment in 1990 was' Thomas red Unilever National Laboratory of the United States  in ﻭوﻳﯾﭘﻥن designer, are already working on the book of Los ﺍاﻳﯾﻠﻣﻭوﺱس National Laboratory ﮈڈﻳﯾﻧﯽ ,former director of the technical ﺍاﻧﮢﭨﻳﯾﻠﺟﻧﺱس written with the cooperation of ﺳﮢﭨﻠﻣﻳﯾﻥن. This book 'nuclear express: political history and the expansion of bomb' has been written, and the two writers have also claimed that the country has made nuclear bomb is he or any other country's nuclear secrets to  ﮯٴٔﻳﯾﺭرﺍاﭼ or that of any other nuclear  power cooperation is achieved. The First Nuclear Test Was in 1990. Thomas red of the United States, the National Laboratory in designer are already working on the book of Los Alamos National Laboratory, former director of the technical intelligence, with the cooperation of Diana steelman wrote. This book under the title of the spread of nuclear expressway: the political history of the bomb and this has been written and the two writers have claimed that the country also has made nuclear bomb or any other country, Korea nuclear secrets, or any of the other nuclear power cooperation. First nuclear test conducted in 1990 Thomas Reed, who has worked as a weapons designer at Livermore National Laboratory in the United States, has written a book in collaboration with Danny Stillman, former director of the technical intelligence division at Los Alamos National Laboratory. In their book, 'The Nuclear Express: A Political History of the Bomb and its Proliferation,' Reed and Stillman claim that every country that has ever produced a nuclear bomb has been able to do so because it stole the nuclear secrets of another country or enjoyed the cooperation of another nuclear power. ’ﺗﻬﮭﺎ ﮐﻳﯾﺎ ﻣﻳﯾﮞں 1990 ﺗﺟﺭرﺑﮧہ ﺟﻭوﮨﺭرﯼی ﭘﮩﻼ’ ﻧﻳﯾﺷﻧﻝل ﻟﻳﯾﻭوﺭر ﮐﯽ ﺍاﻣﺭرﻳﯾﮑﮧہ ﺟﻭو ﻧﮯ  ﺭرﻳﯾﮈڈ ﺗﻬﮭﺎﻣﺱس ﮐﺎﻡم ﭘﺭر ﻁطﻭوﺭر ﮐﮯ ﮈڈﻳﯾﺯزﺍاﺋﻧﺭر ﻭوﻳﯾﭘﻥن ﻣﻳﯾﮞں ﻟﻳﯾﺑﺎﺭرﮢﭨﺭرﯼی ﻧﻳﯾﺷﻧﻝل ﺍاﻳﯾﻠﻣﻭوﺱس ﻻﺱس ﮐﺗﺎﺏب ﻳﯾﮧہ ﮨﻳﯾﮞں ﮐﺭرﭼﮑﮯ ﺳﺎﺑﻕق ﮐﮯ ﺍاﻧﮢﭨﻳﯾﻠﺟﻧﺱس ﮢﭨﻳﯾﮑﻧﻳﯾﮑﻝل ﮐﮯ ﻟﻳﯾﺑﺎﺭرﮢﭨﺭرﯼی ﻟﮑﻬﮭﯽ ﺳﮯ ﺗﻌﺎﻭوﻥن ﮐﮯ ﺳﮢﭨﻠﻣﻳﯾﻥن ﮈڈﻳﯾﻧﯽ ﮈڈﺍاﺋﺭرﻳﯾﮑﮢﭨﺭر ﮨﮯ۔ ﺳﻳﯾﺎﺳﯽ ﮐﯽ ﺑﻡم: ﺍاﻳﯾﮑﺳﭘﺭرﻳﯾﺱس ﻧﻳﯾﻭوﮐﻠﻳﯾﺋﺭر ’ﮐﺗﺎﺏب ﻳﯾﮧہ ﺳﮯ ﻋﻧﻭوﺍاﻥن ﮐﮯ‘ ﭘﻬﮭﻳﯾﻼﺅؤ ﮐﮯ ﺍاﺱس ﺍاﻭوﺭر ﺗﺎﺭرﻳﯾﺦ ﻣﺻﻧﻔﻳﯾﻥن ﺩدﻭوﻧﻭوﮞں ﻣﻳﯾﮞں ﺍاﺱس ﺍاﻭوﺭر ﮨﮯ ﮔﺋﯽ ﻟﮑﻬﮭﯽ ﺍاﻳﯾﮢﭨﻡم ﺑﻬﮭﯽ ﻧﮯ ﻣﻠﮏ ﺟﺱس ﮐﮧہ ﮨﮯ ﮐﻳﯾﺎ ﺩدﻋﻭوﯼی ﻧﮯ ﻣﻠﮏ ﺩدﻭوﺳﺭرﮮے ﮐﺳﯽ ﺗﻭو ﻳﯾﺎ ﻧﮯ ﺍاﺱس ﮨﮯ ﺑﻧﺎﻳﯾﺎ ﺑﻡم ﺍاﺳﮯ ﭘﻬﮭﺭر ﻳﯾﺎ ﮨﻳﯾﮞں ﭼﺭرﺍاﺋﮯ ﺭرﺍاﺯز ﺟﻭوﮨﺭرﯼی ﮐﮯ ﺣﺎﺻﻝل ﺗﻌﺎﻭوﻥن ﮐﺎ ﻁطﺎﻗﺕت ﺟﻭوﮨﺭرﯼی ﺩدﻭوﺳﺭرﯼی ﮐﺳﯽ ﮨﮯ۔ ﺭرﮨﺎ
Who	
  did	
  what	
  to	
  whom?
23Thomas was red when this question why China has provided the nuclear technology to Pakistan, In response, He said as China and India was joint enemy of Pakistan.BaselineSyntactic final systemHe said that China, North Korea, Iran, Syria, Pakistan, through Egypt, Libya and Yemen is to provide nuclear technology.
Thomas red when was this question why China has provided to Pakistan nuclear technology, he said in response to China, Pakistan and India as a common enemy.He said that China would provide nuclear technology to North Korea, Iran, Syria, Pakistan, Egypt, Libya and Yemen.
Example	
  Research	
  ques7ons•How	
  can	
  we	
  learn	
  transla7on	
  rules	
  automa7cally	
  from	
  data?	
  •When	
  one	
  word	
  has	
  several	
  diﬀerent	
  possible	
  transla7ons,	
  which	
  one	
  should	
  we	
  choose?	
  •When	
  languages	
  have	
  diﬀerent	
  word	
  orders,	
  how	
  do	
  we	
  properly	
  re-­‐reorder	
  the	
  words	
  from	
  one	
  language	
  into	
  the	
  other	
  language?	
  •What	
  is	
  the	
  most	
  eﬀec7ve	
  way	
  of	
  searching	
  over	
  all	
  permuta7ons	
  and	
  combina7ons	
  of	
  words?24
Topics	
  in	
  CIS	
  526•Probability	
  and	
  Language	
  Models	
  •Word	
  Alignment	
  and	
  Transla7on	
  Models	
  •Searching	
  for	
  the	
  most	
  probable	
  transla7on	
  •Phrase-­‐based	
  models	
  and	
  discrimina7ve	
  training	
  •Evalua7ng	
  transla7on	
  quality	
  •Syntax-­‐based	
  models	
  of	
  transla7on	
  •Collec7ng	
  training	
  data	
  through	
  crowdsourcing	
  and	
  web	
  crawling25
Who	
  should	
  take	
  this	
  class?•Anyone	
  who	
  is	
  interested	
  in	
  machine	
  transla7on,	
  natural	
  language	
  processing,	
  or	
  AI	
  •I	
  don’t	
  presume	
  any	
  background	
  in	
  linguis7cs	
  or	
  sta7s7cs	
  •The	
  only	
  prerequisite	
  is	
  good	
  programming	
  skills	
  •All	
  the	
  code	
  for	
  the	
  class	
  is	
  wriTen	
  in	
  Python
26
Assignments•Programming	
  assignments	
  are	
  designed	
  to	
  teach	
  you	
  the	
  fundamental	
  algorithms	
  in	
  SMT	
  and	
  illustrate	
  the	
  research	
  challenges
27
the clients and the associates are enemies .los clientes y los asociados son enemigos .the company has three groups .la empresa tiene tres grupos .its groups are in Europe .sus grupos estan en Europa .the modern groups sell strong pharmaceuticals .los grupos modernos venden medicinas fuertes .the groups do not sell zanzanine .los grupos no venden zanzanina .the small groups are not modern .los grupos pequenos no son modernos .Garcia and associates .Garcia y asociados .Carlos Garcia has three associates .Carlos Garcia tiene tres asociados .his associates are not strong .sus asociados no son fuertes .Garcia has a company also .Garcia tambien tiene una empresa .its clients are angry .sus clientes estan enfadados .the associates are also angry .los asociados tambien estan enfadados .Word	
  aligner
the clients and the associates are enemies .los clientes y los asociados son enemigos .the company has three groups .la empresa tiene tres grupos .its groups are in Europe .sus grupos estan en Europa .the modern groups sell strong pharmaceuticals .los grupos modernos venden medicinas fuertes .the groups do not sell zanzanine .los grupos no venden zanzanina .the small groups are not modern .los grupos pequenos no son modernos .Garcia and associates .Garcia y asociados .Carlos Garcia has three associates .Carlos Garcia tiene tres asociados .his associates are not strong .sus asociados no son fuertes .Garcia has a company also .Garcia tambien tiene una empresa .its clients are angry .sus clientes estan enfadados .the associates are also angry .los asociados tambien estan enfadados .Word	
  aligner
Phrase	
  Extractor
30
澳洲是与北韩有邦交的少数国家之一AustraliaisoneofthefewcountriesthathavediplomaticrelationswithNorthKorea
与 北 韩 有 邦交 have diplomatic relations with North Korea邦交 diplomatic relations北 韩North Korea

Phrase-­‐based	
  Decoder
31heergehtjanichtnachhauseit, it, heisaregoesgoyesis, of coursenotdo notdoes notis notaftertoaccording toinhousehomechamberat homenotis notdoes notdo nothomeunder housereturn homedo notit ishe will beit goeshe goesisareis after alldoestofollowingnot afternot tonotis notare notis not a
Phrase-­‐based	
  Decoder
32ergehtjanichtnachhauseergehtjanichtnachhause
areitheergehtjanichtnachhause
areithegoesdoes notyesgotohomehomeergehtjanichtnachhause
areithegoesdoes notyesgotohomehome

Discrimina7ve	
  Re-­‐Ranking
336.0Lex12 cartoons insulting the prophet mohammad4.5LM3.0TM19.012 cartoons attack the prophet mohammad17.610.12.07.07.09.4twelve comics offensive to the prophet mohammadseveral drawings mocking the prophet mohammad8.05.515.423.245.026.0TM2
Assignments•All	
  assignments	
  have	
  the	
  following	
  proper7es:	
  –Clearly	
  deﬁned	
  baseline	
  systems	
  that	
  you	
  can	
  reimplement	
  –Open-­‐ended	
  research	
  problems	
  with	
  no	
  “correct”	
  solu7ons	
  (lots	
  of	
  room	
  for	
  crea7vity)	
  –Objec7ve	
  measures	
  of	
  how	
  accurate	
  a	
  solu7on	
  is
34

Language	
  in	
  10	
  minutes•In-­‐class	
  presenta7on	
  about	
  a	
  language	
  •What	
  proper7es	
  does	
  it	
  have?	
  •What	
  makes	
  it	
  diﬀerent	
  than	
  English?	
  •What	
  are	
  the	
  challenges	
  for	
  machine	
  transla7on?	
  •Jonny	
  will	
  give	
  an	
  example	
  presenta7on
36
Straw	
  Poll•Last	
  7me	
  I	
  ran	
  this	
  class,	
  the	
  grading	
  was:	
  –	
  4	
  homework	
  assignments	
  (10	
  points	
  each	
  =	
  40	
  points)	
  	
  –	
  Language	
  in	
  10	
  minutes	
  (10	
  points)	
  –	
  Quizzes	
  about	
  the	
  reading	
  (10	
  points	
  total)	
  –	
  Self-­‐designed	
  ﬁnal	
  project	
  (40	
  points)	
  •Poll	
  ques7on:	
  Who	
  would	
  prefer	
  more	
  homework	
  assignments	
  instead	
  of	
  a	
  ﬁnal	
  project?37
By	
  next	
  week,	
  please•Buy	
  the	
  textbook	
  (Kindle	
  version	
  is	
  $35)	
  •Sign	
  up	
  on	
  piazza.com/upenn/spring2014/cis526	
  	
  •Fill	
  out	
  the	
  piazza	
  poll	
  about	
  when	
  would	
  be	
  best	
  for	
  us	
  to	
  hold	
  oﬃce	
  hours.	
  	
  •Do	
  assignment	
  0,	
  the	
  setup	
  assignment.	
  	
  I’ll	
  post	
  a	
  link	
  to	
  the	
  assignment	
  on	
  piazza	
  tomorrow.
38
Ques7ons?Chris:	
  	
  ccb@cis.upenn.edu	
  Jus7n:	
  justc@seas.upenn.edu	
  Mitchell:	
  mitstern@seas.upenn.edu
Introduction to Machine Learning (CSCI-UA.0480-007) David Sontag New York University Slides adapted from Luke Zettlemoyer, Pedro Domingos, and Carlos Guestrin 
Logistics • Class webpage: – http://cs.nyu.edu/~dsontag/courses/ml16/ – Sign up for Piazza! • Office hours: TBD • Teaching assistant: Kevin Jiao <jjiao@stern.nyu.edu> • Graders: – Yijun Xiao <ryjxiao@nyu.edu> – Alexandre Sablayrolles <alexandre.sablayrolles@gmail.com> 
Evaluation • 6-7 homeworks (50%) – Both theory and programming – Collaboration policy: • First try to solve the problems on your own  • Then, can discuss with other classmates • Write-up solutions on your own • List names of anyone you talked to  • Midterm exam (25%) • Project (20%) • Course participation (5%) 
Projects • Be creative – think of new problems that you can tackle using machine learning – Scope: ~40 hours/person • Logistics: – 2-3 students per group – Begins mid-March. Project proposal due week after midterm exam – Will still be problem sets during this period! 
Prerequisites  REQUIRED: • Basic algorithms (CS 310) – Dynamic programming, algorithmic analysis – Can be taken concurrently STRONGLY RECOMMENDED: • Linear algebra (Math 140) – Matrices, vectors, systems of linear equations – Eigenvectors, matrix rank – Singular value decomposition • Multivariable calculus (Math 123) – Derivatives, integration, tangent planes – Optimization, Lagrange multipliers • Good programming skills: Python highly recommended 
Source Materials No textbook required. Readings will come from freely available online material. If you really want a book for an additional reference, these are OK options: •  C. Bishop, Pattern Recognition and Machine Learning, Springer, 2007 •  K. Murphy, Machine Learning: a Probabilistic Perspective, MIT Press, 2012 •  … may update this list throughout semester. I wouldn’t buy anything yet. 
What is Machine Learning ? (by examples) 
Classification from data to discrete classes 
Spam filtering  
data prediction Spam vs. Not Spam 
10 ©2009 Carlos Guestrin Face recognition 
Example training images for each orientation 

Weather prediction 

Regression predicting a numeric value 
Stock market 

Weather prediction revisited 
Temperature 72° F 
Ranking comparing items 
Web search 

Given image, find similar images 
http://www.tiltomo.com/ 

Collaborative Filtering 
Recommendation systems 

Recommendation systems 
Machine learning competition with a $1 million prize 

Clustering discovering structure in data 
Clustering Data: Group similar things 

Clustering images 
[Goldberger et al.] 
Set of Images 

Clustering web search results 

Embedding visualizing data 
Embedding images 
26 ©2009 Carlos Guestrin • Images have thousands or millions of pixels. • Can we give each image a coordinate, such that similar images are near each other? 
[Saul & Roweis ‘03] 
Embedding words 
[Joseph Turian] 
Embedding words (zoom in) 
[Joseph Turian] 
Structured prediction from data to discrete classes 
Speech recognition 

Natural language processing 
I need to hide a body noun, verb, preposition, … 

Growth of Machine Learning • Machine learning is preferred approach to – Speech recognition, Natural language processing – Computer vision – Medical outcomes analysis – Robot control – Computational biology – Sensor networks – … • This trend is accelerating – Big data – Improved machine learning algorithms  – Faster computers – Good open-source software 
Course roadmap  • First half of course: supervised learning – SVMs, kernel methods – Learning theory – Decision trees, boosting, deep learning • Second half of course: data science – Unsupervised learning, EM algorithm – Dimensionality reduction – Topic models 
Supervised Learning: find f • Given: Training set {(xi, yi)  | i = 1 … N} • Find: A good approximation to  f  : X ! Y Examples: what are X and Y ? • Spam Detection – Map email to {Spam, Not Spam}  • Digit recognition – Map pixels to {0,1,2,3,4,5,6,7,8,9}  • Stock Prediction – Map new, historic prices, etc. to     (the real numbers) ℜ
A Supervised Learning Problem • Our goal is to find a function f  : X ! Y – X = {0,1}4 – Y = {0,1} • Question 1: How should we pick the hypothesis space, the set of possible functions f ? • Question 2: How do we find the best f  in the hypothesis space? 
Dataset: 
Most General Hypothesis Space  Consider all possible boolean functions over four input features!  
Dataset: 
• 216 possible hypotheses • 29 are consistent with our dataset • How do we choose the best one? 
A Restricted Hypothesis Space  Consider all conjunctive boolean functions. • 16 possible hypotheses • None are consistent with our dataset • How do we choose the best one? 
Dataset: 
Occam’s Razor Principle • William of Occam: Monk living in the 14th century • Principle of parsimony: “One should not increase, beyond what is necessary, the number of entities required to explain anything” • When many solutions are available for a given problem, we should select the simplest one • But what do we mean by simple? • We will use prior knowledge of the problem to solve to define what is  a simple solution [Samy Bengio] Example of a prior: smoothness 
Key Issues in Machine Learning • How do we choose a hypothesis space? – Often we use prior knowledge to guide this choice • How can we gauge the accuracy of a hypothesis on unseen data? – Occam’s razor: use the simplest hypothesis consistent with data! This will help us avoid overfitting. – Learning theory will help us quantify our ability to generalize as a function of the amount of training data and the hypothesis space • How do we find the best hypothesis? – This is an algorithmic question, the main topic of computer science • How to model applications as machine learning problems? (engineering challenge) 
Computation Graphs
From Practical Neural Networks for NLP / Chris Dyer, Yoav Goldberg, Graham Neubig / EMNLP 2016CS5740: Natural Language Processing Spring 2018
Instructor: Yoav Artzi 
Computation Graphs•The descriptive language of deep learning models •Functional description of the required computation •Can be instantiated to do two types of computation: •Forward computation •Backward computation
y=x>Ax+b·x+c
A node is a {tensor, matrix, vector, scalar} valueexpression:
xgraph:
y=x>Ax+b·x+c
xexpression:graph:
An edge represents a function argument(and also data dependency). They are justpointers to nodes.A node with an incoming edge is a function of that edge’s tail node.
f(u)=u>A node knows how to compute its value and the value of its derivative w.r.t each argument (edge) times a derivative of an arbitrary input       .@F@f(u)
@f(u)@u@F@f(u)=✓@F@f(u)◆>
y=x>Ax+b·x+c
xf(u)=u>Af(U,V)=UVexpression:graph:Functions can be nullary, unary,binary, … n-ary. Often they are unary or binary.
y=x>Ax+b·x+c
xf(u)=u>Af(U,V)=UVf(M,v)=Mvexpression:graph:
Computation graphs are directed and acyclic (usually)
y=x>Ax+b·x+c
xf(u)=u>Af(U,V)=UVf(M,v)=MvxAf(x,A)=x>Ax
@f(x,A)@A=xx>@f(x,A)@x=(A>+A)xexpression:graph:
y=x>Ax+b·x+c
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcf(x1,x2,x3)=Xixiexpression:graph:
y=x>Ax+b·x+c
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcyf(x1,x2,x3)=Xixi
expression:graph:
variable names are just labelings of nodes.
Algorithms•Graph construction•Forward propagation•Loop over nodes in topological order •Compute the value of the node given its inputs •Given my inputs, make a prediction (or compute an “error” with respect to a “target output”) •Backward propagation•Loop over the nodes in reverse topological order starting with a ﬁnal goal node •Compute derivatives of ﬁnal goal node value with respect to each edge’s tail node •How does the output change if I make a small change to the inputs?
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcf(x1,x2,x3)=Xixigraph:Forward Propagation
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcf(x1,x2,x3)=Xixigraph:Forward Propagation
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcf(x1,x2,x3)=Xixigraph:Forward Propagation
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcf(x1,x2,x3)=Xixigraph:
x>Forward Propagation
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcf(x1,x2,x3)=Xixigraph:
x>x>AForward Propagation
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcf(x1,x2,x3)=Xixigraph:
x>x>Ab·xForward Propagation
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcf(x1,x2,x3)=Xixigraph:
x>x>Ab·xx>AxForward Propagation
xf(u)=u>Af(U,V)=UVf(M,v)=Mvbf(u,v)=u·vcf(x1,x2,x3)=Xixigraph:
x>x>Ab·xx>AxForward Propagationx>Ax+b·x+c
The MLPh= tanh(Wx+b)y=Vh+a

The MLPh= tanh(Wx+b)y=Vh+a
xf(M,v)=MvWbf(u,v)=u+vhf(u) = tanh(u)Vaf(M,v)=Mvf(u,v)=u+v
Constructing Graphs
Two Software Models•Static declaration•Phase 1: deﬁne an architecture(maybe with some primitive ﬂow control like loops and conditionals) •Phase 2: run a bunch of data through it to train the model and/or make predictions •Dynamic declaration•Graph is deﬁned implicitly (e.g., using operator overloading) as the forward computation is executed 
Hierarchical Structure
Phrases
WordsSentencesAlicegaveamessagetoBobPPNPVPVPS
DocumentsThis film was completely unbelievable.The characters were wooden and the plot was absurd.That being said, I liked it.

Static Declaration•Pros•Ofﬂine optimization/scheduling of graphs is powerful •Limits on operations mean better hardware support •Cons•Structured data (even simple stuff like sequences), even variable-sized data, is complex  •You effectively learn a new programming language (“the Graph Language”) and you write programs in that language to process data. •Examples: Torch, Theano, TensorFlow
Dynamic Declaration•Pros•Library is less invasive •The forward computation is written in your favorite programming language with all its features, using your favorite algorithms •Interleave construction and evaluation of the graph •Cons•Little time for graph optimization •If the graph is static, effort can be wasted •Examples: Chainer, most automatic differentiation libraries, DyNet
Dynamic Structure?•Hierarchical structures exist in language •We might want to let the network reﬂect that hierarchy •Hierarchical structure is easiest to process with traditional ﬂow-control mechanisms in your favorite languages •Combinatorial algorithms (e.g., dynamic programming) •Exploit independencies to compute over a large space of operations tractably
Language ModelsInstructor: Yoav ArtziCS5740: Natural Language ProcessingSpring 2018
Slides adapted from Dan Klein, Dan Jurafsky, Chris Manning, Michael Collins, Luke Zettlemoyer, and YejinChoi
Overview•The language modeling problem•N-gram language models•Evaluation: perplexity•Smoothing–Add-N–Linear interpolation
The Language Modeling Problem•Setup: Assume a (finite) vocabulary of words•We can construct an (infinite) set of strings•Data: given a training set of example sentences             •Problem: estimate a probability distribution•Question: why would we ever want to do this?V†={the,a,the a,the fan,the man,the man with the telescope,. . .}Xx V†p(x)=1andp(x) 0 for allx⇥V†p(the) = 10 12p(a) = 10 13p(the fan) = 10 12p(the fan saw Beckham) = 2⇥10 8p(the fan saw saw) = 10 15...
Speech Recognition•Automatic Speech Recognition (ASR)•Audio in, text out•SOTA: 0.3% error for digit strings, 5% dictation, 50%+ TV•“Wreck a nice beach?”–“Recognize speech”•“Eye eight uh Jerry?”–“I ate a cherry”

The Noisy Channel Model•Goal: predict sentence given acoustics•The noisy channel approach:Acoustic model: Distributions over acoustic waves given a sentence Language model: Distributions over sequences of words (sentences)w⇤= arg maxXP(X|a)w⇤= arg maxXP(X|a)= arg maxXP(a|X)P(X)/P(a)= arg maxXP(a|X)P(X)
Acoustically Scored Hypothesesthe station signs are in deep in english-14732the stations signs are in deep in english-14735the station signs are in deep into english-14739the station 's signs are in deep in english-14740the station signs are in deep in the english-14741the station signs are indeed in english-14757the station 's signs are indeed in english-14760the station signs are indiansin english-14790the station signs are indianin english-14799the stations signs are indiansin english-14807the stations signs are indiansand english-14815
ASR System Componentssource!(#)#%decoderobserved     #%bestchannel!(%|#)Language ModelAcoustic Model
arg maxXP(X|a) = arg maxXP(a|X)P(X)
Translation as Codebreaking“Also knowing nothing official about, but having guessed and inferred considerable about, the powerful new mechanized methods in cryptography—methods which I believe succeed even when one does not know what language has been coded—one naturally wonders if the problem of translation could conceivably be treated as a problem in cryptography. When I look at an article in Russian, I say: ‘This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.’” Warren Weaver (1955:18, quoting a letter he wrote in 1947)

MT System Componentssource!(#)#%decoderobserved     #%bestchannel!(%|#)Language ModelTranslation Model
arg maxeP(e|f) = arg maxeP(f|e)P(e)
Caption Generation System Componentssource!(#)#%decoderobserved     #%bestchannel!(%|#)Language ModelImage Model
arg maxeP(e|i) = arg maxeP(i|e)P(e)
Learning Language Models•Goal:Assign useful probabilities !(#)to sentences #–Input: many observations of training sentences #–Output: system capable of computing !(#)•Probabilities should broadly indicate plausibility of sentences–!(I saw a van)>>!(eyes awe of an)–Not only grammaticality: !(artichokes intimidate zippers)»0–In principle, “plausible” depends on the domain, context, speaker…•One option:empirical distribution over training sentences…•Problem:does not generalize (at all)•Need to assign non-zero probability to previously unseen sentences!p(x1...xn)=c(x1...xn)Nfor sentenceX=x1...xn
Real-­‐Valued	
  inputs	
  What	
  should	
  we	
  do	
  if	
  some	
  of	
  the	
  inputs	
  are	
  real-­‐valued?	
  Infinite number of possible split values!!! 
“One	
  branch	
  for	
  each	
  numeric	
  value”	
  idea:	
  
Hopeless: hypothesis with such a high branching factor will shatter any dataset and overfit 
Threshold	
  splits	
  • Binary	
  tree:	
  split	
  on	
  aIribute	
  X	
  at	
  value	
  t	
  – One	
  branch:	
  X	
  <	
  t	
  – Other	
  branch:	
  X	
  ≥	
  t	
  Year	
  
<78	
  ≥78	
  good bad • Requires small change • Allow repeated splits on same variable along a path Year	
  
<70	
  ≥70	
  good bad 
The	
  set	
  of	
  possible	
  thresholds	
  • Binary	
  tree,	
  split	
  on	
  aIribute	
  X	
  – One	
  branch:	
  X	
  <	
  t	
  – Other	
  branch:	
  X	
  ≥	
  t	
  • Search	
  through	
  possible	
  values	
  of	
  t	
  – Seems	
  hard!!!	
  • But	
  only	
  a	
  ﬁnite	
  number	
  of	
  t’s	
  are	
  important:	
  – Sort	
  data	
  according	
  to	
  X	
  into	
  {x1,…,xm}	
  – Consider	
  split	
  points	
  of	
  the	
  form	
  xi	
  +	
  (xi+1	
  –	
  xi)/2	
  – Morever,	
  only	
  splits	
  between	
  examples	
  of	
  diﬀerent	
  classes	
  maIer!	
  (Figures	
  from	
  Stuart	
  Russell)	
  Optimal splits for continuous attributesInﬁnitely many possible split pointscto deﬁne node testXj>c?No! Moving split point along the empty space between two observed valueshas no e ect on information gain or empirical loss; so just use midpointXjc1c2Moreover, only splitsbetween examples from di erent classescan be optimal for information gain or empirical loss reductionXjc2c1
CS194-10 Fall 2011 Lecture 8 26t1t2Optimal splits for continuous attributesInﬁnitely many possible split pointscto deﬁne node testXj>c?No! Moving split point along the empty space between two observed valueshas no e ect on information gain or empirical loss; so just use midpointXjc1c2Moreover, only splitsbetween examples from di erent classescan be optimal for information gain or empirical loss reductionXjc2c1
CS194-10 Fall 2011 Lecture 8 26t1t2
Picking	
  the	
  best	
  threshold	
  • Suppose	
  X	
  is	
  real	
  valued	
  with	
  threshold	
  t	
  • Want	
  IG(Y	
  |	
  X:t),	
  the	
  informaGon	
  gain	
  for	
  Y	
  when	
  tesGng	
  if	
  X	
  is	
  greater	
  than	
  or	
  less	
  than	
  t	
  • Deﬁne:	
  	
  • H(Y|X:t)	
  =	
  	
  p(X	
  <	
  t)	
  H(Y|X	
  <	
  t)	
  +	
  p(X	
  >=	
  t)	
  H(Y|X	
  >=	
  t)	
  • IG(Y|X:t)	
  =	
  H(Y)	
  -­‐	
  H(Y|X:t)	
  • IG*(Y|X)	
  =	
  maxt	
  IG(Y|X:t)	
  • Use:	
  IG*(Y|X)	
  for	
  conGnuous	
  variables	
  
What	
  you	
  need	
  to	
  know	
  about	
  decision	
  trees	
  • Decision	
  trees	
  are	
  one	
  of	
  the	
  most	
  popular	
  ML	
  tools	
  – Easy	
  to	
  understand,	
  implement,	
  and	
  use	
  – ComputaGonally	
  cheap	
  (to	
  solve	
  heurisGcally)	
  • InformaGon	
  gain	
  to	
  select	
  aIributes	
  (ID3,	
  C4.5,…)	
  • Presented	
  for	
  classiﬁcaGon,	
  can	
  be	
  used	
  for	
  regression	
  and	
  density	
  esGmaGon	
  too	
  • Decision	
  trees	
  will	
  overﬁt!!!	
  – Must	
  use	
  tricks	
  to	
  ﬁnd	
  “simple	
  trees”,	
  e.g.,	
  • Fixed	
  depth/Early	
  stopping	
  • Pruning	
  – Or,	
  use	
  ensembles	
  of	
  diﬀerent	
  trees	
  (random	
  forests)	
  
Ensemble	
  learning	
  
Slides adapted from Navneet Goyal, Tan, Steinbach, Kumar, Vibhav Gogate 
Ensemble	
  methods	
  
Machine learning competition with a $1 million prize 

Bias/Variance	
  Tradeoﬀ	
  
Hastie, Tibshirani, Friedman “Elements of Statistical Learning” 2001	

Reduce	
  Variance	
  Without	
  Increasing	
  Bias	
  • Averaging	
  reduces	
  variance:	
  Average models to reduce model variance One problem:  only one training set where do multiple models come from? (when predictions  are independent) 
Bagging:	
  Bootstrap	
  AggregaGon	
  • Leo	
  Breiman	
  (1994)	
  • Take	
  repeated	
  bootstrap	
  samples	
  from	
  training	
  set	
  D	
  • Bootstrap	
  sampling:	
  Given	
  set	
  D	
  containing	
  N	
  training	
  examples,	
  create	
  D’	
  by	
  drawing	
  N	
  examples	
  at	
  random	
  with	
  replacement	
  from	
  D.	
  • Bagging:	
  – Create	
  k	
  bootstrap	
  samples	
  D1	
  …	
  Dk.	
  – Train	
  disGnct	
  classiﬁer	
  on	
  each	
  Di.	
  – Classify	
  new	
  instance	
  by	
  majority	
  vote	
  /	
  average.	
  
General	
  Idea	
  
Example	
  of	
  Bagging	
  • Sampling	
  with	
  replacement	
  • Build	
  classiﬁer	
  on	
  each	
  bootstrap	
  sample	
  • Each	
  data	
  point	
  has	
  probability	
  (1	
  –	
  1/n)n	
  of	
  being	
  selected	
  as	
  test	
  data	
  • Training	
  data	
  =	
  1-­‐	
  (1	
  –	
  1/n)n	
  of	
  the	
  original	
  data	
  Training Data Data ID 
51 

52 
decision tree learning algorithm; very similar to ID3 
shades of blue/red indicate strength of vote for particular classification 
Random	
  Forests	
  • Ensemble	
  method	
  speciﬁcally	
  designed	
  for	
  decision	
  tree	
  classiﬁers	
  • Introduce	
  two	
  sources	
  of	
  randomness:	
  “Bagging”	
  and	
  “Random	
  input	
  vectors”	
  – Bagging	
  method:	
  each	
  tree	
  is	
  grown	
  using	
  a	
  bootstrap	
  sample	
  of	
  training	
  data	
  – Random	
  vector	
  method:	
  At	
  each	
  node,	
  best	
  split	
  is	
  chosen	
  from	
  a	
  random	
  sample	
  of	
  m	
  aIributes	
  instead	
  of	
  all	
  aIributes	
  
Random	
  Forests	
  

Random	
  Forests	
  Algorithm	
  

Introduction To Machine Learning
David Sontag
New York University
Lecture 21, April 14, 2016
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 1 / 14
Expectation maximization
Algorithm is as follows:
1Write down the complete log-likelihood logp(x;z;) in such a way
that it is linear in z
2Initialize0, e.g. at random or using a good rst guess
3Repeat until convergence:
t+1= arg max
MX
m=1Ep(zmjxm;t)[logp(xm;Z;)]
Notice that log p(xm;Z;) is a random function because Zis unknown
By linearity of expectation, objective decomposes into expectation
terms and data terms
\E" step corresponds to computing the objective (i.e., the
expectations )
\M" step corresponds to maximizing the objective
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 2 / 14
Derivation of EM algorithm
L(θ) l(θ|θn)
θnθn+1L(θn)=l(θn|θn)l(θn+1|θn)L(θn+1)
L(θ)
l(θ|θn)
θ
Figure 2: Graphical interpretation of a single iteration of the EM algorithm:
The function l(θ|θn)i sb o u n d e da b o v eb yt h el i k e l i h o o df u n c t i o n L(θ). The
functions are equal at θ=θn.T h e E M a l g o r i t h m c h o o s e s θn+1as the value of θ
for which l(θ|θn)i sam a x i m u m . S i n c e L(θ)≥l(θ|θn)i n c r e a s i n g l(θ|θn)e n s u r e s
that the value of the likelihood function L(θ)i si n c r e a s e da te a c hs t e p .
We have now a function, l(θ|θn)w h i c hi sb o u n d e da b o v eb yt h el i k e l i h o o d
function L(θ). Additionally, observe that,
l(θn|θn)= L(θn)+∆(θn|θn)
=L(θn)+/summationdisplay
zP(z|X,θn)l nP(X|z,θn)P(z|θn)
P(z|X,θn)P(X|θn)
=L(θn)+/summationdisplay
zP(z|X,θn)l nP(X,z|θn)
P(X,z|θn)
=L(θn)+/summationdisplay
zP(z|X,θn)l n1
=L(θn), (16)
so for θ=θnthe functions l(θ|θn)a n d L(θ)a r ee q u a l .
Our objective is to choose a values of θso that L(θ)i sm a x i m i z e d .W eh a v e
shown that the function l(θ|θn)i sb o u n d e da b o v eb yt h el i k e l i h o o df u n c t i o n L(θ)
and that the value of the functions l(θ|θn)a n d L(θ)a r ee q u a la tt h ec u r r e n t
estimate for θ=θn.T h e r e f o r e , a n y θwhich increases l(θ|θn)w i l la l s oi n c r e a s e
L(θ). In order to achieve the greatest possible increase in the v alue of L(θ), the
EM algorithm calls for selecting θsuch that l(θ|θn)i sm a x i m i z e d . W ed e n o t e
this updated value as θn+1.T h i sp r o c e s si si l l u s t r a t e di nF i g u r e( 2 ) .
7
(Figure from tutorial by Sean Borman)
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 3 / 14
Application to mixture models
i=1t oNd=1t oDwidPrior distributionover topicsT opic of doc dWordβT opic-worddistributionsθzd
This model is a type of (discrete) mixture model
Called multinomial naive Bayes (a word can appear multiple times)
Document is generated from a single topic
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 4 / 14
EM for mixture models
i=1t oNd=1t oDwidPrior distributionover topicsT opic of doc dWordβT opic-worddistributionsθzd
The complete likelihood is p(w;Z;;) =QD
d=1p(wd;Zd;;), where
p(wd;Zd;;) =ZdNY
i=1Zd;wid
Trick #1: re-write this as
p(wd;Zd;;) =KY
k=11[Zd=k]
kNY
i=1KY
k=11[Zd=k]
k;wid
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 5 / 14
EM for mixture models
Thus, the complete log-likelihood is:
logp(w;Z;;) =DX
d=1 KX
k=11[Zd=k] logk+NX
i=1KX
k=11[Zd=k] logk;wid!
In the \E" step, we take the expectation of the complete log-likelihood with
respect to p(zjw;t;t), applying linearity of expectation, i.e.
Ep(zjw;t;t)[logp(w;z;;)] =
DX
d=1 KX
k=1p(Zd=kjw;t;t) logk+NX
i=1KX
k=1p(Zd=kjw;t;t) logk;wid!
In the \M" step, we maximize this with respect to and
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 6 / 14
EM for mixture models
Just as with complete data, this maximization can be done in closed form
First, re-write expected complete log-likelihood from
DX
d=1 KX
k=1p(Zd=kjw;t;t) logk+NX
i=1KX
k=1p(Zd=kjw;t;t) logk;wid!
to
KX
k=1logkDX
d=1p(Zd=kjwd;t;t)+KX
k=1WX
w=1logk;wDX
d=1Ndwp(Zd=kjwd;t;t)
We then have that
t+1
k=PD
d=1p(Zd=kjwd;t;t)
PK
^k=1PD
d=1p(Zd=^kjwd;t;t)
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 7 / 14
Latent Dirichlet allocation (LDA)
Topic models are powerful tools for exploring large data sets and for
making inferences about the content of documents
!"#$%&'()
*"+,#)
+"/,9#)1+.&),3&'(1"65%51:5)2,'0("'1.&/,0,"'1-
.&/,0,"'12,'3$14$3,5)%1&(2,#)16$332,)%1
)+".()165)&65//1)"##&.165)7&(65//18""(65//1--
Many applications in information retrieval, document summarization,
and classication
Complexity+of+Inference+in+Latent+Dirichlet+Alloca6on+David+Sontag,+Daniel+Roy+(NYU,+Cambridge)+
W66+Topic+models+are+powerful+tools+for+exploring+large+data+sets+and+for+making+inferences+about+the+content+of+documents+Documents+
Topics+
poli6cs+.0100+president+.0095+obama+.0090+washington+.0085+religion+.0060+Almost+all+uses+of+topic+models+(e.g.,+for+unsupervised+learning,+informa6on+retrieval,+classiﬁca6on)+require+probabilis)c+inference:+New+document+
What+is+this+document+about?+Words+w1,+…,+wN+✓
Distribu6on+of+topics+ t= p(w|z=t) …+
religion+.0500+hindu+.0092+judiasm+.0080+ethics+.0075+buddhism+.0016+
sports+.0105+baseball+.0100+soccer+.0055+basketball+.0050+football+.0045+…+…+
weather+.50+ﬁnance+.49+sports+.01+
LDA is one of the simplest and most widely used topic models
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 8 / 14
Generative model for a document in LDA
1Sample the document's topic distribution (aka topic vector)
Dirichlet (1:T)
where theftgT
t=1are xed hyperparameters. Thus is a distribution
over Ttopics with mean t=t=P
t0t0
2Fori= 1 to N, sample the topic ziof the i'th word
zij
3... and then sample the actual word wifrom the zi'th topic
wijzizi
whereftgT
t=1are the topics (a xed collection of distributions on
words)
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 9 / 14
Generative model for a document in LDA
1Sample the document's topic distribution (aka topic vector)
Dirichlet (1:T)
where theftgT
t=1are hyperparameters. The Dirichlet density, dened over
 =f~2RT:8tt0;PT
t=1t= 1g, is:
p(1;:::; T)/TY
t=1t 1
t
For example, for T=3 (3= 1 1 2):
α1=α2=α3=
θ1θ2log Pr(θ)
θ1θ2log Pr(θ)α1=α2=α3=
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 10 / 14
Generative model for a document in LDA
3... and then sample the actual word wifrom the zi'th topic
wijzizi
whereftgT
t=1are the topics (a xed collection of distributions on
words)
Complexity+of+Inference+in+Latent+Dirichlet+Alloca6on+David+Sontag,+Daniel+Roy+(NYU,+Cambridge)+
W66+Topic+models+are+powerful+tools+for+exploring+large+data+sets+and+for+making+inferences+about+the+content+of+documents+Documents+
Topics+
poli6cs+.0100+president+.0095+obama+.0090+washington+.0085+religion+.0060+Almost+all+uses+of+topic+models+(e.g.,+for+unsupervised+learning,+informa6on+retrieval,+classiﬁca6on)+require+probabilis)c+inference:+New+document+
What+is+this+document+about?+Words+w1,+…,+wN+✓
Distribu6on+of+topics+ t= p(w|z=t) …+
religion+.0500+hindu+.0092+judiasm+.0080+ethics+.0075+buddhism+.0016+
sports+.0105+baseball+.0100+soccer+.0055+basketball+.0050+football+.0045+…+…+
weather+.50+ﬁnance+.49+sports+.01+
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 11 / 14
Example of using LDA
gene     0.04dna      0.02genetic  0.01.,,life     0.02evolve   0.01organism 0.01.,,brain    0.04neuron   0.02nerve    0.01...data     0.02number   0.02computer 0.01.,,TopicsDocumentsTopic proportions andassignments
Figure 1:The intuitions behind latent Dirichlet allocation.We assume that somenumber of “topics,” which are distributions over words, exist for the whole collection (far left).Each document is assumed to be generated as follows. First choose a distribution over thetopics (the histogram at right); then, for each word, choose a topic assignment (the coloredcoins) and choose the word from the corresponding topic. The topics and topic assignmentsin this ﬁgure are illustrative—they are not ﬁt from real data. See Figure 2 for topics ﬁt fromdata.model assumes the documents arose. (The interpretation of LDA as a probabilistic model isﬂeshed out below in Section 2.1.)We formally deﬁne atopicto be a distribution over a ﬁxed vocabulary. For example thegeneticstopic has words about genetics with high probability and theevolutionary biologytopic has words about evolutionary biology with high probability. We assume that thesetopics are speciﬁed before any data has been generated.1Now for each document in thecollection, we generate the words in a two-stage process.1. Randomly choose a distribution over topics.2. For each word in the document(a) Randomly choose a topic from the distribution over topics in step #1.(b)Randomly choose a word from the corresponding distribution over the vocabulary.This statistical model reﬂects the intuition that documents exhibit multiple topics. Eachdocument exhibits the topics with diﬀerent proportion (step #1); each word in each document1Technically, the model assumes that the topics are generated ﬁrst, before the documents.3θdz1dzNdβ1
βT
(Blei, Introduction to Probabilistic Topic Models , 2011)
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 12 / 14
\Plate" notation for LDA model
αDirichlet hyperparameters
i=1t oNd=1t oDθdwidzidT opic distributionfor documentT opic of word i of doc dWordβT opic-worddistributions
Variables within a plate are replicated in a conditionally independent manner
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 13 / 14
Comparison of mixture and admixture models
i=1t oNd=1t oDwidPrior distributionover topicsT opic of doc dWordβT opic-worddistributionsθzd
αDirichlet hyperparameters
i=1t oNd=1t oDθdwidzidT opic distributionfor documentT opic of word i of doc dWordβT opic-worddistributions
Model on left is a mixture model
Called multinomial naive Bayes (a word can appear multiple times)
Document is generated from a single topic
Model on right (LDA) is an admixture model
Document is generated from a distribution over topics
David Sontag (NYU) Introduction To Machine Learning Lecture 21, April 14, 2016 14 / 14
1Introduction to Information Retrieval Introduction to Information Retrieval 
Introduction to 
Information Retrieval 
Document ingestion Introduction to Information Retrieval Introduction to Information Retrieval 
Recall the basic indexing pipeline 
Tokenizer 
Token stream Friends Romans Countrymen 
Linguistic 
modules 
Modified tokens friend roman countryman 
Indexer 
Inverted index friend 
roman 
countryman 24
2
13 16 1Documents to 
be indexed Friends, Romans, countrymen. 
Introduction to Information Retrieval Introduction to Information Retrieval 
Parsing a document 
/square4What format is it in? 
/square4pdf/word/excel/html? 
/square4What language is it in? 
/square4What character set is in use?
/square4(CP1252, UTF-8, …) 
Each of these is a classification problem, 
which we will study later in the course. 
But these tasks are often done heuristically … Sec. 2.1 Introduction to Information Retrieval Introduction to Information Retrieval 
Complications: Format/language 
/square4Documents being indexed can include docs from 
many different languages 
/square4A single index may contain terms from many languages. 
/square4Sometimes a document or its components can 
contain multiple languages/formats 
/square4French email with a German pdf attachment.
/square4French email quote clauses from an English-language 
contract 
/square4There are commercial and open source libraries that 
can handle a lot of this stuff Sec. 2.1 
Introduction to Information Retrieval Introduction to Information Retrieval 
Complications: What is a document? 
We return from our query “documents” but there are 
often interesting questions of grain size: 
What is a unit document? 
/square4A file? 
/square4An email?  (Perhaps one of many in a single mbox file) 
/square4What about an email with 5 attachments? 
/square4A group of files (e.g., PPT or LaTeX split over HTML pages) Sec. 2.1 Introduction to Information Retrieval Introduction to Information Retrieval 
Introduction to 
Information Retrieval 
Tokens 
2Introduction to Information Retrieval Introduction to Information Retrieval 
Tokenization 
/square4Input : “ Friends, Romans and Countrymen ”
/square4Output : Tokens 
/square4Friends 
/square4Romans 
/square4Countrymen 
/square4A token is an instance of a sequence of characters 
/square4Each such token is now a candidate for an index 
entry, after further processing 
/square4Described below 
/square4But what are valid tokens to emit? Sec. 2.2.1 Introduction to Information Retrieval Introduction to Information Retrieval 
Tokenization 
/square4Issues in tokenization: 
/square4Finland’s capital →
Finland AND s? Finlands ?Finland’s ?
/square4Hewlett-Packard →Hewlett and Packard as two 
tokens? 
/square4state-of-the-art : break up hyphenated sequence.  
/square4co-education 
/square4lowercase , lower-case , lower case ?
/square4It can be effective to get the user to put in possible hyphens 
/square4San Francisco : one token or two?  
/square4How do you decide it is one token? Sec. 2.2.1 
Introduction to Information Retrieval Introduction to Information Retrieval 
Numbers 
/square43/20/91 Mar. 12, 1991 20/3/91 
/square455 B.C. 
/square4B-52 
/square4My PGP key is 324a3df234cb23e 
/square4(800) 234-2333 
/square4Often have embedded spaces 
/square4Older IR systems may not index numbers 
/square4But often very useful: think about things like looking up error 
codes/stacktraces on the web 
/square4(One answer is using n-grams: IIR ch. 3) 
/square4Will often index “meta-data” separately 
/square4Creation date, format, etc. Sec. 2.2.1 Introduction to Information Retrieval Introduction to Information Retrieval 
Tokenization: language issues 
/square4French 
/square4L'ensemble →one token or two? 
/square4L ? L’? Le ?
/square4Want l’ensemble to match with un ensemble 
/square4Until at least 2003, it didn’t on Google 
/square4Internationalization! 
/square4German noun compounds are not segmented 
/square4Lebensversicherungsgesellschaftsangestellter 
/square4‘life insurance company employee’ 
/square4German retrieval systems benefit greatly from a compound splitter 
module 
/square4Can give a 15% performance boost for German Sec. 2.2.1 
Introduction to Information Retrieval Introduction to Information Retrieval 
Tokenization: language issues 
/square4Chinese and Japanese have no spaces between 
words: 
/square4莎拉波娃 现在居住在美国 东南部的佛 罗里达。 
/square4Not always guaranteed a unique tokenization 
/square4Further complicated in Japanese, with multiple 
alphabets intermingled 
/square4Dates/amounts in multiple formats 
フォーチュン フォーチュン フォーチュン フォーチュン 500 社は情報不足のため時間あた 社は情報不足のため時間あた 社は情報不足のため時間あた 社は情報不足のため時間あた $500K( 約約 約約6,000 万円 万円 万円 万円 )
Katakana Hiragana Kanji Romaji 
End-user can express query entirely in hiragana! Sec. 2.2.1 Introduction to Information Retrieval Introduction to Information Retrieval 
Tokenization: language issues 
/square4Arabic (or Hebrew) is basically written right to left, 
but with certain items like numbers written left to 
right 
/square4Words are separated, but letter forms within a word 
form complex ligatures 
/square4 ←  →    ← →                         ← start 
/square4‘Algeria achieved its independence in 1962 after 132 
years of French occupation. ’
/square4With Unicode, the surface presentation is complex, but the 
stored form is  straightforward 
Sec. 2.2.1 
3Introduction to Information Retrieval Introduction to Information Retrieval 
Introduction to 
Information Retrieval 
Terms 
The things indexed in an IR system Introduction to Information Retrieval Introduction to Information Retrieval 
Stop words 
/square4With a stop list, you exclude from the dictionary 
entirely the commonest words. Intuition: 
/square4They have little semantic content: the, a, and, to, be 
/square4There are a lot of them: ~30% of postings for top 30 words 
/square4But the trend is away from doing this: 
/square4Good compression techniques (IIR 5) means the space for including 
stop words in a system is very small 
/square4Good query optimization techniques (IIR 7) mean you pay little at 
query time for including stop words. 
/square4You need them for: 
/square4Phrase queries: “King of Denmark” 
/square4Various song titles, etc.: “Let it be”, “To be or not to be” 
/square4“Relational” queries: “flights to London” Sec. 2.2.2 
Introduction to Information Retrieval Introduction to Information Retrieval 
Normalization to terms 
/square4We may need to “normalize” words in indexed text 
as well as query words into the same form 
/square4We want to match U.S.A. and USA 
/square4Result is terms: a term is a (normalized) word type, 
which is an entry in our IR system dictionary 
/square4We most commonly implicitly define equivalence 
classes of terms by, e.g., 
/square4deleting periods to form a term 
/square4U.S.A. ,USA  USA 
/square4deleting hyphens to form a term 
/square4anti-discriminatory, antidiscriminatory antidiscriminatory Sec. 2.2.3 Introduction to Information Retrieval Introduction to Information Retrieval 
Normalization: other languages 
/square4Accents: e.g., French résumé vs. resume .
/square4Umlauts: e.g., German: Tuebingen vs. Tübingen 
/square4Should be equivalent 
/square4Most important criterion: 
/square4How are your users like to write their queries for these 
words? 
/square4Even in languages that standardly have accents, users 
often may not type them 
/square4Often best to normalize to a de-accented term 
/square4Tuebingen, Tübingen, Tubingen Tubingen Sec. 2.2.3 
Introduction to Information Retrieval Introduction to Information Retrieval 
Normalization: other languages 
/square4Normalization of things like date forms 
/square47月月 月月30 日日 日日vs. 7/30 
/square4Japanese use of kana vs. Chinese characters 
/square4Tokenization and normalization may depend on the 
language and so is intertwined with language 
detection 
/square4Crucial: Need to “normalize” indexed text as well as 
query terms identically Morgen will ich in MIT … Is this 
German “mit”? Sec. 2.2.3 Introduction to Information Retrieval Introduction to Information Retrieval 
Case folding 
/square4Reduce all letters to lower case 
/square4exception: upper case in mid-sentence? 
/square4e.g., General Motors 
/square4Fed vs. fed 
/square4SAIL vs. sail 
/square4Often best to lower case everything, since users will use 
lowercase regardless of ‘correct’ capitalization… 
/square4Longstanding Google example:         [fixed in 2011…] 
/square4Query C.A.T.  
/square4#1 result is for “cats” (well, Lolcats) not Caterpillar Inc. Sec. 2.2.3 
4Introduction to Information Retrieval Introduction to Information Retrieval 
Normalization to terms 
/square4An alternative to equivalence classing is to do 
asymmetric expansion 
/square4An example of where this may be useful 
/square4Enter: window Search: window, windows 
/square4Enter: windows Search: Windows, windows, window 
/square4Enter: Windows Search: Windows 
/square4Potentially more powerful, but less efficient Sec. 2.2.3 Introduction to Information Retrieval Introduction to Information Retrieval 
Thesauri and soundex 
/square4Do we handle synonyms and homonyms? 
/square4E.g., by hand-constructed equivalence classes 
/square4car = automobile color = colour 
/square4We can rewrite to form equivalence-class terms 
/square4When the document contains automobile , index it under car-
automobile (and vice-versa) 
/square4Or we can expand a query 
/square4When the query contains automobile , look under car as well 
/square4What about spelling mistakes? 
/square4One approach is Soundex, which forms equivalence classes 
of words based on phonetic heuristics 
/square4More in IIR 3 and IIR 9 
Introduction to Information Retrieval Introduction to Information Retrieval 
Introduction to 
Information Retrieval 
Stemming and Lemmatization Introduction to Information Retrieval Introduction to Information Retrieval 
Lemmatization 
/square4Reduce inflectional/variant forms to base form 
/square4E.g., 
/square4am, are, is →be 
/square4car, cars, car's , cars' →car 
/square4the boy's cars are different colors →the boy car be 
different color 
/square4Lemmatization implies doing “proper” reduction to 
dictionary headword form Sec. 2.2.4 
Introduction to Information Retrieval Introduction to Information Retrieval 
Stemming 
/square4Reduce terms to their “roots” before indexing 
/square4“Stemming” suggests crude affix chopping 
/square4language dependent 
/square4e.g., automate(s), automatic, automation all reduced to 
automat .
for example compressed 
and compression are both 
accepted as equivalent to 
compress .for exampl compress and 
compress ar both accept 
as equival to compress Sec. 2.2.4 Introduction to Information Retrieval Introduction to Information Retrieval 
Porter’s algorithm 
/square4Commonest algorithm for stemming English 
/square4Results suggest it’s at least as good as other stemming 
options 
/square4Conventions + 5 phases of reductions 
/square4phases applied sequentially 
/square4each phase consists of a set of commands 
/square4sample convention: Of the rules in a compound command, 
select the one that applies to the longest suffix. Sec. 2.2.4 
5Introduction to Information Retrieval Introduction to Information Retrieval 
Typical rules in Porter 
/square4sses →ss 
/square4ies →i
/square4ational →ate 
/square4tional →tion 
/square4Weight of word sensitive rules 
/square4(m>1) EMENT →
/square4replacement → replac 
/square4cement → cement Sec. 2.2.4 Introduction to Information Retrieval Introduction to Information Retrieval 
Other stemmers 
/square4Other stemmers exist: 
/square4Lovins stemmer 
/square4http://www.comp.lancs.ac.uk/computing/research/stemming/general/l ovins.htm 
/square4Single-pass, longest suffix removal (about 250 rules)
/square4Paice/Husk stemmer 
/square4Snowball 
/square4Full morphological analysis (lemmatization) 
/square4At most modest benefits for retrieval Sec. 2.2.4 
Introduction to Information Retrieval Introduction to Information Retrieval 
Language-specificity 
/square4The above methods embody transformations that 
are 
/square4Language-specific, and often 
/square4Application-specific 
/square4These are “plug-in” addenda to the indexing process 
/square4Both open source and commercial plug-ins are 
available for handling these Sec. 2.2.4 Introduction to Information Retrieval Introduction to Information Retrieval 
Does stemming help? 
/square4English: very mixed results. Helps recall for some 
queries but harms precision on others 
/square4E.g., operative (dentistry) ⇒oper 
/square4Definitely useful for Spanish, German, Finnish, … 
/square430% performance gains for Finnish! Sec. 2.2.4 
Introduction to Information Retrieval Introduction to Information Retrieval 
Introduction to 
Information Retrieval 
Faster postings merges: 
Skip pointers/Skip lists Introduction to Information Retrieval Introduction to Information Retrieval 
Recall basic merge 
/square4Walk through the two postings simultaneously, in 
time linear in the total number of postings entries 
128 
31 24841 48 64 
123811 17 21 Brutus 
Caesar 28
If the list lengths are mand n, the merge takes O( m+n )
operations. 
Can we do better? 
Yes (if the index isn’t changing too fast). Sec. 2.3 
6Introduction to Information Retrieval Introduction to Information Retrieval 
Augment postings with skip pointers 
(at indexing time) 
/square4Why? 
/square4To skip postings that will not figure in the search 
results. 
/square4How? 
/square4Where do we place skip pointers? 128 24841 48 64 
31 123811 17 21 31 11 41 128 Sec. 2.3 Introduction to Information Retrieval Introduction to Information Retrieval 
Query processing with skip pointers 
128 24841 48 64 
31 123811 17 21 31 11 41 128 
Suppose we’ve stepped through the lists until we 
process 8 on each list. We match it and advance. 
We then have 41 and 11 on the lower.  11 is smaller. 
But the skip successor of 11 on the lower list is 31 , so 
we can skip ahead past the intervening postings. Sec. 2.3 
Introduction to Information Retrieval Introduction to Information Retrieval 
Where do we place skips? 
/square4Tradeoff: 
/square4More skips →shorter skip spans ⇒more likely to skip.  
But lots of comparisons to skip pointers. 
/square4Fewer skips →few pointer comparison, but then long skip 
spans ⇒few successful skips. Sec. 2.3 Introduction to Information Retrieval Introduction to Information Retrieval 
Placing skips 
/square4Simple heuristic: for postings of length L, use √L
evenly-spaced skip pointers     [Moffat and Zobel 1996] 
/square4This ignores the distribution of query terms. 
/square4Easy if the index is relatively static; harder if Lkeeps 
changing because of updates. 
/square4This definitely used to help; with modern hardware it 
may not unless you’re memory-based   [Bahle et al. 2002] 
/square4The I/O cost of loading a bigger postings list can outweigh 
the gains from quicker in memory merging! Sec. 2.3 
Empirical Risk MinimizationOctober 29, 2015
Outline•Empirical risk minimization view –Perceptron –CRF
Notation for Linear Models•Training data:  {(x1, y1), (x2, y2), …, (xN, yN)} •Testing data:  {(xN+1, yN+1), … (xN+N', yN+N')} •Feature function:  g •Weights:  w •Decoding: •Learning: •Evaluation:
Structured Perceptron•Described as an online algorithm. •On each iteration, take one example, and update the weights according to: •Not discussing today:  the theoretical guarantees this gives, separability, and the averaged and voted versions.
Empirical Risk Minimization•A unifying framework for many learning algorithms. •Many options for the loss function L and the regularization function R.
Solving the Minimization Problem•In some friendly cases, there is a closed form solution for the minimizer of w –E.g., the maximum likelihood estimator for HMMs •Usually, we have to use an iterative algorithm which amounts to progressively finding better versions of w –involves hard/soft inference with each improved value of w on either part or all of the training set
Loss Functions You May KnowNameExpression of Log loss (joint)Log loss (conditional)Zero-one lossExpected zero-one loss
Loss Functions You May KnowNameExpression of Log loss (joint)Log loss (conditional)Zero-one lossExpected zero-one loss
Loss Functions You May KnowNameExpression ofLog loss (joint)Log loss (conditional)CostExpected cost, a.k.a. “risk”
CRFs and Loss•Plugging in the log-linear form (and not worrying at this level about locality of features):‘
CRFs and Loss•Plugging in the log-linear form (and not worrying at this level about locality of features):‘
Training CRFs and Other Linear Models•Early days:  iterative scaling (specialized method for log-linear models only) •~2002:  quasi-Newton methods –(using LBFGS which dates from the late 1980s) •~2006:  stochastic gradient descent •~2010:  adaptive gradient methods
Perceptron and Loss•Not clear immediately what L is, but the “gradient” of L should be: •The vector of above quantities is actually a subgradient of:
Compare•CRF (log-loss): •Perceptron:‘
Loss Functions

Loss Functions You KnowNameExpression ofConvex?Log loss (joint) ✔Log loss (conditional)✔CostExpected cost, a.k.a. “risk”Perceptron loss✔
Loss Functions You KnowNameExpression ofCont.?Log loss (joint) ✔Log loss (conditional)✔CostExpected cost, a.k.a. “risk”✔Perceptron loss✔
Loss Functions You KnowNameExpression ofCost?Log loss (joint) Log loss (conditional)Cost✔Expected cost, a.k.a. “risk”✔Perceptron loss
The Ideal Loss FunctionFor computational convenience: •Convex •Continuous For good performance: •Cost-aware •Theoretically sound
On Regularization•In principle, this choice is independent from the choice of the loss function. •Squared L2 norm is the most common starting place. •L1 and other sparsity-inducing regularizers as well as structured regularizers are of interestλλλλ
Practical Advice•Features still more important than the loss function. –But general, easy-to-implement algorithms are quite useful! •Perceptron is easiest to implement. •CRFs and max margin techniques usually do better. •Tune the regularization constant, λ. –Never on the test data.
1 
Introduction to Information Retrieval  
   
   
Introduction to  
Information Retrieval  
CS276  
Information Retrieval and Web Search  
Pandu Nayak and Prabhakar Raghavan  
Lecture 7: Scoring and results assembly  
Introduction to Information Retrieval  
   
   
Lecture 6 – I introduced a bug  
In my anxiety to avoid taking the log of zero, I 
rewrote  
 
 
 
as  
 
 
2 
  otherwise 0,0   tfif, tf log  1  10 t,d t,d
t,dw
  otherwise 0,0   tfif),tf(1 log  10 t,d t,d
t,dw
Introduction to Information Retrieval  
   
   
Recap: tf -idf weighting  
The tf -idf weight of a term is the product of its tf 
weight and its idf weight.  
 
 
Best known weighting scheme in information retrieval  
Increases with the number of occurrences within a 
document  
Increases with the rarity of the term in the collection  
)df/( log)tf log1( w10 , 10, t dt N
dt 
Introduction to Information Retrieval  
   
   
Recap: Queries as vectors  
Key idea 1:  Do the same for queries: represent them 
as vectors in the space  
Key idea 2:  Rank documents according to their 
proximity to the query in this space  
proximity = similarity of vectors  
Introduction to Information Retrieval  
   
   
Recap: cosine(query,document)  

 
V
iiV
iiV
iii
d qdq
dd
qq
dqdqdq
12
121), cos( 


q,d
 q
 d
q
 d
Introduction to Information Retrieval  
   
   
This lecture  
Speeding up vector space ranking  
Putting together a complete search 
system  
Will require learning about  a number of 
miscellaneous topics and heuristics  

2 
Introduction to Information Retrieval  
   
   
Computing cosine scores  
Introduction to Information Retrieval  
   
   
Efficient cosine ranking  
Find the K docs in the collection “nearest” to the 
query  K largest query -doc cosines.  
Efficient ranking:  
Computing a single cosine efficiently.  
Choosing the K largest cosine values efficiently.  
Can we do this without computing all N cosines?  
Introduction to Information Retrieval  
   
   
Efficient cosine ranking  
What we’re doing in effect: solving the K-nearest 
neighbor problem for a query vector  
In general, we do not know how to do this  efficiently 
for high -dimensional spaces  
But it is solvable for short queries, and standard 
indexes support this well  
Introduction to Information Retrieval  
   
   
Special case – unweighted queries  
No weighting on query terms  
Assume each query term occurs only once  
Then for ranking, don’t need to normalize query 
vector  
Slight simplification of algorithm from Lecture 6  
Introduction to Information Retrieval  
   
   
Computing the K largest cosines: 
selection vs. sorting  
Typically we want to retrieve the top K docs (in the 
cosine ranking for the query)  
not to totally order all docs in the collection  
Can we pick off docs with K highest cosines?  
Let J = number of docs with nonzero cosines  
We seek the K best of these J 
Introduction to Information Retrieval  
   
   
Use heap for selecting top K 
Binary tree in which each node’s value > the values 
of children  
Takes 2J operations to construct, then each of K 
“winners” read off in 2log J steps.  
For J=1M, K=100, this is about 10% of the cost of 
sorting.  

3 
Introduction to Information Retrieval  
   
   
Bottlenecks  
Primary computational bottleneck in scoring: cosine 
computation  
Can we avoid all this computation?  
Yes, but may sometimes get it wrong  
a doc not in the top K may creep into the list of K 
output docs  
Is this such a bad thing?  
Introduction to Information Retrieval  
   
   
Cosine similarity is only a proxy  
User has a task and a query formulation  
Cosine matches docs to query  
Thus cosine is anyway a proxy for user happiness  
If we get a list of K docs “close” to the top K by cosine 
measure, should be ok  
Introduction to Information Retrieval  
   
   
Generic approach  
Find a set A  of contenders , with K < |A| << N  
A does not necessarily contain the top K, but has 
many docs from among the top K 
Return the top K docs in A 
Think of A as pruning  non-contenders  
The same approach is also used for other (non -
cosine) scoring functions  
Will look at several schemes following this approach  
Introduction to Information Retrieval  
   
   
Index elimination  
Basic algorithm cosine computation algorithm only 
considers docs containing at least one query term  
Take this further:  
Only consider high -idf query terms  
Only consider docs containing many query terms  
Introduction to Information Retrieval  
   
   
High -idf query terms only  
For a query such as catcher in the rye  
Only accumulate scores from catcher and rye 
Intuition: in and the contribute little to the scores 
and so don’t alter rank -ordering much  
Benefit:  
Postings of low -idf terms have many docs  these (many) 
docs get eliminated from set A of contenders  
Introduction to Information Retrieval  
   
   
Docs containing many query terms  
Any doc with at least one query term is a candidate 
for the top K output list  
For multi -term queries, only compute scores for docs 
containing several of the query terms  
Say, at least 3 out of 4  
Imposes a “soft conjunction” on queries seen on web 
search engines (early Google)  
Easy to implement in postings traversal  

4 
Introduction to Information Retrieval  
   
   
3 of 4 query terms  
Introduction to Information Retrieval  
   
   
Champion lists  
Precompute for each dictionary term t, the r docs of 
highest weight in t’s postings  
Call this the champion list  for t 
(aka fancy list  or top docs  for t) 
Note that r has to be chosen at index build time  
Thus, it’s possible that r < K 
At query time, only compute scores for docs in the 
champion list of some query term  
Pick the K top-scoring docs from amongst these  
Introduction to Information Retrieval  
   
   
Exercises  
How do Champion Lists relate to Index Elimination? 
Can they be used together?  
How can Champion Lists be implemented in an 
inverted index?  
Note that the champion list has nothing to do with small 
docIDs  
Introduction to Information Retrieval  
   
   
Quantitative  
Static quality scores  
We want top -ranking documents to be both relevant 
and authoritative  
Relevance  is being modeled by cosine scores  
Authority is typically a query -independent property 
of a document  
Examples of authority signals  
Wikipedia among websites  
Articles in certain newspapers  
A paper with many citations  
Many bitly’s, diggs or del.icio.us marks  
(Pagerank)  
Introduction to Information Retrieval  
   
   
Modeling authority  
Assign to each document a query -independent  
quality score  in [0,1] to each document d 
Denote this by g(d) 
Thus, a quantity like the number of citations is scaled 
into [0,1]  
Exercise: suggest a formula for this.  
Introduction to Information Retrieval  
   
   
Net score  
Consider a simple total score combining cosine 
relevance and authority  
net-score( q,d) = g(d) + cosine( q,d) 
Can use some other linear combination  
Indeed, any function of the two “signals” of user happiness 
– more later  
Now we seek the top K docs by net score  

5 
Introduction to Information Retrieval  
   
   
Top K by net score – fast methods  
First idea: Order all postings by g(d) 
Key: this is a common ordering for all postings  
Thus, can concurrently traverse query terms’ 
postings for  
Postings intersection  
Cosine score computation  
Exercise: write pseudocode for cosine score 
computation if postings are ordered by g(d) 
Introduction to Information Retrieval  
   
   
Why order postings by g(d)?  
Under g(d)-ordering, top -scoring docs likely to 
appear early in postings traversal  
In time -bound applications (say, we have to return 
whatever search results we can in 50 ms), this allows 
us to stop postings traversal early  
Short of computing scores for all docs in postings  
Introduction to Information Retrieval  
   
   
Champion lists in g(d)-ordering  
Can combine champion lists with g(d)-ordering  
Maintain for each term a champion list of the r docs 
with highest g(d) + tf-idftd 
Seek top -K results from only the docs in these 
champion lists 
Introduction to Information Retrieval  
   
   
High and low lists  
For each term, we maintain two postings lists called 
high and low 
Think of high  as the champion list  
When traversing postings on a query, only traverse 
high lists first  
If we get more than K docs, select the top K and stop  
Else proceed to get docs from the low lists 
Can be used even for simple cosine scores, without 
global quality g(d) 
A means for segmenting index into two tiers  
Introduction to Information Retrieval  
   
   
Impact -ordered postings  
We only want to compute scores for docs for which 
wft,d is high enough  
We sort each postings list by wft,d 
Now: not all postings in a common order!  
How do we compute scores in order to pick off top K? 
Two ideas follow  
Introduction to Information Retrieval  
   
   
1. Early termination  
When traversing t’s postings, stop early after either  
a fixed number of r docs  
wft,d  drops below some threshold  
Take the union of the resulting sets of docs  
One from the postings of each query term  
Compute only the scores for docs in this union  
 

6 
Introduction to Information Retrieval  
   
   
2. idf -ordered terms  
When considering the postings of query terms  
Look at them in order of decreasing idf  
High idf terms likely to contribute most to score  
As we update score contribution from each query 
term  
Stop if doc scores relatively unchanged  
Can apply to cosine or some other net scores  
Introduction to Information Retrieval  
   
   
Cluster pruning: preprocessing  
Pick N docs  at random: call these leaders  
For every other doc, pre -compute nearest 
leader  
Docs attached to a leader: its followers;  
Likely : each leader has ~ N followers.  
Introduction to Information Retrieval  
   
   
 Cluster pruning: query processing  
Process a query as follows:  
Given query Q, find its nearest leader L.  
Seek K nearest docs from among L’s 
followers.  
 
Introduction to Information Retrieval  
   
   
Visualization  
Query  
Leader  Follower  
Introduction to Information Retrieval  
   
   
Why use random sampling  
Fast 
Leaders reflect data distribution  
Introduction to Information Retrieval  
   
   
General variants  
Have each follower attached to b1=3 (say) nearest 
leaders.  
From query, find b2=4 (say) nearest leaders and their 
followers.  
Can recurse on leader/follower construction.  

7 
Introduction to Information Retrieval  
   
   
Exercises  
To find the nearest leader in step 1, how many cosine 
computations do we do?  
Why did we have N in the first place?  
What is the effect of the constants b1, b2  on the 
previous slide?  
Devise an example where this is likely to  fail – i.e., we 
miss one of the K nearest docs.  
Likely  under random sampling.  
 
 
Introduction to Information Retrieval  
   
   
Parametric and zone indexes  
Thus far, a doc has been a sequence of terms  
In fact documents have multiple parts, some with 
special semantics:  
Author  
Title  
Date of publication  
Language  
Format  
etc. 
These constitute the metadata  about a document  
Introduction to Information Retrieval  
   
   
Fields  
We sometimes wish to search by these metadata  
E.g., find docs authored by William Shakespeare in the 
year 1601, containing alas poor Yorick  
Year = 1601 is an example of a field  
Also, author last name = shakespeare, etc.  
Field or parametric index: postings for each field 
value  
Sometimes build range trees (e.g., for dates)  
Field query typically treated as conjunction  
(doc must be authored by shakespeare)  
Introduction to Information Retrieval  
   
   
Zone  
A zone  is a region of the doc that can contain an 
arbitrary amount of text, e.g.,  
Title  
Abstract  
References …  
Build inverted indexes on zones as well to permit 
querying  
E.g., “find docs with merchant in the title zone and 
matching the query gentle rain”  
Introduction to Information Retrieval  
   
   
Example zone indexes  
Introduction to Information Retrieval  
   
   
Tiered indexes  
Break postings up into a hierarchy of lists  
Most important  
… 
Least important  
Can be done by g(d) or another measure  
Inverted index thus broken up into tiers of decreasing 
importance  
At query time use top tier unless it fails to yield K 
docs  
If so drop to lower tiers  

8 
Introduction to Information Retrieval  
   
   
Example tiered index  
Introduction to Information Retrieval  
   
   
Query term proximity  
Free text queries : just a set of terms typed into the 
query box – common on the web  
Users prefer docs in which query terms occur within 
close proximity of each other  
Let w be the smallest window in a doc containing all 
query terms, e.g.,  
For the query strained mercy  the smallest window in 
the doc The quality of mercy is not strained  is 4 
(words)  
Would like scoring function to take this into account 
– how?  
Introduction to Information Retrieval  
   
   
Query parsers  
Free text query from user may in fact spawn one or 
more queries to the indexes, e.g., query rising 
interest rates  
Run the query as a phrase query  
If <K docs contain the phrase rising interest rates , run the 
two phrase queries rising interest and interest rates  
If we still have < K docs, run the vector space query rising 
interest rates  
Rank matching docs by vector space scoring  
This sequence is issued by a query parser  
Introduction to Information Retrieval  
   
   
Aggregate scores  
We’ve seen that score functions can combine cosine, 
static quality, proximity, etc.  
How do we know the best combination?  
Some applications – expert -tuned  
Increasingly common: machine -learned  
See May 19th lecture  
Introduction to Information Retrieval  
   
   
Putting it all together  
Introduction to Information Retrieval  
   
   
Resources  
IIR 7, 6.1  
CS11-747 Neural Networks for NLP Models of Dialog and ConversationGraham NeubigSitehttps://phontron.com/class/nn4nlp2017/

Types of Dialog•Who is talking? •Human-human •Human-computer •Why are they talking? •Task driven •Chat
Models of Chat
Two Paradigms•Generation-based models •Take input, generate output •Good if you want to be creative •Retrieval-based models •Take input, ﬁnd most appropriate output •Good if you want to be safe
Generation-based Models (Ritter et al. 2011)•Train phrase-based machine translation system to perform translation from utterance to response •Lots of ﬁltering, etc., to make sure that the extracted translation rules are reliable

Neural Models for Dialog Response Generation (Sordoni et al. 2015, Sheng et al. 2015, Vinyals and Le 2015)•Like other translation tasks, dialog response generation can be done with encoder-decoders •Sheng et al. (2015) present simplest model, translating from previous utterance

Problem 1: Dialog More Dependent on Global Coherence•Considering only a single previous utterance will lead to locally coherent but globally incoherent output •Necessary to consider more context! (Sordoni et al. 2015)
•Contrast to MT, where context sometimes is (Matsuzaki et al. 2015) and sometimes isn’t (Jean et al. 2015) helpful
One Solution: Use Standard Architecture w/ More Context•Sordoni et al. (2015) consider one additional previous context utterance concatenated together •Vinyals et al. (2015) just concatenate together all previous utterances and hope an RNN an learn

Hierarchical Encoder-decoder Model (Serban et al. 2016)•Also have utterance-level RNN track overall dialog state

Discourse-level VAE Model(Zhao et al. 2017)•Encode entire previous dialog context as latent variable in VAE •Also meta-information such as dialog acts
Also, bag-of-words loss
Problem 2: Dialog allows Much More Varied Responses•For translation, there is lexical variation but content remains the same •For dialog, content will also be different! (e.g. Li et al. 2016)

Diversity Promoting Objective for Conversation (Li et al. 2016)•Basic idea: we want responses that are likely given the context, unlikely otherwise •Method: subtract weighted unconditioned log probability from conditioned probability (calculated only on ﬁrst few words)

Diversity is a Problem for Evaluation!•Translation uses BLEU score; while imperfect, not horrible •In dialog, BLEU shows very little correlation (Liu et al. 2016)

Using Multiple References with Human Evaluation Scores (Galley et al. 2015)•Retrieve good-looking responses, perform human evaluation, up-weight good ones, down-weight bad ones

Learning to Evaluate•Use context, true response, and actual response to learn a regressor that predicts goodness (Lowe et al. 2017) •Important: similar to model, but has access to reference!
•Adversarial evaluation: try to determine whether response is true or fake (Li et al. 2017) •One caveat from MT: learnable metrics tend to overﬁt
Problem 3: Dialog Agents should have Personality•If we train on all of our data, our agent will be a mish-mash of personalities (e.g. Li et al. 2016)•We would like our agents to be consistent!

Personality Infused Dialog (Mairesse et al. 2007)•Train a generation system with controllable “knobs” based on personality traits •e.g. Extraversion: •Non-neural, but well done and perhaps applicable

Persona-based Neural Dialog Model (Li et al. 2017)•Model each speaker in embedding space
•Also model who the speaker is speaking to in speaker-addressee model
Retrieval-based Models
Dialog Response Retrieval•Idea: many things can be answered with template •Simply ﬁnd most relevant response out of existing ones in corpus
Image Credit: GoogleTemplate responses
Retrieval-based Chat (Lee et al. 2009)•Basic idea: given an utterance, ﬁnd the most similar in the database and return it
•Similarity based on exact word match, plus extracted features regarding discourse
Neural Response Retrieval (Nio et al. 2014)•Idea: use neural models to soften the connection between input and output and do more ﬂexible matching
•Model uses Socher et al. (2011) recursive auto-encoder + dynamic pooling
Smart Reply for Email Retrieval (Kannan et al. 2016)•Implemented in GMail smart reply •Similar response model with LSTM seq2seq scoring, but many improvements •Beam search over response space for scalability •Canonicalization of syntactic variants and clustering of similar responses •Human curation of responses  •Enforcement of diversity through omission of redundant responses and enforcing positive/negative
Task-driven Dialog
Chat vs. Task Completion•Chat is basically to keep the user entertained •What if we want to do an actual task? •Book a ﬂight •Access information from a database
Traditional Task-completion Dialog Framework•In semantic frame based dialog: •Natural language understanding to ﬁll the slots in the frame based on the user utterance •Dialog state tracking to keep track of the overall dialog state over multiple turns •Dialog control to decide the next action based on state •Natural language generation to generate utterances based on current state
NLU (for Slot Filling) w/ Neural Nets (Mesnil et al. 2015)•Slot ﬁling expressed as BIO scheme
•RNN-CRF based model for tags
Dialog State Tracking•Track the belief about our current frame-ﬁlling state (Williams et al. 2013)
•Henderson et al. (2014) present RNN model that encodes multiple ASR hypotheses and generalizes by abstracting details
Language Generation from Dialog State w/ Neural Nets (Wen et al. 2015)•Condition LSTM units based on the dialog input, output English

End-to-end Dialog Control(Williams et al. 2017)•Train an LSTM that takes in text and entities and directly chooses an action to take (reply or API call)
•Trained using combination of supervised and reinforcement learning

Questions?
CS11-747 Neural Networks for NLP AttentionGraham NeubigSitehttps://phontron.com/class/nn4nlp2017/

LSTM
LSTM
LSTM
LSTM
LSTM
</s>
LSTM
LSTM
LSTM
LSTM
argmaxargmaxargmaxargmax</s>argmaxEncoder-decoder Models (Sutskever et al. 2014)Ihatethismoviekonoeigagakirai
IhatethismovieEncoder
Decoder
Sentence Representations
•But what if we could use multiple vectors, based on the length of the sentence.
this is an example
this is an example
“You can’t cram the meaning of a whole %&!$ing sentence into a single $&!*ing vector!” — Ray MooneyProblem!
Attention
Basic Idea (Bahdanau et al. 2015)•Encode each word in the sentence into a vector •When decoding, perform a linear combination of these vectors, weighted by “attention weights” •Use this combination in picking the next word
Calculating Attention (1)•Use “query” vector (decoder state) and “key” vectors (all encoder states) •For each query-key pair, calculate weight •Normalize to add to one using softmax
konoeigagakiraiKey VectorsIhate
Query Vector
a1=2.1
a2=-0.1
a3=0.3
a4=-1.0
softmaxα1=0.76α2=0.08α3=0.13α4=0.03
Calculating Attention (2)•Combine together value vectors (usually encoder states, like key vectors) by taking the weighted sum
konoeigagakiraiValue Vectors
α1=0.76α2=0.08α3=0.13α4=0.03****
•Use this in any part of the model you like
A Graphical Example

Attention Score Functions (1)•q is the query and k is the key •Multi-layer Perceptron (Bahdanau et al. 2015)•Flexible, often very good with large data •Bilinear (Luong et al. 2015)a(q,k)=w|2tanh(W1[q;k])a(q,k)=q|Wk
Attention Score Functions (2)•Dot Product (Luong et al. 2015)•No parameters! But requires sizes to be the same. •Scaled Dot Product (Vaswani et al. 2017) •Problem: scale of dot product increases as dimensions get larger •Fix: scale by size of the vectora(q,k)=q|k
a(q,k)=q|kp|k|
Let’s Try it Out! attention.py
What do we Attend To?
Input Sentence•Like the previous explanation •But also, more directly •Copying mechanism (Gu et al. 2016)•Lexicon bias (Arthur et al. 2016)

Previously Generated Things•In language modeling, attend to the previous words (Merity et al. 2016)•In translation, attend to either input or previous output (Vaswani et al. 2017)

Various Modalities•Images (Xu et al. 2015)•Speech (Chan et al. 2015)

Hierarchical Structures (Yang et al. 2016)•Encode with attention over each sentence, then attention over each sentence in the document

Multiple Sources•Attend to multiple sentences (Zoph et al. 2015)•Libovicky and Helcl (2017) compare multiple strategies •Attend to a sentence and an image (Huang et al. 2016)

Intra-Attention / Self Attention (Cheng et al. 2016)•Each element in the sentence attends to other elements → context sensitive encodings!thisisanexamplethisisanexample

Improvements to Attention
Coverage•Problem: Neural models tends to drop or repeat content •Solution: Model how many times words have been covered •Impose a penalty if attention not approx. 1 (Cohn et al. 2015) •Add embeddings indicating coverage (Mi et al. 2016)
Incorporating Markov Properties(Cohn et al. 2015)•Intuition: attention from last time tends to be correlated with attention this time•Add information about the last attention when making the next decision

Bidirectional Training (Cohn et al. 2015)•Intuition: Our attention should be roughly similar in forward and backward directions •Method: Train so that we get a bonus based on the trace of the matrix product for training in both directions
tr(AX!YA|Y!X)
Supervised Training (Mi et al. 2016)•Sometimes we can get “gold standard” alignments a-priori •Manual alignments •Pre-trained with strong alignment model •Train the model to match these strong alignments
Attention is not Alignment! (Koehn and Knowles 2017)•Attention is often blurred •Attention is often off by one

Specialized Attention Varieties
Hard Attention•Instead of a soft interpolation, make a zero-one decision about where to attend (Xu et al. 2015) •Harder to train, requires methods such as reinforcement learning (see later classes) •Perhaps this helps interpretability? (Lei et al. 2016)

Monotonic Attention (e.g. Yu et al. 2016)•In some cases, we might know the output will be the same order as the input •Speech recognition, incremental translation, morphological inﬂection (?), summarization (?)
•Basic idea: hard decisions about whether to read more •Speech recognition, morphological inﬂection (?), summarization (?)
Convolutional Attention (Allamanis et al. 2016)•Intuition: we might want to be able to attend to “the word after ‘Mr.’”, etc.

Multi-headed Attention•Idea: multiple attention “heads” focus on different parts of the sentence
•Or multiple independently learned heads (Vaswani et al. 2017)
•e.g. Different heads for “copy” vs regular (Allamanis et al. 2016)
An Interesting Case Study: “Attention is All You Need” (Vaswani et al. 2017)
•A sequence-to-sequence model based entirely on attention •Strong results on standard WMT datasets •Fast: only matrix multiplications
Summary of the “Transformer" (Vaswani et al. 2017)
Attention Tricks•Self Attention: Each layer combines words with others •Multi-headed Attention: 8 attention heads learned independently •Normalized Dot-product Attention: Remove bias in dot product when using large networks •Positional Encodings: Make sure that even if we don’t have RNN, can still distinguish positions
Training Tricks•Layer Normalization: Help ensure that layers remain in reasonable range •Specialized Training Schedule: Adjust default learning rate of the Adam optimizer •Label Smoothing: Insert some uncertainty in the training process •Masking for Efﬁcient Training
Masking for Training•We want to perform training in as few operations as possible using big matrix multiplies •We can do so by “masking” the results for the outputkonoeigagakiraiIhatethismovie</s>

Questions?
Phrase-Based MT
Machine TranslationLecture 7 Instructor: Chris Callison-Burch TAs: Mitchell Stern, Justin Chiu Website: mt-class.org/penn
Translational EquivalenceHe insisted on the test, but just barely.He passed the test, but just barely.Er hat die Prüfung bestanden, jedoch nur knappHow do lexical translation models deal withcontextual information?
Translational EquivalenceHe insisted on the test, but just barely.He passed the test, but just barely.Er hat die Prüfung bestanden, jedoch nur knapp
F
E
probbestandeninsisted0.06were0.06existed0.04was0.04been0.04passed0.03consist0.01
Translational EquivalenceHe insisted on the test, but just barely.He passed the test, but just barely.Er hat die Prüfung bestanden, jedoch nur knappWhat is wrong with this?How can we improve this?Lexical Translation
Translation model•What are the atomic units?•Lexical translation: words•Phrase-based translation: phrases •Standard model used by Google, Microsoft ...•Beneﬁts•many-to-many translation•use of local context in translation•Downsides•Where do phrases comes from?
Translation model•With a latent variable, we introduce a decomposition into phrases which translate independently:p(f,a|e)=p(a)Y⇥e,f⇤ ap(f|e)p(f|e)=Xa Ap(a)Y⇥e,f⇤ ap(f|e)We can then marginalize to get p(f|e):e=f=
Morgen
fliege
ich
nach Baltimore
zur Konferenz
Tomorrow
will fly
I
in Baltimore
to the 
conference
Translation model•With a latent variable, we introduce a decomposition into phrases which translate independently:p(f,a|e)=p(a)Y⇥e,f⇤ ap(f|e)p(f|e)=Xa Ap(a)Y⇥e,f⇤ ap(f|e)We can then marginalize to get p(f|e):a=e=f=
fliege
ich
nach Baltimore
zur Konferenz
will fly
I
in Baltimore
to the 
conference
Morgen
Tomorrow
Translation model•With a latent variable, we introduce a decomposition into phrases which translate independently:p(f,a|e)=p(a)Y⇥e,f⇤ ap(f|e)p(f|e)=Xa Ap(a)Y⇥e,f⇤ ap(f|e)We can then marginalize to get p(f|e):a=e=f=
fliege
ich
nach Baltimore
zur Konferenz
will fly
I
in Baltimore
to the conference
Morgen
Tomorrowp(Morgen|T omorrow)
Translation model•With a latent variable, we introduce a decomposition into phrases which translate independently:p(f,a|e)=p(a)Y⇥e,f⇤ ap(f|e)p(f|e)=Xa Ap(a)Y⇥e,f⇤ ap(f|e)We can then marginalize to get p(f|e):a=e=f=
ich
nach Baltimore
zur Konferenz
I
in Baltimore
to the conference
Morgen
Tomorrow
fliege
will flyp(Morgen|T omorrow)x p(ﬂiege|will ﬂy)
Translation model•With a latent variable, we introduce a decomposition into phrases which translate independently:p(f,a|e)=p(a)Y⇥e,f⇤ ap(f|e)p(f|e)=Xa Ap(a)Y⇥e,f⇤ ap(f|e)We can then marginalize to get p(f|e):a=e=f=
nach Baltimore
zur Konferenz
in Baltimore
to the conference
Morgen
Tomorrow
fliege
will fly
p(Morgen|T omorrow)x p(ﬂiege|will ﬂy)
ich
Ix p(ich|I)
`•With a latent variable, we introduce a decomposition into phrases which translate independently:p(f,a|e)=p(a)Y⇥e,f⇤ ap(f|e)p(f|e)=Xa Ap(a)Y⇥e,f⇤ ap(f|e)We can then marginalize to get p(f|e):a=e=f=
zur Konferenz
to the conference
Morgen
Tomorrow
fliege
will fly
p(Morgen|T omorrow)x p(ﬂiege|will ﬂy)
ich
I
x p(ich|I)
nach Baltimore
in Baltimorex ... 
Translation model•With a latent variable, we introduce a decomposition into phrases which translate independently:p(f,a|e)=p(a)Y⇥e,f⇤ ap(f|e)p(f|e)=Xa Ap(a)Y⇥e,f⇤ ap(f|e)Marginalize to get p(f|e):
Phrases•Contiguous strings of words•Phrases are not necessarily syntactic constituents•Usually have maximum limits•Phrases subsume words (individual words are phrases of length 1)
Linguistic Phrases•Model is not limited to linguistic phrases(NPs, VPs, PPs, CPs...)•Non-constituent phrases are useful•Is a “good” phrase more likely to be     [P NP]      or        [governor  P]Why? How would you ﬁgure this out?es gibt    there is | there are
Phrase Tablesdas Themathe issue0.41the point0.72the subject0.47the thema0.99es gibtthere is0.96there are0.72morgentomorrow0.9ﬂiege ichwill I ﬂy0.63will ﬂy0.17I will ﬂy0.13fep(f|e)
p(a)•T wo responsibilities•Divide the source sentence into phrases•Standard approach: uniform distribution over all possible segmentations•How many segmentations are there?•Reorder the phrases•Standard approach: Markov model on phrases (parameterized with log-linear model)
Reordering Model

Learning Phrases•Latent segmentation variable•Latent phrasal inventory•Parallel data•EM?Computational problem: summing over all segmentationsand alignments is #P-completeModeling problem: MLE has a degenerate solution.
Learning Phrases•Three stages•word alignment•extraction of phrases•estimation of phrase probabilities
Consistent Phrases

Phrase ExtractionI   open   the    boxwatashiwahakowoakemasu
Phrase ExtractionI   open   the    boxwatashiwahakowoakemasuakemasu / open 
Phrase ExtractionI   open   the    boxwatashiwahakowoakemasuwatashi wa / I 
Phrase ExtractionI   open   the    boxwatashiwahakowoakemasuwatashi / I 
Phrase ExtractionI   open   the    boxwatashiwahakowoakemasuwatashi / I ✘
Phrase ExtractionI   open   the    boxwatashiwahakowoakemasuhako wo / box
Phrase ExtractionI   open   the    boxwatashiwahakowoakemasuhako wo / the box
Phrase ExtractionI   open   the    boxwatashiwahakowoakemasuhako wo / open the box
Phrase ExtractionI   open   the    boxwatashiwahakowoakemasuhako wo / open the box✘
Phrase ExtractionI   open   the    boxwatashiwahakowoakemasuhako wo akemasu / open the box
Translation Process•Task: translate this sentence from German into Englishergehtjanichtnachhause
Chapter 6: Decoding2
Translation Process•Task: translate this sentence from German into Englishergehtjanichtnachhauseerhe•Pick phrase in input, translateChapter 6: Decoding3
Translation Process•Task: translate this sentence from German into Englishergehtjanichtnachhauseerja nichthedoes not•Pick phrase in input, translate–it is allowed to pick words out of sequence reordering–phrases may have multiple words: many-to-many translationChapter 6: Decoding4
Translation Process•Task: translate this sentence from German into Englishergehtjanichtnachhauseergehtja nichthedoes notgo•Pick phrase in input, translateChapter 6: Decoding5
Translation Process•Task: translate this sentence from German into Englishergehtjanichtnachhauseergehtja nichtnach hausehedoes notgohome•Pick phrase in input, translateChapter 6: Decoding6
Computing Translation Probability•Probabilistic model for phrase-based translation:ebest=argmaxeIYi=1 (¯fi|¯ei)d(starti endi 1 1)plm(e)•Score is computed incrementally for each partial hypothesis•ComponentsPhrase translationPicking phrase¯fito be translated as a phrase¯ei!look up score (¯fi|¯ei)from phrase translation tableReorderingPrevious phrase ended inendi 1,c u r r e n tp h r a s es t a r t sa tstarti!computed(starti endi 1 1)Language modelForn-gram model, need to keep track of lastn 1words!compute scoreplm(wi|wi (n 1),. . . ,wi 1)for added wordswiChapter 6: Decoding7
Translation Optionsheergehtjanichtnachhauseit, it, heisaregoesgoyesis, of coursenotdo notdoes notis notaftertoaccording toinhousehomechamberat homenotis notdoes notdo nothomeunder housereturn homedo notit ishe will beit goeshe goesisareis after alldoestofollowingnot afternot to,
notis notare notis not a•Many translation options to choose from–in Europarl phrase table: 2727 matching phrase pairs for this sentence–by pruning to the top 20 per phrase, 202 translation options remainChapter 6: Decoding8
Translation Optionsheergehtjanichtnachhauseit, it, heisaregoesgoyesis, of coursenotdo notdoes notis notaftertoaccording toinhousehomechamberat homenotis notdoes notdo nothomeunder housereturn homedo notit ishe will beit goeshe goesisareis after alldoestofollowingnot afternot tonotis notare notis not a•The machine translation decoder does not know the right answer–picking the right translation options–arranging them in the right order!Search problem solved by heuristic beam searchChapter 6: Decoding9
Decoding: Precompute Translation Optionsergehtjanichtnachhause
consult phrase translation table for all input phrasesChapter 6: Decoding10
Decoding: Start with Initial Hypothesisergehtjanichtnachhause
initial hypothesis: no input words covered, no output producedChapter 6: Decoding11
Decoding: Hypothesis Expansionergehtjanichtnachhause
arepick any translation option, create new hypothesisChapter 6: Decoding12
Decoding: Hypothesis Expansionergehtjanichtnachhause
areithecreate hypotheses for all other translation optionsChapter 6: Decoding13
Decoding: Hypothesis Expansionergehtjanichtnachhause
areithegoesdoes notyesgotohomehomealso create hypotheses from created partial hypothesisChapter 6: Decoding14
Decoding: Find Best Pathergehtjanichtnachhause
areithegoesdoes notyesgotohomehomebacktrack from highest scoring complete hypothesisChapter 6: Decoding15
Reading•Read Chapter 5 and 6 from the textbook

Announcements•HW2 will be released soon•HW2 due Thursday Feb 19th at 11:59pm
Learning and Generating Paraphrases From Twitter and BeyondWei XuGuest Lecture @ Penn MT class   April-2-2015 Computer)and)Informa/on)Science)University)of)Pennsylvania

Research OverviewTACL 15!!NAACL 15!!TACL 14!!ACL 14!!ACL 13!!BUCC 13!!LSAM 13!!COLING 12!!IJCNLP 11!!EMNLP 11!!ACL 06Social MediaParaphrase
Information Extraction
Paraphrase
Paraphrase
 … the forced resignation of the CEO of Boeing, Harry Stonecipher, for …the king’s speechHis Majesty’s addresswealthyrichwordphrasesentence… after Boeing Co. Chief Executive Harry Stonecipher was ousted from …
 ApplicationInformation Extractionend_job (Harry Stonecipher, Boeing)
Wei)Xu,)Raphael)Hoﬀmann,)Le)Zhao,)Ralph)Grishman.)“Filling)Knowledge)Base)Gaps)for)Distant)Supervision)of)Rela/on)Extrac/on”))In)ACL)(2013)))extract … the forced resignation of the CEO of Boeing, Harry Stonecipher, for …… after Boeing Co. Chief Executive Harry Stonecipher was ousted from …
 ApplicationQuestion AnsweringWho is the CEO stepping down from Boeing?match … the forced resignation of the CEO of Boeing, Harry Stonecipher, for …… after Boeing Co. Chief Executive Harry Stonecipher was ousted from …
ApplicationText Simpliﬁcation They are culturally akin to the coastal peoples of Papua New Guinea.Their culture is like that of the coastal peoples of Papua New Guinea.
Wei)Xu,)Chris)CallisonUBurch.)“Problems)in)Current)Text)Simpliﬁca/on)Research:)New)Data)Can)Help”))to)appear)in)TACL)(2015)))NSF)EAGER:)“Simpliﬁca/on)as)Machine)Transla/on”)(2014)~)2015))
ApplicationStylistic RewritingPalpatine:  If you will not be turned, you will be destroyed!If you will not be turn’d, you will be undone!
Wei)Xu,)Alan)Ri_er,)Bill)Dolan,)Ralph)Grishman,)Colin)Cherry.)“Paraphrasing)for)Style”)In)COLING)(2012)))
Luke:  Father, please! Help me!Father, I pray you! Help me!

Previous Work
But, primarily for formal language usage and well-edited textNumerous publications on paraphrase identiﬁcation, extraction, generation and various applications  
Previous Work
only a few hundreds news agencies  report big events  using formal language(Dolan,)Quirk)and)Brocke_,)2004;)Dolan)and)Brocke_,)2005;)Brocke_)and)Dolan,)2005))
Twitter as a new resource
Wei)Xu,)Alan)Ri_er,)Ralph)Grishman.)“A)Preliminary)Study)of)Tweet)Summariza/on)using)Informa/on)Extrac/on”)in)LASM)(2014)))
Twitter as a powerful resourcethousands of users talk about both big and micro events  using formal, informal, erroneous language
Very%diverse!%
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
Enables new applicationspittsburghpghpittsburgpixburghpitsteelersagainst the steelersagainst pittsburghWei)Xu,)Alan)Ri_er,)Ralph)Grishman.)“Gathering)and)Genera/ng)Paraphrases)from)Twi_er)with)Applica/on)to)Normaliza/on”))In)BUCC)(2013)))Information Retrieval?
Enables new applicationsNoisy Text Normalizationoscar nom’d docOscar-nominated documentarydon’t want fordon’t wait for
Wei)Xu,)Joel)Tetreault,)Mar/n)Chodorow,)Ralph)Grishman,)Le)Zhao.)“Exploi/ng)Syntac/c)and)Distribu/onal)Informa/on)for)Spelling)Correc/on)with)WebUScale)NUgram)Models”)In)EMNLP)(2011)))
Enables new applications
who wants to get a beer?Human-computer Interaction 
Wei)Xu,)Alan)Ri_er,)Ralph)Grishman.)“Gathering)and)Genera/ng)Paraphrases)from)Twi_er)with)Applica/on)to)Normaliza/on”))In)BUCC)(2013)))want to get a beer?who else wants to get a beer?who wants to go get a beer?trying to get a beer?who wants to buy a beer?who else wants to get a beer?… (21 different ways)
Enables new applicationsLanguage Education
Aaaaaaaaand stephen curry is on ﬁreWhat a incredible performance from Stephen Curry
Enables new applicationsWowsers to this nets bulls gameThis nets vs bulls game is greatSentiment AnalysisThis Nets vs Bulls game is nutsThis Nets and Bulls game is a good gamethis Nets vs Bulls game is too live
This NetsBulls series is intenseThis netsbulls game is too good
Learn Paraphrases
Learn Paraphrases Mancini has been sacked by Manchester City Mancini gets the boot from Man City identify parallel sentences automatically !from Twitter’s big data streamWORLD OF JENKS IS ON AT 11World of Jenks is my favorite show on tv
Yes!%No!$

Early Attempts•1242 tweet pairs, tracking celebrity & hashtags (Zanzotto, Pennacchiotti and Tsioutsiouliklis, 2011) •named entity + date(Xu, Ritter and Grishman, 2013)  •bilingual posts (Ling, Dyer, Black and Trancoso, 2013)

Design a ModelTrain it on data
A Challenge
Mancini has been sacked by Manchester City Mancini gets the boot from Man Cityvery short lexically divergent !(less word overlap, even in high-dimensional space) 
Design a Modeltwo sentences about the same topic are paraphrases  if and only if  they contain at least one word pair that is a paraphrase anchor   That boy Brook Lopez with a deep 3brook lopez hit a 3 At-least-one-anchor Assumption
Yes!%
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
Another Challengenot every word pair of similar meaning indicates  sentence-level paraphraseSolution:      a discriminative model using features at word-levelIron Man 3 was brilliant funIron Man 3 tonight see what this is like No!$
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
Multi-instance Learning Paraphrase Model
Z2"0"0"0"1"0"..."man$"|"teo"be"|"is"..."Z1"Z4"Y#paraphrase"Y#non2paraphrase"
Z3"1"next"|"new"
diﬀ_word"same_pos_nn"both_sig"…"same_stem"same_pos_be"not_both_sig"…"diﬀ_word"same_pos_jj"both_sig"…"diﬀ_word"diﬀ_pos_nn"diﬀ_pos_jj"not_both_sig"…"man$"|"li>le"
sentence"pair"
word"pair"
features(Manti bout to be the next Junior SeauTeo is the little new Junior Seau
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
[Mini Tutorial]  Multi-instance LearningNega%ve'Bags'Posi%ve'Bags''
A'bag'is'labeled'posi%ve,'if''there'is'at#least#one'posi%ve'example'A'bag'is'labeled'nega%ve,'if''all'the'examples'in'it'are'nega%ve'Instead of labels on each individual instance,  the learner only observes labels on bags of instances.
(Die_erich)et)al.,)1997))
[Mini Tutorial]  Multi-instance Learning
Z2"?"?"1"Z1"Y"
Z3"1"
bag"label"(observed)"
instance"label"(latent)"
Posi7ve"Bag""A"bag"is"labeled"posi7ve,"if""there"is"at#least#one"posi7ve"example"features"
constraints"Latent Variable Model
[Mini Tutorial]  Multi-instance LearningLatent Variable Model
Z2"0"0"0"Z1"Y"
Z3"0"
instance"label"(latent)"
Nega3ve"Bag""A"bag"is"labeled"nega3ve,"if""all"the"examples"in"it"are"nega3ve"features"
constraints"bag"label"(observed)"

[Mini Tutorial]  Multi-instance Learning
Maria)Pershina,)Bonan)Min,)Wei)Xu,)Ralph)Grishman.)“Infusion)of)Labeled)Data)into)Distant)Supervision)for)Rela/on)Extrac/on”))In)ACL)(2014))))Wei)Xu,)Raphael)Hoﬀmann,)Le)Zhao,)Ralph)Grishman.)“Filling)Knowledge)Base)Gaps)for)Distant)Supervision)of)Rela/on)Extrac/on”))In)ACL)(2013))))Wei)Xu,)Ralph)Grishman,)Le)Zhao.)“Passage)Retrieval)for)Informa/on)Extrac/on)using)Distant)Supervision”))In)IJCNLP)(2011))))Distantly Supervised Information Extraction1. incomplete knowledge base problem2. distant supervision + human-labeled dataG|R|
|xi|nzihiyixi9>>=>>;{relationlevelmentionlevel3. IE + IR
[Recap] Multi-instance Learning Paraphrase Model
Z2"0"0"0"1"0"..."man$"|"teo"be"|"is"..."Z1"Z4"Y#paraphrase"Y#non2paraphrase"
Z3"1"next"|"new"
diﬀ_word"same_pos_nn"both_sig"…"same_stem"same_pos_be"not_both_sig"…"diﬀ_word"same_pos_jj"both_sig"…"diﬀ_word"diﬀ_pos_nn"diﬀ_pos_jj"not_both_sig"…"man$"|"li>le"
sentence"pair"
word"pair"
features(Manti bout to be the next Junior SeauTeo is the little new Junior Seau
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
Joint Word-Sentence Model
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
Zi"
Y#"
W×W""S×S""sentence"pair"
word"pair"
determinis2c"OR"bag"label"(observed)"
instance"label"(latent)"
Model the assumption:!sentence-level paraphrase  is anchored by at-least-one word pair
Zj"

Joint Word-Sentence Model
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
features
parameters
determinis/c)OR
jth)word)pairith)sentence)pair’s)label))(observed)or)to)be)predicated))latent)labels)for)all)word)pairs))in)the)ith)sentence)pair)

Learning AlgorithmObjective:!learn the parameters that maximize  likelihood over the training corpus 
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
ith#training#sentence#pair
all#possible#values#of#the#latent#variables

reward#correct#(condi6oned#on#labels)Learning Algorithm
Perceptron-style Update:!Viterbi approximation + online learning   O(# word pairs)
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
penalize#wrong#(ignoring#labels)
Training Data
AnnotationCrowdsourcing
(Courtesy:)The)Sheep)Market)by)Aaron)Koblin)
Annotation
Wei)Xu.)“DataUdriven)Approaches)for)Paraphrasing)Across)Language)Varia/ons”)PhD)Thesis,)New)York)University.)(2014)))Crowdsourcing

A Problemonly 8% sentence pairs about the same topic have similar meaning  hurts both quantity and quality
non#experts*lower*their*bars*Wei)Xu.)“DataUdriven)Approaches)for)Paraphrasing)Across)Language)Varia/ons”)PhD)Thesis,)New)York)University.)(2014)))
Sentence Selection
NetﬂixJeff GreenRyuThe ClippersReggie Miller00.20.40.60.8Randomw/ SelectionSumBasic Algorithm8%        16%  
percentages of paraphrasesWei)Xu.)“DataUdriven)Approaches)for)Paraphrasing)Across)Language)Varia/ons”)PhD)Thesis,)New)York)University.)(2014)))
Topic SelectionMulti-Armed Bandits
16%        34%  
Wei)Xu.)“DataUdriven)Approaches)for)Paraphrasing)Across)Language)Varia/ons”)PhD)Thesis,)New)York)University.)(2014)))
Twitter Paraphrase Dataset18,762 sentence pairs labeled  cost only $200 !!!1/3 paraphrase, 2/3 non-paraphrase (very balanced)including a very broad range of paraphrases: synonyms, misspellings, slang, acronyms and colloquialismsWei)Xu.)“DataUdriven)Approaches)for)Paraphrasing)Across)Language)Varia/ons”)PhD)Thesis,)New)York)University.)(2014)))
important)but)diﬃcult)to)obtain
Performance
Performance
40557085100
(Das&Smith,2009)(Guo&Diab,2012)(Ji&Eisenstein,2013)Our ModelHuman Upperbound
90.8
72.6
62.8
65.5
63.2
75.2
72.2
66.4
52.5
62.9PrecisionRecall
state-of-the-art of paraphrase identiﬁcation
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
40557085100
(Das&Smith,2009)(Guo&Diab,2012)(Ji&Eisenstein,2013)Our ModelHuman Upperbound
90.8
72.6
62.8
65.5
63.2
75.2
72.2
66.4
52.5
62.9PrecisionRecallPerformance
Wei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))
Our Model(Ji&Eisenstein,2013)
ImpactSemEval 2015 shared task on “Paraphrase in Twitter” 19 + 1 teams participated !100+ research groups  have requested the data since Nov 2014 paraphrase identiﬁcation (0 or 1)       rank 1semantic similarity (0 ~ 1)                  rank 4
our modelWei)Xu,)Chris)CallisonUBurch,)Bill)Dolan.)“SemEvalU2015)Task)1:)Paraphrase)and)Seman/c)Similarity)in)Twi_er)(PIT)”)In)SemEval)(2015))
InnovationsThat boy Brook Lopez with a deep 3brook lopez hit a 3 
Yes!%
Multi-instance Learning Paraphrase Model (MultiP)-Twitter’s big data stream -potential beyond Twitter and English -joint sentence-word alignment -extensible latent variable modelWei)Xu,)Alan)Ri_er,)Chris)CallisonUBurch,)Bill)Dolan,)Yangfeng)Ji.)“Extrac/ng)Lexically)Divergent)Paraphrases)from)Twi_er”)In)TACL)(2014))(a lot of space for future work)
Generate Paraphrases 
Extract Phrasal Paraphrases Mancini has been sacked by Manchester City Mancini gets the boot from Man City align
Wei)Xu,)Alan)Ri_er,)Ralph)Grishman.)“Gathering)and)Genera/ng)Paraphrases)from)Twi_er)with)Applica/on)to)Normaliza/on”))In)BUCC)(2013)))
Extract Phrasal Paraphrases has been sacked bygets the boot frommanchester cityman city4for4fourouttaout ofhosteshostessWei)Xu,)Alan)Ri_er,)Ralph)Grishman.)“Gathering)and)Genera/ng)Paraphrases)from)Twi_er)with)Applica/on)to)Normaliza/on”))In)BUCC)(2013)))
Text-to-text Generation
businessHostesouttais goingbiz..out ofis goingHostesstranslate
Wei)Xu,)Alan)Ri_er,)Ralph)Grishman.)“Gathering)and)Genera/ng)Paraphrases)from)Twi_er)with)Applica/on)to)Normaliza/on”))In)BUCC)(2013)))
Statistical Machine TranslationBilingualMonolingualstudiedsensitive to errorobjectivestraightforwardsophisticatedlessmorelessmorea lotmore recentlyhas standard evaluationyesnot quite yetnaturally available parallel text
Wei)Xu.)“DataUdriven)Approaches)for)Paraphrasing)Across)Language)Varia/ons”)PhD)Thesis,)New)York)University.)(2014)))(Paraphrase =)
Text-to-text Generationcomplexsimplestylisticplainnoisystandard
erroneouscorrectand more (future work) …
(Xu et al. 2013)(Xu et al. 2012)(Xu et al. 2015)(Xu et al. 2011)
Wei)Xu.)“DataUdriven)Approaches)for)Paraphrasing)Across)Language)Varia/ons”)PhD)Thesis,)New)York)University.)(2014)))
Prose to SonnetWandering through rows of stalls examining workhorses and prize hogs may seem to … have been a strange way for a scientist to spend an afternoon, but there was a certain logic to it.hogs may seem a bit strange through rows of stalls
Quanze)Chen,)Chenyang)Lei,)Wei)Xu,)Ellie)Pavlick)and)Chris)CallisonUBurch.)“Poetry)of)the)Crowd:)A)Human)Computa/on)Algorithm)to)Convert)Prose)into)Rhyming)Verse”)In)AAAI's)HCOMP)(2012)
[Rhyme]!balls falls installs walls …

Text Simpliﬁcation
state-of-the-art (since 2010)
NSF)EAGER:)“Simpliﬁca/on)as)Machine)Transla/on”)(2014)~)2015))
Text Simpliﬁcation
state-of-the-art (since 2010)is suboptimal !is not all that simpleWei)Xu,)Chris)CallisonUBurch.)“Problems)in)Current)Text)Simpliﬁca/on)Research:)New)Data)Can)Help”))to)appear)in)TACL)(2015)))NSF)EAGER:)“Simpliﬁca/on)as)Machine)Transla/on”)(2014)~)2015))
•Jointly model word-sentence via latent variables!!•Use Twitter as a powerful paraphrase resource!!•Systemize a framework for language generation!!•Right the direction of text simpliﬁcation research
all#code#and#data#are#available#on#my#homepage:##h<p://www.cis.upenn.edu/~xwe/Main Contributions
The Ideal

CollaboratorsChris Callison-Burch Ralph Grishman Bill Dolan Alan Ritter Raphael Hoffmann Joel Tetreault Le Zhao Maria Pershina Martin Chodorow Colin Cherry Yangfeng Ji Ellie Pavlick Mingkun Gao Quanze ChenUPenn NYU MSR UW / OSU UW / AI2 Incubator ETS / Yahoo! CMU / Google NYU CUNY NRC GaTech UPenn UPenn UPenn
Thank youthanksthanking youappreciate itthnxthxtyvmthank you very muchthanks a lot3xsay thanksam gratefulwawwww thankkkkkkkkkkk you alotttttttttttt!thank u 4 ur timegratitude
Dependency ParsingInstructor: Yoav ArtziCS5740: Natural Language ProcessingSpring 2018
Slides adapted from Dan Klein, Luke Zettlemoyer, Chris Manning, and Dan Jurafsky, and David Weiss
Overview•The parsing problem•Methods–Transition-based parsing•Evaluation•Projectivity
Parse Trees•Part-of-speech Tagging: –Word classes•Parsing:–From words to phrases to sentences–Relations between words•Two views–Constituency–Dependency 
Constituency (Phrase Structure) Parsing•Phrase structure organizes words into nested constituents•Linguists can, and do, argue about details•Lots of ambiguity
new art critics write reviews with computersPPNPNPNNPVPS
Dependency Parsing•Dependency structure shows which words depend on (modify or are arguments of) which other words.Theboyputthetortoiseontherug
Dependency Structure•Syntactic structure consists of:–Lexical items–Binary asymmetric relations àdependenciessubmittedBillswereBrownbackSenatornsubjpassauxpassprepnnimmigrationconjbyccandportspobjpreponpobjRepublicanKansaspobjprepofapposDependencies are typed with name of grammatical relation
Dependency Structure•Syntactic structure consists of:–Lexical items–Binary asymmetric relations àdependenciessubmittedBillsnsubjpassHead (governor, superior, regent)Modifier (dependent, inferior, subordinate)Arrow from head to modifier (but can be reversed)
Dependency Structure•Syntactic structure consists of:–Lexical items–Binary asymmetric relations àdependenciessubmittedBillswereBrownbackSenatornsubjpassauxpassprepnnimmigrationconjbyccandportspobjpreponpobjRepublicanKansaspobjprepofapposDependencies form a tree
Dependency Structure•Syntactic structure consists of:–Lexical items–Binary asymmetric relations àdependenciessubmittedBillswereBrownbackSenatornsubjpassauxpassprepnnimmigrationconjbyccandportspobjpreponpobjRepublicanKansaspobjprepofapposDependencies form a treeRoot
Let’s Parse
He said that the boy who was wearing the blue shirt with the white pockets has left the buildingJohn saw MaryStart with main verb, and draw dependencies. Don’t worry about labels. Just try the modifiers right. 
Methods for Dependency Parsing•Dynamic programming–Eisner (1996): O(n3)•Graph algorithms–McDonald et al. (2005): score edges independently using classifier and use maximum spanning tree•Constraint satisfaction–Start with all edges, eliminate based on hard constraints•“Deterministic parsing”–Left-to-right, each choice is done with a classifierjumpedboyoverthethelittleprepnsubjdetamodpobjfencedet
Making DecisionsWhat are the sources of information for dependency parsing?1.Bilexicalaffinities    –[issues àthe] is plausible2.Dependency distance   –mostly with nearby words3.Intervening material–Dependencies rarely span intervening verbs or punctuation4.Valencyof heads  –How many dependents on which side are usual for a head?ROOT Discussion of the outstanding issues was completed  .
MaltParse(Nivreet al. 2008)•Greedy transition-based parser•Each decision: how to attach each word as we encounter it–If you are familiar: like shift-reduce parser•Select each action with a classifier•The parser has:–a stack σ, written with the top to the right•which starts with the ROOT symbol–a buffer β, written with the top to the left•which starts with the input sentence–a set of dependency arcs A•which starts off empty–a set of actions
Arc-standard Dependency ParsingStart:  σ= [ROOT], β = w1, …, wn, A = ∅•Shiftσ, wi|β, A àσ|wi, β, A•Left-Arcrσ|wi, wj|β, A àσ, wj|β, A∪{r(wj,wi)} •Right-Arcrσ|wi, wj|β, A àσ, wi|β, A∪{r(wi,wj)}Finish:  β = ∅ROOTJoe likes Marry
Arc-standard Dependency ParsingStart:  σ= [ROOT], β = w1, …, wn, A = ∅•Shiftσ, wi|β, A àσ|wi, β, A•Left-Arcrσ|wi, wj|β, A àσ, wj|β, A∪{r(wj,wi)} •Right-Arcrσ|wi, wj|β, A àσ, wi|β, A∪{r(wi,wj)}Finish:  β = ∅ROOTJoe likes Marry[ROOT][Joe, likes, marry]∅Shift[ROOT, Joe][likes, marry]∅Left-Arc[ROOT] [likes, marry]{(likes,Joe)}= A1Shift[ROOT, likes][marry]A1Right-Arc[ROOT][likes]A1∪{(likes,Marry)} = A2Right-Arc[][ROOT]A2∪{(ROOT, likes)} = A3Shift[ROOT][]A3
Arc-standard Dependency ParsingStart:  σ= [ROOT], β = w1, …, wn, A = ∅•Shiftσ, wi|β, A àσ|wi, β, A•Left-Arcrσ|wi, wj|β, A àσ, wj|β, A∪{r(wj,wi)} •Right-Arcrσ|wi, wj|β, A àσ, wi|β, A∪{r(wi,wj)}Finish:  β = ∅ROOT Happy children like to play with their friends .
Arc-eager Dependency ParsingStart:  σ= [ROOT], β = w1, …, wn, A = ∅•Left-Arcrσ|wi, wj|β, A àσ, wj|β, A∪{r(wj,wi)} –Precondition: r’(wk, wi) ∉A, wi≠ ROOT•Right-Arcrσ|wi, wj|β, A àσ|wi|wj, β, A∪{r(wi,wj)}•Reduce     σ|wi, β, A àσ, β, A–Precondition: r’(wk, wi) ∈A•Shift           σ, wi|β, A àσ|wi, β, AFinish:  β = ∅This is the common “arc-eager” variant: a head can immediately take a right dependent, before itsdependents are found
Arc-eager1.Left-Arcrσ|wi, wj|β, A èσ, wj|β, A∪{r(wj,wi)} Precondition: r’(wk, wi) ∉A, wi≠ ROOT2.Right-Arcrσ|wi, wj|β, A èσ|wi|wj, β, A∪{r(wi,wj)}3.Reduce    σ|wi, β, A èσ, β, APrecondition: r’(wk, wi) ∈A4.Shift        σ, wi|β, A èσ|wi, β, AROOT Happy children like to play with their friends .
Arc-eagerROOT Happy children like to play with their friends .[ROOT][Happy, children, …]∅Shift[ROOT, Happy][children, like, …]∅LAamod[ROOT][children, like, …]{amod(children, happy)} = A1Shift[ROOT, children][like, to, …]A1LAnsubj[ROOT][like, to, …]A1∪{nsubj(like, children)} = A2RAroot[ROOT, like][to, play, …]A2∪{root(ROOT, like) = A3Shift[ROOT, like, to][play, with, …]A3LAaux[ROOT, like][play, with, …]A3∪{aux(play, to) = A4RAxcomp[ROOT, like, play][with their, …]A4∪{xcomp(like, play) = A51.Left-Arcrσ|wi, wj|β, A èσ, wj|β, A∪{r(wj,wi)} Precondition: r’(wk, wi) ∉A, wi≠ ROOT2.Right-Arcrσ|wi, wj|β, A èσ|wi|wj, β, A∪{r(wi,wj)}3.Reduce    σ|wi, β, A èσ, β, APrecondition: r’(wk, wi) ∈A4.Shift        σ, wi|β, A èσ|wi, β, A
Arc-eagerROOT Happy children like to play with their friends .RAxcomp[ROOT, like, play][with their, …]A4∪{xcomp(like, play) = A5RAprep[ROOT, like, play, with][their, friends, …]A5∪{prep(play, with) = A6Shift[ROOT, like, play, with, their][friends, .]A6LAposs[ROOT, like, play, with][friends, .]A6∪{poss(friends, their) = A7RApobj[ROOT, like, play, with, friends][.]A7∪{pobj(with, friends) = A8Reduce[ROOT, like, play, with][.]A8Reduce[ROOT, like, play][.]A8Reduce[ROOT, like][.]A8RApunc[ROOT, like, .][]A8∪{punc(like, .) = A9You terminate as soon as the buffer is empty.  Dependencies = A91.Left-Arcrσ|wi, wj|β, A èσ, wj|β, A∪{r(wj,wi)} Precondition: r’(wk, wi) ∉A, wi≠ ROOT2.Right-Arcrσ|wi, wj|β, A èσ|wi|wj, β, A∪{r(wi,wj)}3.Reduce    σ|wi, β, A èσ, β, APrecondition: r’(wk, wi) ∈A4.Shift        σ, wi|β, A èσ|wi, β, A
MaltParser(Nivreet al. 2008)•Selecting the next action:–Discriminative classifier (SVM, MaxEnt, etc.)–Untypedchoices: 4–Typed choices: |R| * 2 + 2•Features: POS tags, word in stack, word in buffer, etc.•Greedy àno search–But can easily do beam search•Close to state of the art•Linear time parser àvery fast!
Parsing with Neural NetworksChen and Manning (2014)•Arc-standard Transitions–Shift–Left-Arcr–Right-Arcr•Selecting the next actions:–Untypedchoices: 3–Typed choices: |R| * 2 + 1–Neural network classifier•With a few model improvements and very careful hyper-parameter tuning gives SOTA results
Parsing with Neural NetworksChen and Manning (2014)

Hyper-parameters
Slide from David Weiss
Slide from David Weiss
Slide from David Weiss
Slide from David Weiss
Slide from David Weiss
EvaluationROOT   She  saw   the   video   lecture 0         1      2       3         4            5Gold12Shensubj20sawroot 35thedet45videonn52    lecturedobjParsed12Shensubj20sawroot 34thedet45videonsubj52    lectureccompAcc=   # correct deps# of depsUAS =  4 / 5  =  80%LAS  =  2 / 5  =  40%
Projectivity•Dependencies from CFG trees with head rules must be projective–Crossing arcs are not allowed •But: theory allows to account for displaced constituents ànon-projective structuresWho did Bill buy the coffee from yesterday ?
Projectivity•Arc-eager transition system:–Can’t handle non-projectivity•Possible directions:–Give up!–Post-processing–Add new transition types–Switch to a different algorithm•Graph-based parsers (e.g., MSTParser)
Machine Learning Lecture 6 Note
Compiled by Abhi Ashutosh, Daniel Chen, and Yijun Xiao
February 16, 2016
1 Pegasos Algorithm
The Pegasos Algorithm looks very similar to the Perceptron Algorithm. In fact,
just by changing a few lines of code in our Perceptron Algorithms, we can get
the Pegasos Algorithm.
Algorithm 1: Perceptron to Pegasos
1initialize w1= 0, t = 0;
2foriter = 1, 2, ..., 20 do
3 forj = 1, 2, ...,jdatajdo
4 t = t + 1;t=1
t;
5 ifyj(wT
txj)<1then
6 wt+1= (1 t)wt+tyjxj;
7 else
8 wt+1= (1 t)wt;
9 end
10 end
11end
Side note: We can optimize both the Pegasos and Perceptron Algorithm by
using sparse vectors in the case of document classication because most entries
in the feature vector xwill be zeros.
As we discussed in the lecture, the original Pegasos algorithm randomly
chooses one data point at each iteration instead of going through each data
point in order as shown in Algorithm 1. Pegasos algorithm is an application of
the stochastic sub-gradient descent method.
2 Using Pegasos to Solve Other SVM Objec-
tives
2.1 Imbalanced data set
Sometimes it may be hard to classify an imbalanced data set where the classi-
cation categories are not equally represented. In this case, we want to weigh
1
each data point dierently by placing more weights on the data points in the
underrepresented categories. We can do this very easily by changing our opti-
mization problem to
min
wkwk2
2+CN
2N+X
j:yj=+1j+CN
2N X
j:yj= 1j
whereN+,N are the number of positive data points and negative data points
respectively. j's are the slack variables.
An intuitive way to think about this is if we want to build a classier that
classies whether a point is blue or red. If in our data set we only have 1 data
point that's labelled as red and 10 data points labelled as blue, then using the
modied objective function is equivalent to duplicating the 1 red point 10 times
without explicitly creating more training data.
2.2 Transfer learning
Suppose we want to build a personalized spam classier for Professor David
Sontag. However, Professor David only has few of his emails labelled. Professor
Rob, on the other hand, has labelled all of the emails he has ever received as
spam or not spam and trained an accurate spam lter on them. Since Professor
David and Professor Rob are both Computer Science Professors and run a lab
together, we hope that they probably share similar standards for spams/non-
spams. In this case, a spam classier built for Professor Rob should work well
to a certain extent for Professor David as well. What should the SVM objective
be? (Class ideas: average the weight vectors of both Professors; combine David
and Rob's data and put more weights on David's data.)
One solution is to solve the following modied optimization problem,
min
wd;bdC
jDdjX
x;y2Ddmax(0;1 y(wT
dx+bd)) +1
2kwd wrk2
The idea here is we assume that the weight for Rob is going to be very close
to that for David. We then try to penalize the distance between the two. C
here can be interpreted as how condent we are that the weights for Rob will
be similar to the weights for David. If we are very condent, a low C, then we
will really try to minimize the distance between the two weight vectors. If we
are not condent, large C, then we are more focused on David's labelled data.
2.3 Multiclass classication
If we want to extend these ideas further to multi-class classication, we have a
number of options. The simplest is called a One-vs-all Classier in which we
learnnclassiers, one for each of the nclasses. We could run into issues if we
want to classify a point that fell in between our classiers as we would need to
2
decide in which class it belongs. We can predict the most probable class using
the formula
^y= argmax
kwT
kx+bk
Another solution is called Multiclass SVM. Here, we put soft restrictions on
predicting correct labels for the training data using:
w(yj)Txj+b(yj)w(y0)Txj+b(y0)+ 1 j;8y06=yj;j0;8j
Notice that we have one slack variable jper data point and one set of weights
w(k);b(k)for each class k. We could derive a similar Pegasos Algorithm for a
multiclass classier.
3 Kernel Trick
What if the data is not linearly separable? We can create a mapping (x)
that takes our feature vector xand converts it into a higher dimensional space.
Creating a linear classication in this higher dimension and projecting that onto
our original feature space will give us a squiggly line classier.
Kernel tricks allow us to perform the aforementioned classication with little
extra cost. For Pegasos algorithm, we can do this by keeping track of just a
single variable per data point, i, and calculating vector wwhen required.
w=X
iiyixi
Let's now derive the updating rule for such i's. Notice in Algorithm 1, the
update rule at each iteration is
wt+1= (1 t)wt+ 1[yjwT
txj<1]tyjxj
where 1[condition ] is the indicator function. Now instead of xj;yj, let us use
x(t);y(t)to denote the data point we randomly selected at iteration t. Substitute
t, we have
wt+1=
1 1
t
wt+1
t1[y(t)wT
tx(t)<1]y(t)x(t)
Multiplying both sides with t, rearranging,
twt+1 (t 1)wt=1
1[y(t)wT
tx(t)<1]y(t)x(t)
As the above equation holds for any t, we have the following tequations
8
>>><
>>>:twt+1 (t 1)wt =1
1[y(t)wT
tx(t)<1]y(t)x(t)
(t 1)wt (t 2)wt 1=1
1[y(t 1)wT
t 1x(t 1)<1]y(t 1)x(t 1)

w2 =1
1[y(1)wT
1x(1)<1]y(1)x(1)
3
Summing over the above tequations and dividing both sides by t, we can
have
wt+1=1
ttX
k=11[y(k)wT
kx(k)<1]y(k)x(k)
written in the form of summation over i:
wt+1=X
i0
@1
ttX
k=11[y(k)wT
kx(k)<1] 1[(xi;yi) = (x(k);y(k))]1
Ayixi
All the stu in the huge parenthesis corresponds to iwe dened earlier.
t(t+1)
i counts the number of times data point iappears before iteration tand
satisesyiwT
kxi<1. This implies a simple updating rule for t(t+1)
i:
t(t+1)
i =(t 1)(t)
i+ 1[(xi;yi) = (x(t);y(t))] 1[yiwT
txi<1]
i.e. suppose we draw data point ( xi;yi) at iteration t, we increment ti
by 1 iyiwT
txi<1. The algorithm is shown in Algorithm 2. To simplify the
notations, we denote (t)
i=(t 1)(t)
i.
Algorithm 2: Kernelized Pegasos
1initialize(1)= 0;
2fort = 1, 2, ..., T do
3 randomly choose ( x(t);y(t)) = (xj;yj) from training data
4 ifyj1
(t 1)P
i(t)
iyixT
ixj<1then
5(t+1)
j =(t)
j+ 1;
6 else
7(t+1)
j =(t)
j;
8 end
9end
After convergence, we can get back i's usingi=1
T(T+1)
i . In testing
time, predictions can be made with
^y= sign0
@X
iiyixT
ix1
A
Now suppose we want to use more complex features (x) which can be ob-
tained by transforming the original features xto a higher dimensional space,
all we need to do is to substitute xT
ixjin both training and testing with
(xi)T(xj).
Further notice that (x) always appears in the form of dot products. Which
indicates we do not necessarily need to explicitly compute it as long as we have
4
a formula to calculate the dot products. This is where kernels come into use.
Instead of dening the function to do the projection, we directly dene a
kernel function Kto calculate the dot product of the projected features.
K(xi;xj) =(xi)T(xj)
We can create dierent kernel functions K(xi;xj) as long as those functions
are based on dot products. We can also create new valid kernel functions using
other valid kernel functions following certain rules. Examples of popular kernel
functions include Polynomial Kernels, Gaussian Kernels, and many more.
References
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, Andrew Cotter. Extended
version: Pegasos: Primal Estimated sub-GrAdient SOlver for SVM. Mathemat-
ical Programming, Series B, 127(1):3-30 , 2011
5
Syntax-­‐based	
  Transla0on	
  Part	
  1:	
  Re-­‐ordering	
  for	
  Phrase-­‐based	
  transla0onThanks to Michael Collins for many of today’s slides.Take a look at Mike’s course: http://www.cs.columbia.edu/~cs4705/and his Coursera course: https://class.coursera.org/nlangp-001Machine TranslationLecture 13 Instructor: Chris Callison-Burch 
Goals•Understand	
  why	
  syntax	
  is	
  important	
  for	
  reordering	
  models	
  •Review	
  non-­‐syntac0c	
  reordering	
  models	
  for	
  phrase-­‐based	
  machine	
  transla0on	
  •Review	
  the	
  “Clause	
  Restructuring”	
  approach	
  of	
  Collins,	
  Koehn,	
  and	
  Kucerova	
  •Understand	
  why	
  it	
  is	
  a	
  good	
  ﬁt	
  for	
  phrase-­‐based	
  machine	
  transla0on	
  •Discuss	
  its	
  limita0ons2
Phrase-­‐based	
  model
•Foreign	
  input	
  is	
  segmented	
  in	
  phrases	
  	
  •Each	
  phrase	
  is	
  translated	
  into	
  English	
  	
  •Phrases	
  are	
  reordered3natuerlichhatjohnspass amspielof coursejohnhasfun with thegame
Some	
  Reordering	
  Already	
  Captured
4natuerlichhatjohnspass amspielof coursejohnhasfun with thegamenatuerlichhat johnspass amspielof coursejohn hasfun with thegame•Local	
  reordering	
  can	
  be	
  captured	
  within	
  phrases
Phrase	
  transla0on	
  table•Main	
  knowledge	
  source:	
  table	
  with	
  phrase	
  transla0ons	
  and	
  their	
  probabili0es	
  	
  •Example:	
  phrase	
  transla0ons	
  for	
  natuerlich
5SourceTransla,onProbability	
  φ(e|f)natuerlichof	
  course0.5natuerlichnaturally0.3natuerlichof	
  course	
  ,0.15natuerlich,	
  of	
  course	
  ,0.05
Probabilis0c	
  Model•Bayes	
  rule	
  –ebest	
  =	
  arg	
  maxe	
  p(e|f)	
  	
  	
  	
  	
  	
  	
  	
  =	
  arg	
  maxe	
  p(f|e)	
  plm(e)	
  –	
  transla0on	
  model	
  p(e|f)	
  	
  –	
  language	
  model	
  plm(e)	
  •Reordering	
  score	
  can	
  be	
  incorporated	
  in	
  the	
  TM	
  –	
  phrase	
  transla0on	
  probability	
  φ	
  	
  –	
  reordering	
  probability	
  d6Probabilistic Model•Bayes ruleebest=argmaxep(e|f)=argmaxep(f|e)plm(e)–translation modelp(e|f)–language modelplm(e)•Decomposition of the translation modelp(¯fI1|¯eI1)=IYi=1 (¯fi|¯ei)d(starti endi 1 1)–phrase translation probability –reordering probabilitydChapter 5: Phrase-Based Models6
Log-­‐linear	
  model
7Weighted Model as Log-Linear Modelp(e, a|f)=exp(  IXi=1log (¯fi|¯ei)+ dIXi=1logd(ai bi 1 1)+ LM|e|Xi=1logpLM(ei|e1...ei 1))
Chapter 5: Phrase-Based Models19
Distance-­‐based	
  Reordering
8Distance-Based Reordering1234567d=0d=-3d=-2d=-1foreignEnglishphrasetranslatesmovementdistance11–3start at beginning026skip over 4–5+234–5move back over 4–6-347skip over 6+1Scoring function:d(x)=↵|x|— exponential with distanceChapter 5: Phrase-Based Models7Scoring function: d(x) = α|x| – exponential with distance
Values	
  of	
  α
90.000.250.500.751.00
012345
0.99
0.75
0.5
0.25
0.1Distance of moveProbabilityα=
Discussion:	
  Distance-­‐based	
  reordering•What	
  do	
  you	
  think	
  of	
  it?	
  •Is	
  it	
  a	
  good	
  model	
  for	
  how	
  reordering	
  works	
  across	
  languages?	
  •What	
  is	
  it	
  missing?(Discuss	
  with	
  your	
  neighbor)
10
Distance-­‐based	
  reordering•Small	
  values	
  of	
  α,	
  severely	
  discourage	
  reordering	
  –Limit	
  reordering	
  to	
  monotonic	
  or	
  a	
  narrow	
  window	
  –OK	
  for	
  languages	
  with	
  very	
  similar	
  word	
  orders	
  –Bad	
  for	
  languages	
  with	
  diﬀerent	
  word	
  orders	
  	
  •The	
  distance-­‐based	
  penalty	
  applies	
  uniformly	
  to	
  all	
  words	
  and	
  all	
  word	
  types	
  –Doesn’t	
  know	
  that	
  adjec0ves	
  and	
  nouns	
  should	
  swap	
  when	
  transla0ng	
  from	
  French	
  to	
  English	
  •Puts	
  most	
  responsibility	
  on	
  the	
  language	
  model11
How	
  else	
  could	
  we	
  model	
  reordering?•Why	
  not	
  assign	
  a	
  dis0nct	
  reordering	
  probability	
  to	
  each	
  word/phrase	
  in	
  the	
  phrase	
  table?	
  –p(reorder	
  |	
  f,	
  e)	
  •This	
  is	
  known	
  as	
  lexicalized	
  reordering	
  •How	
  can	
  we	
  es0mate	
  that	
  probability?
12
Lexicalized	
  Reordering	
  model
13
HowmuchyouforyourFacebookWievielmanaufrgundseines
profile
ProfilsinFacebookcharge
verdienenshouldsollte
m
dmdm
sReordering features are probability estimates of s, d, and m
dmdmm: monotone (keep order)s: swap orderd: become discontinuous
Lexicalized	
  Reordering	
  table•Iden0cal	
  phrase	
  pairs	
  <f,e>	
  as	
  in	
  the	
  phrase	
  transla0on	
  table	
  •Contains	
  values	
  for	
  p(monotone|e,f),	
  p(swap|e,f),	
  p(discon0nuous|e,f)
14SourceTransla,onp(m|e,f)p(s|e,f)p(d|e,f)natuerlichof	
  course0.520.080.4natuerlichnaturally0.420.10.48natuerlichof	
  course	
  ,0.50.0010.499natuerlich,	
  of	
  course	
  0.270.170.56
Discussion:	
  Is	
  this	
  bemer?•Do	
  you	
  think	
  that	
  this	
  is	
  a	
  more	
  sensible	
  reordering	
  model	
  than	
  the	
  distance-­‐based	
  one?	
  •How	
  could	
  you	
  determine	
  if	
  it	
  is	
  bemer	
  or	
  not?	
  •What	
  do	
  you	
  think	
  that	
  it	
  s0ll	
  lacks?(Discuss	
  with	
  your	
  neighbor)
15
Empirically,	
  yes!
160.015.030.045.060.0
ArabicJapaneseKoreanChineseEn-Chinese
16.6
38.6
42.3
47.6
50.9
15.2
34.6
35.7
45.1
49.9BaselineLexicalized ReorderingKoehn et al, IWSLT 2005
The	
  Awful	
  German	
  Language The Germans have another kind of parenthesis, which they make by splitting a verb in two and putting half of it at the beginning of an exciting chapter and the OTHER HALF at the end of it. Can any one conceive of anything more confusing than that? These things are called ‘separable verbs.’ The wider the two portions of one of them are spread apart, the better the author of the crime is pleased with his performance.”“
Mark Twain
German	
  verbs
18Ich  werde  Ihnen   den  Report  aushaendigen .   I    will      to_you  the   report   pass_on          .Ich  werde  Ihnen   die entsprechenden Anmerkungen  aushaendigen .    I    will      to_you  the   corresponding  comments       pass_on          .Ich werde Ihnen die entsprechenden Anmerkungen am Dienstag aushaendigen   I    will   to_you the   corresponding  comments      on  Tuesday   pass_on
German	
  free	
  word	
  order
19I    will   to_you  the report   pass_onTo_you  will   I  the report   pass_onThe report  will   I  to_you  pass_onThe finite verb always appears in 2nd position, but Any constituent (not just the subject) can appear in the 1st position
German	
  verbs
20Ich  werde  Ihnen   den  Report  aushaendigen ,    I    will      to_you  the   report   pass_on          ,Main clause  damit    Sie   den  eventuell  uebernehmen  koennen  . so_that   you    it     perhaps   adopt               can          .Subordinate clause
Collins’	
  Mo0va0on
21Phrase-based models have an overly simplistic way of handling different word orders.We can describe the linguistic differences between different languages. Collins defines a set of 6 simple, linguistically motivated rules, and demonstrates that they result in significant translation improvements.
Collins’	
  Pre-­‐ordering	
  Model
22Ich  werde  Ihnen   den  Report  aushaendigen ,  damit    Sie   den  eventuell  uebernehmen  koennen  . (I will pass_on to_you the report, so_that you can adopt it perhaps .)Ich  werde  aushaendigen Ihnen   den  Report  ,  damit    Sie  koennen uebernehmen den  eventuell   . Step 1: Reorder the source language
Step 2: Apply the phrase-based machine translation pipeline                to the reordered input.
Example	
  Parse	
  Tree
23SPPER-SB IVFIN-HD willVPPPER-DA to_youNP-OAVVINF-HD pass_onART  theNN Report
Clause	
  Restructuring
24VP-OCPDS-OA den thatADJD-MO eventuell perhapsVVINF-HD uebernehmen adoptSVINF-HD koennen can...Rule 1:  Verbs are initial in VPs               Within a VP, move the head to the initial position
Clause	
  Restructuring
25S-MOKOUS-CP damit so-that...VP-OCVVINF-HD uebernehmen adoptPPER-SB Sie youVINF-HD koennen canRule 2:  Verbs follow complementizers               In a subordinated clause mote the head of the clause                       to follow the complementizer
Clause	
  Restructuring
26S-MOKOUS-CP damit so-that...VP-OCVVINF-HD uebernehmen adoptPPER-SB Sie youVMFIN-HD koennen canRule 3:  Move subject               The subject is moved to directly precede the head of                the clause
Clause	
  Restructuring
27SPPER-SB Wir wePTKVZ-SVP auf *PARTICLE*VVFIN-HD fordem acceptRule 4:  Particles               In verb particle constructions, the particle is moved                to precede the finite verbNP-OAART das theNN Praesidium presidency
Clause	
  Restructuring
28SPPER-SB Wir weVMFIN-HD konnten couldRule 5:  Infinitives               Infinitives are moved to directly follow the finite verb               within a clause
VVINF-HD einreichen submitPTK-NEG nicht notVP-OC...OOER-OA es it
Clause	
  Restructuring
29SPPER-SB Wir weVMFIN-HD konnten couldRule 6:  Negation               Negative particle is moved to directly follow the                finite verbPTK-NEG nicht notVP-OC...VVINF-HD einreichen submitOOER-OA es it
A	
  Less	
  Awful	
  German	
  Language
Mark TwainIch werde Ihnen den Report aushaendigen, damit Sie den  eventuell uebernehmen koennen.I will to_you the report pass_on, so_that you it perhaps adopt can.Ich werde aushaendigen Ihnen   den Report, damit Sie  koennen uebernehmen den eventuell.I will pass_on to_you the report, so_that you can adopt it perhaps .
Now that seems less like the ravings of a madman.
Experiments•Parallel	
  training	
  data:	
  Europarl	
  corpus	
  (751k sentence pairs, 15M German words, 16M English) •Parsed German training sentences  •Reordered the German training sentences with their 6 clause reordering rules •Trained a phrase-based model •Parsed and reordered the German test sentences •Translated them •Compared against the standard phrase-based model without parsing/reordering31
Bleu	
  score	
  increase
051015202530
BaselineReordered System
26.8
25.2Significant improvement at p<0.01 using the sign test
Human	
  Transla0on	
  Judgments•100	
  sentences	
  (10-­‐20	
  words	
  in	
  length)	
  •Two	
  annotators	
  •Judged	
  two	
  diﬀerent	
  versions	
  –	
  Baseline	
  system’s	
  transla0on	
  –	
  Reordering	
  system’s	
  transla0on	
  •Judgments:	
  Worse,	
  bemer	
  or	
  equal	
  •Sentences	
  were	
  chosen	
  at	
  random,	
  systems’	
  transla0ons	
  were	
  presented	
  in	
  random	
  order33
Human	
  Transla0on	
  Judgments
34+=–Annotator	
  140%40%20%Annotator	
  244%37%19%+ = reordered translation better – = baseline better = = equal
Examples
35ReferenceI think it is wrong in principle to have such measures in the European UnionReorderedI believe that it is wrong in principle to take such measures in the European UnionBaselineI believe that it is wrong in principle, such measure in the European Union to take.
Examples
36ReferenceThe current difficulties should encourage us to redouble our efforts to promote coorperation in the Euro-Mediterranean framework.BaselineThe current problems should spur us, our efforts to promote coorperation within the framework of the e-prozesses to be intensified.ReorderedThe current problems should spur us to intensify our efforts to promote cooperation within the framework of the e-prozesses.
Examples
37ReferenceTo go on subsidizing tobacco cultivation at the same time is a downright contridiction.BaselineAt the same time, continue to subsidize tobacco growing, it is quite schizophrenic. ReorderedAt the same time, to continue to subsidize tobacco growing is schizophrenic. 
Examples
38ReferenceWe have voted against the report by Mrs. Lalumiere for reasons that include the following: ReorderedWe have voted, amongst other things, for the following reasons against the report by Mrs. Lalumiere:BaselineWe have, among other things, for the following reasons against the report by Mrs. Lalumiere voted:
Discussion:	
  Clause	
  Restructuring•Are	
  you	
  convinced	
  that	
  German-­‐English	
  transla0on	
  has	
  improved?	
  •Do	
  you	
  think	
  that	
  this	
  is	
  a	
  good	
  ﬁt	
  for	
  phrase-­‐based	
  machine	
  transla0on?	
  •What	
  limita0ons	
  does	
  this	
  method	
  have?(Discuss	
  with	
  your	
  neighbor.)39
Limita0ons•Requires	
  a	
  parser	
  for	
  the	
  source	
  language	
  –	
  We	
  have	
  parsers	
  for	
  only	
  a	
  small	
  number	
  of	
  languages	
  	
  –	
  Penalizes	
  “low	
  resource	
  languages”	
  –	
  Fine	
  for	
  transla0ng	
  from	
  English	
  into	
  other	
  languages	
  •Involves	
  hand	
  crared	
  rules	
  •Removes	
  the	
  nice	
  language-­‐independent	
  quali0es	
  of	
  sta0s0cal	
  machine	
  transla0on
40
Learning	
  the	
  Rules	
  Automa0cally•Great	
  term	
  project	
  idea!	
  •“Improving	
  a	
  sta0s0cal	
  MT	
  system	
  with	
  automa0cally	
  learned	
  rewrite	
  pamerns”by	
  Fei	
  Xia	
  and	
  Michael	
  McCord	
  (Coling	
  2004)
41
Syntac0c	
  LMs•Our	
  goal	
  is	
  reorder	
  the	
  translated	
  phrases	
  so	
  that	
  they	
  are	
  gramma0cal	
  English	
  	
  •Isn’t	
  the	
  language	
  model	
  probability	
  supposed	
  to	
  do	
  that	
  already?	
  •Instead	
  of	
  an	
  n-­‐gram	
  model,	
  could	
  we	
  augment	
  the	
  LM	
  with	
  syntac0c	
  informa0on?
42
Right-corner Incremental Parsing
MotivationMachine TranslationIncremental ParsingIntegrationResultsAn Incremental Syntactic Language Model for Statistical Phrase-based Machine TranslationLane SchwartzTransform right-expanding sequences of constituentsinto left-expanding sequences of incomplete constituentsSVPPPNPFridayINonVPNPNNboardDTtheVBmeetsNPNNpresidentDTTheSNPFridayS/NPINonS/PPVPNNboardVP/NNDTtheVP/NPVBmeetsS/VPNPNNpresidentNP/NNDTTheSta0s0cal	
  parsing
43Problem: bottom up parsing requires whole sentenceWe need the LM to be able to score partial translations
One	
  possibility:	
  Incremental	
  parsing
44Right-corner Incremental Parsing
MotivationMachine TranslationIncremental ParsingIntegrationResultsAn Incremental Syntactic Language Model for Statistical Phrase-based Machine TranslationLane SchwartzTransform right-expanding sequences of constituentsinto left-expanding sequences of incomplete constituentsSVPPPNPFridayINonVPNPNNboardDTtheVBmeetsNPNNpresidentDTTheSNPFridayS/NPINonS/PPVPNNboardVP/NNDTtheVP/NPVBmeetsS/VPNPNNpresidentNP/NNDTThe
More	
  later•Next,	
  we	
  move	
  away	
  from	
  phrase-­‐based	
  MT	
  and	
  talk	
  synchronous	
  grammar	
  models	
  •Ques0ons	
  about	
  this	
  material?
45
Announcements•HW3	
  due	
  in	
  a	
  week	
  •Language	
  Research	
  project	
  guidelines	
  have	
  been	
  posted	
  (due	
  dates	
  TBD)	
  •Term	
  project	
  ideas
46
The	
  Awful	
  German	
  LanguageSome German words are so long that they have a perspective. Freundschaftsbezeigungen. Dilettantenaufdringlichkeiten.Stadtverordnetenversammlungen. These things are not words, they are alphabetical processions. And they are not rare; one can open a German newspaper at any time and see them marching majestically across the page–and if he has any imagination he can see the banners and hear the music, too.”“
Mark Twain
The	
  Awful	
  German	
  Language A dog is der Hund; now you put that dog in the genitive case, and is he the same dog he was before? No, sir; he is des Hundes; put him in the dative case and what is he? Why, he is dem Hund. Now you snatch him into the accusative case and how is it with him? Why, he is den Hunden. But suppose he happens to be twins and you have to pluralize him- what then? Why, they'll swat that twin dog around through the 4 cases until he'll think he's an entire international dog show. I don't like dogs, but I wouldn't treat a dog like that.”“
Mark Twain
The	
  Awful	
  German	
  Language The Germans have an inhuman way of cutting up their verbs. Now a verb has a hard time enough of it in this world when it's all together. It's downright inhuman to split it up. But that's just what those Germans do. They take part of a verb and put it down here, like a stake, and they take the other part of it and put it away over yonder like another stake, and between these two limits they just shovel in German.”“
Mark Twain
Introduction to Probability and Statistics
Machine TranslationLecture 2 Instructor: Chris Callison-Burch TAs: Mitchell Stern, Justin Chiu Website: mt-class.org/penn
1) Formulate a model of pairs of sentences.2) Learn an instance of the model from data.3) Use it to infer translations of new inputs.Last time ...
Why Probability?•Probability formalizes ...•the concept of models•the concept of data•the concept of learning•the concept of inference (prediction)Probability is expectation founded upon partial knowledge.

p(x|partial knowledge)“Partial knowledge” is an apt description ofwhat we know about language and translation!
Probability Models•Key components of a probability model•The space of events (Ω or 𝙎)•The assumptions about conditional independence / dependence among events•Functions assigning probability (density) to events•We will assume discrete distributions.
Events and Random Variables
X(!)=!⇢X(x)=(16ifx=1,2,3,4,5,60 otherwise⌦={1,2,3,4,5,6}
A random variable is a function from a random event from a set of possible outcomes (Ω) and a probability distribution (𝘱), a function from outcomes to probabilities.
Events and Random Variables⌦={1,2,3,4,5,6}
⇢Y(y)=(12ify=0,10 otherwiseY(!)=(0i f!2{2,4,6}1 otherwiseA random variable is a function from a random event from a set of possible outcomes (Ω) and a probability distribution (𝘱), a function from outcomes to probabilities.
What is our event space?What are our random variables?
Probability DistributionsA probability distribution (𝘱X) assigns probabilities tothe values of a random variable (X).Xx2X⇢X(x)=1⇢X(x) 08x2XThere are a couple of philosophically different waysto deﬁne probabilities, but we will give only the invariants in terms of random variables.Probability distributions of a random variable may be speciﬁed in a number of ways.
Specifying Distributions•Engineering/mathematical convenience•Important techniques in this course•Probability mass functions•Tables (“stupid multinomials”)•Log-linear parameterizations (maximum entropy, random ﬁeld, multinomial logistic regression)•Construct random variables from other r.v.’s with known distributions
Sampling Notation
Random variableDistributionParameterx=4⇥z+1.7y⇠Distribution(✓)VariableExpression
Sampling Notation
Random variableDistributionParameterx=4⇥z+1.7y⇠Distribution(✓)
Sampling Notationx=4⇥z+1.7y⇠Distribution(✓)y0=y⇥x
Multivariate r.v.’sProbability theory is particularly useful because it letsus reason about (cor)related and dependent events.Z=X(!)Y(!) A joint probability distribution is a probabilitydistribution over r.v.’s with the following form:Xx2X,y2Y⇢Z✓xy ◆=1⇢Z✓xy ◆ 08x2X,y2Y
X(!)=!⌦={1,2,3,4,5,6}
⌦={(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(4,1),(4,2),(4,3),(4,4),(4,5),(4,6),(5,1),(5,2),(5,3),(5,4),(5,5),(5,6),(6,1),(6,2),(6,3),(6,4),(6,5),(6,6),}
X(!)=!1Y(!)=!2⇢X,Y(x, y)=(136if (x, y)2⌦0 otherwise
X(!)=!⌦={1,2,3,4,5,6}
⌦={(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(4,1),(4,2),(4,3),(4,4),(4,5),(4,6),(5,1),(5,2),(5,3),(5,4),(5,5),(5,6),(6,1),(6,2),(6,3),(6,4),(6,5),(6,6),}
X(!)=!1Y(!)=!2⇢X,Y(x, y)=(x+y252if (x, y)2⌦0 otherwise
Marginal Probabilityp(X=x, Y=y)=⇢X(x, y)p(X=x)=Xy0=Yp(X=x, Y=y0)p(Y=y)=Xx0=Xp(X=x0,Y=y)⌦={(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(4,1),(4,2),(4,3),(4,4),(4,5),(4,6),(5,1),(5,2),(5,3),(5,4),(5,5),(5,6),(6,1),(6,2),(6,3),(6,4),(6,5),(6,6),}
p(X= 4) =Xy02[1,6]p(X=4,Y=y0)p(Y= 3) =Xx02[1,6]p(X=x0,Y= 3)

Convolutional  Layer
75
Convolutional  Layer
76
Max-­‐Pooling  Layer
77
Max-­‐Pooling  Layer
78
Convolutional  Neural  Network  (CNN)•Typical  layers  include:–Convolutional  layer–Max-­‐pooling  layer–Fully-­‐connected  (Linear)  layer–ReLUlayer  (or  some  other  nonlinear  activation  function)–Softmax•These  can  be  arranged  into  arbitrarily  deep  topologies
79
Architecture  #1:  LeNet-­‐5
Architecture  #2:  AlexNet
80Figure 2:An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilitiesbetween the two GPUs. One GPU runs the layer-parts at the top of the ﬁgure while the other runs the layer-partsat the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, andthe number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264–4096–4096–1000.neurons in a kernel map). The second convolutional layer takes as input the (response-normalizedand pooled) output of the ﬁrst convolutional layer and ﬁlters it with 256 kernels of size5⇥5⇥48.The third, fourth, and ﬁfth convolutional layers are connected to one another without any interveningpooling or normalization layers. The third convolutional layer has 384 kernels of size3⇥3⇥256connected to the (normalized, pooled) outputs of the second convolutional layer. The fourthconvolutional layer has 384 kernels of size3⇥3⇥192, and the ﬁfth convolutional layer has 256kernels of size3⇥3⇥192. The fully-connected layers have 4096 neurons each.4 Reducing OverﬁttingOur neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRCmake each training example impose 10 bits of constraint on the mapping from image to label, thisturns out to be insufﬁcient to learn so many parameters without considerable overﬁtting. Below, wedescribe the two primary ways in which we combat overﬁtting.4.1 Data AugmentationThe easiest and most common method to reduce overﬁtting on image data is to artiﬁcially enlargethe dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct formsof data augmentation, both of which allow transformed images to be produced from the originalimages with very little computation, so the transformed images do not need to be stored on disk.In our implementation, the transformed images are generated in Python code on the CPU while theGPU is training on the previous batch of images. So these data augmentation schemes are, in effect,computationally free.The ﬁrst form of data augmentation consists of generating image translations and horizontal reﬂec-tions. We do this by extracting random224⇥224patches (and their horizontal reﬂections) from the256⇥256images and training our network on these extracted patches4. This increases the size of ourtraining set by a factor of 2048, though the resulting training examples are, of course, highly inter-dependent. Without this scheme, our network suffers from substantial overﬁtting, which would haveforced us to use much smaller networks. At test time, the network makes a prediction by extractingﬁve224⇥224patches (the four corner patches and the center patch) as well as their horizontalreﬂections (hence ten patches in all), and averaging the predictions made by the network’s softmaxlayer on the ten patches.The second form of data augmentation consists of altering the intensities of the RGB channels intraining images. Speciﬁcally, we perform PCA on the set of RGB pixel values throughout theImageNet training set. To each training image, we add multiples of the found principal components,4This is the reason why the input images in Figure 2 are224⇥224⇥3-dimensional.5CNN  for  Image  Classification(Krizhevsky,  Sutskever&  Hinton,  2012)15.3%  error  on  ImageNetLSVRC-­‐2012  contestInput  image  (pixels)•Five  convolutional  layers  (w/max-­‐pooling)•Three  fully  connected  layers1000-­‐way  softmax
CNNs  for  Image  Recognition
81Lecture 7 -27 Jan 2016Fei-Fei Li & Andrej Karpathy & Justin JohnsonFei-Fei Li & Andrej Karpathy & Justin JohnsonLecture 7 -27 Jan 201678
(slide from Kaiming He’s recent presentation)Slide  from  KaimingHe
CNN  VISUALIZATIONS
83
3D  Visualization  of  CNNhttp://scs.ryerson.ca/~aharley/vis/conv/

Convolution  of  a  Color  Image
85Lecture 7 -27 Jan 2016Fei-Fei Li & Andrej Karpathy & Justin JohnsonFei-Fei Li & Andrej Karpathy & Justin JohnsonLecture 7 -27 Jan 201623A closer look at spatial dimensions:
3232
332x32x3 image5x5x3 filterconvolve (slide) over all spatial locationsactivation map
12828
Figure  from  Fei-­‐FeiLi  &  Andrej  Karpathy&  Justin  Johnson  (CS231N)  •Color  images  consist  of  3  floats  per  pixel  for  RGB  (red,  green  blue)  color  values•Convolution  must  also  be  3-­‐dimensional
Animation  of  3D  Convolution
86Figure  from  Fei-­‐FeiLi  &  Andrej  Karpathy&  Justin  Johnson  (CS231N)  http://cs231n.github.io/convolutional-­‐networks/

MNIST  Digit  Recognition  with  CNNs  (in  your  browser)
87https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html
Figure  from  Andrej  Karpathy
CNN  SummaryCNNs–Are  used  for  all  aspects  of  computer  vision,  and  have  won  numerous  pattern  recognition  competitions–Able  learn  interpretable  features  at  different  levels  of  abstraction–Typically,  consist  of  convolutionlayers,  poolinglayers,  nonlinearities,  and  fully  connected  layersOther  Resources:–Readings  on  course  website–Andrej  Karpathy,  CS231n  Noteshttp://cs231n.github.io/convolutional-­‐networks/88
