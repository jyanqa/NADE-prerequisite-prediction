NLP Neural Summarization Deep Learning Attention-based Summarization [Rush, Chopra, Weston 2015] Extracting Sentences and Words [Cheng and  Lapata  2016] NLP NLP Convolutional Neural Networks Deep Learning Cats’ Brains and CNNs With the exception of distinguishing cucumbers from deadly snakes, cats’ brains are really good at image classification.  Yann  LeCun  et al. (1998) found that a neural network that works like a cat’s visual cortex was good at digit classification.  When machines got powerful enough, it also turned out to be great at image classification . ( Krizhevsky  et al. 2012)   ( Don’t do this to your cat.  It can traumatize them.) Basic Idea of CNNs Input Hidden layer Basic Idea of CNNs Input Hidden layer Basic Idea of CNNs Input Hidden layer Basic Idea of CNNs Input Hidden layer CNN for Image Classification Convolutional Layer CNN for Image Classification Convolutional Layer CNN for Image Classification Convolutional Layer CNN for Image Classification Convolutional Layer CNN for Image Classification Convolutional Layer CNN for Image Classification Max Pooling CNN for Image Classification Max Pooling CNN for Image Classification Max Pooling CNN for Image Classification Max Pooling CNN for Image Classification http://deeplearning.net/tutorial/lenet.html [ LeCun ] CNN for Image Classification How good are CNNs?  “We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of  15.3% , compared to  26.2%  achieved by the second-best entry .” Krizhevsky   et al., 2012: ImageNet Classification with Deep Convolutional Neural  Networks Competition to classify photos from ImageNet,  http ://www.image-net.org/ CNN for Image Classification Learned Filters Zeiler  and Fergus 2014 CNN for Image Classification Zeiler  and Fergus 2014 CNN for Image Classification Zeiler  and Fergus 2014 Layer 4 Using CNNs for NLP Convolutional Neural Network for Paraphrase Identification  (Yin &  Schütze  2015) Summarization-based Video Caption via Deep Neural  Networks (Li et al. 2015) Question Answering over Freebase with Multi-Column Convolutional Neural Networks  (Dong et al. 2015) Convolutional Neural Network Architectures for Matching Natural Language  Sentences (Hu et al. 2015) Learning Semantic Representations Using Convolutional Neural Networks for Web  Search (Sheng et al. 2015) Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts (dos Santos &  Gatti  2014 ) Relation Extraction: Perspective from Convolutional Neural Networks (Nguyen &  Grishman   2015) Modeling Mention, Context and Entity with Neural Networks for Entity  Disambiguation (Sun et al. 2015) Modeling Interestingness with Deep Neural  Networks (Gao 2015) Example Application:  Paraphrase Detection (Yin &  Schütze  2015) NLP NLP Introduction to NLP NLP Tasks (2/2) Semantic Role Labeling [ A0  He ] [ AM-MOD  would ] [ AM-NEG   n't  ] [ V   accept  ] [ A1  anything of value ] from [ A2  those he was writing about ] .  V:  verb A0 :  acceptor   A1:  thing accepted  A2:  accepted-from  A3:  attribute  AM-MOD:  modal  AM-NEG:  negation  http://cogcomp.cs.illinois.edu/page/demo_view/SRL Coreference Resolution Barack Obama  visited China.  The US president  met with his Chinese counterpart. Cynthia  went to see  her aunt  at the hospital.  She  was scheduled for surgery on Monday. Because  he  was sick,  Michael  stayed home on Friday. Question Answering "The antagonist of Stevenson's Treasure Island." (Who is Long John Silver?)  http://blog.reddit.com/2011/02/ibm-watson-research-team-answers-your.html “Watson is powered by 10 racks of IBM Power 750 servers running Linux, and uses 15 terabytes of RAM, 2,880 processor cores and is capable of operating at 80 teraflops. Watson was written in mostly Java but also significant chunks of code are written C++ and Prolog, all components are deployed and integrated using UIMA.” Jeopardy Questions From the competition between the IBM Watson system and two human champions (Ken Jennings and Brad Rutter) Sample questions: On December 8, 2008 this national newspaper raised its newsstand price by 25 cents to $1 :  USA Today In 2010 this former first lady published the memoir "Spoken From the Heart" :  Laura Bush* This person is appointed by a testator to carry out the directions & requests in his will :  Executor* Familiarity is said to breed this, from the Latin for "Despise" :  Contempt* As of 2010, Croatia & Macedonia are candidates but this is the only former Yugoslav republic in the EU :  Slovenia The ancient "Lion of Nimrud" went missing from this city's national museum in 2003 (along with a lot of other stuff) :  Baghdad It's just a bloody nose! You don't have this hereditary disorder once endemic to European royalty :  Haemophilia It's Michelangelo's fresco on the wall of the Sistine Chapel, Depicting the saved and the damned :  The Last  Judgement She "Died in the church and was buried along with her name. Nobody came" :  Eleanor Rigby It's a 4-letter term for a summit; the first 3 letters mean a type of simian :  Apex A camel is a horse designed by this :  Committee Watson’s answers: 66 correct and 9 incorrect   (e.g., the one in the category “US Cities” about a city with two airports named after a World War II hero and a World War II battle) Watson's two day winning streak was $77,147. Ken Jennings ended with $24,000 and Brad Rutter with $21,600.   Watson donated $500,000 to both World Vision and World Community Grid charities from the $1,000,000 prize. http://www.quora.com/What-questions-were-asked-in-the-Jeopardy-episode-involving-Watson Sentiment Analysis “ I like  the camera  because I can edit images so easily, exactly as I do my  iPad . I have found that its difficult to frame a picture when there isn't a zoom function as with the  iPad . With this camera I can adjust my images by cropping as I did with my  iPad  but  better yet , this camera has a built in zoom. A stretch or pinch of the fingers bring in the subject closer or back out again. With this iPhone I can also, as I dido with my  iPad , enhance, crop, rotate, red eye reduce, and set a range of tints.  I am also quite impressed with  the quality of the images . Pretty darn good especially  better than I expected  for low light situations where I can use the built-in flash! Quite frankly  I was quite surprised  with these built in features . I also hope too experiment with and learn what HDR photography is. It's built into this iPhone and can be activated by a the touch of an icon. ” http://www.epinions.com/review/apple_iphone_5c_latest_model_16gb_graphite_unlocked_smartphone/content_640679317124   Machine Translation あけましておめでとうございます。  Happy New Year! Machine Translation Moses www.statmt.org 	Elephants  are social animals. They live with their families, give hugs and call each other by using their trunks as trumpets. They also might know how to help each other. 	In  a recent elephant study by researchers from the United States and Thailand, pairs of giant animals learned to work together to get some ears of corn. Other animals, especially some primates, are already known to work together to complete tasks, but now elephants have joined the club. Perhaps the finding is not too surprising: Scientists suspect that elephants, with their big brains and survival savvy, may be among the smartest animals on the planet. 	Joshua  Plotnik , who worked on the study, told Science News that the animals didn’t just learn a trick. Instead, the ways the elephants behaved show that they understand how working together brings benefits to everyone involved.  Plotnik  is a comparative psychologist now at the University of Cambridge in England. Psychology is the study of behaviors and mental processes, and comparative psychologists study how animals other than humans behave. 	Les  éléphants sont des animaux sociaux . Ils vivent avec leur famille, faire des câlins et appeler les uns les autres en utilisant leurs troncs trompettes. Ils pourraient également savoir comment aider les uns les autres. 	 Dans  une étude récente d'éléphants par des chercheurs des États-Unis et la Thaïlande, des paires d'animaux géants ont appris à travailler ensemble pour obtenir des épis de maïs . D'autres animaux, en particulier des primates, sont déjà connus pour travailler ensemble pour accomplir des tâches, mais maintenant, les éléphants ont rejoint le club. Peut-être le résultat n'est pas trop surprenant: Les scientifiques soupçonnent que les éléphants, avec leurs gros cerveaux et de bon sens de survie, peut-être parmi les plus intelligents des animaux sur la planète. 	Joshua  Plotnick , qui a travaillé sur l'étude, dit Nouvelles de la Science que les animaux n'ont pas seulement appris un truc. Au lieu de cela, les moyens les éléphants se comportent montrent qu'ils comprennent comment travailler ensemble apporte des avantages à toutes les personnes impliquées.  Plotnik  est un psychologue comparative maintenant à l'Université de Cambridge en Angleterre.  La psychologie est l'étude des comportements et des processus mentaux , et étude comparative des psychologues comment les animaux autres que les humains se comportent. https://student.societyforscience.org/article/theres-no-i-elephant 	Elephants  are social animals. They live with their families, give hugs and call each other by using their trunks as trumpets. They also might know how to help each other. 	In  a recent elephant study by researchers from the United States and Thailand, pairs of giant animals learned to work together to get some ears of corn. Other animals, especially some primates, are already known to work together to complete tasks, but now elephants have joined the club. Perhaps the finding is not too surprising: Scientists suspect that elephants, with their big brains and survival savvy, may be among the smartest animals on the planet. 	Joshua  Plotnik , who worked on the study, told Science News that the animals didn’t just learn a trick. Instead, the ways the elephants behaved show that they understand how working together brings benefits to everyone involved.  Plotnik  is a comparative psychologist now at the University of Cambridge in England. Psychology is the study of behaviors and mental processes, and comparative psychologists study how animals other than humans behave. 	 Les  éléphants sont des animaux sociaux. Ils  vivent  avec leur famille,  faire  des câlins et  appeler  les uns les autres en utilisant leurs troncs trompettes. Ils pourraient également savoir comment aider les uns les autres. 	Dans  une étude récente d'éléphants par des chercheurs des États-Unis et la Thaïlande, des paires d'animaux géants ont appris à travailler ensemble pour obtenir des épis de maïs. D'autres animaux, en particulier des primates, sont déjà connus pour travailler ensemble pour accomplir des tâches, mais maintenant, les éléphants ont rejoint le club. Peut-être le résultat n'est pas trop surprenant: Les scientifiques soupçonnent que  les éléphants , avec leurs gros cerveaux et de bon sens de survie,  peut-être  parmi les plus intelligents des animaux sur la planète. 	Joshua  Plotnick , qui a travaillé sur l'étude, dit  Nouvelles de la Science  que les animaux n'ont pas seulement appris un truc. Au lieu de cela, les moyens les éléphants se comportent montrent qu'ils comprennent comment travailler ensemble apporte des avantages à toutes  les personnes  impliquées.  Plotnik  est un psychologue  comparative  maintenant à l'Université de Cambridge en Angleterre. La psychologie est l'étude des comportements et des processus mentaux, et  étude comparative des psychologues  comment les animaux autres que les humains se comportent. https://student.societyforscience.org/article/theres-no-i-elephant Machine Translation Noisy channel model (“Chinese Whispers”) e f e’ E   F F   E encoder decoder e’ = argmax P(e|f) = argmax P(f|e) P(e) e e translation model language model Machine Translation IBM Method IS THIS YOUR FAVORITE           PLAY ?    IS THIS YOUR FAVORITE PLAY  PLAY   PLAY  ? ** ** ** THIS IS YOUR PLAY  PLAY   PLAY  FAVORITE ? EST-CE QUE C’ EST VOTRE PIECE DE THEATRE PREFEREE ? Language model Translation model Text Summarization Health Benefits Eating a diet rich in vegetables and fruits as part of an overall healthy diet may reduce risk for heart disease, including heart attack and stroke. Eating a diet rich in some vegetables and fruits as part of an overall healthy diet may protect against certain types of cancers. Diets rich in foods containing fiber, such as some vegetables and fruits, may reduce the risk of heart disease, obesity, and type 2 diabetes. Eating vegetables and fruits rich in potassium as part of an overall healthy diet may lower blood pressure, and may also reduce the risk of developing kidney stones and help to decrease bone loss. Eating foods such as vegetables that are lower in calories per cup instead of some other higher-calorie food may be useful in helping to lower calorie intake. Nutrients Most vegetables are naturally low in fat and calories. None have cholesterol. (Sauces or seasonings may add fat, calories, or cholesterol.) Vegetables are important sources of many nutrients, including potassium, dietary fiber, folate (folic acid), vitamin A, and vitamin C. Diets rich in potassium may help to maintain healthy blood pressure. Vegetable sources of potassium include sweet potatoes, white potatoes, white beans, tomato products (paste, sauce, and juice), beet greens, soybeans, lima beans, spinach, lentils, and kidney beans. Dietary fiber from vegetables, as part of an overall healthy diet, helps reduce blood cholesterol levels and may lower risk of heart disease. Fiber is important for proper bowel function. It helps reduce constipation and diverticulosis. Fiber-containing foods such as vegetables help provide a feeling of fullness with fewer calories. Folate (folic acid) helps the body form red blood cells. Women of childbearing age who may become pregnant should consume adequate folate from foods, and in addition 400 mcg of synthetic folic acid from fortified foods or supplements. This reduces the risk of neural tube defects,  spina  bifida, and anencephaly during fetal development. Vitamin A keeps eyes and skin healthy and helps to protect against infections. Vitamin C helps heal cuts and wounds and keeps teeth and gums healthy. Vitamin C aids in iron absorption. Summary Eating vegetables is healthy. http://www2.research.att.com/~ttsweb/tts/demo.php Text to Speech Text to Speech www.ivona.com Entailment and Paraphrasing Ido  Dagan, Oren Glickman and Bernardo  Magnini . The PASCAL  Recognising  Textual Entailment Challenge Discourse Analysis Anaphoric relations: 1. Mary helped Peter get out of the car. He thanked her. 2. Mary helped the other passenger out of the car.     The man had asked her for help because of his foot injury. Tom appeared on the sidewalk with a bucket of whitewash and a long-handled brush. He surveyed the fence, and all gladness left him and a deep melancholy settled down upon his spirit. (Tom Sawyer) Dialogue Systems I would like to make a reservation at Sorrento. For when? 8 pm Friday night. We only have availability for 7 pm and 10 pm. Sorry, these don't work for me. NLP NLP Introduction to NLP NLG Systems FOG FOG (Goldberg et al. 1994) Weather forecast reports for the Canadian Weather Service Input Numerical simulation data annotated by humans Narrative Science https://www.youtube.com/watch?v=i6jx42dMyxQ Arria /data2text Automated Insights https://www.youtube.com/watch?v=EEczLkSTSrQ CoGenTex  Chart Explainer http://www.cogentex.com/products/chartex/faq/index.html PlanDoc Function: Produces a report describing the simulation options that an engineer has explored Input A simulation log file Developers Bellcore  and Columbia University Input for  PlanDoc RUNID fiberall FIBER 6/19/93 act yes FA 1301 2 1995 FA 1201 2 1995 FA 1401 2 1995 FA 1501 2 1995 ANF co 1103 2 1995 48 ANF 1201 1301 2 1995 24 ANF 1401 1501 2 1995 24 END. 856.0 670.2 Output This saved fiber refinement includes all DLC changes in Run-ID ALLDLC. RUN-ID FIBERALL demanded that PLAN activate fiber for CSAs 1201, 1301, 1401 and 1501 in 1995 Q2. It requested the placement of a 48-fiber cable from the CO to section 1103 and the placement of 24-fiber cables from section 1201 to section 1301 and from section 1401 to section 1501 in the second quarter of 1995. For this refinement, the resulting 20 year route PWE was $856.00K, a $64.11K savings over the BASE plan and the resulting 5 year IFC was $670.20K, a $60.55K savings over the BASE plan. Halogen:  Reranking Example from Irene  Langkilde -Geary’s thesis NLP NLP Introduction to NLP Information Retrieval Introduction People search the Web daily Search engines Google Bing Baidu Yandex Information Retrieval is about search engines Yahoo Search Amazon Search Library of Congress Search Examples of Search Engines Conventional (library catalog) Search by keyword, title, author, etc. Text-based (Lexis-Nexis, Google, Yahoo!) Search by keywords. Limited search using queries in natural language. Image-based shapes, colors, keywords Question answering systems (ask.com) Search in (restricted) natural language Clustering systems ( Vivísimo ,  Clusty ) Research systems (Lemur,  Nutch ) Sample Queries How to get rid of stretch marks Dodge Kourtney Kardashian How many calories are in pumpkn pie Angelina Jolie and Brad Pitt How to vote Derek Jeter Interstellar trailer What is Ebola? https://www.google.com/trends/topcharts The size of the World Wide Web The size of the indexed world wide web pages (by Nov 2015) Indexed by Google: about 48 Billion pages Indexed by Bing: about 25 Billion pages http://www.worldwidewebsize.com/   Web Statistics Twitter hits 400 million tweets per day June, 2012. Dick  Costolo , CEO at Twitter Over 2.5 billion photos uploaded to Facebook each month (2010) blog.facebook.com Google’s clusters process a total of more than 20 petabytes of data per day.  2008. Jeffrey Dean from Google Challenges Dynamically generated content New pages get added all the time The size of the blogosphere doubles every 6 months Characteristics of User Queries Sessions users revisit their queries Very short queries typically 2 words long A large number of typos A small number of popular queries A long tail of infrequent ones Almost no use of advanced query operators with the exception of double quotes Information Retrieval Baseline Process Given a collection of documents And a user’s query Find the most relevant documents Key Terms Used in IR Query a representation of what the user is looking for - can be a list of words or a phrase. Document an information entity that the user wants to retrieve Collection a set of documents Index a representation of information that makes querying easier Term word or concept that appears in a document or a query Documents Not just printed paper Can be records, pages, sites, images, people, movies Document encoding (Unicode) Document representation Document preprocessing (e.g., removing metadata) Words, terms, types, tokens NLP Information Retrieval Models of Information Retrieval Sample queries (from Excite) In what year did baseball become an offical sport? play station codes . com birth control and depression government "WorkAbility I"+conference kitchen appliances where can I find a chines rosewood tiger electronics 58 Plymouth Fury How does the character Seyavash in Ferdowsi's Shahnameh exhibit characteristics of a hero? emeril Lagasse Hubble M.S Subalaksmi running Key Terms Used in IR QUERY: a representation of what the user is looking for - can be a list of words or a phrase. DOCUMENT: an information entity that the user wants to retrieve COLLECTION: a set of documents INDEX: a representation of information that makes querying easier TERM: word or concept that appears in a document or a query Mappings and abstractions Reality Data Information need Query From Robert Korfhage’s book Search Engine Architecture Decide what to index Collect it Index it (efficiently) Keep the index up to date Provide user-friendly query facilities Search Engine Architecture Documents Not just printed paper Can be records, pages, sites, images, people, movies Document encoding (Unicode) Document representation Document preprocessing (e.g., removing metadata) Words, terms, types, tokens Sample query sessions (from AOL) toley spies grames tolley spies games totally spies games tajmahal restaurant brooklyn ny taj mahal restaurant brooklyn ny taj mahal restaurant brooklyn ny 11209 do you love me like you say do you love me like you say lyrics do you love me like you say lyrics marvin gaye Characteristics of user queries Sessions users revisit their queries Very short queries typically 2 words long A large number of typos A small number of popular queries A long tail of infrequent ones Almost no use of advanced query operators with the exception of double quotes Queries as documents Advantages: Mathematically easier to manage Problems: Different lengths Syntactic differences Repetitions of words (or lack thereof) Document representations Term-document matrix (m  x  n) Document-document matrix (n  x  n) Typical example in a medium-sized collection: 3,000,000 documents (n) with 50,000 terms (m) Typical example on the Web: n=30,000,000,000, m=1,000,000 Boolean vs. integer-valued matrices Storage issues Imagine a medium-sized collection with n=3,000,000 and m=50,000 How large a term-document matrix will be needed? Is there any way to do better? Any heuristic? Tokenizing text (CNN)  -- A tropical storm has strengthened into Hurricane Leslie in the Atlantic Ocean, forecasters said Wednesday. The slow-moving storm could affect Bermuda this weekend, according to the National Hurricane Center in Miami. The Category 1 hurricane was churning Wednesday afternoon about 465 miles (750 kilometers) south-southeast of the British territory and moving north at 2 mph (4  kph ), the hurricane center said. http://www.cnn.com/2012/09/05/world/americas/bermuda-hurricane-leslie/index.html   Inverted index Instead of an incidence vector, use a posting table CLEVELAND:  D1, D2, D6 OHIO: D1, D5, D6, D7 Use linked lists to be able to insert new document postings in order and to remove existing postings. Can be used to compute document frequency Keep everything sorted! This gives you a logarithmic improvement in access. Basic operations on inverted indexes Conjunction (AND) iterative merge of the two postings: O( x+y ) Disjunction (OR) very similar Negation (NOT) can we still do it in O( x+y )?  Example: VERMONT AND NOT MASSACHUSETTS Example: MASSACHUSETTS OR NOT VERMONT Recursive operations Optimization start with the smallest sets Major IR models Boolean Vector Probabilistic Language modeling Fuzzy retrieval Latent semantic indexing Information Retrieval The Boolean Model The Boolean model x w y z D 1 D 2 Venn diagrams Boolean queries Operators: AND, OR, NOT, parentheses Example: CLEVELAND AND NOT OHIO (MICHIGAN AND INDIANA) OR (TEXAS AND OKLAHOMA) Ambiguous uses of AND and OR in human language Exclusive vs. inclusive OR Restrictive operator: AND or OR? Canonical forms of queries De Morgan’s Laws:  NOT (A AND B) = (NOT A) OR (NOT B) NOT (A OR B) = (NOT A) AND (NOT B) Normal forms Conjunctive normal form (CNF) Disjunctive normal form (DNF)  Some people swear by CNF - why? Evaluating Boolean queries Incidence vectors: CLEVELAND: 1100010 OHIO: 1000111 Examples: CLEVELAND AND OHIO CLEVELAND AND NOT OHIO CLEVALAND OR OHIO Information Retrieval Indexing and Ranking (1) Revisit: What Is Text Retrieval (TR)? There exists a collection of text documents User gives a query to express the information need A retrieval system returns relevant documents to users More often called “information retrieval” (IR) , but IR is actually much broader May include non-textual information May include text categorization or summarization… Known as “search technology” in industry  Typical IR System Architecture User query judgments documents results Query Rep Doc Rep Ranking Feedback INDEXING SEARCHING QUERY MODIFICATION INTERFACE Text Retrieval vs. Database Retrieval Database retrieval – library search system Text retrieval – web search engine Information Unstructured/free text vs. structured data Ambiguous vs. well-defined semantics Query  Ambiguous vs. well-defined semantics Incomplete vs. complete specification Answers Relevant documents vs. matched records TR is an  empirically  defined problem! Text Retrieval Is Hard! Under/over-specified query Ambiguous: “buying CDs” (money or music?)  Incomplete: what kind of CDs? What if “CD” is never mentioned in document?  Vague semantics of documents Ambiguity: e.g., word-sense, structural Incomplete: Inferences required H ard even for people!  80% agreement in human judgments Text Retrieval Is “Easy”! Text retrieval  CAN be easy in a particular case Ambiguity in query/document is RELATIVE to the database So, if the query is SPECIFIC enough, just one keyword may get all the relevant documents PERCEIVED text retrieval performance is usually better than the actual performance Users can NOT judge the completeness of an answer History of TR on One Slide Birth of TR 1945: V.  Bush’s article “As we may think” 1957:  H. P. Luhn’s idea of word counting and matching Indexing & Evaluation Methodology (1960’s) Smart system (G. Salton’s group) Cranfield  test collection (C.  Cleverdon’s  group) Indexing: automatic can be as good as manual  TR Models (1970’s & 1980’s) … Large-scale Evaluation & Applications (1990’s-Present) TREC (D. Harman & E. Voorhees, NIST) Web search (Google, Yahoo!, Bing) PubMed (Biomedical Literature Search) Short vs. Long Term Info Need Short-term information need (Ad hoc retrieval) “Temporary need”, e.g., info about used cars Information source is relatively static  User “pulls” information Application example: library search, Web search Long-term information need (Filtering) “Stable need”, e.g., new data mining algorithms Information source is dynamic System “pushes” information to user Applications:  news filter Importance of Ad hoc Retrieval Directly manages any existing large collection of information There are many  many  “ad hoc” information needs A long-term information need can be satisfied through frequent ad hoc retrieval Basic techniques of ad hoc retrieval can be used for filtering and other “non-retrieval” tasks, such as automatic summarization.  Formal Formulation of TR Vocabulary V={w 1 , w 2 , …,  w N } of language Query q = q 1 ,…, q m ,  where q i    V Document d i  = d i1 ,…, d iL ,  where  d ij     V Collection C= {d 1 , …,  d k } Set of relevant documents R(q)  C Generally unknown and user-dependent Query is a “hint” on which doc is in R(q) Task =  compute R’(q), an “approximate R(q)” Because we never know true R(q)… Formal Formulation of TR + + + + - - - - - - - - - - - - - - + - - R’(q) True R(q) + + + + - - + C d i Computing R(q) Strategy 1: Document selection R(q)={ d  C|f ( d,q )=1}, where f( d,q )   {0,1} is an indicator function or classifier System must decide if a doc is relevant or not (“absolute relevance”) Strategy 2: Document ranking R(q) = { d  C|f ( d,q )>  }, where f( d,q )    is a relevance measure function;   is a cutoff System must decide if one doc is more likely to be relevant than another (“relative relevance”) Document Selection vs. Ranking + + + + - - - - - - - - - - - - - - + - - Doc Selection f(d,q)=? - - - Doc Ranking f(d,q)=? 1 0 0.98 d 1  + 0.95 d 2  + 0.83 d 3  - 0.80 d 4  + 0.76 d 5  - 0.56 d 6  - 0.34 d 7  - 0.21 d 8  + 0.21 d 9  - R’(q) True R(q) ¬ R ’(q) Problems of Doc Selection The classifier is unlikely accurate “Over-constrained” query (terms are too specific): no relevant documents found “Under-constrained” query (terms are too general): over delivery It is extremely hard to find the right position between these two extremes Even if it is accurate, all relevant documents are not equally relevant Relevance is a matter of degree! Ranking is often preferred Relevance is a matter of degree A user can stop browsing anywhere, so the boundary is controlled by the user High recall users would view more items High precision users would view only a few Theoretical justification: Probability Ranking Principle  [Robertson 77] Probability Ranking Principle [Robertson 77] As stated by Cooper Robertson provides two formal justifications Assumptions: Independent relevance and sequential browsing (not necessarily all hold in reality) “If a reference retrieval system’s response to each request is a ranking of the documents in the collections in order of decreasing probability of usefulness to the user who submitted the request, where the probabilities are estimated as accurately a possible on the basis of whatever data made available to the system for this purpose, then the overall effectiveness of the system to its users  will be the best that is obtainable on the basis of that data.” According to the PRP, all we need is    “A relevance measure function f”   which satisfies  For all q, d 1 , d 2 ,   f(q,d 1 ) >  f(q,d 2 )  iff    p(Relevance|q,d 1 ) > p(Relevance|q,d 2 ) We will talk about many different ranking methods later… Next Topic What is text retrieval (TR) ? Document selection vs. document ranking How do we compare TR systems? Indexing Evaluation Criteria Effectiveness/Accuracy Precision, Recall Efficiency Space and time complexity  Usability How useful for real user tasks? Methodology: Cranfield Tradition Laboratory testing of system components Precision, Recall Comparative testing Test collections Set of documents Set of questions Relevance judgments Evaluation: Document Selection vs. Ranking + + + + - - - - - - - - - - - - - - + - - Doc Selection f(d,q)=? - - - 1 0 R’(q) True R(q) This is easy: just count the  +  and  –  in  R’( q)  and  ¬ R’(q ) ¬ R ’(q) The Contingency Table Relevant Retrieved Irrelevant Retrieved Irrelevant Rejected Relevant Rejected Relevant Not relevant Retrieved Not Retrieved Doc Action R’(q) ¬ R ’(q) + - Evaluation: Document Selection vs.  Ranking + + + + - - - - - - - - - - - - - - + - - Doc Ranking f(d,q)=? 0.98 d 1  + 0.95 d 2  + 0.83 d 3  - 0.80 d 4  + 0.76 d 5  - 0.56 d 6  - 0.34 d 7  - 0.21 d 8  + 0.21 d 9  - True R(q) Ranking is much harder to evaluate… How to measure ranking quality? Compute the precision at  every recall point (the position where each relevant document is retrieved) d 1   + d 2   + d 3   - d 4   + d 5   - d 6   - d 7   - d 8   + d 9   - Recall =  0.25 , Precision =  1 Recall =  0.5 , Precision =  1 Not a recall point, no calculation Recall  =  0.75 ,  Precision =  0.75   Not a recall point, no calculation Recall =  1 , Precision =  0.5                    When computing MAP, Precision =  0 …… …… this relevant document is  never retrieved  ( not in R’(q))..  How to measure the goodness of a ranking? Compute the precision at  every recall point Plot a precision-recall (PR) curve precision recall x x x x precision recall x x x x Which is better? Summarize a Ranking: MAP Given that n docs are retrieved Compute the precision (at rank) where each (new) relevant document is retrieved => p(1),…,p(k), if we have k rel. docs E.g., if the first rel. doc is at the 2 nd  rank, then p(1)=1/2.  If a relevant document never gets retrieved, we assume the precision corresponding to that rel. doc to be zero  Compute the average over all the relevant documents Average precision = (p(1)+…p(k))/k This gives us an average precision, which captures both precision and recall and is sensitive to the rank of each relevant document Mean Average Precision (MAP)  MAP = arithmetic mean average precision over a set of queries gMAP = geometric mean average precision over a set of queries (more affected by difficult queries) Computing MAP d 1   + d 2   + d 3   - d 4   + d 5   - d 6   - d 7   - d 8   + d 9   - 1 st  rel. doc, Precision =  1 2 nd  rel. doc, Precision =  1 3 rd  rel. doc ,  Precision =  0.75   4 th  rel. doc, Precision =  0.5                     Precision =  0 …… …… this relevant document is  never retrieved  ( not in R’(q))..  MAP = (1 + 1 + 0.75 + 0) / 4 = 0.6875  There are 4 relevant doc in total Summarize a Ranking: DCG What if relevance judgments are on a scale of [1,r]? r>2 Cumulative Gain (CG) at rank n Let the ratings of the n documents be r 1 , r 2 , … r n  (in ranked order) CG = r 1 +r 2 +… r n Discounted Cumulative Gain (DCG) at rank n DCG = r 1  + r 2 /log 2 2 + r 3 /log 2 3 + …  r n /log 2 n We may use any base for the logarithm, e.g., base=b  For rank positions above b, do not discount Summarize a Ranking: NDCG Normalized  Discounted Cumulative  Gain (NDCG) at rank n Normalize DCG at rank n by the DCG value at rank n of the ideal ranking The ideal ranking would first return the documents with the highest relevance level, then the next highest relevance level,  etc. Compute the precision (at rank) where each  (new) relevant document is retrieved => p(1),…,p(k), if we have k rel. docs NDCG is now quite popular in evaluating Web search Other Measures Precision at k documents (e.g., prec@10doc): more meaningful than MAP (why?) also called breakeven precision when k is the same as the number of relevant documents Mean Reciprocal Rank (MRR):  Same as MAP  when there’s only 1 relevant document Reciprocal Rank = 1/Rank-of-the-relevant-doc F-Measure (F1): harmonic mean of precision and recall P: precision R: recall : parameter Precision-Recall  Curve Mean Avg. Precision (MAP) Recall=3212/4728 Breakeven Precision  (precision when  precision=recall ) Out of 4728 rel docs,  we’ve got 3212 D1 + D2 + D3 – D4 – D5 + D6 - Total #  rel  docs = 4 System returns 6 docs Average Prec = (1/1+2/2+3/5+0)/4 about 5.5 docs in the top 10 docs are relevant Precision@10docs Typical TREC Evaluation Result What Query Averaging Hides Slide from Doug  Oard’s  presentation, originally from Ellen Voorhees’ presentation Next Topic What is text retrieval (TR) ? Document selection vs. document ranking How do we compare TR systems? Indexing Information Retrieval Indexing and Ranking (2) Typical IR System Architecture User query judgments documents results Query Rep Doc Rep Ranking Feedback INDEXING SEARCHING QUERY MODIFICATION INTERFACE Let’s Start from Here… query documents results Query Rep Doc Rep Ranking INDEXING SEARCHING Vector representation is powerful! Revisit: Text Representation/Indexing Making it easier to match a query with a document Query and document should be represented using the same units/terms Controlled vocabulary vs. full text indexing Full-text indexing is more practically useful and has proven to be as effective as manual indexing with controlled vocabulary Handling large collections Life is good when every document is mapped into a vector of words, but … Consider  N  = 1 million documents, each with about 1000 words. Avg. 6 bytes/word including spaces/punctuation  6GB of data in the documents. Say there are  M  = 500K  distinct  terms among these. Storage issue 500K x 1M matrix has half-a-trillion elements. 4 bytes for an integer 500K x 1M x 4 = 2T (your laptop would fail) 500K x 100B x 4 = 2*10 5  T (challenging even for Google) But it has no more than one billion positive numbers. matrix is extremely sparse. 1000 x 1M x 4 = 4G What’s a better representation? Another Motivation… Let’s look at the simplest case ( boolean  retrieval) Query = “school” + “information” The only thing we need to do is to return all the documents containing both the term “school” and the term “information” Simple strategy: Scan each document and output anyone who contains both words. Can we afford to scan all the documents for each query? Any faster way to do it? Indexing Indexing = Convert documents to data structures that enable fast search  Inverted index is the dominating indexing method (used by all search engines) Other indices (e.g., document index) may be needed for feedback Instead of an incidence vector, use a posting table CLEVELAND:  D1, D2, D6 OHIO: D1, D5, D6, D7 Use linked lists to be able to insert new document postings in order and to remove existing postings. More efficient than scanning docs (why?) Inverted  iIndex Inverted Index Fast access to all docs containing a given term (along with frequency and position information)  For each term, we get a list of tuples  ( docID ,  freq ,  pos ). Given a query, we can fetch the lists for all query terms and work on the involved documents. Boolean query:  set operation Natural language query: term weight summing  Keep everything sorted! This gives you a logarithmic improvement in access. Inverted index - example For each term  t , we must store a list of all documents that contain  t . Identify each by a  docID , a document serial number Dictionary Postings Posting Brutus Calpurnia Caesar 2 31 54 101 - From Chris Manning’s slides Inverted Index - Example This is a sample  document with one sample sentence Doc 1 This is another  sample document Doc 2 Dictionary Postings - From ChengXiang Zhai’s slides Conjunction (AND) – iterative merge of the two postings: O( x+y ) Disjunction (OR) – very similar Negation (NOT) – can we still do it in O( x+y )?  Example: MICHIGAN AND NOT OHIO Example: MICHIGAN OR NOT OHIO Recursive operations Optimization: start with the smallest sets Basic operations on inverted indexes Data structures for inverted index Dictionary: modest size Needs fast random access Preferred to be in memory Hash table, B-tree, trie, … Postings: huge Sequential access is expected  Can stay on disk May contain docID, term freq., term pos, etc Compression is desirable Constructing inverted index The main difficulty is to build a huge index with limited memory Memory-based methods: not usable for large collections  Sort-based methods:  Step 1: collect local (termID, docID, freq) tuples Step 2: sort local tuples (to make “runs”) Step 3: pair-wise merge runs Step 4: Output inverted file Sort-based inversion ... Term Lexicon: the 1 cold 2 days 3 a 4 ... DocID Lexicon: doc1 1 doc2 2 doc3 3 ... doc1 doc1 doc300 Searching Given a query, how to score documents efficiently? Boolean query Fetch the inverted list for all query terms Perform set operations to get the subset of docs that satisfy the Boolean condition E.g.,  Q1=“info” AND “security” , Q2=“info” OR “security” info: d1, d2, d3, d4 security: d2, d4, d6 Results: {d2,d4} (Q1) {d1,d2,d3,d4,d6} (Q2) Ranking Documents Assumption : score ( d,q )=f[g(w(d,q,t 1 ),…w( d,q,t n )), w(d),w(q)], where,  t i ’s  are the matched terms Maintain a score accumulator for each doc to compute function g For each query term  t i Fetch the inverted list {(d 1 ,f 1 ),…,( d n ,f n )} For each entry ( d j ,f j ), compute  w( d j ,q,t i ), and update score accumulator for doc d i Adjust the score to compute f, and sort  Ranking Documents: Example S( d,q )=g(t 1 )+…+g( t n ) [sum of  freq  of matched terms ] Query  = “info security ”   S(d, q) = g(“info”) + g(“security”) Info: (d1, 3), (d2, 4), (d3, 1), (d4, 5) Security: (d2, 3), (d4,1), (d5, 3) Accumulators:   d1      d2      d3     d4      d5                             0         0        0       0        0         (d1,3)  =>    3           0        0       0        0         (d2,4)  =>   3          4         0       0        0         (d3,1)  =>   3         4         1        0        0         (d4,5)  =>   3         4        1        5         0         (d2,3)  =>   3          7         1       5        0         (d4,1)  =>   3         7        1        6         0         (d5,3)  =>   3         7        1        6         3 info security Further Improving Efficiency Keep only the most promising accumulators Sort the inverted list in decreasing order of weights and fetch only N entries with the highest weights Pre-compute as much as possible  Scaling up to the Web-scale (more about this later) Inverted index compression Compress the postings Observations Inverted list is sorted (e.g., by docid or termfq) Small numbers tend to occur more frequently Implications “d-gap” (store difference): d1, d2-d1, d3-d2-d1,… Exploit skewed frequency distribution: fewer bits for small (high frequency) integers  Binary code, unary code,  -code, -code   Integer compression In general, to exploit skewed distribution Binary: equal-length coding Unary: x 1  is coded as x-1 one bits followed by 0, e.g., 3=> 110; 5=>11110 -code: x=> unary code for 1+log x followed by  uniform code for x-2  log x  in log x  bits, e.g., 3=>101, 5=>11001 -code: same as -code ,but replace the unary prefix with -code. E.g., 3=>1001, 5=>10101 What You Should Know How TR  is different from DB retrieval Why ranking is generally preferred to document selection (justified by PRP) How to compute the major evaluation measure (precision, recall, MAP, NDCG, F1) What is an inverted index Why does an inverted index help make search fast How to construct a large inverted index How  to compress an  index (Optional) Phrase-based Queries Examples “ New   York   City ” “Ann Arbor” “Barack Obama” We don’t want to match York  is a  city  in  New  Hampshire Positional Indexing Keep track of all words and their positions in the documents To find a multi-word phrase, look for the matching words appearing next to each other Document Ranking Compute the similarity between the query and each of the documents Use cosine similarity Use TF*IDF weighting Return the top K matches to the user Typical IR System Architecture User query judgments documents results Query Rep Doc Rep Ranking Feedback INDEXING SEARCHING QUERY MODIFICATION INTERFACE The Basic Question of Ranking Given a query, how do we know if document A is  more  relevant than B? Document Ranking + + + + - - - - - - - - - - - - - - + - - 0.98 d 1  + 0.95 d 2  + 0.83 d 3  - 0.80 d 4  + 0.76 d 5  - 0.56 d 6  - 0.34 d 7  - 0.21 d 8  + 0.21 d 9  - True R(q) Retrieval function f( d,q )=? Measures the degree of relevance Relevance = Similarity Assumptions Query and document are represented in a similar way A query can be regarded as a “document” Relevance(d, q)   similarity(d, q) R’(q) = { d  C  | f( d,q )>  }, f( q,d )=  (Rep(q), Rep(d))   Key issues How to represent a query/document? How to define the similarity measure   ? Revisit: Vector Space Model Represent a doc/query by a term vector Term:  basic concept , e.g., word or phrase Each term defines one dimension N terms define a high-dimensional space Element of vector corresponds to term weight E.g., d=(x 1 ,…, x N ), x i  is “ importance ” of term i Measure relevance by the similarity/distance (e.g., the cosine similarity) between the query vector and document vector in the vector space VS Model: illustration What the VS model doesn’t say How to define/select the “basic concept” Concepts are assumed to be orthogonal We talked about how to select index terms How to assign weights Weight in query indicates importance of term Weight in doc indicates how well the term characterizes the document We talked about simple presence/absence How to define the similarity/distance measure What’s a good “basic concept”? Orthogonal Linearly independent basis vectors “Non-overlapping” in meaning No ambiguity Weights can be assigned automatically and hopefully accurately Many possibilities: Words, stemmed words, phrases, “latent concept”, …   Why Is Being “Orthogonal” Important? Query = {laptop computer pc sale} Document 1 = {computer sale} Document 2 = {laptop computer pc} Ambiguity Is the Killer… Query =  {Jaguar band} Document 1 =  {Jaguar car} Document 2 =  {Jaguar cat} How to Assign Weights? Very, very important! Why weighting Query side: Not all terms are equally important Doc side: Some terms carry more information about contents How?  Two basic heuristics TF (Term Frequency) = Within-doc-frequency IDF (Inverse Document Frequency) TF normalization Weighting Is Very Important…  Query =  {text information management} Document 1 =  {text information} Document 2 =  {information management} Document  3  =  {text  management} What’s a Reasonable Term Weight? How do we weight the multiple occurrences of a term in a document? How do we weight different terms differently? What’s a Reasonable Term Weight? How do we weight the multiple occurrences of a term in a document? By Term Frequency (TF) How do we weight different terms differently? Term Frequency (TF)  Weighting TF Normalization TF Normalization can be even more complicated: TF in Okapi/BM25: 	 Where  doclen  is the length of d and  avg_dl  is the average length of documents in the collection Question: why do we need to normalize TF? TF Normalization Why?  Document length variation “Repeated occurrences” are less informative than the “first occurrence” Two views of document length A doc is long because it uses more words A doc is long because it has more contents Generally we want to penalize long documents, but avoid over-penalizing (pivoted normalization) TF Normalization  (cont.) Normalized   TF Raw TF “Pivoted normalization”: Using avg. doc length to regularize normalization   1-b+b* doclen  /  avg_doclen b varies from 0 to 1 Normalization interacts with the similarity measure What’s a Reasonable Term Weight? How do we weight the multiple occurrences of a term in a document? By Term Frequency (TF) How do we weight different terms differently? By Inverted Document Frequency (IDF) Inverted Document Frequency (IDF)  Weighting Idea: A term is more discriminative if it occurs only in fewer documents Why this is true? Formula: 	 	n – total number of documents in collection 	 k – number of documents with term t  	      (document frequency) Note that IDF is document independent while TF is document dependent! A Formulation of IDF N : number of documents d k : number of documents containing term  k f ik : absolute frequency of term  k  in document  i  w ik : weight of term  k  in document  i  idf k  =  log 2 ( N / d k ) + 1 = log 2 N  - log 2 d k  + 1 TF-IDF Weighting TF-IDF weighting:  weight(t, d)=TF(t, d)*IDF(t) Term is common in doc    high  tf    high weight Term is rare in collection   high  idf  high weight Imagine a word count profile, what kind of terms would have high weights?  How is this related to  Luhn’s  study of term frequency and  Zipf’s  law? How to Measure Similarity? VS Example: Raw TF & Dot Product  query=“information retrieval” How to implement this efficiently? What Works the Best? (Singhal 2001) Use single words  Use stat. phrases Remove stop words Stemming Others(?) [          ] Pivoted Normalization – the Typical VSM Why do we only care about terms in both Q and D? This part is IDF This part is (normalized) TF This part is TF in query, or QTF s is a parameter between 0 to 1,  that has to be set empirically.  Note the TF and IDF part – how are they similar with/different from what we discussed earlier?  # docs that contain t Number of words in D Average doc length Okapi/BM25 A variant form of IDF Variant form of (normalized) TF Normalized QTF k1, k3, and b are all parameters that have to be set empirically.  The best performer in TREC, the core feature of Bing “ Extensions”  of  Vector Space  Model Alternative similarity measures  Many other choices (tend not to be very effective) P-norm (Extended Boolean): matching a Boolean query with a TF-IDF document vector Alternative representation Many choices (performance varies a lot) Latent Semantic Indexing (LSI) [TREC performance tends to be average] Generalized vector space model Theoretically interesting, not seriously evaluated Advantages of VS Model Empirically effective! (Top TREC performance) Intuitive Easy to implement Well-studied/Most evaluated The Smart system Developed at Cornell: 1960-1999 Still widely used  Warning: Many variants of TF-IDF! Disadvantages of VS Model Assume term independence Which is never true! Assume query and document to be the same Lack of “predictive adequacy”  Arbitrary term weighting Arbitrary similarity measure Very sensitive to the selection of weighting.  Lots of parameter tuning! No Way to Predict Performance “k 1 , b and k 3  are parameters which depend on the nature  of the queries and possibly on the database;  k 1  and b default to 1.2 and 0.75 respectively,   but smaller values of b are sometimes advantageous;  in long queries k 3  is often set to 7 or 1000.”     Sophisticated Parameter Tuning [Robertson et al. 1999]   High Parameter Sensitivity How to Find Well Performing Retrieval Functions? Simple approach: try all the variations, and evaluate their performance Time consuming; can we really try “all” variations? If only we can find some formal guidance on how to find good retrieval functions! Axiomatic approach helps to do that A good retrieval function must satisfy a series of axioms; And if it doesn’t … Basic Idea of Axiomatic Approach Define a  set of  retrieval constraints  that any reasonable retrieval function should satisfy An Axiomatic Framework for  Retrieval Functions Component 1: Constraints to be satisfied by an effective retrieval function  (Fang et al. 2004) Component 2: Function space that allows us to efficiently search for an effective function We won’t cover in this lecture  (Fang  and  Zhai . 2005) What Should a Good Retrieval Function Do? A document that matches more query words? A document that matches more discriminative query words? A document that is longer? A document that covers more query words vs. a document that covers more  unique  query words? … Another Way to Think About it What do the good performers have in common? Term Frequency Inverted Document Frequency Document length normalization 1+ln(c( w,d )) Alternative TF transformation Term Frequency Constraints (TFC1) TFC1 TF weighting heuristic I:    			 	Give a higher score to a document with more 	occurrences of a query term. If	  and           ,       then Let  Q  be a query and  D  be a document.  Term Frequency Constraints (TFC2) TFC2 TF weighting heuristic II:    	 Require that the amount of increase in the score due to   	adding a query term must decrease as we add more terms . then Let  Q  be a query with only one query term  q. Let  D 1  be a document.  Length Normalization Constraints (LNCs) Document length normalization heuristic: 		Penalize long documents(LNC1);                          	Avoid over-penalizing long documents (LNC2) . Let  Q  be a query and  D  be a document. If  t  is a non-query term,   then LNC1	 TF-LENGTH Constraint (TF-LNC) TF-LNC TF-LN heuristic:						 Regularize the interaction of TF and document length. Let  Q  be a query and  D  be a document.  then If  q  is a query term,  Is constraint analysis related to the performance of a retrieval function?  Violation of Constraints    Poor Performance Okapi Method Conditional Satisfaction of Constraints   Parameter Bounds Pivoted Normalization Method	 LNC2   s<0.4 Constraints Analysis    Guidance for Improving a Retrieval Function Make Okapi satisfy more constraints; expected to help verbose queries Modified Okapi Method Implementation of  Component 2:  Question:  How can we define a function space that can be searched efficiently? (We won’t cover in the lecture)   What You Should Know Vector space model is a typical (and well-performing) way to define retrieval functions TF-IDF weighting Advantage and disadvantage of VSM Typical open source IR toolkits (another set of slides) NLP Parsing Parsing Noun Sequences Noun-noun compounds Fish tank tank that holds fish Fish net net used to catch fish Fish soup soup made with fish Fish oil oil extracted from fish Fish sauce sauce for fish dishes? sauce made of fish? Noun-noun compounds Head of the compound College junior – a kind of junior Junior college – a kind of college Head first? Attorney general Adjectives? New Mexico, general manager More than two nouns? luxury car dealership Noun phrase consisting of two nouns Detroit Tigers NNP NNP NP Semantics of Noun  Noun  Compounds Detroit Tigers = Tigers  from  Detroit              ( of/in ) [slide from  Nakov  et al. 2015] [slide from  Nakov  et al. 2015] [slide from  Nakov  et al. 2015] [slide from  Nakov  et al. 2015] [slide from  Nakov  et al. 2015] [Example from  Girju  2008 based on Lauer 1995] [slide from  Nakov  et al. 2015] Noun phrase consisting of two nouns Detroit Tigers NNP NNP NP Noun phrase consisting of four nouns Detroit NN NN Tigers general manager NP NNP NNP NP NP Representation using parentheses ((Salt Lake) City) (Salt (Lake City)) Salt Lake City mayor? Solution (((Salt Lake) City) mayor) Representation using parentheses (((Salt Lake) City) mayor) ((Detroit Tigers) (general manager)) Leland Stanford Junior University? Solution (((Leland Stanford) Junior) University) Combinatorics n=2 (A B) n=3 ((A B) C) (A (B C)) n=4 ((A B)(C D)) Solution n=4 ((A B)(C D)) (A (B (C D)) (A ((B C) D)) ((A (B C)) D) (((A B) C) D) What about n>4? n=5 ((A B)((C D)E)) … Solution The general solution is C(n), a notation for the n th   Catalan number  1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, … Sequence A000108  in the On-Line Encyclopedia of Integer Sequences® (OEIS®) https://oeis.org/   Other uses of Catalan numbers the number of different ways a convex polygon with  n  + 2 sides can be cut into triangles by connecting vertices with straight lines. http://en.wikipedia.org/wiki/File:Catalan-Hexagons-example.svg http://en.wikipedia.org/wiki/File:Catalan_number_4x4_grid_example.svg the number of monotonic paths along the edges of a grid with  n  ×  n  square cells, which do not pass above the diagonal. NLP NLP Introduction to NLP Parts of speech Syntactic categories Substitution test: Open (lexical) and closed (functional) categories: No-fly-zone twerk the in  Example Parts of speech eight (or so) general types nouns, verbs, adjectives… The dog chased the yellow bird . Nouns Examples dog, tree, computer, idea Nouns vary in  number (singular, plural) gender (masculine, feminine, neuter) case (nominative, genitive, accusative, dative) Case example in Latin Singular:  puer  (nominative),  puerum  (accusative),  puerī   (genitive) Plural:  puerī   ( nominative),  puerōs   (accusative),  puerōrum  (genitive) Gender example in German Mädchen   (neuter gender) Jabberwocky (Lewis Carroll) What are the parts of speech for the words in bold? 'Twas   brillig ,  and  the  slithy   toves  Did gyre and  gimble  in the  wabe : All  mimsy  were the  borogoves , And the  mome   raths   outgrabe . Answers 'Twas   brillig , and the  slithy   toves  Did gyre and  gimble  in the  wabe : All  mimsy  were the  borogoves , And the  mome   raths   outgrabe . Wabe ,  borogoves   Nouns (after “the”) brillig   adjective? noun?  (“noon”) mimsy   adjective slighty   toves adjective+noun ? noun+verb ? (“the bell tolls”) mome   raths   outgrabe Adjective+noun+verb ? Noun+verb+adverb ? (“birds fly outside”) Why is this an Important Example? Computers see text that they don’t really understand. They have to use some prior knowledge. They reason probabilistically. They use context. The can be wrong. Pronouns Examples she, ourselves, mine Pronouns vary in person  g ender number case (in English: nominative, accusative, possessive, 2nd possessive) Reflexive and anaphoric forms herself, each other Samantha gave  her  a haircut. Samantha  gave  herself   a haircut. Determiners and Adjectives Determiners Articles the, a Demonstratives this, that Adjectives describe properties attributive and predicative adjectives agreement in gender, number comparative and superlative forms  derivative and periphrastic positive form Verbs Describe actions, activities, and states ( throw, walk, have ) English four verb forms Tense present, past, future Other inflection number (including Dual in Slovenian, Arabic), person Forms gerunds and infinitive Aspect progressive, perfective Voice active, passive Verbs Participles, auxiliaries Arguments: The dog sleeps   (intransitive) The dog chased the cat (transitive) Mary  gave the  dog  a  bone (ditransitive ) Irregular verbs sleep, slept Richer inflections e.g., French, Finnish cases in Russian, Greek, Tamil, Latin Verb Conjugation in French Other Parts of Speech Adverbs happily, here, never Prepositions of, through, in P articles Phrasal verbs the plane took off, take it off P articles vs. prepositions She ran up a bill/hill Other Parts of Speech Coordinating conjunctions and, or, but Subordinating conjunctions if, because, that, although Interjections Ouch ! Sample Part of Speech Tags NN    /* singular noun */ IN    /* preposition */ AT    /* article */ NP    /* proper noun */ JJ    /* adjective */ ,     /* comma */  NNS   /* plural noun */ CC    /* conjunction */ RB    /* adverb */ VB    /* un-inflected verb */ VBN   /* verb +en (taken, looked ( passive,perfect )) */ VBD   /* verb + ed  (took, looked (past tense)) */ CS    /* subordinating conjunction */ NLP NLP NLTK NLP External Resources http ://www.nltk.org/book /   https ://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial /   https :// www.youtube.com/embed/FLZvOKSCkxY   http :// textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk   NLP NLP Introduction to NLP Semantics Semantics What is the meaning of: (5+2)*(4+3)? Parse tree 49 Semantics What if we had (5+2)*(4+z)? mult(add(5,2),add(4,z)) What about (English) sentences? Every human is mortal. ?? Goal Capturing the meaning of linguistic utterances using formal notation Linguistic meaning “It is 8 pm” Pragmatic meaning “It is time to leave” Semantic analysis:  Assign each word a meaning Combine the meanings of words into  sentences I bought a book :  	 ∃   x,y : Buying( x )  ^ Buyer( speaker,x ) ^  BoughtItem ( y,x ) ^ Book( y )       	Buying (Buyer=speaker,  BoughtItem =book) Representing Meaning NLP Introduction to NLP Representing and Understanding Meaning Understanding Meaning If an agent hears a sentence and can act accordingly, the agent is said to understand it Example Leave the book on the table Understanding may involve inference Maybe the book is wrapped in paper? And pragmatics Which book? Which table? So, understanding may involve a procedure Properties Verifiability Can a statement be verified against a knowledge base (KB) Example: does my cat Martin have whiskers? Unambiguousness Give me the book Which book? Canonical form Expressiveness Can the formalism express temporal relations, beliefs, …? Is it domain-independent? Inference Representing Meaning One traditional approach use logic representations, e.g., FOL (first order logic) Inference One can then use theorem proving (inference) to determine whether one statement entails another Syntax of Propositional Logic The simplest type of logic The  proposition symbols P 1 ,  P 2 , …   are  sentences If  S is a sentence,   S is a sentence ( negation ) If S 1  and S 2  are sentences, S 1     S 2  is a sentence ( conjunction ) If S 1  and S 2  are sentences, S 1     S 2  is a sentence ( disjunction ) If S 1  and S 2  are sentences, S 1     S 2  is a sentence ( implication ) If S 1  and S 2  are sentences, S 1     S 2  is a sentence ( biconditional ) Propositional Logic in Backus  Naur  Form Sentence  → AtomicSentence | ComplexSentence AtomicSentence → True | False | S | T | U ... ComplexSentence → (Sentence)  							|  Sentence 							| Sentence  Sentence 							| Sentence  Sentence 							| Sentence  Sentence 							| Sentence    Sentence Operator Precedence  (highest)     (lowest) Translating Propositions to English A = Today is a holiday. B = We are going to the  park. A  ⇒  B A ∧ ¬ B ¬ A ⇒ ¬ B ¬ B ⇒ ¬ A B ⇒  A Translating Propositions to English A = Today is a holiday. B = We are going to the park. A ⇒ B  If today is a holiday, we are going to the park. A ∧ ¬ B  Today is a holiday, and we are not going to the park. ¬ A ⇒ ¬ B If today is not a holiday, then we are not going to the park. ¬ B ⇒ ¬ A If we are not going to the park, then today is not a holiday. B ⇒ A If we are going to the park, then today is a holiday. Semantics of Propositional Logic    S			is true iff 	S is false   	S 1     S 2 	 	is true iff 	S 1  is true  and   S 2  is true 	S 1     S 2 	 	is true iff 	S 1 is true  or  	S 2  is true 	S 1     S 2 	 is true iff	S 1  is false  or 	 S 2  is true i.e. S 1      S 2	 is false iff	S 1  is true  and 	 S 2  is false 	S 1     S 2 	 is true iff	S 1  S 2  is true  and   S 2  S 1  is true Recursively, one can compute the truth value of longer formulas Connectives Logical  Equivalence [From Russell and  Norvig ] NLP NLP Introduction to NLP Natural Language Generation Using Features and Unification Systemic Grammars Language is viewed as a resource for expressing meaning in context (Halliday, 1985) Layers: mood, transitivity, theme Example ( :process save-1 :actor system-1 :goal document-1 : speechact  assertion :tense future )  Input is underspecified The Functional Unification Formalism Aka FUF Based on Kay’s (83) formalism Partial information, declarative, uniform, compact Same framework used for all stages: syntactic realization, lexicalization, and text planning Functional Analysis Functional vs. structured analysis “John eats an apple” Actor (John), affected (apple), process (eat) Suitable for generation Partial vs. Complete Specification Voice: An apple is eaten by John Tense: John ate an apple Mode: Did John eat an apple?      Modality: John must eat an apple Unification Target sentence Input FD Grammar Unification process Linearization  process Path notation View an FD as a tree To specify features, use a  path {feature  feature  … feature} value e.g. { prot  number} Also use  relative paths {^ number} value = the feature number for the current node {^ ^ number} value = the feature number for the node above the current node Sample input ((cat s)  (prot ((n ((lex john)))))  (verb ((v ((lex like)))))  (goal ((n ((lex mary)))))) Sample Grammar ((alt top (((cat s)             (prot ((cat np)))             (goal ((cat np)))             (verb ((cat vp)                    (number {prot number})))             (pattern (prot verb goal)))            ((cat np)             (n ((cat noun)                 (number {^ ^ number})))             (alt (((proper yes)                    (pattern (n)))                   ((proper no)                    (pattern (det n))                    (det ((cat article)                          (lex “the”)))))))            ((cat vp)             (pattern (v))             (v ((cat verb))))             ((cat noun))            ((cat verb))            ((cat article))))) Sample Output ((cat s)  (goal ((cat np)         (n ((cat noun)             (lex mary)             (number {goal number})))         (pattern (n))         (proper yes)))  (pattern (prot verb goal))  (prot ((cat np)         (n ((cat noun)             (lex john)             (number {verb number})))         (number {verb number})         (pattern (n))         (proper yes)))  (verb ((cat vp)         (pattern (v))         (v ((cat verb)             (lex like))))))        Unification Example Unify Prot Unify Goal Unify VP Unify Verb Finish The SURGE grammar (Elhadad) Syntactic realization front-end Variable level of abstraction 5,600 branches and 1,600 alts Lexical chooser SURGE Linearizer Morphology Lexicalized FD Syntactic FD Text Links SimpleNLG  ( Gatt  and Reiter) https :// github.com/simplenlg FUF in NLTK https://github.com/nltk/nltk_contrib/tree/master/nltk_contrib/fuf NLP NLP Introduction to NLP Text Classification Classification Assigning documents  or sentences to  predefined categories   topics, languages, users … Input :  a  document (or sentence)  d   a fixed set of classes   C  =   { c 1 ,  c 2 ,…,  c J } Output : a predicted class  c     C Variants of Problem Formulation Binary categorization: only two categories Retrieval: {relevant-doc, irrelevant-doc} Spam filtering: {spam, non-spam} Opinion: {positive, negative} K-category categorization: more than two categories Topic categorization: {sports, science, travel, business,…} Word sense disambiguation:{bar1, bar2, bar3, …} Hierarchical vs. flat Overlapping (soft) vs non-overlapping (hard) Hierarchical Classification Image from  Sch ütze   &  Krisnawati Borges’s Classification The list divides all animals into 14 categories: Those that belong to the emperor Embalmed  ones Those that are trained Sucking pigs Mermaids (or  Sirens ) Fabulous ones Stray dogs Those that are included in this classification Those that tremble as if they were mad Innumerable ones Those drawn with a very fine  camel hair brush Et cetera Those that have just broken the flower vase Those that, at a distance, resemble flies Celestial Emporium of Benevolent Knowledge Hand-coded Rules Rules based on combinations of words or other features   spam: black-list-address OR (“dollars” AND “have been selected”) Accuracy can be high If rules carefully refined by expert But building and maintaining these rules is expensive Supervised Machine Learning A given set of classes C Given  text data x , determine its class  y in  C Spam  Recognition Return-Path: <*****@rediffmail.com> X-Sieve: CMU Sieve 2.2 From: "Ibrahim  Galadima " <*****@rediffmail.com> Reply-To: galadima_esq@netpiper.com To: ***** Subject: Gooday DEAR SIR FUNDS FOR INVESTMENTS THIS LETTER MAY COME TO YOU AS A SURPRISE SINCE I HAD NO PREVIOUS CORRESPONDENCE WITH YOU I AM THE CHAIRMAN TENDER BOARD OF INDEPENDENT NATIONAL ELECTORAL COMMISSION INEC I GOT YOUR CONTACT IN THE COURSE OF MY SEARCH FOR A RELIABLE PERSON WITH WHOM TO HANDLE A VERY  CONFIDENTIAL   TRANSACTION  INVOLVING THE ! TRANSFER OF FUND VALUED AT TWENTY ONE  MILLION  SIX HUNDRED THOUSAND UNITED STATES  DOLLARS  US$20M TO A SAFE FOREIGN ACCOUNT SpamAssassin http://spamassassin.apache.org/   http://wiki.apache.org/spamassassin/HowScoresAreAssigned http :// spamassassin.apache.org/tests_3_3_x.html   Example features: body 		Incorporates a tracking ID number  body 		HTML and text parts are different  header 	 Date : is 3 to 6 hours before Received: date  body 		HTML font size is huge  header 	 Attempt  to obfuscate words in Subject:  header 	 Subject  =~ /^urgent(?:[\s\W]*(dollar) | .{1,40}(?:alert| response| assistance| proposal| reply| warning|  noti (?: ce |  fication )| greeting| matter))/ i   Features for Classification Vector-based Words: “cat”, “dog”, “great”, “horrible”, etc. Meta features: document length, author name, etc. Each document (or sentence) is represented as a vector in an  n -dimensional space Similar documents appear nearby in the vector space (more later)  Classification in NLP Part of speech tagging Sentiment analysis Word sense disambiguation Parsing Optical character recognition Spelling correction Named entity classification Introduction to NLP Vector Space Classification Vector Space Classification Decision surfaces Decision trees Classification Using Centroids Centroid the point most representative of a class Compute centroid by finding vector average of known class members Decision boundary is a line that is equidistant from two centroids. New document on one side of the goes in one class; new document on the other side goes in the other.  Linear boundary Classification Using Centroids Introduction to NLP Linear Classifiers Linear Separators Two-dimensional  line: w 1 x 1 +w 2 x 2 =b  is the linear  separator w 1 x 1 +w 2 x 2 >b  for the positive  class In  n -dimensional spaces : One can also add w 1 =1, x 0 =b (constant) w  is the weight vector x  is the feature vector Decision Boundary Example Bias b=0 (in this example) Sentence is “A D E H” Its score will be 0.6*1+0.4*1+0.4*1+(-0.5)*1 = 0.9>0 How to Find the Linear Boundary? Find the linear boundary = find  Many methods Perceptron Linear least squares Problem: There are infinite number of linear boundaries if the two classes are linearly separable! Maximum margin: Support Vector Machines (SVM) General Training Idea Go through the training data Predict the class y (1 or -1) If the prediction is wrong, update w w (t+1) = w t +yx Used in the perceptron Naïve Bayes Multinomial Naïve Bayes is a linear model x  = 	[1, 				 x 1 , 			      x 2  			     …] w  =     [log P(y), 	log P(w 1 |y), 	log P(w 2 |y)	…] NLP NLP Introduction to NLP Inference Modus ponens:     ⇒     Example: Cat(Martin)  x: Cat(x)  ⇒  EatsFish(x) EatsFish(Martin) Modus Ponens Inference Forward chaining as individual facts are added to the database, all derived inferences are generated Backward chaining starts from queries  Example: the Prolog programming language Prolog example father(X, Y) :- parent(X, Y), male(X). parent(john, bill). parent(jane, bill). female(jane). male (john). ?- father(M, bill). The Kinship Domain Brothers are siblings  x,y   Brother( x,y )     Sibling( x,y ) One's mother is one's female parent  m,c   Mother(c)  = m     (Female(m)     Parent( m,c )) “Sibling” is symmetric  x,y   Sibling( x,y )     Sibling( y,x ) Universal Instantiation Every instantiation of a universally quantified sentence is entailed by it:  v   α Subst ({v/g},  α ) 	 	 for any variable  v  and ground term  g E.g.,   x  Cat ( x )     Fish ( y )    Eats(x,y)  yields: Cat(Martin)   Fish(Blub )   Eats( Martin,Blub )  Existential Instantiation For any sentence  α , variable  v , and constant symbol  k  that  does not appear  elsewhere in the knowledge base:    v   α Subst ({v/k},  α ) E.g.,   x   Cat ( x )     EatsFish ( x ) yields: Cat ( C 1 )     EatsFish ( C 1 ) 	provided  C 1  is a new constant symbol,  called a  Skolem  constant Unification If a substitution  θ  is available, unification is possible Examples: p=Eats(x,y), q=Eats(x,Blub), possible if  θ  = {y/Blub} p=Eats(Martin,y), q=Eats(x,Blub), possible if  θ  = {x/ Martin,y /Blub} p=Eats(Martin,y), q=Eats(y,Blub), fails because Martin ≠ Blub Subsumption Unification works not only when two things are the same but also when one of them subsumes the other one Example: All cats eat fish, Martin is a cat, Blub is a fish NLP NLP Machine Translation Semantics in MT Examples Das  Vorhaben   verwarf  die  Kommission The      plan    rejected the commission OSV reading is more plausible I saw the movie and it is good How do we translate “it”? [Examples from Philipp Koehn] Color Names Russian: light  blue ( голубой ,  goluboy )  dark blue ( синий ,  siniy ) Japanese: 青  ( ao ) – both blue and green historically 緑  ( midori ) – recent addition Davidoff et al. 99 Color Names in English and  Berinmo NLP NLP Discourse Analysis Coherence Coherence Examples I saw Mary in the street. She was looking for a bookstore. ? I saw Mary in the street. She has a cat. ?? I saw Mary in the street. The Pistons won. Rhetorical Structure Theory (Mann and Thompson 1988) Nucleus and Satellite The carpenter was tired. He had been working all day. Nucleus and Satellite The satellite increases the belief in the relation described in the nucleus Some relations have only a nucleus, others have two nuclei, yet others have one nucleus and one satellite Coherence Relations Result The carpenter worked all day. The new cabinet was ready in the evening. Explanation The carpenter was tired. He had spent the entire day building a new cabinet. Parallel The carpenter worked all day. The upholsterer took the day off. Elaboration The carpenter built a cabinet. The cabinet had four drawers and an oversized rear panel. Other relations Nucleus+satellite :  c ircumstance, volitional cause, purpose, interpretation, restatement, summary Multi-nuclear: sequence, contrast, joint [Mann and Thompson 1988] Local Entity Coherence [ Barzilay  and  Lapata  2008] Local Entity Coherence 6 sentences S=subject, O=object, X=neither [ Barzilay  and  Lapata  2008] Local Entity Coherence [ Barzilay  and  Lapata  2008] Discourse Analysis Prosody Prosody Properties of speech Rhythm Intonation Stress Used to express emotions, emphasis, etc. NLP NLP Introduction to NLP Earley  Parser Earley  parser Problems with left recursion in top-down parsing VP    VP PP Background Developed by Jay Earley in 1970 No need to convert the grammar to CNF Left to right Complexity Faster than O(n 3 ) in many cases Earley   Parser Looks for both full and partial constituents When reading word  k , it has already identified all hypotheses that are consistent with words 1 to  k -1 Example: S [ i,j ]   Aux  .  NP VP NP [ j,k ]  N S [ i,k ]  Aux NP  .  VP Earley  Parser It uses a dynamic programming table, just like CKY Example entry in column 1 [0:1] VP -> VP  .  PP Created when processing word 1 Corresponds to words 0 to 1 (these words correspond to the VP part of the RHS of the rule) The dot separates the completed (known) part from the incomplete (and possibly unattainable) part Earley  Parser Three types of entries ‘scan’ – for words ‘predict’ – for non-terminals ‘complete’ – otherwise  Earley  Parser Algorithm Figure from  Jurafsky  and Martin Earley  Parser Algorithm Figure from  Jurafsky  and Martin S  -> NP VP S  -> Aux NP VP S  -> VP NP  -> PRON NP  ->  Det  Nom Nom  -> N Nom  -> Nom N Nom  -> Nom PP PP  -> PRP NP VP  -> V VP  -> V NP VP  -> VP  PP     Det  -> 'the' Det   -> 'a' Det   -> 'this' PRON  -> 'he' PRON  -> 'she' N  -> 'book' N  -> 'boys' N  -> 'girl' PRP  -> 'with' PRP  -> 'in' V  -> 'takes' V  -> 'take' |.    take   .    this   .    book   .| |[-----------]           .           .| [0:1] 'take' |.           [-----------]           .| [1:2] 'this' |.           .           [-----------]| [2:3]  'book’ Example created using NLTK |.    take   .    this   .    book   .| |[-----------]           .           .| [0:1] 'take' |.           [-----------]           .| [1:2] 'this' |.           .           [-----------]| [2:3] 'book' |>           .           .           .| [0:0] S  -> * NP VP |>           .           .           .| [0:0] S  -> * Aux NP VP |>           .           .           .| [0:0] S  -> * VP |>           .           .           .| [0:0] VP -> * V |>           .           .           .| [0:0] VP -> * V NP |>           .           .           .| [0:0] VP -> * VP PP |>           .           .           .| [0:0] V  -> * 'take' |>           .           .           .| [0:0] NP -> * PRON |>           .           .           .| [0:0] NP -> * Det  Nom |.    take   .    this   .    book   .| |[-----------]           .           .| [0:1] 'take' |.           [-----------]           .| [1:2] 'this' |.           .           [-----------]| [2:3] 'book' |>           .           .           .| [0:0] S  -> * NP VP |>           .           .           .| [0:0] S  -> * Aux NP VP |>           .           .           .| [0:0] S  -> * VP |>           .           .           .| [0:0] VP -> * V |>           .           .           .| [0:0] VP -> * V NP |>           .           .           .| [0:0] VP -> * VP PP |>           .           .           .| [0:0] V  -> * 'take' |>           .           .           .| [0:0] NP -> * PRON |>           .           .           .| [0:0] NP -> *  Det  Nom |[-----------]           .           .| [0:1] V  -> 'take' * |[-----------]           .           .| [0:1] VP -> V * |[----------->           .           .| [0:1] VP -> V * NP |.           >           .           .| [1:1] NP -> * PRON |.           >           .           .| [1:1] NP -> * Det  Nom |.           >           .           .| [1:1] Det -> * 'this’ |[-----------]            .           .| [0:1] S  -> VP * |[----------->           .           .| [0:1] VP -> VP * PP |.           >           .           .| [1:1] PP -> * PRP NP |.           [-----------]           .| [1:2]  Det  -> 'this' * |.           [----------->           .| [1:2] NP ->  Det  * Nom |.           .           >           .| [2:2] Nom -> * N |.           .           >           .| [2:2] Nom -> * Nom N |.           .           >           .| [2:2] Nom -> * Nom PP |.           .           >           .| [2:2] N  -> * 'book' |.           .           [-----------]| [2:3] N  -> 'book' * |.           .           [-----------]| [2:3] Nom -> N * |.           [-----------------------]| [1:3] NP ->  Det  Nom * |.           .           [----------->| [2:3] Nom -> Nom * N |.           .           [----------->| [2:3] Nom -> Nom * PP |.           .           .           >| [3:3] PP -> * PRP NP |[===================================]| [0:3] VP -> V NP * |[===================================]| [0:3] S  -> VP * |[----------------------------------->| [0:3] VP -> VP *  PP (S (VP (V take) (NP (Det this) (Nom (N book))))) NLTK Demo nltk  demo: import  nltk nltk.parse.chart.demo (2 ,  print_times =False, trace=1, sent='I saw a dog',  numparses =1 ) Notes CKY fills the table with phantom constituents problem, especially for long sentences Earley  only keeps entries that are consistent with the input up to a given word So far, we only have a recognizer	 For parsing, we need to add  backpointers Just like with CKY, there is no disambiguation of the entire sentence Time complexity n  iterations of size O( n 2 ), therefore O( n 3 ) For unambiguous grammars, each iteration is of size O( n ),  therefore  O( n 2 ) NLP NLP Introduction to NLP Evaluation of QA The TREC Q&A evaluation Run by NIST  Voorhees and Tice 1999, 2000 2GB of input AQUAINT corpus 200 questions in 1999 693 questions in 2000 Essentially fact extraction Who was Lincoln’s secretary of state? What does the Peugeot company manufacture? Assumptions Questions are based on text Answers are assumed to be present Systems return five passages of 50 or 250 bytes After 2002, only a single passage of 50 bytes + confidence score + NIL No inference needed TREC 1999 What  date in 1989 did East Germany open the Berlin  Wall?  Nov 9 Who  was Johnny Mathis' high school track  coach? Lou  Vasquez What  is the shape of a porpoises' tooth? spade-shaped What  is the number of buffaloes thought to have been living in  North America  when Columbus landed in 1492? 60  million  The  Faroes are a part of what northern European country? Denmark The  symptoms of Parkinson's disease are linked to the demise of  cells in  what area of the brain? substantia nigra What was the date of the Chelsea flower show in 1992? May 18 What hotel was used for a setting of the Agatha Christie novel, "And Then There Were None"? Burgh Island Hotel What year was the Magna Carta signed? 1215 Who was Lincoln's Secretary of State? William Seward How long does it take to fly from Paris to New York in a Concorde? 3 1/2 hours TREC 1999 – Test Questions Who is the author of the book, "The Iron Lady: A Biography of Margaret Thatcher"? What was the monetary value of the Nobel Peace Prize in 1989? What does the Peugeot company manufacture? How much did Mercury spend on advertising in 1993? What is the name of the managing director of Apricot Computer? Why did David Koresh ask the FBI for a word processor? What debts did  Qintex  group leave? What is the name of the rare neurological disease with symptoms such as: involuntary movements (tics), swearing, and incoherent vocalizations (grunts, shouts, etc.)? How far is Yaroslavl from Moscow? Name the designer of the shoe that spawned millions of plastic imitations, known as "jellies". Who was President Cleveland's wife? How much did Manchester United spend on players in 1993? How much could you rent a Volkswagen bug for in 1966? What country is the biggest producer of tungsten? When was London's Docklands Light Railway constructed? What two US biochemists won the Nobel Prize in medicine in 1992? How long did the Charles Manson murder trial last? Who was the first Taiwanese President? Who was the leader of the Branch  Davidian  Cult confronted by the FBI in Waco, Texas in 1993? Where is  Inoco  based? Who was the first American in space? When did the Jurassic Period end? When did Spain and Korea start ambassadorial relations? When did Nixon visit China? Who was the lead actress in the movie "Sleepless in Seattle"? What is the name of the "female" counterpart to El Nino, which results in cooling temperatures and very dry weather? What  is the brightest star visible from Earth? What are the Valdez Principles ? Evaluation MRR Mean Reciprocal Rank Introduced by TREC in 1999 Example What is the capital of Canada? 1.Toronto, 2.Ottawa, 3.Albany, 4.Philadelphia, 5.Ottawa Correct answer ranks = 2, 5 (highest rank = 2) MRR = 1/2 = 0.5 TRR = 1/2 + 1/5 = 0.7 = total reciprocal rank Later years Confidence-weighted score System Rankings 1999 Cymphony , SMY, AT&T, IBM, XRCE, Umd 2000 SMU, ISI, Waterloo, IBM, CUNY-QC 2001 InsightSoft , LCC, Oracle, ISI 2002 LCC, InsightSoft, NUS 2003 LCC, NUS, LexiClone, ISI 2004 LCC, NUS, Uwales, IBM 2005 LCC, NUS, IBM, Albany Other Types of Questions Definitional What is a boll weevil? List Which states signed the US Declaration of Independence? Crosslingual E.g., questions in Spanish, documents in English Series Next slide TREC Series Questions 	    What  are prions made of? 	    Who discovered prions? 	    What researchers have worked with prions ? 	    Who is the lead singer/musician in Nirvana? 	    Who are the band members? 	    When was the band formed? 	    What is their biggest hit? 	    What are their albums? 	    What style of music do they play ? 	    What industry is Rohm and Haas in? 	    Where is the company located? 	    What is their annual revenue? 	    How many employees does it have ? 	    What kind of insect is a boll weevil? 	    What type of plant does it damage? 	    What states have had problems with boll weevils? NLP NLP Question Answering System Architecture System Architecture Many questions can be answered by traditional search engines ...   Afghanistan , Kabul, 2,450  ...  Administrative capital and  largest   city  (1997 est  ...  Undetermined. Panama, Panama  City , 450,668.  ...  of the Gauteng,  Northern  Province, Mpumalanga  ...   www.infoplease.com/cgi-bin/id/A0855603 ...  died in Kano,  northern  Nigeria's  largest   city , during two days of anti-American riots led by Muslims protesting the US-led bombing of  Afghanistan , according to  ...   www.washingtonpost.com/wp-dyn/print/world/ ...  air strikes on the  city .  ...  the Taliban militia in  northern   Afghanistan  in a significant blow  ...  defection would be the  largest  since the United States  ...   www.afgha.com/index.php - 60k ...  Kabul is the capital and  largest   city  of  Afghanistan . .  ...  met. area pop. 2,029,889), is the  largest   city  in Uttar Pradesh, a state in  northern  India. .  ...   school.discovery.com/homeworkhelp/worldbook/atozgeography/ k/k1menu.html    ...  Gudermes, Chechnya's second  largest  town. The attack  ...  location in  Afghanistan's  outlying regions  ...  in the  city  of Mazar-i-Sharif, a  Northern  Alliance-affiliated  ...   english.pravda.ru/hotspots/2001/09/17/ ...  Get Worse By RICK BRAGG Pakistan's  largest   city  is getting a jump on the  ...  Region: Education Offers Women in  Northern   Afghanistan  a Ray of Hope.  ...   www.nytimes.com/pages/world/asia/ ...  within three miles of the airport at Mazar-e-Sharif, the  largest   city  in  northern  Afghanistan , held since 1998 by the Taliban. There was no immediate comment  ...   uk.fc.yahoo.com/photos/a/afghanistan.html What is the Largest City in Northern Afghanistan? System Components Source  identification semi-structured vs. text sources Query  modulation best paraphrase of a NL question given the syntax of a search  engine Example: Who wrote Hamlet → author | wrote Hamlet Document retrieval Sentence  ranking n-gram matching,  Okapi Answer extraction question type  classification phrase chunking Answer  ranking question  type, proximity to query words,  frequency Document retrieval Query modulation Sentence ranking Answer extraction Answer ranking What is the largest city in Northern Afghanistan? (largest OR biggest) city “Northern Afghanistan” www.infoplease.com/cgi-bin/id/A0855603 www.washingtonpost.com/wp-dyn/print/world/ Gudermes, Chechnya's second  largest  town … location in  Afghanistan's  outlying regions within three miles of the airport at Mazar-e-Sharif, the  largest   city  in  northern Afghanistan Gudermes Mazer-e-Sharif Mazer-e-Sharif Gudermes Question Type Classification Can help find the right answers in the text Example Who wrote Anna Karenina? Looking for a PERSON/INDIVIDUAL/WRITER  SYN-classes (IBM AnSel) UIUC Question Types ENTITY : entities   animal: animals   body: organs of body   color: colors   creative: inventions, books and other creative pieces   currency: currency names   dis.med.: diseases and medicine   event: events   food: food   instrument: musical instrument   lang: languages   letter: letters like a-z   other: other entities   plant: plants   product: products   religion: religions   sport: sports   substance: elements and substances   symbol: symbols and signs   technique: techniques and methods   term: equivalent terms   vehicle: vehicles   word: words with a special property NUMERIC : numeric values   code: postcodes or other codes   count: number of sth.   date: dates   distance: linear measures   money: prices   order: ranks   other: other numbers   period: the lasting time of sth.   percent: fractions   speed: speed   temp: temperature   size: size, area and volume   weight: weight ABBREVIATION : abbreviation    abb : abbreviation    exp : expression abbreviated  DESCRIPTION : description and abstract concepts   definition: definition of sth.   description: description of sth.   manner: manner of an action   reason: reasons HUMAN : human beings   group: a group or organization of persons   ind: an individual   title: title of a person   description: description of a person LOCATION : locations   city: cities   country: countries   mountain: mountains   other: other locations   state: states UIUC Examples NUM:date When did Rococo painting and architecture flourish ? LOC:country What country 's national passenger rail system is called Via ? HUM:ind Who invented Make-up ? DESC:desc What is the origin of the word `` attic '' ? DESC:desc What did Delilah do to Samson 's hair ? ENTY:animal What kind of animals were in the Paleozoic era ? HUM:ind Which of the following was Rhodes Scholar ? HUM:ind Who comprised the now-defunct comic book team known as the Champions ? DESC:manner How do you make a paintball ? LOC:state What U.S. state is Fort Knox in ? ENTY:animal What is a female rabbit called ? LOC:mount   Where is the highest point in Japan ? DESC:desc   Where do  chihuahuas  come from ? LOC:other   Where does Barney Rubble go to work after he drops Fred off in the “Flintstones” cartoon series? UIUC papers on question classification Xin Li and Dan Roth. Experimental  Data for Question Classification http ://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC /     Xin Li, Dan  Roth.  Learning Question Classifiers: The Role of Semantic Information http://l2r.cs.uiuc.edu/~ danr/Papers/LiRo05a.pdf   http://cogcomp.cs.illinois.edu/page/software_view/LBJava Data set training  ( http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/train_5500.label )  test  ( http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/TREC_10.label )  Techniques for Question Classification Classification task Use standard techniques Regular expressions WHO is|was -> PERSON WHICH STATE -> STATE Query Reformulation [ Radev  et al. 2001] Passage Retrieval Features Proper nouns that match the query Near each other Entities that match the expected answer type Answer Retrieval Use NER to identify the matching phrases E.g., “January 1, 1951” as a DATE Features Distance to query words Answer type Wordnet similarity Redundancy  Redundancy (1/2) What is the capital of Spain? Madrid   is the  capital  of  Spain En  route to  Spain's capital  of  Madrid Madrid ,  Spain's capital  city is situated almost at the geographical epicentre of the country The  capital  of  Spain  is  Madrid Madrid ,  Spain's  sunny  capital Madrid  became  Spain's capital In 1561, it was elevated to status as  Spain's capital  city Madrid  has been the  capital  of  Spain  since 1562 Madrid , the physical and cultural  capital  of  Spain Redundancy (2/2) When  did French revolutionaries storm the  Bastille? The storming of the Bastille occurred in Paris on the morning of 14 July 1789 The storming of the Bastille, 14 July 1789 The storming of the Bastille prison on July 14th 1789 was an event that paved the way to further civil disorder and upheaval in France.  French revolutionaries storm Bastille, 1789 The storming of the Bastille (Louis XVI's prison) by French revolutionaries took place on July 14th, 1789 Who  killed Mahathma Gandhi? Mohandas  Karamchand  Gandhi (often called Mahatma Gandhi) was assassinated on 30 January 1948, shot at point-blank range by  Nathuram   Godse . Nathuram   Godse  killed Gandhi Godse  killed Gandhi Mahatma Gandhi was assassinated on 30 January 1948, shot at point-blank range by  Nathuram   Godse Mohandas Gandhi was shot dead by  Nathuram   Godse , a Hindu extremist Godse  assassinated Mahatma Gandhi on January 30, 1948, approaching him during the evening prayer, bowing, and shooting him three times at close range with a Beretta semi-automatic pistol. NLP Introduction to NLP Question Answering Systems AnSel (Prager et al. 1999) IBM System Built for TREC Components Predictive Annotation Logistic Regression Predictive Annotation When was Yemen reunified?   <TIME>  22 may 1990  </TIME> <p><NUMBER>1</NUMBER></p> <p> <QUERY>Who is the author of the book, "The Iron Lady: A Biography of Margaret Thatcher"?</QUERY> </p> <p><PROCESSED_QUERY>@excwin(*dynamic* @weight(2.00001001 *Ion_Lady) @weight(200 Biography_of_Margaret_Thatcher) @weight(200 Margaret) @weight(100 author) @weight(100 book) @weight(100 iron) @weight(100 lady) @weight(100 :) @weight(100 biography) @weight(100 thatcher) @weight(400  @syn(PERSON$ ORG$ NAME$ ROLE$)  ) )</PROCESSED_QUERY></p> <p><DOC>LA090290-0118</DOC></p> <p><SCORE>1020.8114</SCORE></p> <TEXT><p>THE IRON LADY; A  <span class=“NAME”>   Biography of Margaret Thatcher   </span>  by  <span class=“PERSON”>  Hugo Young   </span>  ( <span class="ORG“>  Farrar , Straus & Giroux  </span>  ) The central riddle revealed here is why, as a woman  <span class="PLACEDEF">   in a man   </span>  's world,  <span class="PERSON">  Margaret Thatcher   </span>  evinces such an exclusionary attitude toward women.</p></TEXT> Predictive Annotation Some Observations Word proximity In documents that contain the answers, the query terms tend to occur in close proximity to each other Phrases The answers to fact-seeking questions are usually phrases These phrases can be categorized by question type The phrases can be identified in text by pattern matching techniques Type : the position of the span type in the list of potential span types. Example:  Type  (“ Lou Vasquez ”) = 1, because the span type of “ Lou Vasquez ”, namely “PERSON” appears first in the list of potential span types, “PERSON ORG NAME ROLE”. Feature Selection Avgdst : the average distance in words between the beginning of the span and the words in the query that also appear in the passage. Example: given the question “ Who was Johnny Mathis' high school track coach ? “ and the passage  “Tim O'Donohue, Woodbridge High School's varsity baseball coach, resigned Monday and will be replaced by assistant Johnny Ceballos, Athletic Director Dave Cowen said.”  and the span “ Tim O’Donohue ”, the value of  avgdst  is equal to 8. Notinq : the number of words in the span that do not appear in the query. Example:  Notinq  (“ Woodbridge high school” ) = 1, because both “high” and “school” appear in the query while “Woodbridge” does not. It is set to –100 when the actual value is 0. Frequency:  number of times a given span appears in the hit list. Sscore : passage relevance as computed by  the search engine . Number : position of the span among all spans returned. Example: “ Lou Vasquez ” was the first span returned by  GuruQA  on the sample question. Rspanno : position of the span among all spans returned within the current passage. Count : number of spans of any span class retrieved within the current passage. IONAUT (Abney et al. 2000) Passage retrieval Uses START (Salton, Buckley) Entity recognition Uses Cass (Abney) – partial parser Entity classification Simple patterns for 8 question types Web-based QA Significantly larger corpus No pre-annotation is possible Search engines only partially helpful Stop words Question types Restrictions on queries Issues with reliability, timeliness, inaccurate answers Mulder (Kwok et al. 2001) First large-scale Web QA system Components Maximum entropy parser (Charniak) PC-Kimmo for unknown words Link parser (Sleator and Temperley) Google Tokenization phrases in quotes Query transformations “When did Nixon visit China” -> “Nixon visited China” NSIR (Radev et al. 2002) Probabilistic phrase reranking P(qtype|signature) Signature = POS sequence (e.g., “NNP NNP” for “Bill Gates”) Search engines AlltheWeb, NorthernLight, Altavista, Google AskMSR (Banko et al. 2002) Assumption Someone has already  answered this question on the Web Components Query rewriting Snippet retrieval N-gram ranking Tiling matches Combining A B C and B C D into A B C D E.g., “Mr. Charles” and “Charles Dickens” into “Mr. Charles Dickens” Echihabi  and  Marcu Based on the noisy channel model Find the sentence S that maximizes p(q|S) Requires simplifying the sentences LCC (e.g. Moldovan et al. 2003) Uses logic form transformations Uses axioms for inference e.g. Lexical chains Ravinchandran  and  Hovy  2002 Characteristics Automatically learn surface patterns Starts with a seed Query Web Find patterns that contain both the question and the answer terms Example Mozart was born in 1756 <NAME> was born on <BIRTHDATE> Watson ( Ferrucci  et al. 2010) Jeopardy winner (2011) Architecture Uses " DeepQA ": a technology that enables computer systems to directly and precisely answer natural language questions over an open and broad range of  knowledge 10  racks of IBM Power 750 servers running  Linux 16  terabytes of  RAM 2,880  processor  cores Capable  of operating at 80 teraflops.  Mostly in Java but also some C++ and Prolog Integrated using UIMA http://blog.reddit.com/2011/02/ibm-watson-research-team-answers-your.html http://www.pcmag.com/article2/0,2817,2380351,00.asp Ferrucci  et al. 2010. Building Watson: An Overview of the  DeepQA  Project. AI Magazine. Fall 2010. 59-79 . Watson ( Ferrucci  et al. 2000) Knowledge sources 200 million pages of structured and unstructured content consuming four terabytes of disk storage  Including Wikipedia, WordNet,  Yago Betting strategy Buzz if at least 50% certain Performance Watson answers 66 correct and 9 incorrect. Watson's two day winning streak was $77,147. Ken Jennings ended with $24,000 and Brad Rutter with $21,600.  Watson ( Ferrucci  et al. 2010) Question types 2,500 of them 200 of them are very common Jeopardy Question Archive http://j-archive.com/   QA Challenges Word Sense Disambiguation Co-reference Resolution Semantic Role Labeling Temporal questions Categories on Jeopardy NLP NLP Introduction to NLP Word Distributions Word  D istributions Words are not distributed evenly! Same goes for letters of the alphabet (ETAOIN SHRDLU), city sizes, wealth, etc. Usually, the 80/20 rule applies 80% of the wealth goes to 20% of the people or it takes 80% of the effort to build the easier 20% of the system more examples coming up… Shakespeare Romeo and Juliet: And, 667; The, 661; I, 570; To, 515; A, 447; Of, 382; My, 356; Is, 343; That, 343; In, 314; You, 289; Thou, 277; Me, 262; Not, 257; With, 234; It, 224; For, 223; This, 215; Be, 207; But, 181; Thy, 167; What, 163; O, 160; As, 156; Her, 150; Will, 147; So, 145; Thee, 139; Love, 135; His, 128; Have, 127; He, 120; Romeo, 115; By, 114; She, 114; Shall, 107; Your, 103; No, 102; Come, 96; Him, 96; All, 92; Do, 89; From, 86; Then, 83; Good, 82; Now, 82; Here, 80; If, 80; An, 78; Go, 76; On, 76; I'll, 71; Death, 69; Night, 68; Are, 67; More, 67; We, 66; At, 65; Man, 65; Or, 65; There, 64; Hath, 63; Which, 60;  … A-bed, 1; A-bleeding, 1; A-weary, 1; Abate, 1; Abbey, 1; Abhorred, 1; Abhors, 1; Aboard, 1;  Abound'st , 1;  Abroach , 1; Absolved, 1; Abuse, 1; Abused, 1; Abuses, 1; Accents, 1; Access, 1; Accident, 1; Accidents, 1; According, 1; Accursed, 1;  Accustom'd , 1; Ache, 1; Aches, 1; Aching, 1; Acknowledge, 1; Acquaint, 1; Acquaintance, 1; Acted, 1; Acting, 1; Action, 1; Acts, 1; Adam, 1; Add, 1; Added, 1; Adding, 1; Addle, 1; Adjacent, 1; Admired, 1; Ado, 1; Advance, 1; Adversary, 1; Adversity's, 1; Advise, 1; Afeard, 1; Affecting, 1; Afflicted, 1; Affliction, 1; Affords, 1; Affray, 1; Affright, 1; Afire, 1; Agate-stone, 1; Agile, 1; Agree, 1; Agrees, 1;  Aim'd , 1; Alderman, 1; All-cheering, 1; All-seeing, 1;  Alla , 1; Alliance, 1; Alligator, 1; Allow, 1; Ally, 1; Although, 1;  http://www.mta75.org/curriculum/english/Shakes/indexx.html (visited in Dec. 2006) The BNC (Adam  Kilgarriff )  1 6187267 the   det  2 4239632 be   v  3 3093444 of   prep  4 2687863 and   conj  5 2186369 a     det  6 1924315 in   prep  7 1620850 to   infinitive-marker  8 1375636 have v  9 1090186 it    pron 10 1039323 to   prep 11  887877 for  prep 12  884599  i      pron 13  760399 that  conj 14  695498 you   pron 15  681255 he    pron 16  680739 on   prep 17  675027 with prep 18  559596 do   v 19  534162 at   prep 20  517171 by   prep Kilgarriff , A. Putting Frequencies in the Dictionary. International Journal of Lexicography  10 (2) 1997. Pp 135--155  Stop  W ords Fact: 250-300 most common words in English account for 50% or more of a given text. Example:  “the” and “of” represent 10% of tokens. “and”, “to”, “a”, and “in” - another 10%. Next 12 words - another 10%.  Moby Dick Ch.1:  859 unique words (types), 2256 word occurrences (tokens). Top 65 types cover 1132 tokens (> 50%). Token/type ratio:  2256/859 = 2.63 Power-law Distribution Power-law Many words with a small frequency of occurrence A few words with a very large frequency High skew (asymmetry) Comparing to a normal distribution: Many people of a medium height Almost nobody of a very high or very low height Symmetry Slide from  Qiaozhu  Mei Scaling the Axes linear scale log-log scale Long-tail on a linear scale - straight line on a log-log plot Power Law Distribution The probability of observing an item of size ‘x’ is given by Straight line on a log-log plot Power Laws Are Seemingly Everywhere note: these are cumulative distributions Source:MEJ  Newman, ’Power laws, Pareto distributions and Zipf’s law’,  Contemporary Physics   46 , 323–351 (2005) Zipf's  law is fairly general!  Frequency of accesses to web pages   in particular the access counts on the Wikipedia page, with  s  approximately equal to 0.3   page access counts on Polish Wikipedia (data for late July 2003)  approximately obey  Zipf's  law with a slope  s  about 0.5   Words in the English language   for instance, in Shakespeare’s play Hamlet with  s  approximately 0.5   Sizes of settlements  Income distributions amongst individuals   Size of earthquakes  Notes in musical performances  http://en.wikipedia.org/wiki/Zipf's_law http://web.archive.org/web/20121101070342/http://www.nslij-genetics.org/wli/zipf/ http://www.cut-the-knot.org/do_you_know/zipfLaw.shtml Another Way to Plot:  Zipf’s  Distribution Words by rank Word frequency p(k ) ~  k - a Zipf’s  Law in Natural Language Rank  x  Frequency     Constant – Constant ≈ 0.1 × Length of collection (in words)  – Not accurate at the tails, but accurate enough for our purposes Heaps’ Law Size of vocabulary: V(n) =  Kn b In English,  K  is between 10 and 100, β is between 0.4 and 0.6.  http://en.wikipedia.org/wiki/Heaps%27_law   Heaps’ Law (cont’d) Related to  Zipf’s  law: generative models Zipf’s  and Heaps’ law coefficients change with language  Alexander  Gelbukh , Grigori  Sidorov .  Zipf  and Heaps Laws’ Coefficients Depend on Language . Proc. CICLing-2001, Conference on Intelligent Text Processing and Computational Linguistics,  February 18–24, 2001, Mexico City. Lecture Notes in Computer Science N 2004,  ISSN 0302-9743, ISBN 3-540-41687-0, Springer- Verlag , pp. 332–335. NLP NLP Introduction to NLP Semantic Parsing Semantic Parsing Converting natural language to a logical form e.g., executable code for a specific application Example: Airline reservations Geographical query systems Stages of Semantic  Parsing Input Sentence Syntactic Analysis Syntactic structure Semantic Analysis Semantic representation Compositional Semantics Add semantic attachments to CFG rules Compositional semantics Parse the sentence syntactically Associate some semantics to each word Combine the semantics of words and non-terminals recursively Until the root of the sentence Example Input Javier likes pizza Output like(Javier , pizza ) Example S  -> NP VP   { VP.Sem ( NP.Sem )}    t VP -> V NP    { V.Sem ( NP.Sem )}     < e,t > NP -> N       { N.Sem }             e V  ->  likes    { λ  x,y  likes( x,y )   <e,< e ,t >> N  -> Javier  {Javier}            e N  -> pizza   {pizza}             e Semantic Parsing Associate a semantic expression with each node Javier likes pizza V:  λ  x,y likes(x,y) N:  pizza VP:  λ x likes( x,pizza ) N:  Javier S:  likes(Javier, pizza) Grammar with Semantic Attachments Example from Jurafsky and Martin Using CCG ( Steedman  1996) CCG representations for semantics ADJ:  λ x.tall (x) (S\NP)/ADJ :  λ f. λ x.f (x) NP:  YaoMing YaoMing          is                  tall         NP       (S\NP)/ADJ       ADJ YaoMing         λ f. λ x.f (x)      λ x. tall (x )                               S\NP                              λ x. tall (x )                           S                   Tall ( YaoMing ) CCG Parsing Example: https ://bitbucket.org/yoavartzi/spf Tutorial by  Artzi , FitzGerald,  Zettlemoyer http://yoavartzi.com/pub/afz-tutorial.acl.2013.pdf GeoQuery ( Zelle  and Mooney 1996)  Zettlemoyer and Collins (2005) Zettlemoyer and Collins (2005) Dong and  Lapata  (2016) Dong and  Lapata  (2016) Dong and  Lapata  (2016) FrameNet Represents Events, relations, states, entities 1,195 semantic frames Example:  Absorb_heat An  Entity (generally food) is exposed to a  Heat_source  whose Temperature may also be specified. Generally, the Entity undergoes some sort of change as a result of this process. Bacon was frying in the pan, and a great heap of eggs already lay steaming on a  plate. I f  it cooks at 400 for an hour, it 'll be nothing but a pile of ash ! 1,774 frame-to-frame relations Links https ://framenet.icsi.berkeley.edu/fndrupal /   http:// naacl.org/naacl-hlt-2015/tutorial-framenet.html   Abstract Meaning Representation (AMR) http://amr.isi.edu / Single  structure that includes: Predicate-Argument  Structure Named Entity Recognition Coreference Resolution Wikification [slide from Jonathan  Kummerfeld ] Example “Lassie ate four bones that she found.” [slide from Jonathan  Kummerfeld ] e / eat-01 a / animal b / bone 4 f / find-01 “Lassie” n / name wiki: “Lassie” Arg-0 Arg-0 Arg- 1 Arg- 1 Example About 14,000 people fled their homes at the weekend after a local tsunami warning was issued, the UN said on its Web site (s / say-01      :ARG0 (g / organization              :name (n / name                      :op1 "UN"))      :ARG1 (f / flee-01              :ARG0 (p / person                      :quant (a / about                               :op1 14000))              :ARG1 (h / home : poss  p)              :time (w / weekend)              :time (a2 / after                     :op1 (w2 / warn-01                             :ARG1 (t / tsunami)                             :location (l / local))))      :medium (s2 / site                : poss  g                :mod (w3 / web)))   Status of AMR AMR currently lacks Multilingual  consideration Quantifier scope Co-references across sentences Grammatical number, tense, aspect, quotation marks Many noun-noun or noun-adjective relations Many detailed frames, e.g. Earthquake (with roles for magnitude, epicenter, casualties,  etc ) [slide from Jonathan  Kummerfeld ] AMR Parsing (Wang et al. 2015,16) AMR Parsing (Wang et al. 2015,16) AMR Parsing (Wang et al. 2015,16) NLP NLP Introduction to NLP Features and Unification Need for Feature-based Grammars Example (number agreement) The dogs bites Example (count/mass nouns) many water Example in French (number and person agreement w/subject) Paul  est   parti , Michelle  est   partie ,  Ils   sont   partis ,  Elles   sont  parties Example in French (number and person agreement w/direct object) Je  l’ai  vu (I saw him), Je  l’ai   vue  (I saw her) Idea S   NP VP (but only if the person of the NP is equal to the person of the VP) Parameterized Grammars Parameterized rules, e.g., S  →  NP[ person,number,”nominative ” VP[ person,number ] VP[ person,number ]  →  V[ person,number ] NP[ person,number,”accusative ” NP[“ first”,number,”nominative ”]  →  DET[number]N[number] Appropriate modifications are needed to the parser Unification Grammars Various unification grammar formalisms LFG, HPSG, FUG Handle agreement e.g., number, gender, person Unification Two constituents can be combined only if their features can ‘unify’ Feature structures (FS or FD) Nested structures that represent all features in an attribute-value matrix Values are typed, so GENDER=PLURAL is not allowed FSs can also be represented as graphs (DAG) Feature paths (from root to a node in the graph) Example in NLTK import  nltk ; from __future__ import  print_function from  nltk.featstruct  import  FeatStruct from  nltk.sem.logic  import Variable,  VariableExpression , Expression fs1  =  FeatStruct (number='singular', person=3 ) print  (fs1) [  number = 'singular' ] [ person = 3           ] fs2  =  FeatStruct (type='NP',  agr =fs1) print (fs2) [  agr   = [ number = 'singular' ] ] [        [ person = 3          ] ] [                                ] [ type = 'NP'                     ] http://www.nltk.org/howto/featstruct.html Feature Unification Graph-matching Recursive definition Two FSs unify if they can be merged into a consistent FS Leaf nodes unify if: They are the same One can “subsume” the other Special case: One or both are blank Feature Unification U Feature Unification U FAILURE Example in NLTK fs2  =  FeatStruct (type='NP',  agr =fs1) print (fs2) [  agr   = [ number = 'singular' ] ] [        [ person = 3          ] ] [                                ] [ type = 'NP'                     ] fs3 =  FeatStruct ( agr = FeatStruct (number=Variable('?n ')), subj= FeatStruct (number=Variable ('?n '))) print(fs3 )  [  agr    = [ number = ?n ] ]  [                        ]  [  subj = [ number = ?n ] ]  print(fs2.unify(fs3 ))  [  agr    =  [ number = 'singular' ] ]  [        [  person = 3           ]  ]  [                                ]  [  subj = [ number = 'singular' ] ]  [                                ]  [  type = 'NP'                     ]  http://www.nltk.org/howto/featstruct.html Agreement with Features S   NP VP 	 {NP PERSON} = {VP PERSON}   S   Aux NP VP 	{Aux PERSON} = {NP PERSON} Verb  bites 	 {Verb PERSON} = 3 Verb  bite {Verb PERSON} = 1  Types in Semantics e   – entities,  t  – facts  < e,t >  : unary predicates – maps entities to facts <e,< e,t >>  : binary predicates << e,t >,t>  : type-raised entities Examples: “Jorge”, “he”, A123: e “Janice likes cats”: t “likes”: <e,< e,t >> “likes cats”: < e,t > “every person”: << e,t >,t> Type Coercion Programming languages How is it done in your favorite programming language? Examples in natural language I had a coffee this morning (-> one cup of coffee) I tried two wines last night (-> two types of wine) I had fish for dinner (-> some fish, not “a fish”) Subtypes and  Selectional  Restrictions Type hierarchy object > edible object > fruit > banana noun > count noun noun > mass noun Selectional  restrictions Some verbs can only take arguments of certain types Example: eat + “edible object”, believe + “idea“ Selectional  restrictions and type coercion (metonymy) I have read this title (“title” -> “book”) I like Shakespeare (“Shakespeare” -> “works by Shakespeare”) Subcategorization with Features VP   Verb 	{VP SUBCAT} = {Verb SUBCAT} 	{VP SUBCAT} = INTRANS VP  Verb NP {VP SUBCAT} = {Verb SUBCAT} {VP SUBCAT} = TRANS VP  Verb NP  NP {VP SUBCAT} = {Verb SUBCAT} {VP SUBCAT} = DITRANS Representing FSs as DAGs FS = feature structure DAG = directed acyclic graph (not a tree and not an arbitrary graph) [Example from  Jurafsky  and Martin] FS Unification [Example from  Jurafsky  and Martin] Unification Procedure [Example from  Jurafsky  and Martin] FS Unification [Example from  Jurafsky  and Martin] Unification with the  Earley  Parser Important to use the constraints during, not after parsing [Example from  Jurafsky  and Martin] [Example from  Jurafsky  and Martin] Subsumption Unification of a more general concept with a more specific concept “undefined” is the most general concept “fail” is the least general concept Subcategorization [Example from  Jurafsky  and Martin] Subcategorization [Example from  Jurafsky  and Martin] Notes FSs can have a special, “head” feature If all features have a finite domain, attribute-value grammars can be converted into a CFG The power-of-2 language doesn’t have the “constant growth property”.  It is a CSL and cannot be recognized by a CFG.  It can, however, be recognized by a mildly-context-sensitive grammar. “Power-of-2 Language” Can be generated by an  LFG (Bresnan 82) [Example from Bob Berwick] Summary Feature structures define constraints Subsumption Nested features structures Cycles are allowed (DAG) Unification Types Type  subsumption NLP NLP Introduction to NLP Syntax Syntax Is language more than just a “bag of words”? Grammatical rules apply to categories and groups of words, not individual words. Example a sentence includes a subject and a predicate. The subject is a noun phrase and the predicate is a verb phrase. Noun phrase: The cat, Samantha, She Verb phrase: arrived, went away, had dinner When people learn a new word, they learn its syntactic usage. Examples:  wug  (n),  cluvious  ( adj ) – use them in sentences Hard to come up with made up words:  forkle ,  vleer , etc. all taken. Defining Parts of Speech What do nouns typically have in common?  E.g.,  can  be preceded by “the”.  What about verbs? Verbs can be preceded by “can’t”.  Adjectives can come between “the” and a noun. How is this different from grade school definitions? Determiners a, the, many, no, five Prepositions for, to, in, without, before The Lexicon How do we think of words like cat, run, five? pronunciation, part of speech, meaning Five:  / faɪv /, numeral, “5” Ambiguity Constituents Constituents are continuous Constituents are non-crossing if two constituents share one word, then one of them must completely contain the other. Each word is a constituent Constituent Tests “coordination” test  She bought a bagel and three chocolate croissants “pronoun” test A small dog is barking in the park. It is barking in the park “question by repetition” test: I have seen blue elephants Blue elephants? * Seen blue? Seen blue elephants? “ topicalization ” test: Blue elephants, I have seen. “question” test: What  have I seen? “deletion” test Last year I saw  a blue elephant in the zoo . “semantic” test “ intuitition ” test How to generate sentences One way: tree structure Generate the tree structure first Then fill the leaf nodes with terminals A Simple Syntactic Rule The simplest rule for a sentence,  e.g. “Birds fly” S    N V Simplest Grammar S    N V N    Samantha | Min | Jorge V    left | sang | walked Sample sentences: 	Samantha sang 	Jorge left Syntax The verbs so far were intransitive (no direct object) What rules are needed next?  Transitive verbs and direct objects (“Jorge saw Samantha”) Determiners (“the cats”) Combinatorial explosion (even for the simplest form of sentences) Need for noun phrases Ditto for verb phrases Latest Grammar S    NP VP NP    DT N VP    V NP DT    the | a N    child | cat | dog V    took | saw | liked | scared | chased Sample sentences: 	a dog chased the cat 	the child saw a dog Alternatives Different expansions of a category are delineated with  ”|” NP   P N | DT CN One rule for proper nouns and another for common nouns Latest Grammar S    NP VP NP    DT CN NP   PN VP    V NP DT    the | a CN    child | cat | dog PN   Samantha | Jorge | Min V    took | saw | liked | scared | chased Sample sentences: 	 a child scared Jorge 	 Min took the child Optional categories Wherever  N  is allowed in a sentence,  DT N JJ N DT JJ N 	are also allowed We can use the notation for alternatives NP    N | DT N | JJ N | DT JJ N Optional categories can be also marked using parentheses: NP   ( DT) (JJ) N Verb Phrases Samantha ran. Samantha ran to the park. Samantha ran away. Samantha bought a cookie. Samantha bought a cookie for John. Overall structure VP    V(NP)(P)(NP) Latest Grammar S    NP VP NP    DT CN NP   PN VP    V (NP) (P) (NP) DT    the | a CN    child | cat | dog PN   Samantha | Jorge | Min P  to | for | from | in V    took | saw | liked | scared | chased | gave Sample sentences: 	 Samantha saw the cat 	 Jorge gave the cat to Min Prepositional Phrases Examples: Mary bought a book for John  in a bookstore . The bookstore sells magazines. The bookstore  on Main St . sells magazines. Mary ran away. Mary ran  down the hill . Changes are needed to both NP and VP to accommodate prepositional phrases Wherever a preposition is allowed, it can be followed by a noun phrase. Run up  NP can contain any number of PPs but only up to two NPs.  How do we revise the grammar accordingly? The Rules So Far S    NP VP NP    (DT) (JJ) N (PP) VP    V (NP) (PP) PP    P (NP) PP Ambiguity The boy saw the woman with the telescope. PP    PREP NP VP    V NP PP VP    V NP NP    DT N NP    DT N PP Repetition (*) (JJ*) = a sequence of zero or more JJ Are all sequences of adjectives allowed? a big red house * a red big house Adjective ordering in English depends on semantics! Exercise The Little Red Riding Hood Three Little Pigs The Three Musketeers The Steadfast Tin Soldier The French Connection Old Macdonald Five Golden Rings The Ancient Mariner Adjective ordering Det Number Strength Size Age Shape Color Origin Material Purpose Noun det  < number < size < color < purpose < noun strength < material < noun origin < noun Nested Sentences Examples: I don’t recall whether I took the dog out. Do you know if the mall is still open? VP    V (NP) (NP) (C S) (PP*) Can (C S) appear inside an NP? Whether he will win the elections remains to be seen. Recursion S can generate VP, VP can generate S NP can generate PP, PP can generate NP What does recursion allow? Is there a longest sentence in English? Conjunction of NPs:  NP    NP and NP Conjunction of PPs:  PP   PP and PP Conjunction of VPs:  VP   VP and VP Meta-patterns S    NP VP NP    (DT) (JJ) N (PP) VP    V (NP) (PP) PP    P (NP) Is there a meta-pattern here? XP    ( specifier ) X’ X’    X (complement) Example: NP    DT N ’ X-bar Theory http://www.unlweb.net/wiki/X-bar_theory   Meta-rules for Conjunctions Conjunction X    X and X This kind of rule even covers entire sentences S    S and S Auxiliaries Is “Aux V” a constituent? I  have seen  blue elephants and  will remember  them forever. Recursion: VP -> Aux VP Raj may have been sleeping. Is such recursion unlimited? Exercise Grammar: S   NP VP | CP VP NP  (DT) (JJ*) N (CP) (PP*) VP  V (NP) (NP) (PP*) | V (NP) (CP) (PP*)  PP  P NP CP  C S What rules are needed to generate these three sentences: 1. The small dog of the neighbors brought me an old tennis ball. 2. That  wugs  have three eyes is unproven by scientists. 3. I saw the gift that the old man gave me at the meeting. Notes Syntax helps with sentences like * The milk drank the cat The milk is drunk by the cat Overgeneration  The girl saw Undergeneration Grammar – between the two Arguments vs. Adjuncts Arguments Mandatory (e.g., “* Romeo likes”, “*likes Juliet”) Cannot be repeated (e.g., “* Juliet likes Romeo John”) Verbs can have more than one  subcategorization  frame Adjuncts Optional Typically prepositional phrases or adverbs Can be repeated (e.g., “Apparently Candace ate pizza yesterday at the restaurant with pleasure”) NLP NLP Introduction to NLP Social Network Extraction [Elson 2012] NLP NLP Introduction to NLP NACLO problems on parsing NACLO Problems Twodee  (by Jason Eisner) http://nacloweb.org/resources/problems/2013/N2013-H.pdf   One, Two, Tree (by Noah Smith,  Kevin  Gimbel , and Jason Eisner) http:// www.nacloweb.org/resources/problems/2012/N2012-R.pdf   CCG  (by Jonathan  Kummerfeld ,  Aleka  Blackwell, and Patrick  Littell ) http:// www.nacloweb.org/resources/problems/2014/N2014-O.pdf Combining categories in  Tok   Pisin  (same authors) http:// www.nacloweb.org/resources/problems/2014/N2014-P.pdf   Grammar Rules (Andrea  Schalley  and Pat  Littell ) http:// www.nacloweb.org/resources/problems/2013/N2013-F.pdf   Sk8 Parser (Pat  Littell ) http:// www.nacloweb.org/resources/problems/2009/N2009-G.pdf   Solutions to the NACLO Problems Twodee http://nacloweb.org/resources/problems/2013/N2013-HS.pdf    One, Two, Tree http:// www.nacloweb.org/resources/problems/2012/N2012-RS.pdf    CCG  http:// www.nacloweb.org/resources/problems/2014/N2014-OS.pdf   Combining categories in  Tok   Pisin   http://www.nacloweb.org/resources/problems/2014/N2014-PS.pdf    Grammar Rules  http:// www.nacloweb.org/resources/problems/2013/N2013-FS.pdf    Sk8 Parser  http:// www.nacloweb.org/resources/problems/2009/N2009-GS.pdf    NLP NLP Introduction to NLP Combinatory  Categorial   Grammar (CCG) Combinatory Categorial Grammar (CCG) Complex types E.g.,  X/Y  and  X\Y These take an argument of type  Y  and return an object of type  X .  X/Y – means that Y should appear on the right X\Y – means that Y should appear on the left Structure of CCG Categories Combinatory rules Lexicon CCG Rules Function composition X/Y   Y/Z   ->   X/Z X\Y   Z\X   ->   Z\Y X/Y   Y\Z   ->   X\Z X/Y   Z\X   ->   Z/Y Type raising X -> Y/(Y\X) X -> Y\(Y/X) Coordination Example Example from  Jonathan Kummerfeld, Aleka Blackwell, and Patrick Littell    Expressive power CCGs can generate the language a n b n c n d n , n>0  Interesting examples: I like New York I like and hate New York I like and would rather be in New York I gave a book to Chen and a laptop to Jorge I want Chen to stay and Jorge to leave I like and Chen hates, New York Where are the verb phrases? Examples from Steedman 1996 Examples from Steedman 1996 Examples from Steedman 1996 CCG in NLTK http://www.nltk.org/howto/ccg.html from  nltk.ccg  import chart, lexicon lex  =  lexicon.parseLexicon (''' :- S, NP, N, VP Det  :: NP/N Pro :: NP Modal :: S\\NP/VP TV :: VP/NP DTV :: TV/NP the =>  Det that =>  Det that => NP I => Pro you => Pro we => Pro chef => N cake => N children => N dough =>  N will => Modal should  => Modal might => Modal must => Modal and =>  var \\., var /., var to  => VP[to]/VP without => (VP\\VP)/VP[ ing ] be => TV cook => TV eat => TV cooking => VP[ ing ]/NP give => DTV is => (S\\NP)/NP prefer => (S\\NP)/NP which => (N\\N)/(S/NP) persuade => (VP/VP[to])/NP ''') CCG in NLTK http:// www.nltk.org/howto/ccg.html   parser  =  chart.CCGChartParser ( lex ,  chart.DefaultRuleSet ) for parse in  parser.parse ("you prefer that  cake".split ()):      chart.printCCGDerivation (parse)      break  you    prefer      that   cake   NP   ((S\NP)/NP)  (NP/N)   N    -----  NP       -------------       ((S\NP)/NP)                    --------                    (NP/N)                            ------                             N                   -------------->                         NP      --------------------------->                (S\NP) --------------------------------<                S http:// www.nltk.org/howto/ccg.html   for parse in  parser.parse ("that is the cake which you  prefer".split ()):       chart.printCCGDerivation (parse)     break   that      is        the    cake      which       you    prefer       NP   ((S\NP)/NP)  (NP/N)   N    ((N\N)/(S/NP))  NP   ((S\NP)/NP)  ------   NP        -------------        ((S\NP)/NP)                     --------                     (NP/N)                             ------                              N                                   ----------------                                   ((N\N)/(S/NP))                                                   -----                                                   NP                                                  ----->T                                               (S/(S\NP))                                                        -------------                                                        ((S\NP)/NP)                                                  ------------------>B                                                        (S/NP)                                  ---------------------------------->                                                (N\N)                            ----------------------------------------<                                               N                    ------------------------------------------------>                                           NP       ------------------------------------------------------------->                                  (S\NP) -------------------------------------------------------------------<                                  S CCG NACLO problem from 2014 Authors : Jonathan  Kummerfeld ,  Aleka  Blackwell, and Patrick  Littell http:// www.nacloweb.org/resources/problems/2014/N2014-O.pdf http:// www.nacloweb.org/resources/problems/2014/N2014-OS.pdf http:// www.nacloweb.org/resources/problems/2014/N2014-P.pdf http:// www.nacloweb.org/resources/problems/2014/N2014-PS.pdf CCG problem in NACLO CCG CCG Answer CCG CCG CCG CCG CCG CCG CCG Parsing CKY works fine http://openccg.sourceforge.net /   http:// 4.easy-ccg.appspot.com/do_parse?sentence=Fruit+flies+like+a+banana&nbest=5   Exercise How do you represent the following categories in CCG Nouns              Adjectives      Articles           Prepositions   Transitive verbs Intransitive verbs Exercise How do you represent the following categories in CCG Nouns             N Adjectives      N/N Articles           NP/N Prepositions   (NP\NP)/NP Transitive verbs (S\NP)/NP Intransitive verbs CCG for Dutch Cross-Serial Dependencies … because I 1  Cecilia 2  Henk 3  the hippopotamuses 4  saw 1  help 2  feed 3,4 . [Example from Stephen Clark] Notes CCG is mildly context-sensitive It can handle some phenomena that are not CFG But it can be parsed efficiently CCG rules are monotonic Movement is not allowed Complexity O(n 3 ) – with restricted type raising Unbounded – with unrestricted type raising CCG gets closer to semantics and lambda calculus CCGBank Hockenmaier   and Steedman (2005) NLP NLP LILY  Projects Deep Learning Current Projects Dialogue Systems for student advising Dialogue modeling Database scheme embedding Text Ordering for natural language generation Collective Discourse Multi-document summarization Survey generation Computational Creativity Funny caption generation Crossword puzzle solving Sentence Ordering Using Recurrent Neural Networks [ Lajanugen  et al. 2017] Sentence Ordering Using Recurrent Neural Networks [ Lajanugen  et al. 2017] Sentence Ordering Using Recurrent Neural Networks [ Lajanugen  et al. 2017] Sentence Ordering Using Recurrent Neural Networks [ Lajanugen  et al. 2017] Sentence Ordering Using Recurrent Neural Networks [ Lajanugen  et al. 2017] Sentence Ordering Using Recurrent Neural Networks [ Lajanugen  et al. 2017] Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and  Documents [Zhang et al. 2016] Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and  Documents [Zhang et al. 2016] Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and  Documents [Zhang et al. 2016] Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and  Documents [Zhang et al. 2016] Parsing Natural Language to SQL with Sequence-to-Sequence  and Sequence-to-Tree  LSTMs and Attention to Database  Schema [ Finegan-Dollak  et al., in progress] Parsing Natural Language to SQL with Sequence-to-Sequence  and Sequence-to-Tree  LSTMs and Attention to Database  Schema [ Finegan-Dollak  et al., in progress] Parsing Natural Language to SQL with Sequence-to-Sequence  and Sequence-to-Tree  LSTMs and Attention to Database  Schema [ Finegan-Dollak  et al., in progress] Parsing Natural Language to SQL with Sequence-to-Sequence  and Sequence-to-Tree  LSTMs and Attention to Database  Schema [ Finegan-Dollak  et al., in progress] NLP NLP Neural  Discourse Modeling Deep Learning Skip-Thoughts [ Kiros  et al. 2015] Skip-Thoughts [ Kiros  et al. 2015] Skip-Thoughts [ Kiros  et al. 2015] Neural Generation Deep Learning Neural Generation Very recent work (e.g., EMNLP 2015) Example: Language Generation Semantically  Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems  [Wen et al. 2015] Wen et al., 2015 (continued) [Wen et al. 2015] [Wen et al. 2015] [Wen et al. 2015] Wen et al., 2015 (continued) NLP NLP Introduction to NLP Dependency Grammars Dependency structure blue modifier, dependent, child, subordinate house head, governor, parent, regent blue house Dependency Structure Unionized workers are usually better paid than their non-union counterparts.        1               2        3       4          5       6     7      8         9                10         Dependency Structure Unionized workers are usually better paid than their non-union counterparts.        1               2        3       4          5       6     7      8         9                10         Dependency Structure Unionized workers are usually better paid than their non-union counterparts.        1               2        3       4          5       6     7      8         9                10         Dependency Structure Unionized workers are usually better paid than their non-union counterparts.        1               2        3       4          5       6     7      8         9                10         Other notations Unionized workers are usually better paid than their non-union counterparts.        1               2        3       4          5       6     7      8         9                10         PRP$ JJ NNS VBP RB RBR IN VBN JJ NNS amod poss nsubjpass auxpass advmod advmod prep poss amod Phrase Structure Unionized workers are usually better paid than their non-union counterparts.        1               2        3       4          5       6     7      8         9                10         PRP$ JJ NNS VBP RB RBR IN VBN JJ NNS NP NP PP ADVP VP VP S Dependency grammars Characteristics Lexical/syntactic dependencies between words The top-level predicate of a sentence is the root Simpler to parse than context-free grammars Particularly useful for free word order languages How to identify the heads H=head, M=modifier H determines the syntactic category of the construct H determines the semantic category of the construct H is required; M may be skipped Fixed linear position of M with respect to H Head Rules from Collins’s Thesis Non- Projectivity Rare in English Topicalization Cats, I like a lot. Extraposition The pizza is ready with pepperoni. Non-projectivity 1       Where    Where    WRB      WRB      -       2       SBJ     -       - 2       did      did      VBD      VBD      -       0       ROOT    -       - 3       you      you      PRP      PRP      -       4       SBJ     -       - 4       come     come     VBP      VBP      -       2       VC      -       - 5       from     from     IN      IN      -       4       DIR     -       - 6       last     last     JJ       JJ       -       7       NMOD    -       - 7       year     year     NN       NN       -       5       PMOD    -       - 8       ?       ?       .       .       -       2       P       -       - did Where you from come last  year  Output of  (the non-projective) MSTParser advmod (come-4, Where-1)  aux(come-4, did-2)  nsubj (come-4, you-3)  root(ROOT-0, come-4)  prep(come-4, from-5)  amod (year-7, last-6)  pobj (from-5, year-7) Output of Stanford parser Notes How to extend a projective method for non-projective parses Use a SWAP operator ( Nivre  2009) Not clear what to do with conjunctions “cats, dogs, and hamsters” Options: “cats” or “and” Rate of Non- Projectivity [ CoNLL -X data: Hall and Nilsson 2006] NLP NLP Introduction to NLP Language Models Probabilistic Language Models Assign a probability to a sentence P(S) = P(w 1 ,w 2 ,w 3 ,...,w n ) Different from deterministic methods using CFG The sum of the probabilities of all possible sentences must be 1. Predicting the Next Word Example Let’s meet in Times … General Electric has lost some market … Formula P(w n |w 1 ,w 2 ,...,w n-1 ) Predicting the Next Word What word follows “your”?  http://norvig.com/ngrams/count_2w.txt your abilities 160848  your ability 1116122  your ablum 112926  your academic 274761  your acceptance 783544  your access 492555  your accommodation 320408  your account 8149940  your accounting 128409  your accounts 257118  your action 121057  your actions 492448 your activation 459379  your active 140797  your activities 226183  your activity 156213  your actual 302488  your ad 1450485  your address 1611337  your admin 117943  your ads 264771  your advantage 242238  your adventure 109658  your advert 101178  your advertisement 172783 Uses of Language Models Speech recognition P(“recognize speech”) > P(“wreck a nice beach”) Text generation P(“three houses”) > P(“three house”) Spelling correction P(“my cat eats fish”) > P(“my xat eats fish”) Machine translation P(“the blue house”) > P(“the house blue”) Other uses OCR Summarization Document classification Usually coupled with a translation model (later) Probability of a Sentence How to compute the probability of a sentence? What if the sentence is novel? What we need to estimate: P(S)=P(w 1 ,w 2 ,w 3 … w n ) Using the chain rule: P(S)= P(w 1 ) P(w 2 |w 1 ) P(w 3 |w 1 ,w 2 )… P(w n |w 1 ,w 2 …w n-1 ) Example: P(“I would like the pepperoni and spinach pizza”)=? N-Gram Models Predict the probability of a word based on the words before: P( square|Let’s  meet in Times) Markov assumption Only look at limited history N-gram models Unigram – no context: P(square) Bigram: P( square|Times ) Trigram: P( square|in  Times) Random Text (Brown Corpus) 2-grams: 	The 53-year-old Shea was no acceptable formula to help the abuse of events were a wall in 1908 , called upon his hand in Southern New Orleans , Miss Garson was named Maurice Couve De Havilland signed a privilege resolution had had happened on a tax applied to the Chisholm , the thriving systems of the `` Pride and musician , and Moscow made good team spirit of the culmination of the metal tube through the amateur , but rather than a special prosecutor . This knowledge of each member of these savings of golf course can see the 13 straight 69 . Since 1927 by Harry Truman Cleveland of railroad retirement age groups . No Vacancy '' . `` I have to congressmen . The remainder of the rear bumper and on a benefit in U.S. amateur , as far as a thrill a $100 U.S. if not indicted . The state's occupation tax dollars over the newest product of the address he attended Arlington State University will pay half years .  Random Text (Brown Corpus) 3-grams:  	The Fulton County Jail and `` a very strong central government of Laos that the presence of picket lines and featuring a flared skirt and lace jacket with bateau neckline and princesse skirt accented by lace appliques . Her acting began with the members of the government -- such control is necessary to build in a final exchange between Moscow and Washington last week . Of course , since the views of another one . It urged that the games are not essential to provide federal contributions to the 85-student North Carolina group to play , was addressing a meeting in the manufacture of a tax bill since most of his uncle and aunt , also was particularly struck by the reams came in from shareholders of these co-operative systems , the 9th precinct of the guiding spirits of the Armed Services Committee . Davis received 1,119 votes in Saturday's election , the executive organs of participation can hardly escape the impression that he made no attempt to get agreement among the conference's top four in rushing… Random Text (Brown Corpus) 4-grams: 	The broadcast said Anderson , a Seattle ex-marine and Havana businessman , and McNair , of Miami , were condemned on charges of smuggling arms to Cuban rebels . Anderson operated three Havana automobile service stations and was commander of the Havana American Legion post before it disbanded since the start of August have shown gains averaging nearly 10% above last year . That , too , in improving motorists' access to many turnpikes . The Kansas Turnpike offers an illustration . Net earnings of that road rose from 62 per cent of the prices that the avid buyers bid it up to . Dallas and North Texas is known world-wide as the manufacturing and distribution center of cotton gin machinery and equipment . The firm is design-conscious , sales-conscious , advertising-conscious . `` Hodges predicted : ' I think we should certainly follow through on it '' . Rep. Henry C. Grover , who teaches history in the Houston public schools , would reduce from 24 to 12 semester hours the so-called `` blue law '' … Higher Order N-grams It is possible to go to 3,4,5-grams Longer n-grams suffer from sparseness N-Grams Shakespeare unigrams 29,524 types, approx. 900K tokens Bigrams 346,097 types, approx. 900K tokens How many bigrams are never seen in the data? Notice! very sparse data! Google 1-T Corpus 1 trillion word tokens Number of tokens 1,024,908,267,229  Number of sentences 95,119,665,584  Number of unigrams 13,588,391  Number of bigrams 314,843,401  Number of trigrams 977,069,902  Number of  fourgrams 1,313,818,354  Number of  fivegrams 1,176,470,663 https://catalog.ldc.upenn.edu/ldc2006t13 Estimation Can we compute the conditional probabilities directly? No, because the data is sparse Markov assumption P(“musical” | “I would like two tickets for the”) = P(“musical | the”) or P(“musical” | “I would like two tickets for the”) = P(“musical | for the”) Maximum Likelihood Estimates Use training data Count how many times a given context appears in it. Unigram example: The word “pizza” appears 700 times in a corpus of 10,000,000 words. Therefore the MLE for its probability is P’(“pizza”) = 700/10,000,000 = 0.00007 Bigram example: The word “with” appears 1,000 times in the corpus. The phrase “with spinach” appears 6 times Therefor the MLE for P’( spinach|with ) = 6/1,000 = 0.006 These estimates may not be good for corpora from other genres Example P(“<S> I will see you on Monday</S>”) = P(I|<S>)  x P(will|I)  x P(see|will)  x P(you|see)  x P(on|you)  x P(Monday|on)  x P(</S>|Monday) Example from Jane Austen P(“Elizabeth looked at Darcy”) Use maximum likelihood estimates for the n-gram probabilities unigram: P( w i )=c( w i )/V bigram: P(w i |w i-1 ) = c(w i-1 ,w i )/c(w i-1 ) Values P(“Elizabeth”) = 474/617091 = .000768120 P(“ looked|Elizabeth ”) = 5/474 = .010548523 P(“ at|looked ”) = 74/337 = .219584569 P(“ Darcy|at ”) = 3/4055 = .000739827 Bigram probability P(“Elizabeth looked at Darcy”) = .000000001316 = 1.3 x 10 -9 Unigram probability P(“Elizabeth looked at Darcy”) = 474/617091 * 337/617091 * 4055/617091 * 304/617091 = .000000000001357 = 1.3 x 10 -12 P(“looked Darcy Elizabeth at”) = ? Generative Models Unigram:  generate a word, then generate the next one, until you generate </S>. Bigram:  generate <S>, generate a word, then generate the next one based on the previous one, etc., until you generate </S>. Engineering Trick The MLE values are often on the order of 10 -6  or less Multiplying 20 such values gives a number on the order of 10 -120 This leads to underflow Use logarithms instead  10 -6  (in base 10) becomes -6 Use sums instead of products Tools http://www.speech.cs.cmu.edu/SLM_info.html   http://www.speech.sri.com/projects/srilm/ https://kheafield.com/code/kenlm/   http://htk.eng.cam.ac. uk/   NLP NLP Introduction to NLP Introduction to Parsing Parsing programming languages #include < stdio.h >     int  main()  {     int  n, reverse = 0;        printf ("Enter a number to reverse \n ");    scanf ("% d",&n );       while (n != 0)    {      reverse = reverse * 10;      reverse = reverse + n%10;      n = n/10;    }       printf ("Reverse of entered number is = %d \n ", reverse);       return 0;  } Parsing human languages Rather different than computer languages Can you think in which ways? Parsing human languages Rather different than computer languages No types for words No brackets around phrases Ambiguity Words Parses  Implied information The parsing problem Parsing means associating tree structures to a sentence, given a grammar (often a CFG) There may be exactly one such tree structure There may be many such structures There may be none Grammars (e.g., CFG) are declarative They don’t specify how the parse tree will be constructed Syntactic ambiguities PP attachment I saw the man with the telescope Gaps Mary likes Physics but hates Chemistry Coordination scope Small boys and girls are playing Particles vs. prepositions She ran up a large bill Gerund vs. adjective Frightening kids can cause trouble Applications of parsing Grammar checking I want to return this shoes. Question answering How many people in sales make $40K or more per year? Machine translation E.g., word order – SVO vs. SOV Information extraction Breaking Bad  takes place in New Mexico. Speech generation Speech understanding NLP Introduction to NLP Context-free grammars Context-free grammars A context-free grammar is a 4-tuple (N,  ,R,S) N: non-terminal symbols  : terminal symbols (disjoint from N) R: rules (A      ), where   is a string from (  N)* S: start symbol from N ["the", "child", "ate", "the", "cake", "with", "the", "fork"]       S -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' Example ["the", "child", "ate", "the", "cake", "with", "the", "fork"]       S -> NP VP       NP -> DT  N  | NP PP       PP ->  PRP  NP       VP ->  V  NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' Heads marked in bold face Example Phrase-structure grammars (1/2) Sentences are not just bags of words Alice bought Bob flowers Bob bought Alice flowers Context-free view of language A prepositional phrase looks the same whether it is part of the subject NP or part of the VP Constituent order SVO (subject verb object) SOV (subject object verb) Phrase-structure grammars (2/2) Auxiliary verbs The dog may have eaten my homework Imperative sentences Leave the book on the table Interrogative sentences Did the customer have a complaint? Who had  a complaint? Negative sentences The customer didn’t have a complaint     S -> NP VP | Aux NP VP | VP     NP -> PRON |  Det  Nom     Nom -> N | Nom N | Nom PP     PP -> PRP NP     VP -> V | V NP | VP PP      Det  -> 'the' | 'a' | 'this'     PRON -> 'he' | 'she'     N -> 'book' | 'boys' | 'girl'     PRP -> 'with' | 'in'     V -> 'takes' | 'take' A longer example What changes were made to the grammar?     S -> NP VP | Aux NP VP | VP     NP -> PRON |  Det  Nom     Nom -> N | Nom N | Nom PP     PP -> PRP NP     VP -> V | V NP | VP PP      Det  -> 'the' | 'a' | 'this'     PRON -> 'he' | 'she'     N -> 'book' | 'boys' | 'girl'     PRP -> 'with' | 'in'     V -> 'takes' | 'take' A longer example     S -> NP VP |  Aux NP VP  | VP     NP -> PRON |  Det  Nom     Nom ->  N  | Nom N | Nom PP     PP -> PRP NP     VP -> V | V NP | VP PP      Det  -> 'the' | 'a' | 'this'     PRON -> 'he' | 'she'     N -> 'book' | 'boys' | 'girl'     PRP -> 'with' | 'in'     V -> 'takes' | 'take' A longer example Penn Treebank Example ( (S      (NP-SBJ        (NP (NNP Pierre) (NNP  Vinken ) )       (, ,)        (ADJP          (NP (CD 61) (NNS years) )         (JJ old) )       (, ,) )     (VP (MD will)        (VP (VB join)          (NP (DT the) (NN board) )         (PP-CLR (IN as)            (NP (DT a) (JJ nonexecutive) (NN director) ))         (NP-TMP (NNP Nov.) (CD 29) )))     (. .) )) ( (S      (NP-SBJ (NNP Mr.) (NNP  Vinken ) )     (VP (VBZ is)        (NP-PRD          (NP (NN chairman) )         (PP (IN of)            (NP              (NP (NNP Elsevier) (NNP N.V.) )             (, ,)              (NP (DT the) (NNP Dutch) (VBG publishing) (NN group) )))))     (. .) )) Center Embedding Center Embedding The rat ate the seed. The rat that the cat ate  ate  the seed. The rat that the cat that the dog ate  ate   ate  the seed. ... Is this language a CFL? Notes CFG cannot describe bounded recursion Competence vs. performance CFGs are equivalent to PDAs Example Consider the language  x n y n stack is empty, input= xxxyyy push * onto stack, input= xxyyy push * onto stack, input= xyyy push * onto stack, input= yyy pop * from stack, input= yy pop * from stack, input=y pop * from stack, input=“” Leftmost derivation A leftmost derivation is a sequence of strings s 1 , s 2 , ..., s n s 1  = S, the start symbol s n  includes only terminal symbols Example: [S] [S] [NP VP]  [S] [NP VP] [DT N VP]  … [S] [NP VP] [DT N VP] ... [the child ate the cake with the fork] S Leftmost derivation NLP Introduction to NLP Semi-Supervised Learning on Graphs Random walks and harmonic functions Drunkard’s walk: Start at position 2 on a line What is the prob. of reaching 5 before reaching 0?  Harmonic functions: P(0) = 0 P(N) = 1 P(x) = ½*p(x-1)+ ½*p(x+1), for 0<x<N (in general, replace ½ with the bias in the walk) 0 1 2 3 4 5 Learning harmonic functions The method of relaxations Discrete approximation. Assign fixed values to the boundary points. Assign arbitrary values to all other points. Adjust their values to be the average of their neighbors. Repeat until convergence. Monte Carlo method Perform a random walk on the discrete representation. Compute f as the probability of a random walk ending in a particular fixed point. Eigenvector methods Look at the stationary distribution of a random walk Eigenvectors and eigenvalues An eigenvector is an implicit “direction” for a matrix   where  v  (eigenvector)   is non-zero, though  λ  (eigenvalue) can be any complex number in principle Computing eigenvalues: Eigenvectors and eigenvalues Example: Det  (A- l I ) = (-1- l )*(- l )-3*2=0 Then:  l + l 2 -6=0;    l 1 =2;    l 2 =-3 For  l 1 =2: Solutions: x 1 =x 2 Stochastic matrices Stochastic matrices: each row (or column) adds up to 1 and no value is less than 0. Example: The largest eigenvalue of a stochastic matrix  E  is real:  λ 1  = 1.  For  λ 1 , the left (principal) eigenvector is  p , the right eigenvector =  1 In other words,  G T p  =  p . Ergodic (connected) Markov chain with transition matrix  P 1  Ω 1  Ω 1  Ω 0.5  Ω 0.5  Ω a b c d w=Pw From Doyle and Snell 2000 Electrical networks and random walks Electrical networks and random walks 1 V v x   is the probability that a random walk starting at  x  will reach  a  before reaching  b .  The random walk interpretation allows us to use Monte Carlo methods to solve electrical circuits. 1  Ω 1  Ω 1  Ω 0.5  Ω 0.5  Ω a c d b Markov  Chains A homogeneous Markov chain is defined by an initial distribution  x  and a Markov kernel  E . Path = sequence ( x 0 ,  x 1 , …,  x n ). X i  =  x i-1 * E   The probability of a path can be computed as a product of probabilities for each step  i . Random walk = find  X j   given  x 0 , E, and  j . Stationary solutions The fundamental Ergodic Theorem for Markov chains [ Grimmett  and  Stirzaker  1989] says that the Markov chain with kernel  E  has a stationary distribution  p  under three conditions: E  is stochastic E  is irreducible  E  is aperiodic To make these conditions true: All rows of  E  add up to 1 (and no value is negative) Make sure that  E  is strongly connected Make sure that  E  is not bipartite Example: PageRank [ Brin  and Page 1998]: use “teleportation”           Example This graph  E  has a second graph  E’ (not drawn)  superimposed on it: E’  is the uniform transition graph.  Computing the  Stationary Distribution function   PowerStatDist  ( E ): begin     p (0)  =  u ;   (or  p (0)  = [1,0,…0])     i =1;     repeat        p ( i )  =  E T p (i-1)        L  = || p ( i ) -p (i-1 ) || 1 ;        i   =  i  +  1 ;     until   L  <       return   p ( i ) end Solution for the stationary distribution Convergence rate is O(m) Example Other Interesting Concepts of Random Walks Hitting time h  is the expected time in a random walk to reach  vertex v starting from vertex u Commute time The expected time to start from u, reach v, and then return to u Cover time The expected time to start from u and visit every other vertex at least once Meeting time The expected time of two random walks to start from u and v and meet at some vertex in between Handwritten digit  recognition with pixel-wise  similarity - Graph from Jerry Zhu’s tutorial in ICML 07 Learning on graphs Search for a lower dimensional manifold Relaxation method Monte Carlo method Supervised vs. semi-supervised Example from Zhu et al. 2003 Learning on graphs Example: Example from Zhu et al. 2003 Semi-supervised Classification  d1 is democratic  d2 is republican  What can we say about d3 and d4? - Graph from Jerry Zhu’s tutorial in ICML 07 Semi-supervised learning:  Start with limited training data Leverage large scale unlabeled data  Different methods vary on how to use the unlabeled data Absorbing Random Walk Absorbing states: A random walk cannot get out once it reaches that node Absorbing random walk: Random walk with absorbing states i s t Absorption probability P i (s):  If the random walk starts from vertex i, how likely it will be eventually absorbed by vertex s? Computing Absorption Probabilities i s t For the absorbing state: P s (s) = 1 For other absorbing states:  P t (s) = 0 For  all other vertices:  Iterative computation Absorption  Probability for Classification Make the positive nodes and negative nodes as absorbing states; For each vertex, compute the probability that it will be absorbed by the positive states Classify this vertex as positive if p > 0.5, and negative otherwise Semi-supervised passage retrieval Graph-based semi-supervised learning. The idea is to propagate information from labeled nodes to unlabeled nodes using the graph connectivity. A passage can be either positive (labeled as relevant) or negative (labeled as not relevant), or unlabeled. Otterbacher, Erkan and Radev 2005 Network based approaches No clear notion of “distance” now, but the “scores” are estimated based on some propagation process.  Score propagation based on random walk Two approaches: Starting from the user, how likely/how long can I reach the object? Starting from the object, how likely/how long can I hit the user?  Natural Language Processing Spectral Clustering Case Study: Spectral  Clustering Ng et al On Spectral clustering: analysis and algorithm K-means Spectral  Clustering Graph Partitioning with Graph Cuts Build document network using similarity Text clustering = cutting edges to get k (e.g., k = 2) disconnected components But where to cut? (what to optimize?) What to Optimize: Minimum Cut MinCut - minimize the number (or weight) of edges to cut Objective = s(A, B) =  s(A, B): (weight) of edges from community A to community B Other Graph Cut Objectives Ratio Cut Minimize the weights of cut edges and balance the size of each community Objective:  Normalized Cut Minimize the weights of cut edges and balance the degree of each community Objective:   Min-Max-Cut Minimize the weights of cut edges and balance the edge weights inside each community Objective:     How to Optimize? – Spectral Clustering Find the actual solution of graph cuts using spectral clustering. Spectral Algorithms The spectrum of a matrix is the list of all eigenvectors of a matrix. The eigenvectors in the spectrum are sorted by the absolute value of their corresponding eigenvalues. In spectral methods, eigenvectors are based on the Laplacian of the original matrix. Spectral Clustering Algorithms that cluster points using eigenvectors of matrices derived from the data Obtain data representation in the low-dimensional space that can be easily clustered Variances on: What matrix to use?  Which eigenvectors to use?  How to derive clusters from these eigenvectors? Very hot in machine learning, but math intensive. Good tutorial:  http://ranger.uta.edu/~chqding/Spectral/   Laplacian Matrix The Laplacian L of a matrix is a symmetric matrix. L = D – G, where D is the degree matrix corresponding to G. Example: A B C G F E D Fiedler Vector The Fiedler vector is the eigenvector of L(G) with the second smallest eigenvalue. A B C G F E D Spectral Bisection Algorithm Compute  l 2 Compute the corresponding v2 For each node n of G If v2(n) < 0 Assign n to cluster C1 Else if v2(n) > 0 Assign n to cluster C2 Else if v2(n) = 0 Assign n to cluster C1 or C2 at random Spectral Clustering: Pros and Cons Pros: Solid mathematics background Explicit objective function to optimize Yields to very good results in general Effective to handle complex shapes Cons: Usually not efficient Not sure which objective is the right one to use Molistic NACLO problem (2007) Molistic NLP Natural Language Processing Dialogue Systems Dialogue Systems Architecture Understanding Dialogue manager Task manager Generation Policies System initiative The system controls the dialogue It tries to collect all the information needed to complete the task It asks follow up questions It may ignore some of the irrelevant answers Some universals: e.g., help, quit, restart Mixed initiative NLP NLP Linear  Algebra 2 NLP External Links http :// cs229.stanford.edu/section/cs229-linalg.pdf   https :// www.khanacademy.org/math/linear-algebra   http :// ocw.mit.edu/courses/mathematics/18-06sc- linear-algebra-fall-2011/   https ://www.math.ucdavis.edu /~ linear/linear.pdf   NLP NLP Text Similarity Vector Semantics What does “acerola” mean? acerola  is  a significant source of vitamin  C. the  pulp of the  acerola  is very soft acerola  are now found growing in most sub-tropical regions of the  world. acerola   can be eaten fresh or used to make jams or jellies . Distributional similarity Two words that appear in similar contexts are likely to be semantically related, e.g., schedule a test  drive  and investigate  Honda 's financing options Volkswagen  debuted a new version of its front-wheel- drive  Golf the  Jeep  reminded me of a recent  drive Our test  drive  took place at the wheel of loaded  Ford  EL model “You will know a word by the company that it keeps.” (J.R. Firth 1957 ) Basic Ideas Represent words as vectors For example, based on nearby words Similar words (synonyms) should have similar representations Different senses of the same word should have different representations Relations should be preserved For example, “cat”-”kitten” should be similar to “dog”-”puppy” Context Features The context  features can  be any of the following: The word before the target word The word after the target word Any word within  n  words of the target word Any word within a specific syntactic relationship with the target word (e.g., the head of the dependency or the subject of the sentence) Any word within the same sentence Any word within the same document Example S1: schedule  a test  drive  and investigate  Honda 's financing options S2:  Volkswagen   debuted a new version of its front-wheel- drive  Golf S3: the  Jeep  reminded me of a recent  drive S4: Our  test  drive  took place at the wheel of loaded  Ford  EL model Association Strength Frequency matters we want to ignore spurious word pairings. however, frequency alone is not sufficient, e.g., being near the words “and” or “the is not very informative Pointwise mutual information (PMI).  Here  w  is a word and  c  is a feature from the context Association Strength Positive PMI (PPMI): If PMI( w,c )<0, that indicates that the word and the feature are negatively associated In PPMI, negative values are replaced by zeros Smoothing may be needed If the vectors are probabilities Use  Kullback-Leibler  Divergence Or Jensen-Shannon Divergence NLP NLP Introduction to NLP Information Extraction Information Extraction Usually from unstructured or semi-structured data Examples News stories Scientific papers Resumes  Entities Who did what, when, where, why Build knowledge  base (KBP Task) Named Entities Types: People Locations Organizations Teams, Newspapers, Companies  Geo-political entities Ambiguity: London can be a person, city, country (by metonymy) etc. Useful for interfaces to databases, question answering, etc. Times and Events Times Absolute expressions Relative expressions (e.g., “last night”) Events E.g., a plane went past the end of the runway Named Entity Recognition (NER) Segmentation Which words belong to a named entity? Brazilian football legend  Pele 's condition has improved, according to a  Thursday evening  statement from a  Sao Paulo  hospital. Classification What type of named entity is it? Use gazetteers, spelling, adjacent words, etc. Brazilian football legend [ PERSON  Pele]'s condition has improved, according to a [ TIME  Thursday evening] statement from a [ LOCATION  Sao Paulo] hospital. NER, Time, and Event extraction Brazilian football legend [ PERSON  Pele]'s condition has improved, according to a [ TIME  Thursday evening] statement from a [ LOCATION  Sao Paulo] hospital. There had been earlier concerns about Pele's health after [ ORG  Albert Einstein Hospital] issued a release that said his condition was "unstable.“ [ TIME  Thursday night]'s release said [ EVENT  Pele was relocated] to the intensive care unit because a kidney dialysis machine he needed was in ICU. Event Extraction Event Extraction Named Entities Named Entity Recognition (NER) IOB Representation Sample Input for NER ( (S      (NP-SBJ-1        (NP (NNP Rudolph) (NNP Agnew) )       (, ,)        (UCP          (ADJP            (NP (CD 55) (NNS years) )           (JJ old) )         (CC and)          (NP            (NP (JJ former) (NN chairman) )           (PP (IN of)              (NP (NNP Consolidated) (NNP Gold) (NNP Fields) (NNP PLC) ))))       (, ,) )     (VP (VBD was)        (VP (VBN named)          (S            (NP-SBJ (-NONE- *-1) )           (NP-PRD              (NP (DT a) (JJ nonexecutive) (NN director) )             (PP (IN of)                (NP (DT this) (JJ British) (JJ industrial) (NN conglomerate) ))))))     (. .) )) Sample Output for NER (IOB format) file_id   sent_id   word_id   iob_inner   pos  word     0002  1  0 B-PER   NNP   Rudolph  0002  1  1 I-PER   NNP   Agnew  0002  1  2 O       COMMA  COMMA  0002  1  3 B-NP    CD    55  0002  1  4 I-NP    NNS   years  0002  1  5 B-ADJP  JJ    old  0002  1  6 O       CC    and  0002  1  7 B-NP    JJ    former  0002  1  8 I-NP    NN    chairman  0002  1  9 B-PP    IN    of   0002  1 10 B-ORG   NNP   Consolidated  0002  1 11 I-ORG   NNP   Gold  0002  1 12 I-ORG   NNP   Fields  0002  1 13 I-ORG   NNP   PLC  0002  1 14 O       COMMA  COMMA  0002  1 15 B-VP    VBD   was  0002  1 16 I-VP    VBN   named  0002  1 17 B-NP    DT    a  0002  1 18 I-NP    JJ    nonexecutive  0002  1 19 I-NP    NN    director  0002  1 20 B-PP    IN    of  0002  1 21 B-NP    DT    this  0002  1 22 I-NP    JJ    British  0002  1 23 I-NP    JJ    industrial  0002  1 24 I-NP    NN    conglomerate      0002  1 25 O       .     . NER Demos http://nlp.stanford.edu:8080/ner / http:// cogcomp.org/page/demo_view/ner http:// demo.allennlp.org/named-entity-recognition NER Extraction Features NER Extraction Features Feature Encoding in NER NER as Sequence Labeling Many NLP problems can be cast as sequence labeling problems POS – part of speech tagging NER – named entity recognition SRL – semantic role labeling  Input Sequence w 1 w 2 w 3 Output Labeled words Classification methods Can use the categories of the previous tokens as features in classifying the next one Direction matters NER as Sequence Labeling Temporal Expressions Temporal Lexical Triggers TempEx  Example TimeML TimeBank The Message Understanding Conference (MUC) MUC Example MUC Annual competition DARPA, 1990s Events in news stories Terrorist events Joint ventures Management changes Evaluation metrics Precision Recall F-measure MUC Example Example from  Grishman  and  Sundheim  1996 MUC in FASTUS Biomedical example Gene labeling Sentence: [ GENE  BRCA1] and [ GENE  BRCA2] are human genes that produce tumor suppressor proteins Other Examples Job announcements Location, title, starting date, qualifications, salary Seminar announcements Time, title, location, speaker Medical papers Drug, disease, gene/protein, cell line, species, substance  Filling the Templates Some fields get filled by text from the document E.g., the names of people Others can be pre-defined values  E.g., successful/unsuccessful merger Some fields allow for multiple values Perl Regular Expressions Perl Regular Expressions Perl Regular Expressions Sample Patterns Price (e.g., $14,000.00) \$[0-9,]+(\.[0-9]{2})? Date (e.g., 2015-02-01) ^(19|20)\d\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])$ Email ^[_a-z0-9-]+(\.[_a-z0-9-]+)*@[a-z0-9-]+(\.[a-z0-9-]+)*(\.[a-z]{2,4})$ Person May include HTML code May include POS information May include  Wordnet  information Evaluating  T emplate-Based NER For each test document Number of correct template extractions Number of slot/value pairs extracted Number of extracted slot/value pairs that are correct NLP NLP Introduction to NLP Feature Selection Feature selection: The   2  test For a term  t: C=class, i t  = feature Testing for independence: P(C=0,I t =0) should be equal to P(C=0) P(I t =0) P(C=0) = (k 00 +k 01 )/n P(C=1) = 1-P(C=0) = (k 10 +k 11 )/n P(I t =0) = (k 00 +K 10 )/n P(I t =1) = 1-P(I t =0) = (k 01 +k 11 )/n Feature Selection: The   2  test High values of   2  indicate lower belief in independence. In practice, compute   2  for all words and pick the top  k  among them. Mutual Information Measures amount of information X = word; Y = class if  the distribution is the same as the background distribution, then  MI=0 Documents  are assumed to be generated according to the multinomial  model NLP NLP Sequence-to-sequence and Neural Machine Translation Deep Learning Machine Translation (MT) Goal: Translate text from one language to the other Both Language Understanding and Language Generation Progress in Machine Translation https:// nlp.stanford.edu /projects/ nmt /Luong-Cho-Manning-NMT-ACL2016-v4.pdf Statistical Machine Translation Rise in early 90s Exploit Parallel Text Word  alignment Slide credit to Karl  Stratos Neural Machine Translation Modeling the machine translation using neural networks Encoder for language understanding in source language Decoder for language generation in target language https:// nlp.stanford.edu /projects/ nmt /Luong-Cho-Manning-NMT-ACL2016-v4.pdf Use RNNs as Encoder and Decoder A Recurrent Neural Network, or RNN, is a network that operates on a sequence and uses its own output as input for subsequent steps. A sequence-to-sequence network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and the decoder. The encoder reads an input sequence and outputs vectors, and the decoder reads encoder outputs as input to produce an output sequence. Slides from Rui Zhang Sequence to Sequence Model Sutskever , Le, and  Vinyals  2014 Sutskever , Le, and  Vinyals  2014 Sequence to Sequence Model Sequence to Sequence Model Sutskever , Le, and  Vinyals  2014 Sutskever , Le, and  Vinyals  2014 Sequence to Sequence Model From Language Modeling to MT [ Bengio  et al. JMLR 2003] Multilingual Embeddings [Hermann and  Blunsom   2013:  https:// arxiv.org/abs/1312.6173] Use RNNs as Encoder and Decoder https:// nlp.stanford.edu /pubs/luong2016iclr_multi.pdf  [Koehn 2017] [Koehn 2017] [Koehn 2017] [Koehn 2017] [Koehn 2017] Beam Decoding [Koehn 2017] [Koehn 2017] Beam Decoding [Koehn 2017] Domain Transfer [Koehn 2017] [Koehn 2017] GRU Encoder For every input word in the sentence, it is first used to index a word embedding matrix to get its embedding. Then the encoder produces an output vector and a hidden state from the word embedding and the previous hidden state.  The hidden state is used for the next input word. The  initial hidden state is initialized as a zero vector. GRU Decoder The decoder is another RNN that outputs a sequence of words.  The simple decoder uses only the last output of the encoder, which is called context vector. At each step, the decoder takes an input word and the previous hidden state.  At the very beginning, the input token is the start-of-sentence<SOS>token, and the hidden state is the context vector encoding the meaning of the source sentence.  Then the decoder will work as illustrated below to produce an output vector and a hidden state for next step.  The output vector is a probability distribution over the target language vocabulary. Adding  Attention  to the Decoder The simple decoder takes the final hidden state of the encoder and uses that to decode the target  sentence. This  requires to encode the entire sentence into a single fixed-size vector, which is difficult. To  solve this, we use the attention mechanism such that all the hidden states of the encoder are used to decode the target sentence. Attention Illustration At each step of decoding, the decoder attention focuses on different parts of the input sentence. [ Bahdanau , Cho,  Bengio   ICLR 2015] Google Neural Translation System https://research.googleblog.com/2016/09/a-neural-network-for-machine.html Google Neural Translation System https://research.googleblog.com/2016/09/a-neural-network-for-machine.html Data Needs: SMT vs. NMT Slide from Philipp Koehn Software/Code in Homework5 - NMT Data Processing Read normalize, filter sentence pairs prepare_data.py PyTorch Build, Train, Evaluate NMT model run_nmt.py ,  network.py Matplotlib Plot Attention produced by NMT model Plot.py BLEU evaluator Calcualte  BLEU scores given system outputs and gold standard sentences Bleu.py NLP NLP Natural Language Processing Introduction and Class Logistics Quiz Where is this quote from? Dave Bowman : Open the pod bay doors, HAL. HAL : I’m sorry Dave. I’m afraid I can’t do that. Quiz Answer “2001: A Space Odyssey”  1968 film by Stanley Kubrick  based on a joint screenplay with Arthur C. Clarke. Watson Example http://www.geekwire.com/2013/ibm-takes-watson-cloud/   What is Natural Language Processing Natural Language Processing (NLP)  It is the study of the computational treatment of natural (human) language. In other words, teaching computers how to understand (and generate) human language. Modern Applications Search engines Google, Yahoo, Bing,  Baidu Question  answering IBM’s Watson Natural language assistants Apple’s Siri, MS Cortana Translation systems Google Translate News digest Yahoo! Automated earthquake reports LA Times Automated stock market reports Narrative Science Notes Computers are confused by (human) language Specific techniques are needed NLP draws on research in  many fields Linguistics , Theoretical Computer Science, Mathematics, Statistics, Artificial Intelligence, Psychology, Databases, etc . CPSC 477/577 Instructor: Dragomir Radev dragomir.radev@yale.edu Class times: TTh  1-2:15 LORIA building TF/ULA: Corina  Grigore ,  Rui  Zhang Elaine  Hou , Will Merrill,  Soumya   Kambhampati ,  Eren   Orbey Others  tba NLP Textbook:  Speech and Language Processing by  Jurafsky  and Martin Second edition, 2009 http://web.stanford.edu/~jurafsky/slp3 /   Additional readings: Natural Language Processing using NLTK (Bird et al.)  http://www.nltk.org    Other Available Books Foundations  of Statistical Natural Language  Processing Chris Manning and  Hinrich   Sch ü tze http://nlp.stanford.edu/fsnlp /   Natural Language Understanding James Allen (older book) Course Dates Jan 16  18 23 25 30 Feb 1  6 8 13 15 20 22 27 Mar 1  6 8 27 29 Apr 3  5 10 12 17 19 24  26 Exams Midterm exam around March 9 (date  tba ) Final  exam scheduled  May  8  at 2 pm Structure of the Course Linguistic, mathematical, and computational background Computational models of morphology, syntax, semantics, discourse, pragmatics Core NLP technology: parsing, part of speech tagging, text generation, semantic analysis, etc. Applications: text classification, sentiment analysis, text summarization, question answering, machine translation, information extraction, etc. Neural Networks and Deep Learning:  autoencoders , recurrent NN, LSTM, etc. Major Goals of the Class Learn the basic principles and theoretical issues underlying natural language processing  Understand why language processing is hard Learn techniques and tools used to develop practical, robust systems that can understand text and communicate with users in one or more languages  Understand the limitations of these techniques and tools Gain insight into some open research problems in natural language processing Syllabus Book  sections (as of the Second Edition) Introduction (chapter 1) Words (chapters 2-6) Syntax (chapters 12-16) Semantics and Pragmatics (chapters 17-21) Applications (chapters 22-25) Draft Syllabus Relevant Background Linear algebra vectors and matrices Probabilities random variables discrete and continuous distributions Bayes’ theorem Programming Python in a UNIX environment. text manipulation Background Links Matrix multiplication https :// www.intmath.com/matrices-determinants/matrix-multiplication-examples.php   Bayes theorem https ://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem /   Derivative of the sigmoid function https ://beckernick.github.io/sigmoid-derivative-neural-network /     Grading Assignments (50%) HW0+HW1 =  2+8=10 % HW2 = 10% HW3 = 10% HW4 = 10% HW5  = 10 % Exams (45%) midterm = 20% final  exam = 25% Class  participation ( 5%) In-class participation , asking questions on Piazza, answering question on Piazza, office  hours Sample Programming Assignments Language Modeling and Part of Speech Tagging Dependency Parsing Vector Semantics and Word Sense Disambiguation Question Answering Deep Learning  Machine Translation Sentiment Analysis Natural Language Interface to a Database Semantic Parsing How to get the most out of the class? Attend the lectures and study the slides Course syllabus + slides  = road map  Some material may not be found in any of the readings Hands on experience Implement what you’ve learned  Ask questions in and after class Questions? Use the right channel for communication Piazza In special cases (e.g., sickness, regrading), use email Include [CPSC477] or [CPSC577] or [NLP Class] in the subject line Office Hours:  TBA Courses at Other Places Brick-and-Mortar Johns Hopkins (Jason Eisner) Cornell (Lillian Lee) Stanford (Chris Manning, Dan  Jurafsky , Richard  Socher , Chris Potts) Maryland (Hal  Daumé ) Berkeley (Dan Klein) Texas (Ray Mooney) Harvard (Sasha Rush) Illinois (Julia  Hockenmaier ) Coursera Manning/ Jurafsky  (2012, survey) Michael Collins (2013, more advanced) Radev (2015-2016, survey) The Association for Computational Linguistics (ACL) The Alphabet Soup NLP (Natural Language Processing) CL (Computational Linguistics) IR (Information Retrieval) SP (Speech Processing) HLT (Human Language Technology) NLE (Natural Language Engineering) ML (Machine Learning) Research in NLP Conferences:  ACL, NAACL, EMNLP, SIGIR, AAAI/IJCAI, COLING, HLT, EACL, AMTA/MT Summit, ICSLP,  Interspeech , NIPS, ICLR Journals:  Computational Linguistics, TACL, Natural Language Engineering, Information Retrieval, Information Processing and Management, ACM Transactions on Information Systems, ACM TALIP, ACM TSLP University centers:  Berkeley,  Columbia, Stanford , CMU, JHU, Brown, UMass, MIT,  UPenn , USC/ISI, Illinois, Michigan, Yale, Washington, Maryland, NYU, etc. Toronto, Edinburgh, Cambridge,  Sheffield, Saarland , Trento, Prague, QCRI, NUS, and many others Industrial research sites:  G oogle ,  Facebook, MSR ,  Yahoo, IBM, SRI, BBN, MITRE, Baidu, Salesforce The ACL Anthology http://www.aclweb.org/anthology The ACL Anthology Network (AAN) http ://tangra.cs.yale.edu/newaan     LILY Projects Text summarization Crosslingual  Information Retrieval Deep learning Citation analysis Survey generation Collective discourse Paraphrasing Dialogue systems Multilingual computing Academic Honesty Unless otherwise specified in an assignment all submitted work must be your own, original work. Any excerpts, statements, or phrases from the work of others must be clearly identified as a quotation, and a proper citation provided.  Any  violation of the  University’s  policy on Academic and Professional Integrity  will  result in serious penalties, which might range from failing an assignment, to failing a course, to being expelled from the program.  Violations  of academic and professional integrity will be reported to  Student  Affairs. Consequences impacting assignment or course grades are determined by the faculty instructor; additional sanctions may be  imposed. Student Mental Health and Wellbeing Yale University is  committed to advancing the mental health and wellbeing of its students.  If  you or someone you know is feeling overwhelmed, depressed, and/or in need of support, services are available.  Students with Disabilities If you think you need an accommodation for a disability, please let me know at your earliest convenience.  Some  aspects of this course, the assignments, the in-class activities, and the way we teach may be modified to facilitate your participation and progress.  I  will treat any information that you provide in as confidential a manner as possible.  NLP NLP Introduction to NLP Background for NLP Linguistic Knowledge Constituents: Children  eat pizza. They  eat pizza. My cousin’s neighbor’s children  eat pizza. Eat pizza! Collocations: Strong beer but *powerful beer Big sister but *large sister Stocks rise but ?stocks ascend  in the past: 225,000 hits vs. 47 hits on Google, now 550,000 vs 57,000 How to get this knowledge in the system: Manual rules Automatically acquired from large text collections (corpora) Linguistic knowledge Knowledge about language: Phonetics and phonology - the study of sounds Morphology - the study of word components Syntax - the study of sentence and phrase structure Lexical semantics - the study of the meanings of words Compositional semantics - how to combine words Pragmatics - how to accomplish goals Discourse conventions - how to deal with units larger than utterances Separate  lecture Finite-state  Automata Theoretical Computer Science Automata Deterministic and non-deterministic finite-state automata Push-down automata Grammars Regular grammars Context-free grammars Context-sensitive grammars Complexity Algorithms Dynamic programming Artificial Intelligence Logic First-order logic Agents Speech acts Search Planning Constraint satisfaction Machine  learning Neural Networks Reinforcement Learning Mathematics and Statistics Statistics Probabilities Statistical models Hypothesis testing Mathematics Linear algebra (e.g., vectors) Calculus (e.g., gradients) Optimization Numerical methods Mathematical and Computational Tools Language models Estimation methods Context-free  grammars ( CFG) for  trees Hidden Markov Models ( HMM) for  sequences Conditional  Random Fields (CRF) Optimization Statistical Techniques Vector space representation for WSD Noisy channel models for MT Random walk methods for sentiment analysis NLP NLP Introduction to NLP Word  Sense Disambiguation Introduction Polysemy Words have multiple senses (about 2 on average in WordNet) Example Let’s have a drink in the bar I have to study for the bar Bring me a chocolate bar Homonymy May I come in? Let’s meet again in May Part of speech ambiguity Joe won the first round Joe has a round  toy Senses of the word “bar” S:  (n)  barroom ,  bar ,  saloon ,  ginmill ,  taproom  (a room or establishment where alcoholic drinks are served over a counter)  "he drowned his sorrows in whiskey at the bar"   S:  (n)  bar  (a counter where you can obtain food or drink)  "he bought a hot dog and a coke at the bar"   S:  (n)  bar  (a rigid piece of metal or wood; usually used as a fastening or obstruction or weapon)  "there were bars in the windows to prevent escape"   S:  (n)  measure ,  bar  (musical notation for a repeating pattern of musical beats)  "the orchestra omitted the last twelve bars of the song"   S:  (n)  bar  (an obstruction (usually metal) placed at the top of a goal)  "it was an excellent kick but the ball hit the bar"   S:  (n)  prevention ,  bar  (the act of preventing)  "there was no bar against leaving"; "money was allocated to study the cause and prevention of influenza"   S:  (n)  bar  ((meteorology) a unit of pressure equal to a million dynes per square centimeter)  "unfortunately some writers have used bar for one dyne per square centimeter"   S:  (n)  bar  (a submerged (or partly submerged) ridge in a river or along a shore)  "the boat ran aground on a submerged bar in the river"   S:  (n)  legal profession ,  bar ,  legal community  (the body of individuals qualified to practice law in a particular jurisdiction)  "he was admitted to the bar in New Jersey"   S:  (n)  stripe ,  streak ,  bar  (a narrow marking of a different color or texture from the background)  "a green toad with small black stripes or bars"; "may the Stars and Stripes forever wave"   S:  (n)  cake ,  bar  (a block of solid substance (such as soap or wax))  "a bar of chocolate"   S:  (n)  Browning automatic rifle ,  BAR  (a portable .30 caliber automatic rifle operated by gas pressure and fed by cartridges from a magazine; used by United States troops in World War I and in World War II and in the Korean War)  S:  (n)  bar  (a horizontal rod that serves as a support for gymnasts as they perform exercises)  S:  (n)  bar  (a heating element in an electric fire)  "an electric fire with three bars"   S:  (n)  bar  ((law) a railing that encloses the part of the courtroom where the judges and lawyers sit and the case is tried)  "spectators were not allowed past the bar"   Word Sense Disambiguation Task given a word and its context determine which sense it is Useful for Machine Translation e.g., translate “play” into Spanish p lay the violin =  tocar  el  v iolín   p lay tennis =  jugar  al  tenis Other uses Document retrieval (jaguar) Accent restoration (cote) Text to speech generation (lead) Spelling correction (aid/aide) Capitalization restoration (Turkey) Dictionary Method ( Lesk ) Match sentences to dictionary definitions Examples of plant (m-w.com): plant 1  = a  living thing that grows in the ground, usually has leaves or flowers, and needs sun and water to  survive plant 2  = a building or factory where something is  made Examples of leaf leaf 1  = a lateral outgrowth from a plant stem that is typically a flattened expanded variably shaped greenish organ, constitutes a unit of the foliage, and functions primarily in food manufacture by  photosynthesis leaf 2  = a part of a book or folded sheet containing a page on each side  Find the pair of meanings that have the most overlapping definitions “The  leaf  is the  food making  factory  of green  plants.”   Decision Lists ( Yarowsky ) Method introduced by  Yarowsky  (1994) Two senses per word Ordered rules:  collocation -> sense Formula Decision Lists ( Yarowsky ) fish  within window -> bass1 striped bass  -> bass1 guitar  within window -> bass2 bass player -> bass2 play /V  bass   -> bass2 Naïve Bayes E.g., work by Bill Gale Correct sense =  argmax i  P( sense i |context ) P( context|sense i )     P j  P( word j |sense i ) Example:  bar-legal  near {lawyer, trial, judge, exam} bar-restaurant  near {drink, tab, beer} Applying Naive Bayes to WSD P(c)  i s the prior probability of that sense Counting in a labeled training set. P( w|c )   conditional probability of  a word given  a particular sense P( w|c )  =  count( w,c )/count(c) We get  both of these from a tagged  corpus like  SemCor Can  also generalize to look at other features besides words. Then it would be P( f|c )  Conditional probability of a feature given a sense [slide from Dan  Jurafsky ] Choosing a class: P(f|d5)  P(g|d5)    1/4 *  2/9 * (2/9) 2  *  2/9    	≈  0.0006 Conditional Probabilities: P( line| f ) = P( guitar| f )    = P( jazz| f )     = P( line| g ) = P( guitar| g )     = P( jazz| g )      =  Priors: P ( f )=  P ( g )=  3 4 1 4 ( 1 +1) / (8+6) = 2/14 (0+1) / (8+6) = 1/14 (1 +1) / (3+6) = 2/9  (0+1) / (8+6) = 1/14 (1 +1) / (3+6) = 2/9  (1 +1) / (3+6) = 2/9   3/4 *  2/14  *  (1/14) 2   *  1/14  	≈ 0.00003 V = {fish, smoked, line, haul, guitar, jazz} [slide from Dan  Jurafsky ] Classification Features Adjacent words (collocations) e.g., chocolate bar, bar exam, bar stool, bar fight, foreign aid, presidential aide Position e.g., plant pesticide vs. pesticide plant Adjacent parts of speech Nearby words e.g., within 10 words Syntactic information e.g., object of the verb “play” Topic of the text Classification Methods K-nearest neighbor (memory-based) Using Euclidean distance Find the k most similar examples and return the majority class for them See Lecture on Classification Bootstrapping Start with two senses and seeds for each sense  e.g., plant1:leaf, plant2:factory Use these seeds to label the data using a supervised classifier (decision list) Add some of the newly labeled examples to the training data Repeat until no more examples can be labeled Bootstrapping Two principles: one sense per collocation one sense per discourse (e.g., document) The  Yarowsky  Method Training Data for WSD Senseval / Semcor 234,000 words tagged with WordNet senses http://www.senseval.org/senseval3 Two tasks: Lexical Sample and All Words Available for many languages Pseudo-words E.g., banana/door Multilingual corpora Aligned at the sentence level Use the translations as an indication of sense WSD Evaluation Extrinsic (task-based) E.g., using Machine Translation or Information Retrieval Intrinsic Sense accuracy Baseline Most frequent sense Senseval-1 Evaluation Metric A = number of assigned senses C = number of words assigned correct senses T = total number of test words Precision = C/A; Recall = C/T Results b est recall around 77P/77R human lexicographer 97P/96R (for binary classification) but only 75% for humans on WordNet senses most common sense 57P/50R (decent but depends on domain) Notes There hasn’t been enough evidence that WSD helps downstream applications such as MT. N-grams (e.g., “bar exam”) are enough. NLP NLP Introduction to NLP TAG The Chomsky Hierarchy Regular languages Context-free languages Context-sensitive languages Recursively enumerable languages The Chomsky Hierarchy Regular languages Context-free languages Mildly context-sensitive languages Context-sensitive languages Recursively enumerable languages Mildly Context-Sensitive Grammars Superset of CFG Polynomial parsing O(n 6 ) Constant growth property (string length grows linearly) Cannot handle the language of strings with the same number of as,  bs , and cs. [Example from Julia  Hockenmaier ] [https://en.wikipedia.org/wiki/Mildly_context-sensitive_grammar_formalism] Other Formalisms Tree Substitution Grammar (TSG) Terminals generate entire tree fragments TSG and CFG are formally equivalent Mildly Context-Sensitive Grammars More powerful than TSG Examples: Tree Adjoining Grammar (TAG) Combinatory Categorial Grammar (CCG) (Tree) Operations in TAG Substitution Insert an initial tree to the bottom of a tree Adjunction (not in TSG) Insert an auxiliary tree fragment in the middle of a tree Used for long-distance dependencies and for optional modifiers Features Each elementary tree has features that can be associated with the top half and with the bottom half Unification is needed (Lexicalization) LTAG: each initial or auxiliary tree is labeled with a lexical item Tree Adjoining Grammar (TAG) It can generate languages like  a n b n c n d n  or ww (cross-serial dependencies):  e.g., Mary gave a book and a magazine to Chen and Mike, respectively. Expressive power TAG is formally more powerful than CFG  TAG is less powerful than CSG Card game online ( broken links ) http://www.ltaggame.com/   http://www.ltaggame.com/family.html TAG Example Maria sings Maria  often  sings  (optional modifier) [example from  Jungo  Kasai] [example from  Jungo  Kasai] Cross-serial dependencies ( a n b n ) Notes Embedded pushdown automaton (Vijay- Shanker  1988) Similar to the usual pushdown automaton for CFG, but the stack includes stacks of symbols, instead of symbols directly It can generate the language { a n b n c n d n } NLP NLP Text Similarity Dimensionality Reduction Issues with Vector Similarity Polysemy ( sim  < cos) bar, bank, jaguar, hot Synonymy ( sim  > cos) building/edifice, large/big, spicy/hot Relatedness (people are really good at figuring this) doctor/patient/nurse/treatment Semantic Matching Which one should we rank higher? Query vocabulary & doc vocabulary mismatch! If only we can represent documents/queries as concepts! That’s where dimensionality reduction helps Semantic Concepts Semantic Concepts Concept Space = Dimension Reduction Number of concepts (K) is  smaller  than the number of words (N) or number of documents (M). If we represent a document as a  N-dimensional  vector; and the corpus as an M*N matrix…  The goal is to reduce the  dimensionality  from N to K. But how can we do that?   TOEFL Synonyms and SAT Analogies  Word similarity vs. analogies Example from Peter  Turney 29 degrees 29 degrees Vectors and Matrices A matrix is an  m  x  n  table of objects (in our case, numbers) Each row (or column) is a vector. Matrices of compatible dimensions can be multiplied together.  What is the result of the multiplication below? Answer to the Quiz Eigenvectors and Eigenvalues An eigenvector is an implicit “direction” for a  matrix A   v  (the eigenvector )   is  non-zero λ  (the eigenvalue ) can be any complex  number,  in  principle. Computing eigenvalues: Eigenvectors and Eigenvalues Example: det   (A- l I ) = (-1- l )*(- l )-3*2=0 Then :  l + l 2 -6=0;    l 1 =2;    l 2 =-3 For  l 1 =2: Solutions:  v 1 =v 2 Matrix decomposition If  S   is a square matrix, it can be decomposed into  U L U -1 , where       U  = matrix of eigenvectors 	 L  = diagonal matrix of eigenvalues S U  = U L U -1 S U  =  L S   = U L U -1 Example SVD: Singular Value Decomposition A=U S V T U is the matrix of orthogonal eigenvectors of AA T V is the matrix of orthogonal eigenvectors of A T A (co-variance matrix) The components of  S  are the eigenvalues of A T A Properties This decomposition exists for all matrices and is unique U, V are column orthonormal U T  U = I; V T  V = I S  is diagonal and sorted by absolute value of the singular values (large to small) Each column (row) of  S  corresponds to a principal component  If A has 5 columns and 3 rows,  then U will be 5x5 and V will be 3x3 Example (Berry and Browne) T1: baby T2: child T3: guide T4: health  T5: home T6: infant T7: proofing T8: safety T9: toddler D1:  infant  &  toddler  first aid D2:  babies  &  children ’s room (for your home) D3:  child   safety  at  home D4: your  baby ’s  health  and  safety : from  infant  to  toddler D5:  baby   proofing  basics D6: your  guide  to easy rust  proofing D7: beanie  babies  collector’s  guide Example D1 :  T6, T9 D2 :  T1, T2 D3 :  T2, T5, T8 D4 :  T1, T4, T6, T8, T9 D5 :  T1, T7 D6 :  T3, T7 D7 : T1, T3 Example D1 :  T6, T9 D2 :  T1, T2 D3 :  T2, T5, T8 D4 :  T1, T4, T6, T8, T9 D5 :  T1, T7 D6 :  T3, T7 D7 : T1, T3 D1 D2 D3 D4 D5 D6 D7 T2 T3 T4 T5 T6 T7 T8 T9 T1 Document-Term Matrix raw normalized SVD Decomposition u  = [[-0.70 -0.09  0.02 -0.70  0.00  0.02  0.14 -0.00  0.00]  [-0.26  0.30  0.47  0.20  0.00 -0.25 -0.16 -0.64  0.31]  [-0.35 -0.45 -0.10  0.40  0.71 -0.01 -0.05  0.00  0.00]  [-0.11  0.14 -0.15 -0.07 -0.00  0.48 -0.84  0.00 -0.00]  [-0.26  0.30  0.47  0.20  0.00 -0.25 -0.16  0.64 -0.31]  [-0.19  0.37 -0.50  0.13  0.00 -0.23  0.03 -0.31 -0.64]  [-0.35 -0.45 -0.10  0.40 -0.71 -0.01 -0.05 -0.00  0.00]  [-0.21  0.33  0.10  0.28  0.00  0.73  0.47 -0.00  0.00]  [-0.19  0.37 -0.50  0.13  0.00 -0.23  0.03  0.31  0.64 ]] v’   = [[-0.17 -0.45 -0.27 -0.40 -0.47 -0.32 -0.47]  [ 0.42  0.23  0.42  0.40 -0.30 -0.50 -0.30]  [-0.60  0.46  0.50 -0.39 -0.05 -0.12 -0.05]  [ 0.23 -0.22  0.49 -0.13 -0.26  0.71 -0.26]  [ 0.00  0.00  0.00  0.00 -0.71 -0.00  0.71]  [-0.57 -0.49  0.25  0.61  0.01 -0.02  0.01]  [ 0.24 -0.50  0.45 -0.37  0.34 -0.35  0.34 ]] SVD Decomposition S   = [ [  1.58   0.00  0.00  0.00  0.00  0.00  0.00]  [  0.00  1.27   0.00  0.00  0.00  0.00  0.00]  [  0.00   0.00   1.19   0.00  0.00  0.00  0.00]  [  0.00   0.00  0.00   0.80   0.00  0.00  0.00]  [  0.00   0.00  0.00  0.00   0.71   0.00  0.00]  [  0.00   0.00  0.00  0.00  0.00   0.57   0.00]  [  0.00   0.00  0.00  0.00  0.00  0.00   0.20]  [ 0.00  0.00  0.00  0.00  0.00  0.00   0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00   0.00]]   SVD Decomposition u*S*v’  =   [[ 0.00  0.58    0.00   0.45  0.71    0.00   0.71]  [ 0.00  0.58  0.58    0.00    0.00    0.00    0.00 ]  [ 0.00    0.00   0.00  0.00    0.00   0.71  0.71]  [ 0.00    0.00   0.00  0.45  0.00    0.00   0.00]  [ 0.00  0.58  0.58    0.00    0.00   0.00    0.00 ]  [ 0.71  0.00  0.00  0.45  0.00  0.00  0.00]   [   0.00    0.00   0.00  0.00  0.71  0.71    0.00 ]  [ 0.00    0.00   0.58  0.45    0.00   0.00    0.00] ] Dimensionality Reduction Low rank matrix approximation A [m*n]  = U [m*m] S [ m*n ] V T [ n*n ] S  is a diagonal matrix of eigenvalues If we only keep the largest  r  eigenvalues  A ≈ U [m*r] S [ r*r ] V T [ n*r ] Rank-4 Approximation of  S S   = [ [  1.58   0.00  0.00  0.00  0.00  0.00  0.00]  [  0.00  1.27   0.00  0.00  0.00  0.00  0.00]  [  0.00   0.00   1.19   0.00  0.00  0.00  0.00]  [  0.00   0.00  0.00   0.80   0.00  0.00  0.00]  [  0.00   0.00  0.00  0.00   0.00   0.00  0.00]  [  0.00   0.00  0.00  0.00  0.00   0.00   0.00]  [  0.00   0.00  0.00  0.00  0.00  0.00   0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00   0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00   0.00]]   Rank-4 Approximation of A u*S*v’  =   [[ 0.00  0.58    0.00  0.45  0.71    0.00  0.71]  [ 0.00  0.58  0.58    0.00    0.00    0.00    0.00]  [ 0.00    0.00  0.00  0.00    0.00  0.71  0.71]  [ 0.00    0.00  0.00  0.45  0.00    0.00  0.00]  [ 0.00  0.58  0.58    0.00    0.00  0.00    0.00]  [ 0.71  0.00  0.00  0.45  0.00  0.00  0.00]  [   0.00    0.00  0.00  0.00  0.71  0.71    0.00]  [ 0.00    0.00  0.58  0.45    0.00  0.00    0.00] u*S4*v’  =   [[-0.00   0.60 -0.01  0.45  0.70  0.01  0.70]   [-0.07   0.49  0.63  0.07  0.01 -0.01  0.01]  [ 0.00 -0.01  0.01 -0.00  0.36  0.70  0.36]  [ 0.20  0.05  0.01  0.22  0.05 -0.05  0.05]  [-0.07  0.49  0.63  0.07  0.01 -0.01  0.01]  [ 0.63 -0.06  0.03  0.53 -0.00  0.00 -0.00]  [ 0.00 -0.01  0.01 -0.00  0.36  0.70  0.36]  [ 0.22  0.25  0.43  0.23 -0.04  0.04 -0.04]  [ 0.63 -0.06  0.03  0.53 -0.00  0.00 -0.00]] Rank-2  Approximation of  S S   = [ [  1.58   0.00  0.00  0.00  0.00  0.00  0.00]  [  0.00  1.27   0.00  0.00  0.00  0.00  0.00]  [  0.00   0.00   0.00   0.00  0.00  0.00  0.00]  [  0.00   0.00  0.00   0.00   0.00  0.00  0.00]  [  0.00   0.00  0.00  0.00   0.00   0.00  0.00]  [  0.00   0.00  0.00  0.00  0.00   0.00   0.00]  [  0.00   0.00  0.00  0.00  0.00  0.00   0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00   0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00   0.00]]   Rank-2 Approximation of A u*S*v’  =   [[ 0.00  0.58    0.00  0.45  0.71    0.00  0.71]  [ 0.00  0.58  0.58    0.00    0.00    0.00    0.00]  [ 0.00    0.00  0.00  0.00    0.00  0.71  0.71]  [ 0.00    0.00  0.00  0.45  0.00    0.00  0.00]  [ 0.00  0.58  0.58    0.00    0.00  0.00    0.00]  [ 0.71  0.00  0.00  0.45  0.00  0.00  0.00]  [   0.00    0.00  0.00  0.00  0.71  0.71    0.00]  [ 0.00    0.00  0.58  0.45    0.00  0.00    0.00] u*S2*v’  =   [[ 0.14  0.47  0.25  0.39  0.55  0.41  0.55]  [ 0.23  0.27  0.27  0.31  0.08 -0.06  0.08]  [-0.14  0.12 -0.09 -0.01  0.43  0.46  0.43]  [ 0.10  0.12  0.12  0.14  0.03 -0.03  0.03]  [ 0.23  0.27  0.27  0.31  0.08 -0.06  0.08]  [ 0.25  0.24  0.28  0.31 -0.00 -0.14 -0.00]  [-0.14  0.12 -0.09 -0.01  0.43  0.46  0.43]  [ 0.23  0.24  0.27  0.30  0.03 -0.11  0.03]  [ 0.25  0.24  0.28  0.31 -0.00 -0.14 -0.00]] Rank-2 Representation u*S2   =   [[-1.10 -0.12  0.00  0.00  0.00  0.00  0.00]  [-0.41  0.38  0.00  0.00  0.00  0.00  0.00]  [-0.56 -0.57  0.00  0.00  0.00  0.00  0.00]  [-0.18  0.18  0.00  0.00  0.00  0.00  0.00]  [-0.41  0.38  0.00  0.00  0.00  0.00  0.00]  [-0.30  0.47  0.00  0.00  0.00  0.00  0.00]  [-0.56 -0.57  0.00  0.00  0.00  0.00  0.00]  [-0.33  0.42  0.00  0.00  0.00  0.00  0.00]  [-0.30  0.47  0.00  0.00  0.00  0.00  0.00 ]] S2*v’ =  [[-0.26 -0.71 -0.42 -0.62 -0.74 -0.50 -0.74]  [ 0.53  0.29  0.54  0.51 -0.38 -0.64 -0.38]  [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]  [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]] T3,T7 D6 D7 T3,T7 D6 D7 T1 Example D1 :  T6, T9 D2 :  T1 , T2 D3 :  T2, T5, T8 D4 :  T1 , T4, T6, T8, T9 D5 :  T1 ,  T7 D6 :  T3 ,  T7 D7 :  T1 ,  T3 D1 D2 D3 D4 D5 D6 D7 T2 T3 T4 T5 T6 T7 T8 T9 T1 Semantic Concepts Semantic Concepts Quiz Can you explain why this graphic looks this way? Quiz Compare with this: Adding Noise Quiz Let A be a document x term matrix. What is A*A’? What about A’*A? Interpretation of SVD Best direction to project on The principal eigenvector is the dimension that explains most of the variance Finding hidden concepts Mapping documents, terms to a lower-dimensional space Turning the matrix into block-diagonal form (same as finding bi-partite cores) In the NLP/IR literature, SVD is called LSA (LSI) Latent Semantic Analysis (Indexing) Keep as many dimensions as necessary to explain 80-90% of the data (energy) In practice, use 300 dimensions or so fMRI example fMRI functional MRI (magnetic resonance imaging) Used to measure activity in different parts of the brain when exposed to various stimuli Factor analysis  Paper Just , M. A.,  Cherkassky , V. L.,  Aryal , S., & Mitchell, T. M. (2010). A  neurosemantic  theory of concrete noun representation based on the underlying brain codes.  PLoS  ONE, 5, e8622 [Just et al. 2010] [Just et al. 2010] [Just et al. 2010] External pointers http://lsa.colorado.edu   http://www.cs.utk.edu/~lsi   Example of LSI data inf retrieval brain lung = CS MD x x CS-concept MD-concept Term rep of concept Strength of CS-concept Dim. Reduction         A             =       U                L               V T [Example modified from Christos  Faloutsos ] Mapping Queries and Docs to the Same Space q T concept  =  q T  V                 d T concept  =  d T  V = Similarity with  CS-concept CS-concept d T = 0  1   1  0   0 1.16   0 [Example modified from Christos  Faloutsos ] NLP NLP ML Resources NLP External Resources https :// www.coursera.org/learn/machine-learning   https://github.com/thejakeyboy/umich-eecs545-lectures Scipy Sklearn   NLP NLP Introduction to NLP Classic parsing methods       S  -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' Parsing as search There are two types of constraints on the parses From the input sentence From the grammar Therefore, two general approaches to parsing Top-down Bottom-up S Top down parsing       S  -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' S VP NP Top down parsing       S  -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' S VP NP Top down parsing PP NP       S  -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' S VP NP Top down parsing PP NP       S  -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' S VP NP Top down parsing N DT       S  -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' S Top down parsing       S  -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' the fork the child ate cake the with Bottom up parsing       S  -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' Bottom  up  vs. top  down  methods Bottom up explores options that won’t lead to a full parse Example: shift-reduce ( srparser  in  nltk ) Example: CKY ( Cocke - Kasami -Younger) Top down explores options that don’t match the full sentence Example: recursive descent ( rdparser  in  nltk ) Example:  Earley  parser Dynamic programming caches of intermediate results ( memoization )  Recursive Descent Parser In  nltk >>>  from  nltk.app  import  rdparser ; >>>  rdparser ()) Introduction to NLP Shift-Reduce Parsing Shift-Reduce Parsing A bottom-up parser Tries to match the RHS of a production until it can build an S Shift  operation Each word in the input sentence is pushed onto a stack Reduce-n  operation If the top  n  words on the top of the stack match the RHS of a production, then they are popped and replaced by the LHS of the production  Breadth-first search Stopping condition The process stops when the input sentence has been processed and S has been popped from the stack Shift-Reduce Parsing Example      [  * the child ate the cake]   S [ 'the' * child ate the cake]   R [ DT * child ate the cake]   S [ DT 'child' * ate the cake]   R [ DT N * ate the cake]   R [ NP * ate the cake]   S [ NP 'ate' * the cake]   R [ NP V * the cake]   S [ NP V 'the' * cake]   R [ NP V DT * cake]   S [ NP V DT 'cake' * ]   R [ NP V DT N * ]   R [ NP V NP * ]   R [ NP VP * ]   R [ S *  ] ( S (NP (DT the) (N child)) (VP (V ate) (NP (DT the) (N cake )))) Shift-Reduce Parsing In  nltk >>>  from  nltk.app  import  srparser ; >>>  srparser ()) NLP NLP Introduction to NLP Linguistics IPA Chart (consonants) By IPA (http://www.langsci.ucl.ac.uk/ipa/ipachart.html) [CC-BY-SA-3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons IPA Chart (vowels) By IPA (http://www.langsci.ucl.ac.uk/ipa/ipachart.html) [CC-BY-SA-3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons (Many) Languages are Related Cognates night (English),  nuit  (French),  Nacht  (German),  nacht  (Dutch), nag (Afrikaans),  nicht  (Scots),  natt  (Swedish, Norwegian),  nat  (Danish),  nátt  (Faroese),  nótt  (Icelandic),  noc  (Czech, Slovak, Polish),  ночь,  noch  (Russian),  ноќ,  noć  (Macedonian),  нощ,  nosht  (Bulgarian),  ніч,  nich  (Ukrainian),  ноч,  noch / noč  (Belarusian),  noč  (Slovene),  noć  (Serbo-Croatian),  νύξ,  nyx  (Ancient Greek,  νύχτα/ nychta  in Modern Greek),  nox / nocte  (Latin),  nakt - (Sanskrit),  natë  (Albanian),  noche  (Spanish),  nos  (Welsh),  nueche  ( Asturian ),  noite  (Portuguese and Galician),  notte  (Italian), nit (Catalan),  nuèch / nuèit  (Occitan),  noapte  (Romanian),  nakts  (Latvian) and  naktis  (Lithuanian), all meaning "night" and derived from the Proto-Indo-European (PIE) * nókʷts , "night". From  wikipedia Some Indo-European languages Some non-Indo-European Languages Altaic  Turkish Uralic  ( Finno-Ugric) Finnish Hungarian Semitic Arabic Hebrew Uto-Aztecan By  Industrius  at English Wikipedia. Later version(s) were uploaded by  Mttll  at English Wikipedia. ( Image:BlankMap-World.png  by  User:Vardion ) [GFDL (www.gnu.org/copyleft/fdl.html)], via Wikimedia Commons Language Families Ethnologue  (7358  languages) Language Diversity Language Changes Grimm’s Law Voiceless  stops  turn  into voiceless  fricatives Voiced  stops become voiceless  stops Voiced  aspirated stops  change to  voiced stops or  fricatives Example 1 Ancient Greek:  πούς , Latin:  pēs ,  Sanskrit:  pāda English:  foot , German:  Fuß , Swedish:  fot Example 2 Ancient Greek:  κύων , Latin:  canis , Welsh:  ci English:  hound , Dutch:  hond , German:  Hund NACLO Problem http:// nacloweb.org/resources/problems/2012/N2012-D.pdf http:// nacloweb.org/resources/problems/2012/N2012-DS.pdf   Problem  by  Dragomir Radev http://unicode.org/udhr/assemblies/first_article_all.html Basque Karelian Sardinian Lithuanian Welsh Romanian Romansch Breton Slovenian Latin English Slovak Corsican Irish Latvian Finnish Polish Language Families Question Can you guess the source, language, and period of this text? Answer Beowulf Epic poem 8 th -11 th  Century Old English Beowulf Hwæt ! We Gardena in  geardagum ,  þeodcyninga ,  þrym   gefrunon ,  hu   ða   æþelingas   ellen   fremedon .  Oft  Scyld   Scefing   sceaþena   þreatum , monegum   mægþum ,  meodosetla   ofteah ,  egsode   eorlas .  Syððan   ærest   wearð   feasceaft   funden , he  þæs   frofre   gebad ,  weox  under  wolcnum ,  weorðmyndum   þah ,  oðþæt  him  æghwylc   þara   ymbsittendra   erst  (as in  erstwhile ) = first Lo! the Spear-Danes’ glory through splendid achievements The folk-kings’ former fame we have heard of, How princes displayed then their prowess-in-battle. Oft  Scyld  the  Scefing  from  scathers  in numbers From many a people their mead-benches tore. Since  first  he found him friendless and wretched, The earl had had terror: comfort he got for it, Waxed ’ neath  the welkin, world-honor gained, Till all his neighbors o’er sea were compelled  to … http://lit.genius.com/  http://www8.georgetown.edu/departments/medieval/labyrinth/library/oe/texts/a4.1.html http://www.gutenberg.org/files/16328/16328-h/16328-h.htm   http://www.nvcc.edu/home/vpoulakis/Translation/beowulf1.htm http://en.wikipedia.org/wiki/File:Beowulf.firstpage.jpeg   LIEF.—Dear, valued.  MERE.—Sea; in compounds, ‘mere-ways,’ ‘mere-currents,’ etc.  MICKLE.—Much.  NATHLESS.—Nevertheless.  NAZE.—Edge (nose).  NESS.—Edge.  NICKER.—Sea-beast.  QUIT, QUITE.—Requite.  RATHE.—Quickly.  REAVE.—Bereave, deprive.  SAIL-ROAD.—Sea.  SETTLE.—Seat, bench.  SKINKER.—One who pours.  SOOTHLY.—Truly.  SWINGE.—Stroke, blow.  TARGE, TARGET.—Shield.  THROUGHLY.—Thoroughly.  TOLD.—Counted.  UNCANNY.—Ill-featured, grizzly.  UNNETHE.—Difficult.  WAR-SPEED.—Success in war.  WEB.—Tapestry (that which is ‘woven’).  WEEDED.—Clad (cf. widow’s weeds).  WEEN.—Suppose, imagine.  WEIRD.—Fate, Providence.  WHILOM.—At times, formerly, often.  WIELDER.—Ruler. Often used of God;  WIGHT.—Creature.  WOLD.—Plane, extended surface.  WOT.—Knows.  YOUNKER.—Youth.  ATHELING.—Prince, nobleman.  BAIRN.—Son, child.  BARROW.—Mound, rounded hill, funeral-mound.  BATTLE-SARK.—Armor.  BEAKER.—Cup, drinking-vessel.  BEGEAR.—Prepare.  BIGHT.—Bay, sea.  BILL.—Sword.  BOSS.—Ornamental projection.  BRACTEATE.—A round ornament on a necklace.  BRAND.—Sword.  BURN.—Stream.  BURNIE.—Armor.  CARLE.—Man, hero.  EARL.—Nobleman, any brave man.  EKE.—Also.  EMPRISE.—Enterprise, undertaking.  ERST.—Formerly.  ERST-WORTHY.—Worthy for a long time past.  FAIN.—Glad.  FERRY.—Bear, carry.  FEY.—Fated, doomed.  FLOAT.—Vessel, ship.  FOIN.—To lunge ( Shaks .).  GLORY OF KINGS.—God.  GREWSOME.—Cruel, fierce.  HEFT.—Handle, hilt; used by synecdoche for ‘sword.’  HELM.—Helmet, protector.  HENCHMAN.—Retainer, vassal.  HIGHT.—Am (was) named.  HOLM.—Ocean, curved surface of the sea.  HIMSEEMED.—(It) seemed to him.  Diversity of languages Articles Cases (e.g., in Latin) Puer   puellam   vexat Sound systems Glottal stop (the middle sound in “uh-oh”) - pro Velar fricatives -  articulated with the back of the tongue at the soft palate Voiceless /x/ - used e.g., in Russian Voiced  /ɣ/ - used e.g., in Modern Greek Social status (e.g., in Japanese) otousan ,  お父さん  =  someone else‘s father chichi,  父  = one’s  own father Kinship systems (e.g., in Warlpiri) – see next slide NACLO Problem Warlpiri Kinship – by Alan Chang http :// www.nacloweb.org/resources/problems/2013/N2013-O.pdf     NACLO Solution Warlpiri Kinship http:// www.nacloweb.org/resources/problems/2013/N2013-OS.pdf    Language Universals Two types unconditional conditional Examples All languages have  verbs and nouns All spoken languages have consonants  and  vowels [Greenberg 1]  “In  declarative sentences with nominal subject and object, the dominant order is almost always one in which the subject precedes the object .“ [Greenberg 29]  “If  a language has inflection, it always has derivation .“ WALS: the World Atlas of Language Structures http://wals.info Feature 83A: Order of Object and Verb  by Matthew S. Dryer OV (713 languages), VO (705), no dominant order (101) http:// wals.info/feature/83A#2/18.0/152.9   Other features: 18A Absence of common consonants (by Ian  Maddieson ):  no bilabials (5 languages), no fricatives (49), no nasals (12) 67A Inflectional future tense (by  Östen  Dahl,  Viveka   Velupillai ):  yes (110), no (112) Links about World Languages Ethnologue http ://www.ethnologue.com /     Number words in many languages http :// www.zompist.com/numbers.shtml     Endangered languages http://www.endangeredlanguages.com/   Google fights to save 3,054 dying languages http://www.cnn.com/2012/06/21/tech/web/google-fights-save-language-mashable/index.html   NLP NLP Introduction to NLP Deep Learning for NLP Deep Learning for NLP Input: words, sentences, documents Neural networks expect continuous vectors: word embeddings, sentence embeddings, document embeddings For sequence of variable length Use recurrent neural nets (RNN), e.g., LSTM [Thang et al. 2013] [Bowman et al. 2014] NLP Examples Machine Translation (Sequences) Sequence-to-sequence Sutskever  et al. 2014 Sentiment Analysis (Trees) Socher  et al. (2013) Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank NLP Introduction to NLP Evaluation of IR Evaluation Different criteria Size of index Speed of indexing Speed of retrieval Accuracy Timeliness Ease of use Expressiveness of search language Contingency Table w= tp x= fn y= fp z= tn n 2  = w + y n 1  = w + x N Precision and Recall Recall: Precision: Issues Why not use accuracy A=( w+z )/N? Average precision Report when P=R F measure:  F=( b 2 +1)PR/( b 2 P+R) F1 measure:  F1 = 2/(1/R+1/P) harmonic mean of P and R Sample TREC query <top> < num > Number: 305 <title> Most Dangerous Vehicles  < desc > Description:  Which are the most crashworthy, and least crashworthy,  passenger vehicles?   < narr > Narrative:  A relevant document will contain information on the crashworthiness of a given vehicle or vehicles that can be used to draw a comparison with other vehicles.  The document will have to describe/compare vehicles, not drivers.  For instance, it should be expected that vehicles preferred by 16-25 year-olds would be involved in more crashes, because that age group is involved in more crashes.  I would view number of fatalities per 100 crashes to be more revealing of a vehicle's crashworthiness than the number of crashes per 100,000 miles, for example. </top> <DOCNO> LA031689-0177 </DOCNO> <DOCID> 31701 </DOCID> <DATE><P>March 16, 1989, Thursday, Home Edition </P></DATE> <SECTION><P>Business; Part 4; Page 1; Column 5; Financial Desk </P></SECTION> <LENGTH><P>586 words </P></LENGTH> <HEADLINE><P>AGENCY TO LAUNCH STUDY OF FORD BRONCO II AFTER HIGH RATE OF ROLL-OVER ACCIDENTS </P></HEADLINE> <BYLINE><P>By LINDA WILLIAMS, Times Staff Writer </P></BYLINE> <TEXT> <P>The federal government's highway safety watchdog said Wednesday that the Ford Bronco II appears to be involved in more fatal roll-over accidents than other vehicles in its class and that it will seek to determine if the vehicle itself contributes to the accidents. </P> <P>The decision to do an engineering analysis of the Ford Motor Co. utility-sport vehicle grew out of a federal accident study of the Suzuki Samurai, said Tim Hurd, a spokesman for the National Highway Traffic Safety Administration. NHTSA looked at Samurai accidents after Consumer Reports magazine charged that the vehicle had basic design flaws. </P> <P>Several Fatalities </P> <P>However, the accident study showed that the "Ford Bronco II appears to have a higher number of single-vehicle, first event roll-overs, particularly those involving fatalities," Hurd said. The engineering analysis of the Bronco, the second of three levels of investigation conducted by NHTSA, will cover the 1984-1989 Bronco II models, the agency said. </P> <P>According to a Fatal Accident Reporting System study included in the September report on the Samurai, 43 Bronco II single-vehicle roll-overs caused fatalities, or 19 of every 100,000 vehicles. There were eight Samurai fatal roll-overs, or 6 per 100,000; 13 involving the Chevrolet S10 Blazers or GMC Jimmy, or 6 per 100,000, and six fatal Jeep Cherokee roll-overs, for 2.5 per 100,000. After the accident report, NHTSA declined to investigate the Samurai. </P> ... </TEXT> <GRAPHIC><P> Photo, The Ford Bronco II "appears to have a higher number of single-vehicle, first event roll-overs," a federal official said. </P></GRAPHIC> <SUBJECT> <P>TRAFFIC ACCIDENTS; FORD MOTOR CORP; NATIONAL HIGHWAY TRAFFIC SAFETY ADMINISTRATION; VEHICLE INSPECTIONS; RECREATIONAL VEHICLES; SUZUKI MOTOR CO; AUTOMOBILE SAFETY </P> </SUBJECT> </DOC > TREC (cont’d) http://trec.nist.gov/tracks.html http://trec.nist.gov/presentations/presentations.html Most Used Reference Collections Generic retrieval OHSUMED, CRANFIELD, CACM Text classification Reuters, 20newsgroups Question answering TREC-QA Web DOTGOV, wt100g Blogs Buzzmetrics  datasets TREC ad hoc collections, 2-6 GB TREC Web collections, 2-100GB Comparing two systems Comparing A and B One query? Average performance? Need: A to consistently outperform B [Example from James  Allan] The Sign Test Example 1: A > B (12 times) A = B (25 times) A < B (3 times) p < 0.035 (significant at the 5% level) Example 2: A > B (18 times) A < B (9 times) p < 0.122 (not significant at the 5% level) External link: http://www.fon.hum.uva.nl/Service/Statistics/Sign_Test.html     Other Tests Student t-test takes into account the actual performances, not just which system is better http://www.fon.hum.uva.nl/Service/Statistics/Student_t_Test.html   http://www.socialresearchmethods.net/kb/stat_t.php Wilcoxon Matched-Pairs Signed-Ranks Test http://www.fon.hum.uva.nl/Service/Statistics/Signed_Rank_Test.html   NLP Information Retrieval Evaluation of IR Evaluation Size of index Speed of indexing Speed of retrieval Accuracy Timeliness Ease of use Expressiveness of search language Contingency table w= tp x= fn y= fp z= tn n 2  = w + y n 1  = w + x N Precision and Recall Recall: Precision: Exercise Go to  http://www.google.com    and search for documents on Tolkien’s “Lord of the Rings”.  Note! Before starting the exercise, have a clear idea of what a relevant document for your query should look like. Try different information needs. Try  different ways of phrasing the query: e.g.,  Tolkien ,  “JRR Tolkien” ,  +“JRR Tolkien” +“Lord  of the Rings” , etc. For each query, compute the precision (P) based on the first 10 documents returned by  Google.  Later , try different queries. [From Salton’s book] Interpolated average precision (e.g., 11pt) Interpolation – what is precision at recall=0.5? Issues Why not use accuracy A=( w+z )/N? Average precision Report when P=R F measure:  F=( b 2 +1)PR/( b 2 P+R) F1 measure:  F1 =  2(PR)/(1*P + R) =  2/(1/R+1/P) : harmonic mean of P and R Kappa –  interannotator  agreement N: number of items  (index i) n: number of categories (index j) k: number of annotators Kappa example Kappa (cont’d) P(A) = 370/400 = 0.925 P (-) = (10+20+70+70)/800 = 0.2125 P (+) = (10+20+300+300)/800 = 0.7875 P (E) = 0.2125 * 0.2125 + 0.7875 * 0.7875 = 0.665 K = (0.925-0.665)/(1-0.665) = 0.776 Kappa higher than 0.67 is tentatively acceptable; higher than 0.8 is good  Comparing two systems Comparing A and B One query? Average performance? Need: A to consistently outperform B [Example from James  Allan] The Sign Test Example 1: A > B (12 times) A = B (25 times) A < B (3 times) p < 0.035 (significant at the 5% level) Example 2: A > B (18 times) A < B (9 times) p < 0.122 (not significant at the 5% level) External link: http://www.fon.hum.uva.nl/Service/Statistics/Sign_Test.html     Other tests Student t-test: takes into account the actual performances, not just which system is better http://www.fon.hum.uva.nl/Service/Statistics/Student_t_Test.html   http://www.socialresearchmethods.net/kb/stat_t.php Wilcoxon Matched-Pairs Signed-Ranks Test http://www.fon.hum.uva.nl/Service/Statistics/Signed_Rank_Test.html   Information Retrieval System Evaluation How do I evaluate my system if… The task is new? No benchmark data is available? No real users/real queries yet? No ground truth? No well-established competitors? The Cranfield Paradigm Build test collections Start with creating a document collection that is representative of the task Usually a sample of the real domain (or the whole thing if affordable) Size varies from thousands of documents to billions (Trec ClueWeb) From document collections  to test collections Still need Test queries Relevance assessments Test queries Must be germane to docs available Best designed by domain experts Random query terms generally not a good idea Relevance assessments Human judges, time-consuming Are human panels perfect? - Slide from Pandu  Nayak and Prabhakar Raghavan Two Issues Too many documents for human annotation Is sampling the solution? Too many irrelevant documents Waste of human annotation effort Random sampling doesn’t solve this problem… Trec’s  Solution: pooling Pooling Before evaluation: every participating team submit a ranked list of results per query For a query, select top K results of each team  P ool the top K results from all teams together, remove duplicates This is called a document “pool.” Human annotators judge the documents If the pool is large, sampling from the pool Evaluate each team (and incoming teams) using the judgments. Why It Works Top-ranked results of a candidate system ensures a good presence of relevant documents in the sample Multiple (diverse) candidate systems are needed so that the sample isn’t biased. In your project, you don ’ t have participating teams…  Employ different (diversity is important!) IR approaches and treat them as candidate teams! E.g.,  boolean  &  tf-idf ; w or w/o feedback; synonym, … Information Retrieval Evaluation Collections Sample TREC query <top> < num > Number: 305 <title> Most Dangerous Vehicles  < desc > Description:  Which are the most crashworthy, and least crashworthy,  passenger vehicles?   < narr > Narrative:  A relevant document will contain information on the crashworthiness of a given vehicle or vehicles that can be used to draw a comparison with other vehicles.  The document will have to describe/compare vehicles, not drivers.  For instance, it should be expected that vehicles preferred by 16-25 year-olds would be involved in more crashes, because that age group is involved in more crashes.  I would view number of fatalities per 100 crashes to be more revealing of a vehicle's crashworthiness than the number of crashes per 100,000 miles, for example. </top> <DOCNO> LA031689-0177 </DOCNO> <DOCID> 31701 </DOCID> <DATE><P>March 16, 1989, Thursday, Home Edition </P></DATE> <SECTION>< P>Business ; Part 4; Page 1; Column 5; Financial Desk </P></SECTION> <LENGTH><P>586 words </P></LENGTH> <HEADLINE><P>AGENCY TO LAUNCH STUDY OF FORD BRONCO II AFTER HIGH RATE OF ROLL-OVER ACCIDENTS </P></HEADLINE> <BYLINE><P>By LINDA WILLIAMS, Times Staff Writer </P></BYLINE> <TEXT> <P>The federal government's highway safety watchdog said Wednesday that the Ford Bronco II appears to be involved in more fatal roll-over accidents than other vehicles in its class and that it will seek to determine if the vehicle itself contributes to the accidents. </P> <P>The decision to do an engineering analysis of the Ford Motor Co. utility-sport vehicle grew out of a federal accident study of the Suzuki Samurai, said Tim Hurd, a spokesman for the National Highway Traffic Safety Administration. NHTSA looked at Samurai accidents after Consumer Reports magazine charged that the vehicle had basic design flaws. </P> <P>Several Fatalities </P> <P>However, the accident study showed that the "Ford Bronco II appears to have a higher number of single-vehicle, first event roll-overs, particularly those involving fatalities," Hurd said. The engineering analysis of the Bronco, the second of three levels of investigation conducted by NHTSA, will cover the 1984-1989 Bronco II models, the agency said. </P> <P>According to a Fatal Accident Reporting System study included in the September report on the Samurai, 43 Bronco II single-vehicle roll-overs caused fatalities, or 19 of every 100,000 vehicles. There were eight Samurai fatal roll-overs, or 6 per 100,000; 13 involving the Chevrolet S10 Blazers or GMC Jimmy, or 6 per 100,000, and six fatal Jeep Cherokee roll-overs, for 2.5 per 100,000. After the accident report, NHTSA declined to investigate the Samurai. </P> ... </TEXT> <GRAPHIC><P> Photo, The Ford Bronco II "appears to have a higher number of single-vehicle, first event roll-overs," a federal official said. </P></GRAPHIC> <SUBJECT> <P>TRAFFIC ACCIDENTS; FORD MOTOR CORP; NATIONAL HIGHWAY TRAFFIC SAFETY ADMINISTRATION; VEHICLE INSPECTIONS; RECREATIONAL VEHICLES; SUZUKI MOTOR CO; AUTOMOBILE SAFETY </P> </SUBJECT> </DOC > TREC (cont’d) http://trec.nist.gov/tracks.html http://trec.nist.gov/presentations/presentations.html Common Reference Collections Generic retrieval OHSUMED, CRANFIELD, CACM Text classification Reuters, 20newsgroups Question answering TREC-QA Web DOTGOV, wt100g Blogs Buzzmetrics  datasets TREC ad hoc collections, 2-6 GB TREC Web collections, 2-100GB NLP Introduction to NLP Perceptron: Discriminative Classifier The Perceptron A simple but very important classifier Model a neuron Input excitations If excitation > inhibition, send an electrical signal out the axon Earliest neural network invented in 1957! Perceptron Idea x 0 x 1 x 2 Input Σ w 0 w 1 w 2 > threshold? -1 1 no yes Question: Where can we get these weights? Quick Reminder: Dot Products 3 x 2 y 5 z 7 This equation can be written as a dot product of two  vectors :  3 x 2 y 5 z So we can rewrite x 0 x 1 x 2 Σ w 0 w 1 w 2 as a dot product of two vectors Derivatives and Gradients Updating Parameters Basic approach We want to increase the probability of the entire data set Gradient ascent Take the derivative of the log likelihood with respect to the parameters  Make a little change to the parameters in the direction the derivative tells you is uphill: α  here is the learning  rate how much do you want to change each time?  Perceptron Algorithm Input: Algorithm: Output: [Example: Chris Bishop] Gradient Ascent Example Starting value w  = (1,-1,-2,3 ) accuracy 66% Next value w  =  (2,-2,-1,2) accuracy 73% Next value w  =  (3,-2,-1,4) accuracy 80% etc. Regularization Penalize large weights (we don’t want weights of 500,000) L 2   regularizer L 1   regularizer Updating Parameters Stochastic Gradient Ascent Batch mode Consider each data point for each update of  w This is slow Stochastic mode Update  w  after each data point Logistic regression stochastic update (p is between 0 and 1) Perceptron stochastic update (y is 0 or 1; approximation for LR) Notes Datasets: http://blog.webkid.io/datasets-for-machine-learning /   NLP NLP Introduction to NLP Probabilistic Parsing Main Tasks with PCFGs Given a grammar G and a sentence s, let T(s) be all parse trees that correspond to s Task 1 find which tree t among T(s) maximizes the probability p(t) Task 2 find the probability of the sentence p(s) as the sum of all possible tree probabilities p(t) Probabilistic Parsing Methods Probabilistic  Earley  algorithm Top-down parser with a dynamic programming table Probabilistic Cocke-Kasami-Younger (CKY) algorithm Bottom-up parser with a dynamic programming table Probabilistic Grammars Probabilities can be learned from a training corpus Treebank Intuitive meaning Parse #1 is twice as probable as parse #2 Possible to do  reranking Possible to combine with other stages E.g., speech recognition, translation Maximum Likelihood Estimates Use the parsed training set for getting the counts P ML (α β)  =  Count   (α β) / Count (α) Example:  P ML (S NP VP) =  Count  (S NP VP)/ Count (S) Example from  Jurafsky  and Martin Sample Probabilistic Grammar       S  -> NP  VP    [p0=1]       NP -> DT  N    [p1=.8]       NP -> NP PP   [p2=.2]       PP -> PRP  NP  [p3=1]       VP -> V  NP    [p4=.7]       VP -> VP PP   [p5=.3]       DT ->  'a'     [p6=.25]       DT -> 'the'   [p7=.75]       N ->  'child'  [p8=.5]       N -> 'cake'   [p9=.3]       N -> 'fork'   [p10=.2]       PRP -> 'with'  [p11=.1]       PRP -> 'to'   [p12=.9]       V ->  'saw'    [p13=.4]       V -> 'ate'    [p14=.6] Example the child ate the cake with the fork the child ate the cake with the fork DT .75 DT .75 N .5 the child ate the cake with the fork DT .75 N .5 NP .8 the child ate the cake with the fork DT .75 N .5 NP .8*.5*.75 the child ate the cake with the fork DT .75 N .5 NP .8*.5*.75 the child ate the cake with the fork Keep only the highest score in each cell Question Now, on your own, compute the probability of the entire sentence using Probabilistic CKY. Don’t forget that there may be multiple parses, so you will need to add the corresponding probabilities. Notes Stanford Demo http ://nlp.stanford.edu:8080/parser / PTB statistics 50,000 sentences (40,000 training; 2,400 testing) PTB peculiarities includes traces and other null elements Flat NP structure (e.g., NP -> DT JJ  JJ  NNP NNS) Parent transformation Subject NPs are more likely to be modified than object NPs E.g., replace NP with NP^S NLP NLP Neural Parsing Deep Learning Combining Embeddings and Parsing [Hermann and  Blunsom   2013] Combining Embeddings and Parsing [Hermann and  Blunsom   2013] Neural Dependency Parsing [Chen and Manning, EMNLP 2014] Compositional Vector Grammar (CVG) [ Socher  et al. ACL 2013] Compositional Vector Grammar (CVG) [ Socher  et al. ACL 2013] RNTN for Sentiment Analysis [ Socher  et al. EMNLP 2013] RNTN for Sentiment Analysis [ Socher  et al. EMNLP 2013] http://nlp.stanford.edu/sentiment/treebank.html Neural Semantic Parsing [Dong and  Lapata  2016] Neural Semantic Parsing [Dong and  Lapata  2016] NLP NLP Introduction to NLP First Order Logic Properties of Propositional Logic Pros Compositional Declarative Cons Limited expressive power Represents facts First Order Logic Used to represent Objects – Martin the cat Relations – Martin and Moses are brothers Functions – Martin’s age Formula   AtomicFormula | Formula Connective Formula  					| Quantifier Variable Formula |  ¬  Formula | (Formula) AtomicFormula  Predicate (Term…) Term  Function (Term…) | Constant | Variable Connective   ∧   |  ⋁  |  ⇒ Quantifier   ∀  |   Constant  M | Martin Variable   x  |  y  | … Predicate  Likes | Eats | … Function  AgeOf | ColorOf | … First Order Logic Common Mistake (1)   is the main connective with   Common mistake: using    as the main connective with   :  x Cat(x)    EatsFish(x) means “Everyone is a cat and everyone eats fish” Common Mistake (2)   is the main connective with   Common mistake: using    as the main connective with   :  x  Cat(x)    EatsFish(x) 	is true if there is anyone who is not a cat! First Order Logic NACLO problem from 2014 Author: Ben King http:// www.nacloweb.org/resources/problems/2014/N2014-H.pdf   http:// www.nacloweb.org/resources/problems/2014/N2014-HS.pdf   First Order Logic Solutions Lambda Expressions Example inc(x) =  λ x x+1 then inc(4) = ( λ x x+1)(4) = 5 Example add(x,y) =  λ x, λ y(x+y) then add(3,4) = ( λ x, λ y(x+y))(3)(4)= ( λ y 3+y)(4) = 3+4 =  7 Useful for semantic parsing (see later) NLP NLP Parsing Prepositional Phrase Attachment  (3/3) Algorithm 2a Some Observations (1/2) First, even though the expected performance of rule 3 was 52%, its actual performance on the training set dropped to 39% after rules 1 and 2 were applied.  In other words, these rules used up some of the information hidden in the data ahead of rule 3 and left it less useful information to rely upon.  Even more, one can see that a better decision would have been to replace rule 3 with its exact opposite, label everything left at this stage as “high”, which would have boosted the combined performance.  Algorithm 2a would achieve 5,527 + 2,172 + 7,714 = 15,413 correct decisions for an overall accuracy of 74% on the training set. Second, one cannot help but notice that Algorithms 2 and 2a each have only three rules.  We  can imagine a classifier with 20,801 rules, one per training example, each rule of the form “if the preposition is “of” and the nouns are such and such and the verbs are such and such, then classify the data point as the actual class observed in the training set”.  Some Observations (2/2) Third, we so far reported performance on the training set.  Can we project the performance on the training set to the test set?  Let’s start with Algorithms 1 and 3.  Algorithm 1 labels everything as low attachment. It achieved 52% on the training set. We expect its performance on the test set to be similar. In fact it is 59% (1,826/3,097 ). This clearly demonstrates the variability of text across subsets of the data.  In this case, this variability favors Algorithm 1 since its performance actually goes up when moving to the test set. In other cases (e.g., if we had swapped the training and test sets), its performance would have gone down.  On average though, its performance on the test data is expected to vary around the performance on the training data.  Algorithm 3 Some Observations 1/5 Now, let’s consider Algorithm 3.  It  achieved a very high performance on the training data (way above the “upper bound” achieved by humans).  However, we will now see the meaning of the word  overfitting  in action.  Algorithm 3 was so specific to the training data that most of the rules it learned don’t apply at all in the test set.  Only 117 combinations (out of 3032) of words in the test set match a combination previously seen in the training set.  In other words, Algorithm 3 learned a lot of good rules, but it failed to learn many more. In fact, its accuracy on the test data is only around 4%.  Some Observations 2/5 An  alternative to Algorithm 3 would be to combine it with a default rule (just like rule 3 in Algorithm 2) that labels everything that Algorithm 3 missed as noun attachment.  Unfortunately, even this algorithm (let’s call it Algorithm 3a) would only achieve a performance slightly above the baseline (Algorithm 1) of 59% on the test data.  The lesson to learn here is that, on unseen data, a simple algorithm (Algorithm 1) is much better than a really complicated one that  overfits  (Algorithm 3). Also, the combination of the two ( overfitting  + baseline) just barely outperforms the baseline itself and is nowhere close to competitive.  Some Observations 3/5 Clearly this algorithm (Algorithm 3) would achieve close to 100% accuracy on the training set.  Why “close to 100%” and not “100%”?  It turns out that the training set there are mutually inconsistent labels for the same data point.  For example, “won verdict in case” appears once as high and once as low attachment.  There are a total of 56 such “discrepancies” in the training set.  Some of them are caused by inconsistent annotators whereas others would require more context (e.g., the entire paragraph or document) to be correctly disambiguated . Some Observations 4/5 Next, let’s see how algorithms 2 and 2a will fare on the test set.  First, let’s look at Algorithm 2.  There are 3097 items to classify in the test set.  Rule 1 correctly classifies 918 out of 926 instances of “of” (99% accuracy) while rule 2 gets 70% accuracy (234/332 correctly classified).  Rule 3 achieves 810/1,839 = 44%.  Overall the accuracy of Algorithm 2 on the test set is 63% (1,962/3,097).  Again, on the test data, Algorithm 2a outperforms Algorithm 2. Its Rule 3 gets 1,029/1,839 = 56% accuracy and the overall accuracy of Algorithm 2a on the test set is 70% (2,181/3,097).  Some Observations 5/5 Let’s  now summarize  the performance  of the five algorithms that we have looked at so far. What’s Next? So far, so good. We have been able to go from 59% test set accuracy to 70% with two simple rules.  What  additional sources of information can we use to improve the algorithm?  Here  are some  ideas: use  a few more  good  word features (e.g., more prepositions, perhaps some verb and  nouns) use  clever ways to deal with missing  information use  lexical semantic information (e.g., synonyms ) use  additional context beyond the four feature types used so far. What’s Next? Let’s first consider a combination of the first two ideas above: looking for ways to use all possible information that can be extracted from the training data.  This is the approach that was used by Collins and Brooks (1995).  Their method was based on a principle called  backoff  which is somewhat of a combination of all the algorithms used so far (e.g., Algorithms 1, 2, and 3).  Backoff  allows us to use the most specific evidence from the training data, when available but then make reasonable approximations for the missing evidence . Backoff  Method Collins and Brooks algorithm: If a 4-tuple is available, use it.  If not, combine the evidence from the triples that form the 4-tuple (looking only at the triples that include the preposition).  If that is not available, look at the pairs, then the singletons, and finally use a default class.  A 4-tuple is just a set of 4 features in a particular order, e.g., (verb, noun1, preposition, noun2).  The matching term for 3 features is a triple; for 2 features it is a pair; and for 1 feature, the word singleton is used . What’s Next? The idea behind Algorithm 3 was quite reasonable – assume that if the same object appear again (as defined by the same set of four features), it will likely have the same tag.  The problem with this approach is that there is not enough data in the training set to learn the likely classes of all possible combinations of features.  Let’s do the math. To cover all the data points in the test set, we’d need information in the training set for a total of 102,998,280,840 combinations (more than 100 Billion combinations)!  How did we arrive at this number?  It  is simply the product of the numbers 1123, 1295, 52, and 1362, which are, respectively, the numbers of distinct verbs, noun1s, prepositions, and noun2s in the test set.  It is impossible to label so much data and even if it could be done, there would be billions more combinations needed to cover a new test set.  Other Methods Zhao and Lin  2004 nearest  neighbors Find most similar examples – 86.5% best accuracy Similar to  Zavrel ,  Daelemans , and  Veenstra  1997 – memory-based learning Abney  et al.  1999 Boosting Stetina   and Nagao  1997 Semantics Toutanova   et al.  2004 Graph-based  method Comparative Results NLP NLP Text Similarity Thesaurus-based Word Similarity Methods Quiz Which pair of words exhibits the greatest similarity? 1. Deer-elk 2. Deer-horse 3. Deer-mouse 4. Deer-roof Quiz Answer Which pair of words exhibits the greatest similarity? 1. Deer-elk 2. Deer-horse 3. Deer-mouse 4. Deer-roof Why? Remember  Wordnet ruminant deer giraffe wapiti caribou elk okapi even-toed ungulate odd-toed ungulate equine ungulate horse zebra mule pony Path Similarity Version 1 Sim   ( v,w ) = -  pathlength  ( v,w ) Version 2 Sim  ( v,w ) = - log  pathlength  ( v,w ) Problems with this Approach There may be no tree for the specific domain or language A specific word (e.g., a term or a proper noun) may not be in any tree IS-A ( hypernym ) edges are not all equally apart in similarity space Path similarity between two words Version  3  (Philip  Resnik ) Sim  ( v,w ) = - log  P (LCS( v,w )) where  LCS = lowest common  subsumer ,  e.g ., 	 ungulate  for deer and horse 	deer for deer and  elk Information content Version 4 ( Dekang  Lin) Wordnet  augmented with probabilities (Lin 1998) IC(c) = -log P(c ) Sim  ( v,w ) =  2 x  log  P (LCS( v,w )) / (log  P (v) + log  P (w)) = 0.59 Wordnet  Similarity in NLTK NLTK >>>  dog.lin_similarity (cat,  brown_ic ) 0.879 >>>  dog.lin_similarity (elephant,  brown_ic ) 0.531 >>>  dog.lin_similarity (elk,  brown_ic ) 0.475 NLP NLP Machine Translation The Noisy Channel Model The Noisy Channel Model Source-channel model of communication Parametric probabilistic models of language and translation Statistics Given f, guess e Statistical MT Translate from French: “ une  fleur rouge”? Statistical MT Translate from French: “ une  fleur rouge”? Statistical MT Translate from French: “ une  fleur rouge”? Statistical MT Translate from French: “ une  fleur rouge”? Statistical MT Translate from French: “ une  fleur rouge”? Statistical MT Translate from French: “ une  fleur rouge”? Noisy Channel Model Applications Text-to-text (e.g., text summarization) Speech recognition Spelling Correction Optical Character Recognition P( text|pixels ) = P(text) P( pixels|text ) Machine Translation Word Alignment Examples From [Brown et al. 1993] Representing Word Alignments Complexity of Alignment Finding the optimal alignment is NP-hard Reduction from Traveling Salesman Problem Each word is a city Each bigram is a distance from one city to another Each translation is a complete tour of all cities NLP NLP Introduction to NLP Why is NLP hard? Example How many different interpretations does the above sentence have?  How many of them are reasonable/grammatical? Time flies like an arrow. Quiz Answer The most obvious meaning is time flies very fast; as fast as an arrow. This is a metaphorical interpretation.  Computers are not really good at metaphors. Other interpretations: Flies like honey -> flies like an arrow -> fruit flies like an arrow Take a stopwatch and time the race -> time the flies  More Classic Examples Beverly Hills Beverly Sills The box is in the pen The pen is in the box Mary and Sue are mothers Mary and Sue are sisters Every American has a mother Every American has a president We gave the monkeys the bananas because they were hungry We gave the monkeys the bananas because they were over-ripe http://specgram.com/CLIII.4/08.phlogiston.cartoon.zhe.html   Syntax vs. Semantics *  Little a has Mary lamb.  ?  Colorless green ideas sleep furiously . [Chomsky 1957] Ambiguous Words ball, board, plant meaning fly , rent, tape part  of speech address , resent, entrance,  number, unionized pronunciation – give it a try Answer to the quiz a ddress Th e stress can be on either syllable. Compare with transport, effect, outline r esent As a verb infinitive or as “re-sent” a letter e ntrance As a noun or as a verb meaning to put someone in a trance n umber As a noun but also as th e comparative of the adjective “numb” Ambiguity Not in computer languages (by design)! Or  Lojban   Noun-noun phrases: (XY)Z vs. X(YZ) science  fiction writer customer  service representative state  chess  tournament NACLO Problem One Two Tree, by  Noah Smith, Kevin  Gimbel , and Jason  Eisner http://www.nacloweb.org/resources/problems/2012/N2012-R.pdf      Solution NACLO Problem Fakepapershelfmaker , by Willie Costello http://www.nacloweb.org/resources/problems/2008/N2008-F.pdf      Solution NACLO Problem Solutions One Two Tree http://www.nacloweb.org/resources/problems/2012/N2012-RS.pdf   ‎ Fakepapershelfmaker   http://www.nacloweb.org/resources/problems/2008/N2008-FS.pdf    Types of Ambiguity Morphological:  Joe is quite impossible. Joe is quite important. Phonetic:  Joe’s finger got number. Part of speech:  Joe won the first round. Syntactic:  Call Joe a taxi. Prepositional phrase attachment:  Joe ate pizza with a fork / with meatballs / with Samantha / with pleasure. Sense:  Joe took the bar exam. Other Sources of Difficulty Subjectivity:  Joe believes that stocks will rise. Cc attachment:  Joe likes ripe apples and pears. Negation:  Joe likes his pizza with no cheese and tomatoes. Referential:  Joe yelled at Mike. He had broken the bike. Joe yelled at Mike. He was angry at him. Reflexive:  John bought him a present.  John bought himself a present. Ellipsis and parallelism:  Joe gave Mike a beer and Jeremy a glass of wine. Metonymy:  Boston called and left a message for Joe. Other Sources of Difficulties Non-standard, slang, and novel words and usages A360, 7342.67, +1-646-555-2223 “spam” or “friend” as verbs yolo, selfie, chillax – recently recognized as dictionary words www.urbandictionary.com  – (Parental Warning!) Inconsistencies junior college, college junior pet spray, pet llama Typoes  and  gramattical   erorz    reciept , John Hopkins, should of Parsing problems Selbständigkeit  (self-reliance) cup holder Federal Reserve Board Chairman Other Sources of Difficulties Complex sentences Counterfactual sentences Humor and sarcasm Implicature /inference/world knowledge:  I was late because my car broke down.  Implies I have a car, I use the car to get to places, the car has wheels, etc. What is not explicitly mentioned, what is world knowledge? Semantics vs. pragmatics  Do you know the time? L anguage is hard even for humans  Both first language and second language Synonyms and  Paraphrases The S&P 500 climbed 6.93, or 0.56 percent, to 1,243.72,           its  best close       since June 12, 2001.    The Nasdaq  gained 12.22, or 0.56 percent, to 2,198.44   for   its best showing since June 8, 2001.       The DJIA     rose  68.46, or 0.64 percent, to 10,705.55,       its  highest level    since March 15. Synonyms and  Paraphrases The S&P 500  climbed  6.93, or 0.56 percent, to 1,243.72,            its  best close        since June 12, 2001.    The Nasdaq   gained  12.22, or 0.56 percent, to 2,198.44   for   its best showing  since June 8, 2001.       The DJIA      rose   68.46, or 0.64 percent, to 10,705.55,        its  highest level     since March 15. NLP NLP Introduction to NLP Summarization Text Summarization Health Benefits Eating a diet rich in vegetables and fruits as part of an overall healthy diet may reduce risk for heart disease, including heart attack and stroke. Eating a diet rich in some vegetables and fruits as part of an overall healthy diet may protect against certain types of cancers. Diets rich in foods containing fiber, such as some vegetables and fruits, may reduce the risk of heart disease, obesity, and type 2 diabetes. Eating vegetables and fruits rich in potassium as part of an overall healthy diet may lower blood pressure, and may also reduce the risk of developing kidney stones and help to decrease bone loss. Eating foods such as vegetables that are lower in calories per cup instead of some other higher-calorie food may be useful in helping to lower calorie intake. Nutrients Most vegetables are naturally low in fat and calories. None have cholesterol. (Sauces or seasonings may add fat, calories, or cholesterol.) Vegetables are important sources of many nutrients, including potassium, dietary fiber, folate (folic acid), vitamin A, and vitamin C. Diets rich in potassium may help to maintain healthy blood pressure. Vegetable sources of potassium include sweet potatoes, white potatoes, white beans, tomato products (paste, sauce, and juice), beet greens, soybeans, lima beans, spinach, lentils, and kidney beans. Dietary fiber from vegetables, as part of an overall healthy diet, helps reduce blood cholesterol levels and may lower risk of heart disease. Fiber is important for proper bowel function. It helps reduce constipation and diverticulosis. Fiber-containing foods such as vegetables help provide a feeling of fullness with fewer calories. Folate (folic acid) helps the body form red blood cells. Women of childbearing age who may become pregnant should consume adequate folate from foods, and in addition 400 mcg of synthetic folic acid from fortified foods or supplements. This reduces the risk of neural tube defects,  spina  bifida, and anencephaly during fetal development. Vitamin A keeps eyes and skin healthy and helps to protect against infections. Vitamin C helps heal cuts and wounds and keeps teeth and gums healthy. Vitamin C aids in iron absorption. Summary Eating vegetables is healthy . News Summarization Book Summaries Cliff’s notes Book  a minute ( http://www.rinkworks.com/bookaminute / ) Which book is this? (Some BOYS crash on an ISLAND.) Ralph  We need a fire.  (They make a fire. It goes out.) Ralph  We need a fire.  (They make a fire. It goes out.) Ralph  We need a fire.  Jack  Forget the fire. Let's kill each other.  Other Boys  Yeah!  (They do.)   THE END Ultra-Condensed by David J. Parker and Samuel Stoddard Movie Summaries Titanic Beginning  with genuine footage of the departure of the Titanic on its fateful voyage, this epic movie tells the events of that tragic night from the perspective of fictional survivor Rose. As an old lady of 100 she recounts her story of duty, love and disaster to a salvage crew searching for a lost gem. Winner of 11 Oscars, James Cameron's effects-driven blockbuster puts a human face on a tragedy of epic proportions by wedding the historical tale of the doomed ocean liner with a fictional romance between two of the ship's ill-fated passengers. Society girl Rose Dewitt  Bukate  and penniless artist Jack Dawson struggle to survive both the sinking ship -- and the wrath of Rose's wealthy fiancé. Search Engine Snippets Genres of Summaries headlines outlines minutes biographies abridgments sound bites movie summaries chronologies, etc. [Mani and  Maybury  1999] Types of Summaries Input Output Purpose Indicative, informative, and critical summaries Form Extracts (representative paragraphs/sentences/phrases) Abstracts: “a concise summary of the central subject matter of a document” [Paice90]. Dimensions Single-document vs. multi-document Context Query-specific vs. generic Stages of Summarization Three stages (typically) content identification conceptual organization realization BAGHDAD, Iraq (CNN) 6 July 2004  --  Three U.S. Marines have died in al Anbar Province west of Baghdad, the Coalition Public Information Center said Tuesday. According to CPIC, "Two Marines assigned to [1st] Marine Expeditionary Force were killed in action and one Marine died of wounds received in action Monday in the Al Anbar Province while conducting security and stability operations.“ Al Anbar Province -- a hotbed for Iraqi insurgents -- includes the restive cities of Ramadi and Fallujah and runs to the Syrian and Jordanian borders. Meanwhile, officials said eight people died Monday in a U.S. air raid on a house in Fallujah that American commanders said was used to harbor Islamic militants. A senior U.S. military official told CNN the target was a group of people suspected of planning suicide attacks using vehicles. The strike was the latest in a series of raids on the city to target what U.S. military spokesmen have called  safehouses  for the network led by fugitive Islamic militant leader Abu  Musab  al-Zarqawi. A statement from  Allawi  said: "The people of Iraq will not tolerate terrorist groups or those who collaborate with any other foreign fighters such as the Zarqawi network to continue their wicked ways. "The sovereign nation of Iraq and our international partners are committed to stopping terrorism and will continue to hunt down these evil terrorists and weed them out, one by one. I call upon all Iraqis to close ranks and report to the authorities on the activities of these criminal cells.“ American planes dropped two 1,000-pound bombs and four 500-pound bombs on the house about 7:15 p.m. (11:15 a.m. ET), according to a statement from the U.S.-led Multi-National Force-Iraq. At least four previous air raids have targeted suspected Zarqawi  safehouses  in Fallujah. BAGHDAD, Iraq (CNN) 6 July 2004  --  Three U.S. Marines have died in al Anbar Province west of Baghdad , the Coalition Public Information Center said Tuesday. According to CPIC, "Two Marines assigned to [1st] Marine Expeditionary Force were killed in action and one Marine died of wounds received in action Monday in the Al Anbar Province while conducting security and stability operations.“ Al Anbar Province   -- a hotbed for Iraqi insurgents -- includes the restive cities of Ramadi and Fallujah and  runs to the Syrian and Jordanian borders . Meanwhile , officials said  eight people died Monday in a U.S. air raid on a house in Fallujah  that American commanders said was used to harbor Islamic militants. A senior U.S. military official told CNN the target was a group of people suspected of planning suicide attacks using vehicles. The strike was the latest in a series of raids on the city   to target what U.S. military spokesmen have called  safehouses  for the network led by fugitive Islamic militant leader Abu  Musab  al-Zarqawi. A statement from  Allawi  said: "The people of Iraq will not tolerate terrorist groups or those who collaborate with any other foreign fighters such as the Zarqawi network to continue their wicked ways. "The sovereign nation of Iraq and our international partners are committed to stopping terrorism and will continue to hunt down these evil terrorists and weed them out, one by one. I call upon all Iraqis to close ranks and report to the authorities on the activities of these criminal cells.“ American planes dropped two 1,000-pound bombs and four 500-pound bombs on the house about 7:15 p.m. (11:15 a.m. ET), according to a statement from the U.S.-led Multi-National Force-Iraq. At least four previous air raids have targeted suspected Zarqawi  safehouses  in Fallujah . Human Summarization and Abstracting What professional abstractors do Ashworth (1973): “To take an original article, understand it and pack it neatly into a nutshell without loss of substance or clarity presents a challenge which many have felt worth taking up for the joys of achievement alone. These are the characteristics of an art form”. Extractive Summarization Selecting units of the original text Usually sentences No simplification No rewriting Baseline Extract the first few sentences NLP NLP Introduction to NLP Cocke - Kasami -Younger (CKY) Parsing Notes on Left Recursion Problematic for many parsing methods Infinite loops when expanding But appropriate linguistically NP -> DT N NP -> PN DT -> NP ‘s Mary’s mother’s sister’s friend Chart Parsing Top-down parsers have problems with expanding the same non-terminal In particular, pre-terminals such as POS Bad idea to use top-down (recursive descent) parsing as is Bottom-up parsers have problems with generating locally feasible subtrees that are not viable globally Chart parsing will address these issues Dynamic Programming Motivation A lot of the work is repeated Caching intermediate results improves the complexity Dynamic programming Building a parse for a substring [i,j] based on all parses [i,k] and [k, j] that are included in it. Complexity O( n 3 ) for recognizing an input string of length  n Dynamic Programming CKY ( Cocke - Kasami -Younger) bottom-up requires a normalized ( binarized ) grammar Earley  parser top-down more complicated (separate lecture) CKY Algorithm function   cky  (sentence W, grammar G)  returns  table    for   i  in 1..length(W)  do     table[i-1,i] = {A|A->Wi in G}    for  j in 2..length(W)  do      for   i  in j-2 down to 0  do        for  k in (i+1) to (j-1)  do         table[ i,j ] = table[ i,j ] union {A|A->BC in G, B in table [ I,k ], C in table [ k,j ]} If the start symbol S is in table [0,n] then W is in L(G) ["the", "child", "ate", "the", "cake", "with", "the", "fork"]       S -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate' Example the child ate the cake with the fork the child ate the cake with the fork DT DT N the child ate the cake with the fork DT N NP the child ate the cake with the fork DT N NP the child ate the cake with the fork DT N V the child ate the cake with the fork NP DT N V DT the child ate the cake with the fork NP DT N V DT N NP the child ate the cake with the fork DT N V DT N NP NP the child ate the cake with the fork DT N V DT N NP NP the child ate the cake with the fork DT N V DT N NP VP NP the child ate the cake with the fork DT N V DT N NP VP NP the child ate the cake with the fork DT N V DT N NP S VP NP the child ate the cake with the fork DT N V DT N NP S VP NP the child ate the cake with the fork DT N V DT N PRP NP S VP NP the child ate the cake with the fork DT N V DT N PRP DT N NP S VP NP NP PP NP the child ate the cake with the fork DT N V DT N PRP DT N NP S VP VP NP NP PP NP the child ate the cake with the fork DT N V DT N PRP DT N NP S VP VP NP NP PP NP the child ate the cake with the fork DT N V DT N PRP DT N NP S S VP VP NP NP PP NP the child ate the cake with the fork DT N V DT N PRP DT N NP S S VP VP NP NP PP NP the child ate the cake with the fork [0]  DT [1]   N [2] ==> [0]  NP [2] [3]  DT [4]   N [5] ==> [3]  NP [5] [6]  DT [7]   N [8] ==> [6]  NP [8] [2]   V [3]  NP [5] ==> [2]  VP [5] [5] PRP [6]  NP [8] ==> [5]  PP [8] [0]  NP [2]  VP [5] ==> [0]   S [5] [3]  NP [5]  PP [8] ==> [3]  NP [8] [2]   V [3]  NP [8] ==> [2]  VP [8] [2]  VP [5]  PP [8] ==> [2]  VP [8] [0]  NP [2]  VP [8] ==> [0]   S [8] What is the  meaning  of each of these sentences? (S   (NP (DT the) (N child))   (VP     (VP (V ate) (NP (DT the) (N cake)))     (PP (PRP with) (NP (DT the) (N fork))))) (S   (NP (DT the) (N child))   (VP     (VP (V ate) (NP (DT the) (N cake)))     (PP (PRP with) (NP (DT the) (N fork))))) (S   (NP (DT the) (N child))   (VP     (V ate)     (NP       (NP (DT the) (N cake))       (PP (PRP with) (NP (DT the) (N fork)))))) Complexity of CKY Space complexity There are O( n 2 ) cells in the table Single parse Each cell requires a linear lookup. Total time complexity is O( n 3 ) All parses Total time complexity is exponential ["take", "this", "book"]     S -> NP VP | Aux NP VP | VP     NP -> PRON |  Det  Nom     Nom -> N | Nom N | Nom PP     PP -> PRP NP     VP -> V | V NP | VP PP      Det  -> 'the' | 'a' | 'this'     PRON -> 'he' | 'she'     N -> 'book' | 'boys' | 'girl'     PRP -> 'with' | 'in'     V -> 'takes' | 'take' A longer example ["take", "this", "book"]     S -> NP VP |  Aux NP VP  |  VP     NP ->  PRON  |  Det  Nom     Nom ->  N  | Nom N | Nom PP     PP -> PRP NP     VP ->  V  | V NP | VP PP      Det  -> 'the' | 'a' | 'this'     PRON -> 'he' | 'she'     N -> 'book' | 'boys' | 'girl'     PRP -> 'with' | 'in'     V -> 'takes' | 'take' Non-binary productions Chomsky Normal Form (CNF) All rules have to be in binary form: X    Y Z    or    X    w This introduces new non-terminals for  hybrid rules n- ary  rules unary rules epsilon rules (e.g., NP     e ) Any CFG can be converted to CNF See  Aho  & Ullman p. 152  ATIS grammar S  → NP VP S  → Aux NP VP S → VP NP → Pronoun NP → Proper-Noun NP →  Det  Nominal Nominal → Noun Nominal → Nominal Noun Nominal → Nominal PP VP → Verb VP → Verb NP VP → VP PP PP → Prep NP Original version From  Jurafsky  and Martin  ATIS grammar in CNF S  → NP VP S  → Aux NP VP S → VP NP → Pronoun NP → Proper-Noun NP →  Det  Nominal Nominal → Noun Nominal → Nominal Noun Nominal → Nominal PP VP → Verb VP → Verb NP VP → VP PP PP → Prep NP Original version CNF version S  → NP VP S  → X1 VP X1 → Aux NP S → book | include | prefer S → Verb NP S → VP PP NP → I  | he | she | me NP → Houston | NWA NP →  Det  Nominal Nominal → book | flight | meal | money Nominal → Nominal Noun Nominal → Nominal PP VP → book | include | prefer VP → Verb NP VP → VP PP PP → Prep NP  ATIS grammar in CNF S  → NP VP S  → Aux NP VP S → VP NP → Pronoun NP → Proper-Noun NP →  Det  Nominal Nominal → Noun Nominal → Nominal Noun Nominal → Nominal PP VP → Verb VP → Verb NP VP → VP PP PP → Prep NP Original version CNF version S  → NP VP S  → X1 VP X1 → Aux NP S → book | include | prefer S → Verb NP S → VP PP NP → I  | he | she | me NP → Houston | NWA NP →  Det  Nominal Nominal → book | flight | meal | money Nominal → Nominal Noun Nominal → Nominal PP VP → book | include | prefer VP → Verb NP VP → VP PP PP → Prep NP Chomsky Normal Form All rules have to be in binary form: X    Y Z    or    X    w New non-terminals for hybrid rules, n-ary and unary rules: INF-VP   to VP      becomes INF-VP   TO VP TO  to S   Aux NP VP      becomes S  R1 VP R1  Aux NP S    VP      VP    Verb    VP    Verb NP      VP    Verb PP        becomes S    book  S    buy S    R2 PP S    Verb PP     etc. Issues with CKY Weak equivalence only Same language, different structure If the grammar had to be converted to CNF, then the final parse tree doesn’t match the original grammar However, it can be converted back using a specific procedure Syntactic ambiguity (Deterministic) CKY has no way to perform syntactic disambiguation Notes Demo: http://lxmls.it.pt/2015/cky.html Recognizing vs. parsing Recognizing just means determining if the string is part of the language defined by the CFG Parsing is more complicated – it involves producing a parse tree  NLP NLP Introduction to NLP Practical Issues in Text Classification Pitfall: Overfitting What happens when your model learns your training data a little too well? Development Test Sets and Cross-validation Metric: P/R/F1  or Accuracy Unseen test set avoid  overfitting  (‘tuning to the test set’) more conservative estimate of  performance Cross-validation over multiple splits Handle sampling errors from different datasets Pool results over each split Compute pooled  dev  set performance Training set Development   Test Set Test Set Test Set Underflow Prevention: log space Multiplying lots of probabilities can result in floating-point underflow. Since log( xy ) = log( x ) + log( y ) Better to sum logs of probabilities instead of multiplying probabilities. Class with highest un-normalized log probability score is still most probable. Model  is now just max of sum of weights No labeled data: Manual Rules If  (wheat or grain) and not (whole or bread) then Categorize as  grain Need careful crafting  Human tuning on development data Time-consuming: 2 days per class Very little data? Use  N a ï ve  Bayes Naïve  Bayes  is a “high-bias” algorithm  ( Ng and Jordan 2002 NIPS) Get more  labeled data  Find clever ways to get humans to  label data for  you Try semi-supervised training methods: Bootstrapping, EM over unlabeled documents,  … A reasonable amount of data? Perfect for all the clever classifiers SVM Regularized Logistic Regression You can even use user-interpretable decision trees Users  like to  hack M anagement  likes  quick fixes A huge amount of data? Can achieve high accuracy! At a cost: SVMs  (train time) or  kNN  (test time)  can be too slow R egularized  logistic  regression can be somewhat better So Naïve  Bayes can come back into its own again ! Or you can play with deep learning… NLP NLP Machine Translation Sentence Alignment Sentence Alignment Tokenization Sentence alignment (1-1, 2-2, 2-1 mappings) Church and Gale 1993 based on sentence length similar to previous work by Brown et al. 1988 Sentence Alignment [Church/Gale 1993] Machine Translation The IBM Models Questions If the word order is fixed Align  strings using  the  Levenshtein  method What about the following: How to deal with word  reorderings ? How to deal with phrases? We need a systematic (and feasible) approach Generative Story (almost IBM) I watched an interesting play I watched  watched  an interesting play  play   play I watched  watched  an play  play   play  interesting J’  ai  vu  une  pièce de  théâtre   intéressante IBM’s EM trained models (1-5) Word translation Local alignment Fertilities Class-based alignment Non-deficient algorithm (avoid overlaps, overflow) Model 1 Alignments La  maison   bleue The blue house Alignments: {1,2,3}, {1,3,2}, {1,3,3}, {1,1,1} A priori, all are equally likely Conditional probabilities P( f|A,e ) = ? Model 1 (cont’d) Algorithm Pick length of translation (uniform probability) Choose an alignment (uniform probability) Translate the foreign words (only depends on the word) That gives you P( f,A|e ) We need P( f|A,e ) Use EM (expectation-maximization) to find the hidden variables Model 1 (cont’d) Length probability Alignment probability Translation probability Finding the Optimal Alignment Training Model 1 Goal: Learn the translation probabilities p( f|e ) EM Algorithm Used to estimate the translation probabilities from a training corpus Guess p( f|e )           (could be uniform) Repeat until convergence: E-step: compute counts M-step:  recompute  p( f|e ) green house               the house casa verde                  la casa Corpus: Uniform translation model: Example E-step 1: compute the expected counts E[count(t( f|e ))] for all word pairs ( f j ,e aj ) E-step 1a: compute P( a,f|e ) by multiplying all  t  probabilities using E-step 1b: normalize P( a,f|e ) to get P( a|e,f ) using   E-step 1c: compute expected fractional counts, by weighting each count by P( a|e,f )   M-step 1: Compute the  MLE  probability params by normalizing the tcounts to sum to 1. E-step 2a: Recompute P(a,f|e) again by multiplying the  t  probabilities More iterations are needed (until convergence) import  itertools corpus = [('green  house','casa   verde '),('the  house','la  casa ')] # Print corpus: vocab1 = [] vocab2 = [] print "Sentence pairs" for  i  in range( len (corpus)):      tup  = corpus[ i ]     print  i ,     print '%s\ t%s ' %  tup       vocab1 +=  tup [0].split()     vocab2 +=  tup [1].split() # Print Vocabulary vocab1 = list(set(vocab1)) vocab2 = list(set(vocab2)) print print "Vocabulary" print "Source Language:", print vocab1 print "Target Language:", print  vocab2 print print " EM initialization " prob  = {} for w in vocab1:     for v in vocab2:          prob [( w,v )] = 1. /  len (vocab2)         print "P(%s|%s) = %.2f\t" % ( v,w,prob [( w,v )]),      print Code by  Rui  Zhang def   E_step ( prob ):     print " E_step "      def   compute_align ( a,sent_pair ):         print "\t Alignment:",         p = 1.         s =  sent_pair [0].split()         t =  sent_pair [1].split()         for  i  in range( len (a)):             w = s[ i ]             v = t[a[ i ]]             print ( w,v ),             p = p *  prob [( w,v )]         print          print "\t p( a,f|e ): %.2f" % p         return p               new_prob  = {}     for w in vocab1:         for v in vocab2:              new_prob [( w,v )] = 0.     for  i  in range( len (corpus)):         print "Sentence Pair", i          sent_pair  = corpus[ i ]          sent_l  =  len ( sent_pair )              total_i  = []         for a in  itertools.permutations (range( sent_l )):              total_i.append ( compute_align (a,  sent_pair ))         #normalize         #print "\ tp ( a,f|e ):", total_i          total_i_sum  = sum( total_i )          total_i  = [t /  total_i_sum  for t in  total_i ]         print "\n\t Normalizing"         print "\t p( a|e,f ):", total_i         print          s  =  sent_pair [0].split()         t =  sent_pair [1].split()          cnt  = 0         for a in  itertools.permutations (range( sent_l )):             for j in range( len (a)):                 w = s[j]                 v = t[a[j]]                  new_prob [( w,v )] +=  total_i [ cnt ]              cnt  += 1                for w in vocab1:          total_w  = 0.         for v in vocab2:              total_w  +=  new_prob [( w,v )]             print "P(%s|%s) = %.2f\t" % ( v,w,new_prob [( w,v )]),         print "total(%s) = %2.f" % ( w,total_w )     return  new_prob def   M_step ( prob ):     print " M_step "     for w in vocab1:          total_w  = sum([ prob [ w,v ] for v in vocab2])         for v in vocab2:              prob [( w,v )] =  prob [( w,v )] /  total_w             print "P(%s|%s) = %.2f\t" % ( v,w,prob [( w,v )]),         print     return  prob for  i  in range(0,10):     print "step: ", i      prob  =  E_step ( prob )      prob  =  M_step ( prob ) Sentence pairs 0 green house   casa  verde 1 the house     la casa Vocabulary Source Language: ['house', 'the', 'green'] Target Language: [' verde ', 'casa', 'la'] EM  initialization P( verde|house ) = 0.33   P( casa|house ) = 0.33    P( la|house ) = 0.33 P( verde|the ) = 0.33     P( casa|the ) = 0.33      P( la|the ) = 0.33 P( verde|green ) = 0.33   P( casa|green ) = 0.33    P( la|green ) =  0.33 step :   0 E_step Sentence Pair 0          Alignment: ('green', 'casa') ('house', ' verde ')          p( a,f|e ): 0.11          Alignment: ('green', ' verde ') ('house', 'casa')          p( a,f|e ): 0.11          Normalizing          p( a|e,f ): [0.5, 0.5] Sentence Pair 1          Alignment: ('the', 'la') ('house', 'casa')          p( a,f|e ): 0.11          Alignment: ('the', 'casa') ('house', 'la')          p( a,f|e ): 0.11          Normalizing          p( a|e,f ): [0.5, 0.5] P( verde|house ) = 0.50   P( casa|house ) = 1.00    P( la|house ) = 0.50      total(house) =  2 P( verde|the ) = 0.00     P( casa|the ) = 0.50      P( la|the ) = 0.50        total(the) =  1 P( verde|green ) = 0.50   P( casa|green ) = 0.50    P( la|green ) = 0.00      total(green) =   1 M_step P( verde|house ) = 0.25   P( casa|house ) = 0.50    P( la|house ) = 0.25 P( verde|the ) = 0.00     P( casa|the ) = 0.50      P( la|the ) = 0.50 P( verde|green ) = 0.50   P( casa|green ) = 0.50    P( la|green ) = 0.00 step:   1 E_step Sentence Pair 0          Alignment: ('green', 'casa') ('house', ' verde ')          p( a,f|e ): 0.12          Alignment: ('green', ' verde ') ('house', 'casa')          p( a,f|e ): 0.25          Normalizing          p( a|e,f ): [0.3333333333333333, 0.6666666666666666] Sentence Pair 1          Alignment: ('the', 'la') ('house', 'casa')          p( a,f|e ): 0.25          Alignment: ('the', 'casa') ('house', 'la')          p( a,f|e ): 0.12          Normalizing          p( a|e,f ): [0.6666666666666666, 0.3333333333333333] P( verde|house ) = 0.33   P( casa|house ) = 1.33    P( la|house ) = 0.33      total(house) =  2 P( verde|the ) = 0.00     P( casa|the ) = 0.33      P( la|the ) = 0.67        total(the) =  1 P( verde|green ) = 0.67   P( casa|green ) = 0.33    P( la|green ) = 0.00      total(green) =  1 M_step P( verde|house ) = 0.17   P( casa|house ) = 0.67    P( la|house ) = 0.17 P( verde|the ) = 0.00     P( casa|the ) = 0.33      P( la|the ) = 0.67 P( verde|green ) = 0.67   P( casa|green ) = 0.33    P( la|green ) = 0.00 step:   2 E_step Sentence Pair 0          Alignment: ('green', 'casa') ('house', ' verde ')          p( a,f|e ): 0.06          Alignment: ('green', ' verde ') ('house', 'casa')          p( a,f|e ): 0.44          Normalizing          p( a|e,f ): [0.11111111111111112, 0.888888888888889] Sentence Pair 1          Alignment: ('the', 'la') ('house', 'casa')          p( a,f|e ): 0.44          Alignment: ('the', 'casa') ('house', 'la')          p( a,f|e ): 0.06          Normalizing          p( a|e,f ): [0.888888888888889, 0.11111111111111112] P( verde|house ) = 0.11   P( casa|house ) = 1.78    P( la|house ) = 0.11      total(house) =  2 P( verde|the ) = 0.00     P( casa|the ) = 0.11      P( la|the ) = 0.89        total(the) =  1 P( verde|green ) = 0.89   P( casa|green ) = 0.11    P( la|green ) = 0.00      total(green) =  1 M_step P( verde|house ) = 0.06   P( casa|house ) = 0.89    P( la|house ) = 0.06 P( verde|the ) = 0.00     P( casa|the ) = 0.11      P( la|the ) = 0.89 P( verde|green ) = 0.89   P( casa|green ) = 0.11    P( la|green ) = 0.00 step:   3 E_step Sentence Pair 0          Alignment: ('green', 'casa') ('house', ' verde ')          p( a,f|e ): 0.01          Alignment: ('green', ' verde ') ('house', 'casa')          p( a,f|e ): 0.79          Normalizing          p( a|e,f ): [0.007751937984496124, 0.9922480620155039] Sentence Pair 1          Alignment: ('the', 'la') ('house', 'casa')          p( a,f|e ): 0.79          Alignment: ('the', 'casa') ('house', 'la')          p( a,f|e ): 0.01          Normalizing          p( a|e,f ): [0.9922480620155039, 0.007751937984496124] P( verde|house ) = 0.01   P( casa|house ) = 1.98    P( la|house ) = 0.01      total(house) =  2 P( verde|the ) = 0.00     P( casa|the ) = 0.01      P( la|the ) = 0.99        total(the) =  1 P( verde|green ) = 0.99   P( casa|green ) = 0.01    P( la|green ) = 0.00      total(green) =  1 M_step P( verde|house ) = 0.00   P( casa|house ) = 0.99    P( la|house ) = 0.00 P( verde|the ) = 0.00     P( casa|the ) = 0.01      P( la|the ) = 0.99 P( verde|green ) = 0.99   P( casa|green ) = 0.01    P( la|green ) = 0.00 corpus  = [('green  house','casa   verde '),('the  house','la  casa'),('my  house','mi  casa ')] Sentence pairs 0 green house   casa  verde 1 the house     la casa 2 my house      mi casa Vocabulary Source Language: ['house', 'the', 'green', 'my'] Target Language: ['mi', ' verde ', 'casa', 'la'] EM  initialization P( mi|house ) = 0.25      P( verde|house ) = 0.25   P( casa|house ) = 0.25    P( la|house ) = 0.25 P( mi|the ) = 0.25        P( verde|the ) = 0.25     P( casa|the ) = 0.25      P( la|the ) = 0.25 P( mi|green ) = 0.25      P( verde|green ) = 0.25   P( casa|green ) = 0.25    P( la|green ) = 0.25 P( mi|my ) = 0.25          P( verde|my ) = 0.25      P( casa|my ) = 0.25       P( la|my ) = 0.25 step :   0 E_step Sentence Pair 0          Alignment: ('green', 'casa') ('house', ' verde ')          p( a,f|e ): 0.06          Alignment: ('green', ' verde ') ('house', 'casa')          p( a,f|e ): 0.06           Normalizing          p( a|e,f ): [0.5, 0.5] Sentence Pair 1          Alignment: ('the', 'la') ('house', 'casa')          p( a,f|e ): 0.06          Alignment: ('the', 'casa') ('house', 'la')          p( a,f|e ): 0.06           Normalizing          p( a|e,f ): [0.5, 0.5 ] Sentence  Pair 2          Alignment: ('my', 'mi') ('house', 'casa')          p( a,f|e ): 0.06          Alignment: ('my', 'casa') ('house', 'mi')          p( a,f|e ): 0.06           Normalizing          p( a|e,f ): [0.5, 0.5] P( mi|house ) = 0.50      P( verde|house ) = 0.50   P( casa|house ) = 1.50    P( la|house ) = 0.50      total(house) =  3 P( mi|the ) = 0.00        P( verde|the ) = 0.00     P( casa|the ) = 0.50      P( la|the ) = 0.50        total(the) =  1 P( mi|green ) = 0.00      P( verde|green ) = 0.50   P( casa|green ) = 0.50    P( la|green ) = 0.00      total(green) =  1 P( mi|my ) = 0.50          P( verde|my ) = 0.00      P( casa|my ) = 0.50       P( la|my ) = 0.00          total(my ) =  1 M_step P( mi|house ) = 0.17      P( verde|house ) = 0.17   P( casa|house ) = 0.50    P( la|house ) = 0.17 P( mi|the ) = 0.00        P( verde|the ) = 0.00     P( casa|the ) = 0.50      P( la|the ) = 0.50 P( mi|green ) = 0.00      P( verde|green ) = 0.50   P( casa|green ) = 0.50    P( la|green ) = 0.00 P( mi|my ) = 0.50          P( verde|my ) = 0.00      P( casa|my ) = 0.50       P( la|my ) =  0.00 ... step:   3 ... M_step P( mi|house ) = 0.00      P( verde|house ) = 0.00   P( casa|house ) = 1.00    P( la|house ) = 0.00 P( mi|the ) = 0.00        P( verde|the ) = 0.00     P( casa|the ) = 0.00      P( la|the ) = 1.00 P( mi|green ) = 0.00      P( verde|green ) = 1.00   P( casa|green ) = 0.00    P( la|green ) = 0.00 P( mi|my ) = 1.00          P( verde|my ) = 0.00      P( casa|my ) = 0.00       P( la|my ) =  0.00 corpus = [('green  house','casa   verde '),('the  house','la  casa'),('my  house','mi  casa'),('my houses',' mis  casas ')] Sentence pairs 0 green house   casa  verde 1 the house     la casa 2 my house      mi casa 3 my houses      mis  casas Vocabulary Source Language: ['house', 'the', 'green', 'my', 'houses'] Target Language: ['casa', 'la', 'mi', ' verde ', 'casas', ' mis '] EM initialization P( casa|house ) = 0.17    P( la|house ) = 0.17      P( mi|house ) = 0.17      P( verde|house ) = 0.17   P( casas|house ) = 0.17   P( mis|house ) =  0.17 P( casa|the ) = 0.17      P( la|the ) = 0.17        P( mi|the ) = 0.17        P( verde|the ) = 0.17     P( casas|the ) = 0.17     P( mis|the ) =  0.17 P( casa|green ) = 0.17    P( la|green ) = 0.17      P( mi|green ) = 0.17      P( verde|green ) = 0.17   P( casas|green ) = 0.17   P( mis|green ) =  0.17 P( casa|my ) = 0.17       P( la|my ) = 0.17          P( mi|my ) = 0.17          P( verde|my ) = 0.17      P( casas|my ) = 0.17      P( mis|my ) = 0.17 P( casa|houses ) = 0.17   P( la|houses ) = 0.17     P( mi|houses ) = 0.17     P( verde|houses ) = 0.17  P( casas|houses ) = 0.17  P( mis|houses )  =   0.17 step :   0 E_step Sentence Pair 0          Alignment: ('green', 'casa') ('house', ' verde ')          p( a,f|e ): 0.03          Alignment: ('green', ' verde ') ('house', 'casa')          p( a,f|e ): 0.03          Normalizing          p( a|e,f ): [0.5, 0.5] Sentence Pair 1          Alignment: ('the', 'la') ('house', 'casa')          p( a,f|e ): 0.03          Alignment: ('the', 'casa') ('house', 'la')          p( a,f|e ): 0.03          Normalizing          p( a|e,f ): [0.5, 0.5] Sentence Pair 2          Alignment: ('my', 'mi') ('house', 'casa')          p( a,f|e ): 0.03          Alignment: ('my', 'casa') ('house', 'mi')          p( a,f|e ): 0.03          Normalizing          p( a|e,f ): [0.5, 0.5] Sentence Pair 3          Alignment: ('my', ' mis ') ('houses', 'casas')          p( a,f|e ): 0.03          Alignment: ('my', 'casas') ('houses', ' mis ')          p( a,f|e ): 0.03          Normalizing          p( a|e,f ): [0.5, 0.5] P( casa|house ) = 1.50    P( la|house ) = 0.50      P( mi|house ) = 0.50      P( verde|house ) = 0.50   P( casas|house ) = 0.00   P( mis|house ) =  0.00     total(house) =  3 P( casa|the ) = 0.50      P( la|the ) = 0.50        P( mi|the ) = 0.00        P( verde|the ) = 0.00     P( casas|the ) = 0.00     P( mis|the ) =  0.00       total(the) =  1 P( casa|green ) = 0.50    P( la|green ) = 0.00      P( mi|green ) = 0.00      P( verde|green ) = 0.50   P( casas|green ) = 0.00   P( mis|green ) =  0.00     total(green) =  1 P( casa|my ) = 0.50       P( la|my ) = 0.00          P( mi|my ) = 0.50          P( verde|my ) = 0.00      P( casas|my ) = 0.50      P( mis|my ) = 0.50        total(my ) =  2 P( casa|houses ) = 0.00   P( la|houses ) = 0.00     P( mi|houses ) = 0.00     P( verde|houses ) = 0.00  P( casas|houses ) = 0.50  P( mis|houses )  =  0.50   total(houses) =  1 M_step P( casa|house ) = 0.50    P( la|house ) = 0.17      P( mi|house ) = 0.17      P( verde|house ) = 0.17   P( casas|house ) = 0.00   P( mis|house ) =  0.00 P( casa|the ) = 0.50      P( la|the ) = 0.50        P( mi|the ) = 0.00        P( verde|the ) = 0.00     P( casas|the ) = 0.00     P( mis|the ) =  0.00 P( casa|green ) = 0.50    P( la|green ) = 0.00      P( mi|green ) = 0.00      P( verde|green ) = 0.50   P( casas|green ) = 0.00   P( mis|green ) =  0.00 P( casa|my ) = 0.25       P( la|my ) = 0.00          P( mi|my ) = 0.25          P( verde|my ) = 0.00      P( casas|my ) = 0.25      P( mis|my ) = 0.25 P( casa|houses ) = 0.00   P( la|houses ) = 0.00     P( mi|houses ) = 0.00     P( verde|houses ) = 0.00  P( casas|houses ) = 0.50  P( mis|houses )  =  0.50 step 3: M_step P( casa|house ) = 1.00    P( la|house ) = 0.00      P( mi|house ) = 0.00      P( verde|house ) = 0.00   P( casas|house ) = 0.00   P( mis|house ) = 0.00 P( casa|the ) = 0.00      P( la|the ) = 1.00        P( mi|the ) = 0.00        P( verde|the ) = 0.00     P( casas|the ) = 0.00     P( mis|the ) = 0.00 P( casa|green ) = 0.00    P( la|green ) = 0.00      P( mi|green ) = 0.00      P( verde|green ) = 1.00   P( casas|green ) = 0.00   P( mis|green ) = 0.00 P( casa|my ) = 0.00       P( la|my ) = 0.00          P( mi|my ) = 0.50          P( verde|my ) = 0.00      P( casas|my ) = 0.25      P( mis|my ) = 0.25 P( casa|houses ) = 0.00   P( la|houses ) = 0.00     P( mi|houses ) = 0.00     P( verde|houses ) = 0.00  P( casas|houses ) = 0.50  P( mis|houses ) = 0.50 Model 2 Distortion parameters d( i|j,l,m )  i  and j are words in the two sentences l and m are the lengths of these sentences Example d(“boy”|”garçon”,5,6) The distortion parameters are also learned by EM Model 3 Fertility f(  i |e ) f 0   is an extra parameter that defines   0 Examples                 program =  programme                f(1|program)    1   NOUN     play = pi è ce de  th éâ tre                f(3|play_N)    1         VERB      place =  mettre   en  place              f(3|place_V)    1 [ Brown et al. 1993] IBM Models 4 and 5 Model 4 Deals with relative reordering Model 5 Fixes problems in models 1-4 that allow multiple words to appear in the same position References http://www.isi.edu/natural-language/mt/wkbk.rtf 		 (an awesome tutorial by Kevin Knight) http://www.statmt.org/   		 (a comprehensive site, including references to the old IBM papers, pointers to Moses, etc.) NLP NLP Introduction to NLP Evaluation of Language Models Evaluation of LM Extrinsic Use in an application Intrinsic Cheaper Correlate the two for validation purposes Information Theory Uncertainty	 Entropy Motivating Entropy Adding Probabilities Average Surprise Entropy Example Simplified Polynesian Entropy Calculation Designing a Better Code Joint Entropy Conditional Entropy Chain Rule for Entropy Syllables Syllables (cont’d) Polynesian Syllables Polynesian Syllables (cont’d) Polynesian Syllables (cont’d) Exercise Pointwise  Mutual Information Mutual Information Mutual Information (Cont’d) Relative Entropy Notes on Relative Entropy Divergence as Mutual Information Perplexity Does the model fit the data? A good model will give a high probability to a real sentence Perplexity Average branching factor in predicting the next word Lower is better (lower perplexity -> higher probability) N = number of words Perplexity Example: A sentence consisting of N equiprobable words: p(wi) = 1/k Per = ((k -1 ) N ) (-1/N)  = k Perplexity is like a branching factor Logarithmic version the exponent is = #bits to encode each word) The Shannon Game Consider the Shannon game: New York governor Andrew Cuomo said ... What is the perplexity of guessing a digit if all digits are equally likely? Do the math. 10 How about a letter? 26 How about guessing A (“operator”) with a probability of 1/4, B (“sales”) with a probability of 1/4 and 10,000 other cases with a probability of 1/2 total example modified from Joshua Goodman. Perplexity Across Distributions What if the actual distribution is very different from the expected one? Example: All of the 10,000 other cases are equally likely but P(A) = P(B) = 0. Cross-entropy = log (perplexity), measured in bits Sample Values for Perplexity Wall Street Journal (WSJ) corpus 38 M words (tokens) 20 K types Perplexity Evaluated on a separate 1.5M sample of WSJ documents Unigram 962 Bigram 170 Trigram 109 Word Error Rate Another evaluation metric Number of insertions, deletions, and substitutions Normalized by sentence length Same as Levenshtein Edit Distance Example: governor Dan Malloy met with the mayor the governor met the senator 3 deletions + 1 insertion + 1 substitution = WER of 5 Issues Out of vocabulary words (OOV) Split the training set into two parts Label all words in part 2 that were not in part 1 as <UNK> Clustering e.g., dates, monetary amounts, organizations, years Long Distance Dependencies This is where n-gram language models fail by definition Missing syntactic information The students  who participated in the game  are  tired The student  who participated in the game  is  tired Missing semantic information The pizza  that I had last night was  tasty The class  that I had last night was  interesting Other Ideas in LM Syntactic models Condition words on other words that appear in a specific syntactic relation with them Caching models Take advantage of the fact that words appear in bursts External Resources SRI-LM http://www.speech.sri.com/projects/srilm/ CMU-LM http://www.speech.cs.cmu.edu/SLM/toolkit.html Google n-gram corpus http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html Google book n-grams http://ngrams.googlelabs.com/ Example Google n-grams house a		302435 house after	118894 house all	105970 house and	3880495 house are	136475 house arrest	254629 house as	339590 house at	694739 house before	102663 house built	189451 house but	137151 house by	249118 house can	133187 house cleaning	125206 house design	120500 house down	109663 house fire	112325 house for	1635280 house former	112559 house from	249091 house had	154848 house has	440396 house he	115434 house hotel	139282 house in	3553052 house is	1962473 house music	199346 house near	131889 house now	127043 house of	3164591 house on	1077835 house or	1172783 house party	162668 house plan	172765 house plans	434398 house price	158422 house prices	643669 house rental	209614 house rules	108025 house share	101238 house so	133405 house that	687925 house the	478204 house to	1452996 house training	163056 house value	135820 N-gram External Links http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html http://norvig.com/mayzner.html   http://storage.googleapis.com/books/ngrams/books/datasetsv2.html https://books.google.com/ngrams/ http://www.elsewhere.org/pomo/   http://pdos.csail.mit.edu/scigen/   http://www.magliery.com/Band/   http://www.magliery.com/Country/ http://johno.jsmf.net/knowhow/ngrams/index.php http://www.decontextualize.com/teaching/rwet/n-grams-and-markov-chains/ http://gregstevens.com/2012/08/16/simulating-h-p-lovecraft   http://kingjamesprogramming.tumblr.com/   NLP NLP Long Short-Term Memory Networks (LSTM) Deep Learning LSTM Motivation Remember how we update an RNN? The c at sat Cost w y [slides from Catherine  Finegan-Dollak ] The Vanishing Gradient Problem Deep neural networks use backpropagation. Back propagation uses the chain rule. The chain rule multiplies derivatives. Often these derivatives between 0 and 1. As the chain gets longer, products get smaller until they disappear. Or do they explode? With gradients larger than 1, you encounter the opposite problem with products becoming larger and larger  as the chain becomes longer and longer, causing overlarge updates to parameters. This is the exploding gradient problem. Vanishing/Exploding Gradients  Are Bad. If we cannot  backpropagate  very far through the network, the network cannot learn long-term dependencies.  My dog [chase/chases] squirrels. vs. My dog, whom I adopted in 2009, [chase/chases] squirrels.  LSTM Solution Use  memory cell to  store information at each time  step. Use “gates”  to control the flow of information through the  network. Input gate: protect the current step from irrelevant inputs Output gate: prevent the current step from passing irrelevant outputs to later steps Forget gate: limit information passed from one cell to the next Transforming RNN to LSTM Transforming RNN to LSTM c 0 Transforming RNN to LSTM Transforming RNN to LSTM Transforming RNN to LSTM Transforming RNN to LSTM Transforming RNN to LSTM Transforming RNN to LSTM LSTM for Sequences The c at sat LSTM Applications Language identification (Gonzalez-Dominguez et al., 2014) Paraphrase detection ( Cheng  &  Kartsaklis , 2015) Speech recognition (Graves, Abdel-Rahman, & Hinton, 2013) Handwriting recognition (Graves   &  Schmidhuber , 2009) Music composition ( Eck  &  Schmidhuber , 2002) and lyric generation (Potash, Romanov, &  Rumshisky , 2015) Robot control (Mayer et al., 2008) Natural language generation (Wen et al. 2015) (best paper at EMNLP) Named entity recognition ( Hammerton , 2003) http://www.cs.toronto.edu/~graves/handwriting.html Related Architectures: GRU w x w h x 1 h 0 ĥ 1 σ h 1 z 1 + 1-z 1 r 1 Chung et al. (2014) reports comparable performance to LSTM Related Architectures: Tree LSTMs h 0 c 0 h 1 c 1 x 2 h 2 c 2 u 2 i o f f Tai,  Socher , Manning 2015 External Links http://colah.github.io/posts/2015-08-Understanding-LSTMs /   NLP NLP Introduction to NLP Probabilistic  Grammars Need for Probabilistic Parsing Time flies like an arrow Many parses Some (clearly) more likely than others Need for a probabilistic ranking method Probabilistic Context-Free Grammars Just like (deterministic) CFG, a 4-tuple (N,  ,R,S) N: non-terminal symbols  : terminal symbols (disjoint from N) R: rules (A      ) [p]  is a string from (  N)* p is the probability P(|A) S: start symbol (from N) Example        S  -> NP VP       NP -> DT N | NP PP       PP -> PRP NP       VP -> V NP | VP PP       DT -> 'a' | 'the'       N -> 'child' | 'cake' | 'fork'       PRP -> 'with' | 'to'       V -> 'saw' | 'ate'       S  -> NP  VP           NP -> DT  N           NP -> NP PP          PP -> PRP  NP         VP -> V NP           VP -> VP PP          DT ->  'a'            DT -> 'the'          N ->  'child'         N -> 'cake'          N -> 'fork'          PRP ->  'with'        PRP -> 'to'           V ->  'saw'           V -> 'ate'     Example       S  -> NP  VP           NP -> DT  N           NP -> NP PP          PP -> PRP  NP         VP -> V NP           VP -> VP PP          DT ->  'a'            DT -> 'the'          N ->  'child'         N -> 'cake'          N -> 'fork'          PRP ->  'with'        PRP -> 'to'           V ->  'saw'           V -> 'ate'     Example       S  -> NP  VP    [p0=1]       NP -> DT  N    [p1]       NP -> NP PP   [p2]       PP -> PRP  NP  [p3=1]       VP -> V  NP    [p4]       VP -> VP PP   [p5]       DT ->  'a'     [p6]       DT -> 'the'   [p7]       N ->  'child'  [p8]       N -> 'cake'   [p9]       N -> 'fork'   [p10]       PRP -> 'with'  [p11]       PRP -> 'to'   [p12]       V ->  'saw'    [p13]       V -> 'ate'    [p14] Example Probability of a Parse Tree The probability of a parse tree  t  given all  n  productions used to build it: The most likely parse is determined as follows: The probabilities are obtained using MLE from the training corpus The probability of a  sentence  is the  sum  of the probabilities of all of its parses       S  -> NP  VP    [p0=1]       NP -> DT  N    [p1]       NP -> NP PP   [p2]       PP -> PRP  NP  [p3=1]       VP -> V  NP    [p4]       VP -> VP PP   [p5]       DT ->  'a'     [p6]       DT -> 'the'   [p7]       N ->  'child'  [p8]       N -> 'cake'   [p9]       N -> 'fork'   [p10]       PRP -> 'with'  [p11]       PRP -> 'to'   [p12]       V ->  'saw'    [p13]       V -> 'ate'    [p14] Example S  -> NP  VP    [p0=1] NP  -> DT  N    [p1] NP -> NP PP   [p2] PP  -> PRP  NP  [p3=1] VP  -> V  NP    [p4] VP -> VP PP   [p5] DT  ->  'a'     [p6] DT -> 'the'   [p7] N  ->  'child'  [p8] N -> 'cake'   [p9] N -> 'fork'   [p10] PRP  -> 'with'  [p11] PRP -> 'to'   [p12] V  ->  'saw'    [p13] V -> 'ate'    [p14] Example t 1 Example t 1 t 2 S  -> NP  VP    [p0=1] NP  -> DT  N    [p1] NP -> NP PP   [p2] PP  -> PRP  NP  [p3=1] VP  -> V  NP    [p4] VP -> VP PP   [p5] DT  ->  'a'     [p6] DT -> 'the'   [p7] N  ->  'child'  [p8] N -> 'cake'   [p9] N -> 'fork'   [p10] PRP  -> 'with'  [p11] PRP -> 'to'   [p12] V  ->  'saw'    [p13] V -> 'ate'    [p14] Example t 1 t 2 S  -> NP  VP    [p0=1] NP  -> DT  N    [p1] NP -> NP PP   [p2] PP  -> PRP  NP  [p3=1] VP  -> V  NP    [p4] VP -> VP PP   [p5] DT  ->  'a'     [p6] DT -> 'the'   [p7] N  ->  'child'  [p8] N -> 'cake'   [p9] N -> 'fork'   [p10] PRP  -> 'with'  [p11] PRP -> 'to'   [p12] V  ->  'saw'    [p13] V -> 'ate'    [p14] Example t 1 t 2 S  -> NP  VP    [p0=1] NP  -> DT  N    [p1] NP -> NP PP   [p2] PP  -> PRP  NP  [p3=1] VP  -> V  NP    [p4] VP -> VP PP   [p5] DT  ->  'a'     [p6] DT -> 'the'   [p7] N  ->  'child'  [p8] N -> 'cake'   [p9] N -> 'fork'   [p10] PRP  -> 'with'  [p11] PRP -> 'to'   [p12] V  ->  'saw'    [p13] V -> 'ate'    [p14] Example t 1 t 2 S  -> NP  VP    [p0=1] NP  -> DT  N    [p1] NP -> NP PP   [p2] PP  -> PRP  NP  [p3=1] VP  -> V  NP    [p4] VP -> VP PP   [p5] DT  ->  'a'     [p6] DT -> 'the'   [p7] N  ->  'child'  [p8] N -> 'cake'   [p9] N -> 'fork'   [p10] PRP  -> 'with'  [p11] PRP -> 'to'   [p12] V  ->  'saw'    [p13] V -> 'ate'    [p14] Example t 1 t 2 S  -> NP  VP    [p0=1] NP  -> DT  N    [p1] NP -> NP PP   [p2] PP  -> PRP  NP  [p3=1] VP  -> V  NP    [p4] VP -> VP PP   [p5] DT  ->  'a'     [p6] DT -> 'the'   [p7] N  ->  'child'  [p8] N -> 'cake'   [p9] N -> 'fork'   [p10] PRP  -> 'with'  [p11] PRP -> 'to'   [p12] V  ->  'saw'    [p13] V -> 'ate'    [p14] NLP NLP Introduction to NLP Information Retrieval Toolkits Open Source IR Toolkits Smart (Cornell) MG (RMIT & Melbourne, Australia; Waikato, New Zealand),  Lemur (CMU/Univ. of Massachusetts) Terrier (Glasgow) Lucene /SOLR  (Apache ) Ivory (University of Maryland – cloud computing) Smart The most influential IR system/toolkit Developed at Cornell since 1960’s  Vector space model with lots of weighting options Written in C  The Cornell/AT&T groups have used the Smart system to achieve top TREC performance MG A highly efficient toolkit for retrieval of text and images  Developed by people at Univ. of Waikato, Univ. of Melbourne, and RMIT in 1990’s Written in C, running on Unix Vector space model with lots of compression and speed up tricks People have used it to achieve good TREC performance Lemur/Indri An IR toolkit emphasizing language models Developed at CMU and Univ. of Massachusetts in 2000’s Written in C++, highly extensible Vector space and probabilistic models including language models Achieving good TREC performance with a simple language model Terrier A large-scale retrieval toolkit with lots of applications (e.g., desktop search) and TREC support Developed at University of Glasgow, UK Written in Java, open source “Divergence from randomness” retrieval model and other modern retrieval formulas Lucene Open Source IR toolkit  Initially developed by Doug Cutting in Java Now has been ported to some other languages Good for building IR/Web applications Many applications have been built using  Lucene  (e.g.,  Nutch  and SOLR) NLP NLP Introduction to NLP Brief History of NLP The Turing Test Alan Turing: the  Turing test language as test for intelligence Three participants a computer and two humans (one is an interrogator) Interrogator’s goal to tell the machine and human apart Machine’s goal to fool the interrogator into believing that a person is responding Other human’s goal to help the interrogator reach his goal Q: Please write me a sonnet on the topic of the Forth Bridge . A: Count me out on this one. I never could write poetry. Q: Add 34957 to 70764. A: 105621 (after a pause) Eliza Longer example here: 	 http :// thoughtcatalog.com/oliver-miller/2012/08/a-conversation-with-eliza   Some Brief History Foundational insights (1940’s and 1950’s) automaton (Turing) probabilities information theory (Shannon) formal languages (Backus and  Naur ) noisy channel and decoding (Shannon) first systems (Davis et al., Bell Labs) Two camps (1957-1970): symbolic and stochastic Transformation Grammar (Harris, Chomsky) Artificial Intelligence (Minsky, McCarthy, Shannon, Rochester) automated theorem proving and problem solving (Newell and Simon) Bayesian reasoning ( Mosteller  and Wallace) Corpus work ( Kučera  and Francis) Some Brief History Four paradigms (1970-1983) stochastic (IBM) logic-based ( Colmerauer , Pereira and Warren, Kay, Bresnan) nlu  ( Winograd ,  Schank , Fillmore) discourse modelling (Grosz and Sidner) Empiricism and finite-state models  redux  (83-93) Kaplan and Kay (phonology and morphology) Church (syntax) Late years (1994-2010) integration of different techniques different areas (including speech and IR) probabilistic models machine learning structured prediction topic models The Most Recent Years Machine learning methods SVM, logistic regression ( maxent ), CRFs Shared tasks TREC, DUC, TAC, SEMEVAL Semantic tasks RTE, SRL Semi-supervised and unsupervised methods Deep Learning LSTM, Convolutional NN, RNN Summarizing 30 years of ACL Discoveries Using Citing Sentences Summarizing 30 years of ACL Discoveries Using Citing Sentences Summarizing 30 years of ACL Discoveries Using Citing Sentences Introduction to NLP NACLO NACLO and IOL The North American Computational Linguistics Olympiad Competition held  s ince 2007 in the USA and Canada http://www.nacloweb.org   Best individual US performers so far: Adam  Hesterberg  (2007) Hanzhi  Zhu (2008) Rebecca Jacobs (2007-2009) – 3 team  golds  + 2 individual medals Ben  Sklaroff  (2010) Morris  Alper  (2011) Alex Wade (2012, 2013) – 2 team golds + 2 individual golds + 1 individual silver Tom McCoy (2013) – Yale grad Darryl Wu (2012, 2014) James Wedgwood (2015, 2016) – Yale sophomore Other strong countries:  Russia, UK, Netherlands, Poland, Bulgaria, South Korea, Canada, China, Sweden IOL  – the International  contest Since 2003 IOL 2013 in  the UK,  IOL 2014 in  China,  IOL 2015 in  Bulgaria, IOL 2016 in India, IOL 2017 in Ireland http://www.ioling.org   [A Donkey in Every House, by  Todor   Tchervenkov , NACLO 2007] [Spare the Rod, by Dragomir Radev, NACLO 2008]  [ Tenji  Karaoke, by Patrick  Littell , NACLO 2009] [aw-TOM-uh- tuh ,  by Patrick  Littell , NACLO 2008] [Lost in Yerevan, by Dragomir Radev, NACLO 2010] On her visit to Armenia, Millie has gotten lost in Yerevan, the nation’s  capital. She  is now at the  Metropoliten  (subway) station named  Shengavit ,   but  her friends are waiting for her at the station named  Barekamutyun .  Can  you help Millie meet up with her friends ? 1. Assuming  Millie takes a train in the right direction,  which  will be the first  stop  after  Shengavit ?   Note  that all names of stations listed below appear on the map. a.  Gortsaranayin     b.  Zoravar   Andranik c.  Charbakh d.  Garegin   Njdehi   Hraparak e. none of the above   2. After boarding at  Shengavit , how many stops will it take Millie to get  to  Barekamutyun   (don’t include  Shengavit  itself in the number of stops)?   NACLO: Computational Problems http :// www.nacloweb.org/resources/problems/2016/N2016-B.pdf   http:// www.nacloweb.org/resources/problems/2016/N2016-H.pdf   http:// www.nacloweb.org/resources/problems/2016/N2016-K.pdf   http :// www.nacloweb.org/resources/problems/2016/N2016-P.pdf   http:// www.nacloweb.org/resources/problems/2015/N2015-E.pdf   http:// www.nacloweb.org/resources/problems/2015/N2015-K.pdf   http:// www.nacloweb.org/resources/problems/2015/N2015-M.pdf   http:// www.nacloweb.org/resources/problems/2015/N2015-P.pdf   http :// www.nacloweb.org/resources/problems/2015/N2015-G.pdf   http://www.nacloweb.org/resources/problems/2014/N2014-O.pdf    http:// www.nacloweb.org/resources/problems/2014/N2014-P.pdf    http://www.nacloweb.org/resources/problems/2014/N2014-C.pdf     http://www.nacloweb.org/resources/problems/2014/N2014-J.pdf    http:// www.nacloweb.org/resources/problems/2014/N2014-H.pdf   	 http://www.nacloweb.org/resources/problems/2014/N2014-L.pdf    http://www.nacloweb.org/resources/problems/2013/N2013-C.pdf    http://www.nacloweb.org/resources/problems/2013/N2013-F.pdf    http://www.nacloweb.org/resources/problems/2013/N2013-H.pdf    http://www.nacloweb.org/resources/problems/2013/N2013-L.pdf    http://www.nacloweb.org/resources/problems/2012/N2012-C.pdf    http:// www.nacloweb.org/resources/problems/2013/N2013-N.pdf    http:// www.nacloweb.org/resources/problems/2013/N2013-Q.pdf    http :// www.nacloweb.org/resources/problems/2012/N2012-K.pdf    http:// www.nacloweb.org/resources/problems/2012/N2012-O.pdf    http:// www.nacloweb.org/resources/problems/2012/N2012-R.pdf   http://www.nacloweb.org/resources/problems/2011/F.pdf    http://www.nacloweb.org/resources/problems/2011/M.pdf    http://www.nacloweb.org/resources/problems/2010/D.pdf    http://www.nacloweb.org/resources/problems/2010/E.pdf    http://www.nacloweb.org/resources/problems/2010/I.pdf    http://www.nacloweb.org/resources/problems/2010/K.pdf    http://www.nacloweb.org/resources/problems/2009/N2009-E.pdf    http://www.nacloweb.org/resources/problems/2009/N2009-G.pdf    http://www.nacloweb.org/resources/problems/2009/N2009-J.pdf    http://www.nacloweb.org/resources/problems/2009/N2009-M.pdf    http://www.nacloweb.org/resources/problems/2008/N2008-F.pdf    http://www.nacloweb.org/resources/problems/2008/N2008-H.pdf    http://www.nacloweb.org/resources/problems/2008/N2008-I.pdf    http://www.nacloweb.org/resources/problems/2008/N2008-L.pdf    http://www.nacloweb.org/resources/problems/2007/N2007-A.pdf    http://www.nacloweb.org/resources/problems/2007/N2007-H.pdf    http://www.nacloweb.org/resources.php   NLP NLP Introduction to NLP Discourse Analysis Issues with Discourse Anaphora I went to see my grandfather at the hospital.  The old man  has been there for weeks.  He  had surgery a few days ago. Entities, referring expressions and antecedents Issues with both single sentences and  multi-sentential text, e.g., coherence Needed Models of discourse Cohesion Cohesion (Halliday and Hasan 1976) text units must make sense together different from coherence example:  reference example: discourse connectors lexical chains Topic Segmentation Identifying where topic shifts occur Examples Text  Tiling ( Hearst, 1994, 1997) Lexical chains (Galley et al., 2004) SVD (Choi et al., 2001) Coreference Sample use of anaphora John saw Mary in the park. As every morning, she was walking her dog. What does “she” refer to? Candidate referents John Mary The park Every morning Her dog MUC-7 Coreference Task < COREF ID="6" TYPE="IDENT" REF="5" MIN="Aeroflot">The Russian airline Aeroflot</COREF> has been hit with <COREF ID="19">a writ</COREF> for  loss and  damages, filed in <COREF ID="15">Hong Kong</COREF> by <COREF ID="7" TYPE="IDENT" REF="4" MIN="families">the families of <COREF ID="22" MIN="passengers"> seven passengers killed  in <COREF ID="8" TYPE="IDENT" REF="9" MIN="crash">an air crash</COREF></COREF></COREF >.  All  75 people on board <COREF ID="12" MIN="Airbus">the <COREF ID="10" TYPE="IDENT" REF="6">Aeroflot</COREF> <COREF ID="25">Airbus</COREF > </ COREF> died  when <COREF  ID="11" TYPE="IDENT" REF="12">it</COREF>  ploughed into  a Siberian mountain in March 1994. MUC-7 Coreference Task < COREF ID="6" TYPE="IDENT" REF="5" MIN="Aeroflot">The Russian airline Aeroflot</COREF> has been hit with <COREF ID="19">a writ</COREF> for  loss and  damages, filed in <COREF ID="15">Hong Kong</COREF> by <COREF ID="7" TYPE="IDENT" REF="4" MIN="families">the families of <COREF ID="22" MIN="passengers"> seven passengers killed  in <COREF ID="8" TYPE="IDENT" REF="9" MIN="crash">an air crash</COREF></COREF></COREF >.  All  75 people on board  <COREF ID="12" MIN="Airbus">the <COREF ID="10" TYPE="IDENT" REF="6">Aeroflot</COREF> <COREF ID="25">Airbus</COREF >  </ COREF> died  when  <COREF  ID="11" TYPE="IDENT" REF="12">it</COREF>  ploughed into  a Siberian mountain in March 1994. Screwdriver on Wikipedia A screwdriver is a tool, manual or powered, for turning (driving or removing) screws. A typical simple screwdriver has a handle and a shaft, and a tip that the user inserts into the screw head to turn it. The shaft is usually made of tough steel to resist bending or twisting. The tip may be hardened to resist wear, treated with a dark tip coating for improved visual contrast between tip and screw—or ridged or treated for additional 'grip'.  Handles  are typically wood, metal, or  plastic  and usually hexagonal, square, or oval in cross-section to improve grip and prevent the tool from rolling when set down. Some manual screwdrivers have interchangeable tips that fit into a socket on the end of the shaft and are held in mechanically or magnetically. These often have a hollow handle that contains various types and sizes of tips, and a reversible ratchet action that allows multiple full turns without repositioning the tip or the user's hand. Screwdriver on Wikipedia A screwdriver is a tool, manual or powered, for turning (driving or removing) screws. A typical simple screwdriver has a handle and a shaft, and a tip that the user inserts into the screw head to turn it.  The shaft  is usually made of tough steel to resist bending or twisting.  The tip  may be hardened to resist wear, treated with a dark tip coating for improved visual contrast between tip and screw—or ridged or treated for additional 'grip'.  Handles  are typically wood, metal, or  plastic  and usually hexagonal, square, or oval in cross-section to improve grip and prevent the tool from rolling when set down. Some manual screwdrivers have interchangeable tips that fit into a socket on the end of the shaft and are held in mechanically or magnetically.  These  often have a hollow handle that contains various types and sizes of tips, and a reversible ratchet action that allows multiple full turns without repositioning the tip or the user's hand. Coreference  Resolution Agreement constraints gender, number,  animacy Syntactic constraints e.g., parallelism Sentence ordering recency Salience weights [ Lappin  and  Leass  1994, example from  Jurafsky  and Martin] Lappin and Leass (cont’d) Recency  handling weights are cut in half after each sentence is processed. Examples: An Acura Integra is parked in the lot. (subject) There is an Acura Integra parked in the lot. (existential predicate nominal) John parked an Acura Integra in the lot. (object) John gave Susan an Acura Integra. (indirect object) In his Acura Integra, John showed Susan his new CD player. (demarcated adverbial PP) Resolution of Anaphora Procedure (RAP)  Collect the potential referents (up to four sentences back). Remove potential referents that do not agree in number or gender with the pronoun. Remove potential referents that do not pass  intrasentential  syntactic coreference constraints. Compute the total salience value of the referent by adding any applicable values for role parallelism (+35) or  cataphora  (-175). Select the referent with the highest salience value. In case of a tie, select the closest referent in terms of string position. When moving to a new sentence, halve all scores for the existing entities on the list. http://wing.comp.nus.edu.sg/~ qiu/NLPTools/JavaRAP.html   Example John saw a beautiful Acura Integra at the dealership last week. He showed it to Bill. He bought it. Example from  Jurafsky  and Martin Example (cont’d) Example (cont’d) Example (cont’d) Example (cont’d) Example (cont’d) Centering Goal: understand the local coherence of discourse Why some texts are considered more coherent Inference load associated with badly chosen referring expressions Too much focus shift makes the text hard to understand. Centering Center: an entity that links an utterance to the next one. The most salient centers can be used in  coreference . Every utterance U n  has a backwards looking center  C b , which connects U n  with the previous utterance U n-1 .  Every utterance also has a partially ordered set of forward looking centers  C f  related to the next utterance U n+1 . The order depends on syntax (e.g., subject>object) The preferred center  C p  is the highest ranking element of C f . NLP NLP Introduction to NLP Sentiment Analysis Reviews of 1Q84 by Haruki Murakami “ 1Q84  is a tremendous feat and a triumph . . . A must-read for anyone who wants to come to terms with contemporary Japanese culture.” —Lindsay Howell,  Baltimore Examiner “Perhaps one of the most important works of science fiction of the year . . .  1Q84  does not disappoint . . . [It] envelops the reader in a shifting world of strange cults and peculiar characters that is surreal and entrancing.” —Matt Staggs, Suvudu.com Ambitious, sprawling and thoroughly stunning . . . Orwellian dystopia, sci-fi, the modern world (terrorism, drugs, apathy, pop novels)—all blend in this dreamlike, strange and wholly unforgettable epic.” — Kirkus Reviews  (starred review) Reviews of 1Q84 by Haruki Murakami “ 1Q84  is a  tremendous feat  and  a triumph  . . . A  must-read  for anyone who wants to come to terms with contemporary Japanese culture.” —Lindsay Howell,  Baltimore Examiner “Perhaps one of the most important works of science fiction of the year . . .  1Q84   does not disappoint  . . . [It] envelops the reader in a shifting world of strange cults and peculiar characters that is surreal and  entrancing .” —Matt Staggs, Suvudu.com Ambitious , sprawling and thoroughly  stunning  . . . Orwellian dystopia, sci-fi, the modern world (terrorism, drugs, apathy, pop novels)—all blend in this dreamlike, strange and wholly  unforgettable epic .” — Kirkus Reviews  (starred review) Sentiment about Companies Other Examples Movie reviews Product reviews Debates www.createdebate.com Introduction Many posts, blogs Expressing personal opinions Research questions Subjectivity analysis Polarity analysis (positive/negative, number of stars) Viewpoint analysis (Chelsea vs. Manchester United, republican vs. democrat) Sentiment target entity aspect Introduction Level of granularity Document Sentence Attribute Opinion words Base Comparative (better, slower) Introduction Just counting negative words is not enough Negation analysis Reviews of 1Q84 by Haruki Murakami “ 1Q84   is a tremendous feat and a triumph . . . A must-read for anyone who wants to come to terms with contemporary Japanese culture.” —Lindsay Howell,  Baltimore Examiner “Perhaps one of the most important works of science fiction of the year . . .  1Q84  does not  disappoint  . . . [It] envelops the reader in a shifting world of strange cults and peculiar characters that is surreal and entrancing.” —Matt Staggs, Suvudu.com Ambitious, sprawling and thoroughly stunning . . . Orwellian dystopia, sci-fi, the modern world (terrorism, drugs, apathy, pop novels)—all blend in this dreamlike, strange and wholly unforgettable epic.” — Kirkus Reviews  (starred review) Reviews of 1Q84 by Haruki Murakami “ 1Q84   is a tremendous feat and a triumph . . . A must-read for anyone who wants to come to terms with contemporary Japanese culture.” —Lindsay Howell,  Baltimore Examiner “Perhaps one of the most important works of science fiction of the year . . .  1Q84   does not disappoint  . . . [It] envelops the reader in a shifting world of strange cults and peculiar characters that is surreal and entrancing.” —Matt Staggs, Suvudu.com Ambitious, sprawling and thoroughly stunning . . . Orwellian dystopia, sci-fi, the modern world (terrorism, drugs, apathy, pop novels)—all blend in this dreamlike, strange and wholly unforgettable epic.” — Kirkus Reviews  (starred review) Product Reviews Twitter Sentiment Problems Subtlety Concession Manipulation Sarcasm and irony SA as a Classification Problem Set of features Words Presence is more important than frequency Punctuation Phrases Syntax  A lot of training data is available E.g., movie review sentences and stars Techniques MaxEnt SVM Naive Bayes Resources CMU Twitter parser http://www.ark.cs.cmu.edu/TweetNLP/ Stanford Sentiment Treebank https://nlp.stanford.edu/sentiment/treebank.html Stanford Sentiment Treebank Stanford Sentiment Treebank Recursive Neural Tensor Networks (RNTN) for Sentiment Analysis [ Socher  et al. 2013] Recursive Neural  Tensor Networks (RNTN)  for Sentiment Analysis [ Socher  et al. 2013] RNTN Results [ Socher  et al. 2013] Dealing with Negation [ Socher  et al. 2013] Dealing with Negation [ Socher  et al. 2013] Dealing with Negation [ Socher  et al. 2013] NLP NLP Recursive  Neural Networks Deep Learning Recursive Neural Tensor Networks Socher  et al. (2013) Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank Recursive Neural Tensor Networks Socher  et al. (2013) Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank Recursive Neural Networks Socher  et al. (2013) Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank NLP NLP Text Similarity Wordnet Wordnet Wordnet  is a project run by George Miller (1920-2012) and Christiane  Fellbaum  at Princeton University. It includes a database of words (mainly nouns and verbs but also adjectives and adverbs) and semantic relations between them. The main relation is  hypernymy , so the overall structure of the database is more tree-like (see next slide). References: George  A. Miller (1995).  WordNet : A Lexical Database for English.  Communications  of the ACM Vol. 38, No. 11: 39-41.  Christiane  Fellbaum  (1998, ed.)  WordNet : An Electronic Lexical Database. Cambridge, MA: MIT Press. Tree-like structure of  Wordnet ruminant deer giraffe wapiti caribou elk okapi even-toed ungulate odd-toed ungulate equine ungulate horse zebra mule pony Wordnet  Example (1/6) The  noun  bar has 11  senses                      1.  barroom , bar, saloon,  ginmill , taproom -- (a room where alcoholic drinks are served over a counter) 2.  bar  -- (a counter where you can purchase food or drink) 3.  bar  -- (a rigid piece of metal) 4.  measure , bar -- (notation for a repeating pattern of musical beats; written followed by a vertical bar) 5.  bar  -- (usually metal placed in windows to prevent escape) 6.  prevention , bar -- (the act of preventing) 7. bar -- (a unit of pressure equal to a million dynes per square centimeter) 8. bar -- (a submerged (or partly submerged) ridge in a river or along a shore) 9. legal profession, bar, legal community -- (the body of individuals qualified to practice law) 10. cake, bar -- (a block of soap or wax) 11. bar -- ((law) a railing that encloses the part of the courtroom where the  the  judges and lawyers sit and the case is tried) The  verb  bar has 4  senses                                      1.  bar,  debar, exclude -- (prevent from entering; keep out; "He was barred from membership in the club") 2.  barricade , block, blockade, block off, block up, bar -- (render unsuitable for passage; "block the way"; "barricade the streets") 3.  banish , relegate, bar -- (expel, as if by official decree; "he was banished from his own country") 4.  bar  -- (secure with, or as if with, bars; "He barred the door") Wordnet  Example (2/6) Sense 1 barroom, bar, saloon,  ginmill , taproom        => room            => area                => structure, construction                    => artifact, artefact                        => object, physical object                            => entity, something Sense 2 bar        => counter            => table                => furniture, piece of furniture, article of furniture                    => furnishings                        => instrumentality, instrumentation                            => artifact, artefact                                => object, physical object                                    => entity, something Wordnet  Example (3/6) Sense 3 bar        => implement            => instrumentality, instrumentation                => artifact, artefact                    => object, physical object                        => entity, something Sense 4 measure, bar        => musical notation            => notation, notational system                => writing, symbolic representation                    => written communication, written language                        => communication                            => social relation                                => relation                                    => abstraction Wordnet  Example (4/6) Sense 5 bar        => obstruction, impediment, impedimenta            => structure, construction                => artifact, artefact                    => object, physical object                        => entity, something Sense 6 prevention, bar        => hindrance, interference, interfering            => act, human action, human activity Sense 7 bar        => pressure unit            => unit of measurement, unit                => definite quantity                    => measure, quantity, amount, quantum                        => abstraction Wordnet  Example (5/6) Sense 8 bar        => ridge            => natural elevation, elevation                => geological formation, geology, formation                    => natural object                        => object, physical object                            => entity, something        => barrier            => mechanism                => natural object                    => object, physical object                        => entity, something Wordnet  Example (6/6) Sense 9 legal profession, bar, legal community        => profession, community            => occupation, vocation, occupational group                => body                    => gathering, assemblage                        => social group                            => group, grouping Sense 10 cake, bar        => block            => artifact, artefact                => object, physical object                    => entity, something board used as a noun is familiar (polysemy count = 9) bird used as a noun is common (polysemy count = 5) cat used as a noun is common (polysemy count = 7) house used as a noun is familiar (polysemy count = 11) information used as a noun is common (polysemy count = 5) retrieval used as a noun is uncommon (polysemy count = 3) serendipity used as a noun is very rare (polysemy count = 1) Familiarity and Polysemy Top-Level Categories [Example from J&M, based on Schneider and Smith 2013] Text Similarity Other Lexical Networks BabelNet  Example MeSH Medical Subject Headings http://www.nlm.nih.gov/mesh/MBrowser.html External  Links EuroWordNet http ://www.illc.uva.nl/EuroWordNet /   Open Thesaurus http ://www.openthesaurus.de /   Freebase http://www.freebase.com DBPedia http://www.dbpedia.org    BabelNet   http:// babelnet.org   Various thesauri https://sites.google.com/site/openrogets /   NLP NLP Introduction to NLP Generative vs. Discriminative Models Generative vs. Discriminative Models Learn a model of the joint probability p(d, c) Use Bayes’ Rule to calculate p( c|d )  Build a model of each class; given example, return the model most likely to have generated that example Examples: Naïve Bayes, Gaussian Discriminant Analysis, HMM, Probabilistic CFG Generative Discriminative Model posterior probability p( c|d ) directly Class  is a function of document vector Find the exact  function  that minimizes classification errors on the training data  Examples:  Logistic  r egression, Neural  Networks (NNs ), Support  Vector Machines (SVMs ), Decision Trees Assumptions of Discriminative Classifiers Data examples (documents) are represented as vectors of features (words, phrases, ngrams, etc) Looking for a function that maps each vector into a class.  This function can be found by minimizing the errors on the training data (plus other various criteria) Different classifiers vary on what the function looks like, and how they find the function Discriminative vs. Generative Classifiers Discriminative classifiers are generally more effective, since they directly optimize the classification accuracy. But They are all sensitive to the choice of features, and so far these features are extracted heuristically Also, overfitting can happen if data is sparse Generative classifiers are the “opposite” They directly model text, an unnecessarily harder problem than classification They can easily exploit unlabeled data Introduction to NLP Generative Classifier: Naïve Bayes Naïve Bayes Intuition Simple (“ na ï ve ”) classification method based on Bayes rule Relies on very simple representation of document Bag of words Bag of Words Remember Bayes’ Rule? d  is  the  document (represented as a list  of features of a document,  x 1 , …,  x n ) c   is  a class (e.g., “not spam”) Bayes’  Rule: Na ï ve  Bayes Classifier (I) MAP is “maximum a posteriori”  = most likely class Bayes Rule Dropping the denominator Na ï ve  Bayes Classifier (II) Document d represented as features x1..xn But where will we get these  probabilitites ? Naïve Bayesian classifiers Naïve Bayesian classifier Assuming statistical independence Features = words (or phrases) typically Multinomial Na ï ve  Bayes Independence Assumptions Bag of Words assumption Assume position doesn’t matter Conditional Independence Assume the feature probabilities  P ( x i | c ) are independent given the class  c. [ Jurafsky  and Martin] Multinomial Na ï ve  Bayes Classifier [ Jurafsky  and Martin] Learning the Multinomial Na ï ve  Bayes Model First attempt: maximum likelihood estimates simply use the frequencies in the data [ Jurafsky  and Martin] Create  mega-document for topic  j  by concatenating all docs in this topic Use frequency of  w  in mega- document Parameter  Estimation fraction of times  word  w i   appears  among all words  in documents of topic  c j [ Jurafsky  and Martin] Problem with Maximum Likelihood What if we have seen no training documents with the word  fantastic     and classified in the  topic  positive  ( thumbs-up) ? Zero  probabilities cannot be conditioned away, no matter the other evidence! [ Jurafsky  and Martin] Laplace Smoothing [ Jurafsky  and Martin] Multinomial Naïve Bayes: Learning Calculate  P ( c j )   terms For each  c j   in  C  do   docs j      all docs with  class = c j Calculate  P ( w k   |   c j )   terms Text j    single doc containing all  docs j For   each word  w k   in  Vocabulary      n k    # of occurrences of  w k   in  Text j From training corpus, extract  Vocabulary [ Jurafsky  and Martin] Example Features = {I hate love this book} Training I hate this book Love this book What  is P(Y|X)? Prior p(Y)  Testing hate book Different conditions a  = 0 (no smoothing) a = 1 (smoothing) Example Ways Naive Bayes Is Not So Naive Very  f ast , low storage requirements Robust to irrelevant features Irrelevant features cancel each other without affecting results Very good in domains with many equally important features Decision trees suffer from  fragmentation  in such cases – especially if little data Optimal if the independence  a ssumptions hold If assumed independence is correct, then it is the Bayes Optimal Classifier for problem A good,  dependable baseline for text  classification B ut other classifiers give better accuracy [ Jurafsky  and Martin] NLP NLP Introduction to NLP Summarization Techniques 2/3 Mani/ Bloedorn  (1997, 1999) Graph-based method for identifying similarities and differences between documents Single event or sequence of events Content entities (nodes) and relations (edges) Relation types SAME, ADJACENT, ALPHA (WordNet,  NetOwl ), COREF Spreading activation Barzilay  & M.  Elhadad  (1997) Lexical chains Mr. Kenny is the person that invented the anesthetic machine which uses micro-computers to control the rate at which an anesthetic is pumped into the blood. Such machines are nothing new. But his device uses two micro-computers to  achieve  much closer monitoring of the pump feeding the anesthetic into the patient.  Barzilay  and M.  Elhadad  (1997) WordNet-based Three types of relations: extra-strong (repetitions) strong (WordNet relations) medium-strong (link between  synsets  is longer than one + some additional constraints) Barzilay  and M.  Elhadad  (1997) Scoring chains: Length Homogeneity index: =  1  – (#  distinct words in chain, divided  by  length)   Score = Length * Homogeneity  Score > Average + 2 *  st.dev . http://research.microsoft.com/en-us/um/people/cyl/download/papers/lexical-chains.pdf Marcu  (1997-1999) Focuses on text coherence Based on Rhetorical Structure Theory (Mann and Thompson 1988) Nucleus+Satellite Example: evidence N+S combination increases reader’s belief in N [The truth is that the pressure to smoke in junior high is greater than it will be any other time of one’s life :] N  [we  know that 3,000 teens start smoking each day .] S 2 Elaboration 2 Elaboration 8 Example 2 Background Justification 3 Elaboration 8 Concession 10 Antithesis Mars experiences frigid weather conditions (2) Surface temperatures typically average about -60 degrees Celsius (-76 degrees Fahrenheit) at the equator and can dip to -123 degrees C near the poles (3) 4 5 Contrast Although the atmosphere holds a small amount of water, and water-ice clouds sometimes develop, (7) Most Martian weather involves blowing dust and carbon monoxide. (8) Each winter,  for example , a blizzard of frozen carbon dioxide rages over one pole, and a few meters of this dry-ice snow accumulate as previously frozen carbon dioxide evaporates from the opposite polar cap. (9) Yet  even on the summer pole,  where  the sun remains in the sky all day long, temperatures never warm enough to melt frozen water. (10) With its distant orbit (50 percent farther from the sun than Earth) and slim atmospheric blanket, (1) Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion, (4) 5 Evidence Cause but any liquid water formed in this way would evaporate almost instantly (5) because of the low atmospheric pressure (6) 2 Elaboration 2 Elaboration 8 Example 2 Background Justification 3 Elaboration 8 Concession 10 Antithesis Mars experiences frigid weather conditions (2) Surface temperatures typically average about -60 degrees Celsius (-76 degrees Fahrenheit) at the equator and can dip to -123 degrees C near the poles (3) 4 5 Contrast Although the atmosphere holds a small amount of water, and water-ice clouds sometimes develop, (7) Most Martian weather involves blowing dust and carbon monoxide. (8) Each winter,  for example , a blizzard of frozen carbon dioxide rages over one pole, and a few meters of this dry-ice snow accumulate as previously frozen carbon dioxide evaporates from the opposite polar cap. (9) Yet  even on the summer pole,  where  the sun remains in the sky all day long, temperatures never warm enough to melt frozen water. (10) With its distant orbit (50 percent farther from the sun than Earth) and slim atmospheric blanket, (1) Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion, (4) 5 Evidence Cause but any liquid water formed in this way would evaporate almost instantly (5) because of the low atmospheric pressure (6) 2 Elaboration 2 Elaboration 8 Example 2 Background Justification 3 Elaboration 8 Concession 10 Antithesis Mars experiences frigid weather conditions (2) Surface temperatures typically average about -60 degrees Celsius (-76 degrees Fahrenheit) at the equator and can dip to -123 degrees C near the poles (3) 4 5 Contrast Although the atmosphere holds a small amount of water, and water-ice clouds sometimes develop, (7) Most Martian weather involves blowing dust and carbon monoxide. (8) Each winter,  for example , a blizzard of frozen carbon dioxide rages over one pole, and a few meters of this dry-ice snow accumulate as previously frozen carbon dioxide evaporates from the opposite polar cap. (9) Yet  even on the summer pole,  where  the sun remains in the sky all day long, temperatures never warm enough to melt frozen water. (10) With its distant orbit (50 percent farther from the sun than Earth) and slim atmospheric blanket, (1) Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion, (4) 5 Evidence Cause but any liquid water formed in this way would evaporate almost instantly (5) because of the low atmospheric pressure (6) Noisy Channel Models Source/target language Coding process Berger and Mittal 2000 Source language full document Target language Summary Gisting  (OCELOT ) Berger & Mittal 2000 Training on 100K  summary+document  pairs Testing on 1046 pairs Use Viterbi-type search Evaluation: word overlap (0.2-0.4) No word ordering Berger & Mittal 2000 Sample output: Audubon society  atlanta  area savannah  georgia   chatham  and local birding savannah keepers chapter of the  audubon   georgia  and leasing Carbonell  and Goldstein (1998) Maximal marginal relevance Greedy selection method Query-based summaries Law of diminishing returns C = doc collection Q = user query R = IR(C,Q,  ) S = already retrieved documents Sim  = similarity metric used Mead (Radev et al. 2000) Salience-based extractive summarization  Centroid-based method  Vector space model Features: position, length, centroid Reranker  – similar to MMR Open source  library-  www.summarization.com/mead Mead Mead Input Cluster of  d  documents with  n  sentences (compression rate =  r )  Output (n * r) sentences from the cluster with the highest scores 	SCORE (s) =  S i  ( w c C i  +  w p P i  +  w f F i )  NewsInEssence  (Radev et al. 2001) Web-based multi-document news summarization system Other News Summarization Systems Newsblaster  ( McKeown  et al. 2002) Google News NLP NLP Text similarity Introduction Text Similarity Motivation People can express the same concept (or related concepts) in many different ways. For example, “the plane leaves at 12pm” vs “the flight departs at noon” Text similarity is a key component of Natural Language Processing Uses in NLP If the user is looking for information about cats, we may want the NLP system to return documents that mention kittens even if the word “cat” is not in them.  If the user is looking for information about “fruit dessert”, we want the NLP system to return documents about “peach tart” or “apple cobbler”. A speech recognition system should be able to tell the difference between similar sounding words like the “Dulles” and “Dallas” airports. Human Judgments of Similarity [Lev Finkelstein,  Evgeniy   Gabrilovich , Yossi Matias, Ehud  Rivlin , Zach  Solan ,  Gadi   Wolfman , and  Eytan   Ruppin , "Placing Search in Context: The Concept Revisited", ACM Transactions on Information Systems, 20(1):116-131, January 2002] tiger      cat           7.35 tiger       tiger         10.00 book       paper         7.46 computer   keyboard      7.62 computer   internet      7.58 plane      car           5.77 train      car           6.31 telephone  communication 7.50 television radio         6.77 media      radio         7.42 drug       abuse         6.85 bread      butter        6.19 cucumber   potato        5.92 http://wordvectors.org/suite.php   Human Judgments of Similarity [SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation. 2014. Felix Hill,  Roi   Reichart  and Anna  Korhonen . Preprint  pubslished  on  arXiv . arXiv:1408.3456] delightful   wonderful       A       8.65 modest       flexible        A       0.98 clarify      explain         V       8.33 remind       forget          V       0.87 get          remain          V       1.6 realize      discover        V       7.47 argue        persuade        V       6.23 pursue       persuade        V       3.17 plane        airport         N       3.65 uncle        aunt            N       5.5 horse        mare            N       8.33 Automatic Similarity Computation Words most similar to “France” Computed using word2vec [ Mikolov  et al. 2013]           spain     0.679         belgium     0.666     netherlands     0.652           italy     0.633     switzerland     0.622      luxembourg     0.610        portugal     0.577          russia     0.572         germany     0.563       catalonia     0.534 Types of Text Similarity Many types of text similarity exist: Morphological similarity (e.g., respect-respectful) Spelling similarity (e.g., theater-theatre) Synonymy (e.g., talkative-chatty) Homophony (e.g., raise-raze-rays) Semantic similarity (e.g., cat-tabby) Sentence similarity (e.g., paraphrases) Document similarity (e.g., two news stories on the same event) NLP NLP Introduction to NLP The Noisy Channel Model The Noisy Channel Model Example: Input: Written English (X) Encoder: garbles the input (X->Y) Output: Spoken English (Y) More examples: Grammatical English to English with mistakes English to bitmaps (characters) P(X,Y) = P(X)P(Y|X) Encoding and Decoding Given f, guess e Example Translate “la  maison  blanche” Example Translate “la  maison  blanche” Example Translate “la  maison  blanche” Uses of the Noisy Channel Model Handwriting recognition Text generation Text summarization Machine translation Spelling correction See separate lecture on text similarity and edit distance Spelling Correction From Peter  Norvig :  http:// norvig .com/ngrams/ch14.pdf  ‎ NLP NLP Question Answering System Architecture System Architecture Many questions can be answered by traditional search engines ...   Afghanistan , Kabul, 2,450  ...  Administrative capital and  largest   city  (1997 est  ...  Undetermined. Panama, Panama  City , 450,668.  ...  of the Gauteng,  Northern  Province, Mpumalanga  ...   www.infoplease.com/cgi-bin/id/A0855603 ...  died in Kano,  northern  Nigeria's  largest   city , during two days of anti-American riots led by Muslims protesting the US-led bombing of  Afghanistan , according to  ...   www.washingtonpost.com/wp-dyn/print/world/ ...  air strikes on the  city .  ...  the Taliban militia in  northern   Afghanistan  in a significant blow  ...  defection would be the  largest  since the United States  ...   www.afgha.com/index.php - 60k ...  Kabul is the capital and  largest   city  of  Afghanistan . .  ...  met. area pop. 2,029,889), is the  largest   city  in Uttar Pradesh, a state in  northern  India. .  ...   school.discovery.com/homeworkhelp/worldbook/atozgeography/ k/k1menu.html    ...  Gudermes, Chechnya's second  largest  town. The attack  ...  location in  Afghanistan's  outlying regions  ...  in the  city  of Mazar-i-Sharif, a  Northern  Alliance-affiliated  ...   english.pravda.ru/hotspots/2001/09/17/ ...  Get Worse By RICK BRAGG Pakistan's  largest   city  is getting a jump on the  ...  Region: Education Offers Women in  Northern   Afghanistan  a Ray of Hope.  ...   www.nytimes.com/pages/world/asia/ ...  within three miles of the airport at Mazar-e-Sharif, the  largest   city  in  northern  Afghanistan , held since 1998 by the Taliban. There was no immediate comment  ...   uk.fc.yahoo.com/photos/a/afghanistan.html What is the Largest City in Northern Afghanistan? System Components Source  identification semi-structured vs. text sources Query  modulation best paraphrase of a NL question given the syntax of a search  engine Example: Who wrote Hamlet → author | wrote Hamlet Document retrieval Sentence  ranking n-gram matching,  Okapi Answer extraction question type  classification phrase chunking Answer  ranking question  type, proximity to query words,  frequency Document retrieval Query modulation Sentence ranking Answer extraction Answer ranking What is the largest city in Northern Afghanistan? (largest OR biggest) city “Northern Afghanistan” www.infoplease.com/cgi-bin/id/A0855603 www.washingtonpost.com/wp-dyn/print/world/ Gudermes, Chechnya's second  largest  town … location in  Afghanistan's  outlying regions within three miles of the airport at Mazar-e-Sharif, the  largest   city  in  northern Afghanistan Gudermes Mazer-e-Sharif Mazer-e-Sharif Gudermes Question Type Classification Can help find the right answers in the text Example Who wrote Anna Karenina? Looking for a PERSON/INDIVIDUAL/WRITER  SYN-classes (IBM AnSel) UIUC Question Types ENTITY : entities   animal: animals   body: organs of body   color: colors   creative: inventions, books and other creative pieces   currency: currency names   dis.med.: diseases and medicine   event: events   food: food   instrument: musical instrument   lang: languages   letter: letters like a-z   other: other entities   plant: plants   product: products   religion: religions   sport: sports   substance: elements and substances   symbol: symbols and signs   technique: techniques and methods   term: equivalent terms   vehicle: vehicles   word: words with a special property NUMERIC : numeric values   code: postcodes or other codes   count: number of sth.   date: dates   distance: linear measures   money: prices   order: ranks   other: other numbers   period: the lasting time of sth.   percent: fractions   speed: speed   temp: temperature   size: size, area and volume   weight: weight ABBREVIATION : abbreviation    abb : abbreviation    exp : expression abbreviated  DESCRIPTION : description and abstract concepts   definition: definition of sth.   description: description of sth.   manner: manner of an action   reason: reasons HUMAN : human beings   group: a group or organization of persons   ind: an individual   title: title of a person   description: description of a person LOCATION : locations   city: cities   country: countries   mountain: mountains   other: other locations   state: states UIUC Examples NUM:date When did Rococo painting and architecture flourish ? LOC:country What country 's national passenger rail system is called Via ? HUM:ind Who invented Make-up ? DESC:desc What is the origin of the word `` attic '' ? DESC:desc What did Delilah do to Samson 's hair ? ENTY:animal What kind of animals were in the Paleozoic era ? HUM:ind Which of the following was Rhodes Scholar ? HUM:ind Who comprised the now-defunct comic book team known as the Champions ? DESC:manner How do you make a paintball ? LOC:state What U.S. state is Fort Knox in ? ENTY:animal What is a female rabbit called ? LOC:mount   Where is the highest point in Japan ? DESC:desc   Where do  chihuahuas  come from ? LOC:other   Where does Barney Rubble go to work after he drops Fred off in the “Flintstones” cartoon series? UIUC papers on question classification Xin Li and Dan Roth. Experimental  Data for Question Classification http ://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC /     Xin Li, Dan  Roth.  Learning Question Classifiers: The Role of Semantic Information http://l2r.cs.uiuc.edu/~ danr/Papers/LiRo05a.pdf   http://cogcomp.cs.illinois.edu/page/software_view/LBJava Data set training  ( http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/train_5500.label )  test  ( http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/TREC_10.label )  Techniques for Question Classification Classification task Use standard techniques Regular expressions WHO is|was -> PERSON WHICH STATE -> STATE Query Reformulation [ Radev  et al. 2001] Passage Retrieval Features Proper nouns that match the query Near each other Entities that match the expected answer type Answer Retrieval Use NER to identify the matching phrases E.g., “January 1, 1951” as a DATE Features Distance to query words Answer type Wordnet similarity Redundancy  Redundancy (1/2) What is the capital of Spain? Madrid   is the  capital  of  Spain En  route to  Spain's capital  of  Madrid Madrid ,  Spain's capital  city is situated almost at the geographical epicentre of the country The  capital  of  Spain  is  Madrid Madrid ,  Spain's  sunny  capital Madrid  became  Spain's capital In 1561, it was elevated to status as  Spain's capital  city Madrid  has been the  capital  of  Spain  since 1562 Madrid , the physical and cultural  capital  of  Spain Redundancy (2/2) When  did French revolutionaries storm the  Bastille? The storming of the Bastille occurred in Paris on the morning of 14 July 1789 The storming of the Bastille, 14 July 1789 The storming of the Bastille prison on July 14th 1789 was an event that paved the way to further civil disorder and upheaval in France.  French revolutionaries storm Bastille, 1789 The storming of the Bastille (Louis XVI's prison) by French revolutionaries took place on July 14th, 1789 Who  killed Mahathma Gandhi? Mohandas  Karamchand  Gandhi (often called Mahatma Gandhi) was assassinated on 30 January 1948, shot at point-blank range by  Nathuram   Godse . Nathuram   Godse  killed Gandhi Godse  killed Gandhi Mahatma Gandhi was assassinated on 30 January 1948, shot at point-blank range by  Nathuram   Godse Mohandas Gandhi was shot dead by  Nathuram   Godse , a Hindu extremist Godse  assassinated Mahatma Gandhi on January 30, 1948, approaching him during the evening prayer, bowing, and shooting him three times at close range with a Beretta semi-automatic pistol. NLP NLP Introduction to NLP Logistic Regression Example with Interpolation Linear interpolation for language modeling Estimating the trigram probability P( c|ab ) using P MLE ( c|a ), P MLE (c), etc. Weights   1 ,   2 , etc. We may want to consider other features E.g., POS tags of previous words, heads, word endings, etc. General idea Compute the conditional probability P( y|x ) P( y|x ) = sum of weights*features Label Y for a given history x in X Logistic Regression Logistic Regression Similar to Naïve Bayes (but discriminative!) Log-linear model Features don’t have to be independent Examples of features Anything of use Linguistic and non-linguistic Count of “good” Count of “not good” Sentence  length Classification using LR Compute the feature vector  x Multiply with weight vector  w Compute the logistic function Examples Example 1 x  = (2,1,1,1) w  = (1,-1,-2,3) z  = 2-1-2+3=2 f(z)  = 1/(1+e -2 ) Example 2 x  = ( 2,1,0,1 ) w  =  (0,0,-3,0) z  =  0 f(z)  = 1/( 1+e 0 ) =  1/2 Logistic Regression NLP NLP Introduction to NLP Dependency Parsing Techniques (1) Constraint-based methods Maruyama 1990, Karlsson 1990 Example word( pos (x)) = DET ⇒ (label(X) = NMOD, word(mod(x)) = NN, pos(x) < mod(x)) A determiner (DET) modifies a noun (NN) on the right with the label NMOD. NP-complete problem; heuristics needed Constraint graph For initial constraint graph using a core grammar: nodes, domains, constraints Find an assignment that doesn’t contradict any constraints. If more than one assignment exists, add more constraints. Techniques (2) Dynamic programming CKY – similar to lexicalized PCFG, cubic complexity (Eisner 96) → nsubj dobj nsubj dobj Techniques (3) Deterministic parsing  Covington 2001 Graph-based methods Maximum spanning trees (MST) MST Parser by McDonald et al. Transition-based  MaltParser   by  Nivre  et al.  and its variants The Eisner (1996) Method [Figure from  Nivre  2013] Eisner 1996 Split-head representation Represent half-trees in the CKY table Keep track whether the head is on the left or the right Two operations Combine two half-trees by adding a dependency arc between their heads. This creates an incomplete half-tree. Then combine an incomplete half-tree with a complete half-tree Eisner 1996 [Figure from  Nivre  2013] The Eisner (1996) Method [Figure from  Nivre  2013] Incomplete half-tree [Slide from McDonald and  Nivre ] The Eisner (1996) Method [Slide from McDonald and  Nivre ] The Eisner (1996) Method Introduction to NLP Graph-based Dependency Parsing Graph-based Dependency Parsing Background McDonald et al. 2005 Idea Dependency parsing is equivalent to search for a maximum spanning tree in a directed graph. Chu and Liu (1965) and Edmonds (1967) give an efficient algorithm for finding MST for directed graphs. MST Parser example Consider the sentence “John saw Mary” Recursively remove cycle The Chu-Liu-Edmonds algorithm gives the MST on the right hand side (right). This is in general  a non-projective tree. Notes Complexity Interestingly, it’s O(n 2 ), compared with O(n 3 ) for Eisner, even though MST is non-projective. NLP NLP Introduction to NLP Summarization Techniques 1/3 Baxendale  (1958) Positional method Analysis of 200 paragraphs Pick the first and last sentences of the paragraph That’s where the topic sentences are located Naïve but decent approach Luhn  (1958) Technical documents Stemming Stop words are removed Frequency of content  terms Luhn  (1958) Sentence-level significance factor Look for concentrations of salient content terms Edmundson  (1969) Technical documents Position and frequency Cue words (bonus and stigma words) Significant, hardly, impossible Document structure Is the sentence a title or heading or right under one of these Linear combination of the four features  1 C +   2 K +   3 T +   4 L Frump ( deJong  1979, 1982) Knowledge-based Slot-filling based on UPI news stories Based on 50 sketchy scripts Inputs matched to scripts based on manually selected keywords Difficult to port to other domains Missing scripts for many inputs Frump $ demonstration script The demonstrators arrive at the demonstration location. The demonstrators march. Police arrive on the scene. The demonstrators communicate with the target of the demonstration. The demonstrators attack the target of the demonstration. The demonstrators attack the police . G.  DeJong  (1979) FRUMP: Fast Reading Understanding and Memory Program Paice  (1990) Survey up to 1990 Techniques that (mostly) failed Syntactic criteria (Earl 1970) Indicator phrases Problems with extracts Lack of balance Lack of cohesion Paice  (1990) Lack of balance later approaches based on text rhetorical  structure Lack of cohesion anaphoric reference lexical or definite reference rhetorical  connectives recognition of anaphors [Liddy et al. 87 ] Example: “that” is nonanaphoric  if preceded by a research-verb (e.g., “ demonstrat -”), nonanaphoric  if followed by a pronoun, article, quantifier,…, external  if no later than 10th word, else internal Brandow  et al. (1995) ANES: commercial news from 41  publications “Lead” achieves acceptability of 90% vs. 74.4% for “intelligent”  summaries 20,997 documents words selected based on  tf * idf  (term frequency * inverse document frequency) sentence-based features: signature words location anaphora words length of  abstract Sentences with no signature words are included if between two selected sentences Evaluation done at 60, 150, and 250 word  length Non-task-driven evaluation : “ Most summaries judged less-than-perfect would not be detectable as such to a user” Kupiec  et al. (1995) First trainable method 20% extract 188 documents from scientific journals Naïve Bayes classifier New features Sentence length (|S|>5) Presence of uppercase words (except common acronyms) Thematic words Set of 26 manually fixed phrases Sentence position in paragraph Kupiec  et al. (1995) Uses Naïve  Bayes  classifier Assuming statistical independence Kupiec  et al. (1995) Performance: For 25% summaries, 84% precision For smaller summaries, 74% improvement over Lead Summons ( McKeown  & Radev 1995) First work on multi-document summarization Approach Knowledge-based Information extraction (MUC templates) Text generation Summons ( McKeown   and  Radev 1995 ) NICOSIA, Cyprus (AP) –  Two bombs exploded near government  ministries in Baghdad , but there was no immediate word of any  casualties,  Iraqi dissidents reported Friday .  There was no independent confirmation of the claims   by the Iraqi National Congress .  Iraq’s state-controlled media have not mentioned any bombings. Summons Summons Summons Reuters reported that 18 people were killed on  Sunday  in a bombing in Jerusalem.  The next day , a bomb in Tel Aviv killed at least 10 people and wounded 30 according to Israel radio. Reuters reported that  at least 12 people  were killed and  105  wounded  in the second incident .  Later the same day , Reuters reported that Hamas has claimed responsibility for the act. Summons If there are two templates 	AND the location is the same 	AND the time of the second template is after the time of the first template 	AND the source of the first template is different from the source of the second template 	AND at least one slot differs 	THEN combine the templates using the contradiction operator... Summons Change of perspective March 4th,  Reuters  reported that a bomb in Tel Aviv killed at least 10 people and wounded 30.  Later the same day ,  Reuters  reported that  exactly 12 people  were  actually  killed and  105  wounded. Precondition: The same source reports a change in a small number of slots Summons Contradiction The afternoon of February 26, 1993,  Reuters  reported that a suspected bomb killed  at least six people  in the World Trade Center.  However ,  Associated Press  announced that  exactly five people  were killed in the blast. Precondition: Different sources report contradictory values for a small number of slots Summons Refinement On Monday morning,  Reuters  announced that a suicide bomber killed at least 10 people in Tel Aviv.  In the afternoon ,  Reuters  reported that  Hamas  claimed responsibility for the act. Agreement The morning of March 1st 1994,  both   UPI  and Reuters reported that a man was kidnapped in the Bronx.  Summons Generalization According to UPI, three terrorists were arrested in  Medellín  last Tuesday. Reuters announced that the police arrested two drug traffickers in Bogotá the next day.  A total of five criminals were arrested in Colombia last week. Mitra /Allen/Salton Encyclopedia articles Semantic hyperlinks Between pairs of paragraphs with lexical similarity above a threshold Paths linking highly connected paragraphs are more likely to contain information central to the topic of the article NLP NLP Recurrent Neural Networks Deep Learning … the cat w x w h Recurrent Neural Networks x 1 h 0 h 1 σ w x w h RNN x 1 h 0 h 1 σ y 1 softmax w y RNN The c at sat w y Updating Parameters of an RNN The c at sat Cost w y Backpropagation through time Notes RNNs are used to keep “memory”, just like finite-state automata They can be used as generators, acceptors, transducers, etc. Application to Machine  Translation Sequence-to-sequence learning ( Sutskever ,  Vinyals , and Le, 2014) Other Applications Language Modeling ( Mikolov  2012) Character-level RNNs for text generation Semantic Role Labeling: http :// www.aclweb.org/anthology/P15-1109 Dependency  parsing: http :// www.aclweb.org/anthology/K/K15/K15-1015.pdf   (“ producing state-of-art dependency parsing results while requiring minimal feature  engineering”) NLP NLP Machine Translation Phrase Based Translation Phrase Alignment Example Spanish to English Phrase Alignment Example Spanish to English Phrase Alignment Example Intersection Phrase Alignment Example Combine  alignments from  union Search in Phrase Models [Example from Schafer/Smith 06] Deshalb haben wir allen Grund , die Umwelt in die Agrarpolitik zu integrieren That is why we have every reason to integrate the environment in the agricultural policy Translate in target language order to ease language modeling. One segmentation out of 4096 One reordering out of 40,320 One phrase translation out of 581 Search in Phrase Models [Example from Schafer/Smith 06] And many, many more…even before reordering Search in Phrase Models Many ways of segmenting source Many ways of translating each segment Restrict  phrases > e.g. 7 words, long-distance reordering Prune  away unpromising partial translations or we’ll run out of space and/or run too long How to compare partial translations? Some start with easy stuff: “in”, “das”, ... Some with hard stuff: “ Agrarpolitik ”, “ Entscheidungsproblem ”, … Phrase-based Translation Models Segmentation of the target sentence Translation of each phrase Rearrange the translated phrases Alignment Templates Alignment Templates [Example from  Och /Ney 2002] Machine Translation Evaluation of Machine Translation Evaluation Human judgments a dequacy grammaticality [expensive] Automatic methods Edit cost   (at the word, character, or minute level) BLEU ( Papineni  et al. 2002) BLEU Simple n-gram precision log  BLEU = min (0,1-reflen/ candlen ) + mean of log  precisions Multiple human references Brevity penalty Correlates with human assessments of automatic systems Doesn’t correlate well when comparing human and automatic translations Example from MTC Chinese: Napster 执行长希尔柏斯辞职 English Napster CEO  Hilbers  Resigns Napster CEO  Hilbers  resigned Napster Chief Executive  Hilbers  Resigns Napster CEO Konrad  Hilbers  resigns Full text http ://clair.si.umich.edu/~radev/nlp/mtc /     “Good” Compared to What? Idea  #1: a human translation.  OK , but Good translations can be very dissimilar We’d need to find hidden features (e.g. alignments) Idea #2: other top  n  translations (the “n-best list”).  Better  in practice, but Many entries in n-best list are the same apart from hidden links Compare with a  loss function   L 0/1: wrong or right; equal to reference or not Task-specific metrics (word error rate, BLEU, …) [Example from  Schafer&Smith  2006] [Example from  Doddington  2002] Correlation: BLEU and Humans Tools for Machine Translation Language modeling toolkits SRILM, CMULM Translation systems Giza++, Moses Decoders Pharaoh NLP NLP Introduction to NLP Lexicalized Parsing Limitations of PCFGs The probabilities don’t depend on the specific words E.g.,  give  someone something (2 arguments) vs.  see  something (1 argument) It is not possible to disambiguate sentences based on semantic information E.g., eat pizza with  pepperoni  vs. eat pizza with  fork Lexicalized grammars - idea Use the head of a phrase as an additional source of information VP[ate] -> V[ate] Fundamental idea in syntax, cf. X-bar theory, HPSG Constituents receive their heads from their head child Parent Annotation [Johnson 1998] Lexicalization cake with fork ate child ate ate Head Extraction Example (Collins) NP -> DT NNP  NN         (rightmost) NP -> DT NN  NNP         (rightmost) NP ->  NP  PP                 (leftmost) NP -> DT  JJ                   (rightmost) NP ->  DT                        (rightmost leftover child) Collins Parser (1997) 1/2 Generative, lexicalized model Horizontal  markovization only condition on the head (also on the distance  D  from the head) Types of rules LHS  → L n L n 1 …L 1 H R 1 …R m 1 R m   H gets generated first L gets generated next R gets generated last Collins Parser (1997) 2/2 Maximum likelihood estimates Smoothing (lexicalized,  unlexicalized , “ unheaded ”) smoothedP   (PP of-IN  |  VP think -VB ) =   1  P   (PP of-IN  | VP think-VB )  +                            +   2  P   (PP of-IN  | VP- VB ) + (1  1  2 )  P   (PP of-IN  | VP))  P ML  ( PPof -IN |  VPthink -VB) =         Count ( PPof -IN right of the head  VPthink -VB) /            Count (symbols right of the head  VPthink -VB) Issues with Lexicalized Grammars Sparseness of training data Many probabilities are difficult to estimate from the Penn Treebank E.g., WHADJP (when not “how much” or “how many” only appears 6 times out of 1M constituents) Smoothing is essential Combinatorial explosion Parameterization is essential Discriminative  Reranking Issues with statistical parsers A parser may return many parses of a sentence, with small differences in probabilities The top returned parse may not necessarily be the best because the PCFG may be deficient Other considerations may need to be taken into account parse tree depth left attachment vs. right attachment discourse structure Can you think of others features that may affect the  reranking ? Answer Considerations that may affect the  reranking parse tree depth left attachment vs. right attachment discourse structure Can you think of others? consistency across sentences or other stages of the NLU pipeline Discriminative  Reranking n-best list Get the parser to produce a list of n-best parses (where n can be in the thousands) Reranking Train a discriminative classifier to rerank these parses based on external information such as a bigram probability score or the amount of right branching in the tree Statistical Parser Performance F1 (sentences <= 40 words) Charniak (2000) – 90.1% Charniak and Johnson (2005) – 92% (discriminative reranking)  All words Charniak  and Johnson (2005) – 91.4% Fossum  and Knight (2009) – 92.4% (combining constituent parsers) Notes Complexity of lexicalized parsing  O(N 5 g 3 V 3 ), instead of O(N 3 ) because of the lexicalization N = sentence length g = number of non-terminals V = vocabulary size Use beam search ( Charniak ; Collins) Sparse data 40,000 sentences; 12,409 rules (Collins) 15% of all test sentences contain a rule not seen in training (Collins) Notes Complements (arguments) vs. adjuncts (additional information) NP-C (Collins) Subcategorization E.g., transitive vs. intransitive verbs Parent annotation NP^S (Johnson 1998) Example from Michael Collins Notes Learning PCFG without an annotated corpus Use EM (inside-outside) (Baker 1979), limited success Summary Lexicalization takes F1 from 75% to  90+% Most errors come from  attachment ambiguities: PP and CC Markovization Horizontal (forgetful  binarization ) Vertical (generalized parent annotation) Note: infinite vertical  markovization  is inefficient (Klein and Manning 2003) Collins and  Charniak  are generative models Reranking  is a discriminative model NLP NLP Natural Language Processing Semantic  Similarity Synonyms and paraphrases Example: post-close market announcements The S&P 500  climbed  6.93, or 0.56 percent, to 1,243.72,         its best close        since June 12, 2001.    The Nasdaq   gained  12.22, or 0.56 percent, to 2,198.44    for its best showing  since June 8, 2001.       The DJIA      rose   68.46, or 0.64 percent, to 10,705.55,     its highest level     since March 15. Synonyms Different words (and also word compounds) can have similar meanings.  tepid  and  lukewarm  have very similar meanings and can be substituted for one another ( tepid water  vs.  lukewarm water). True synonyms are actually relatively rare.  even though  big  and  large  are often thought of as synonyms, consider the difference between  Big Leagues  and  Large Leagues .   The verbs  sweat  and  perspire  are also near synonyms.  However, they differ in their frequency of use and the type of text in which they are likely to appear. Polysemy Polysemy Polysemy Polysemy is the property of words to have multiple senses. For example, the noun  book  can refer to the following: A literary work (e.g., “Anna Karenina”) A stack of pages (e.g., a notebook) A record of business transactions (think “bookkeeper”) A record of bets (think “bookmaker”) A list of buy and sell orders in a financial market Polysemy The same word can also have multiple parts of speech, each with its own set of senses. For example, the word  book , as a verb can mean “make a reservation for” or “occupy”. The different senses of the same word don’t have to be equally frequent. Some of the senses may overlap (e.g., the first two senses of  book  on the previous slide). That’s partially why different dictionaries list different sets of word senses for the same word. “My favorite books are Anna Karenina and my father’s checkbook”   Some words can be highly  polysemous  (e.g., the verb “get” has at least 35 different meanings, according to Wordnet). Other Semantic Relations Antonymy  (near opposites)  raise - lower Hypernymy a  deer  is a  hypernym  for  elk Hyponymy (the inverse of  hypernymy ) Membership  Meronymy :  a  flock  includes  sheep  (or  birds) Part  Meronymy :  a  table  has  legs Synsets Semantic relations hold between word senses, not between words. Examples: the antonym of  hot  can be either  mild  or  cold  (or  unattractive)  depending on the specific sense of  hot . the immediate  hypernym  of  bar  can be one of the following, among others:  room ,  musical notation ,  obstruction ,  profession , depending on the sense of  bar . The term  synset  is used to group together all synonyms of the same word. If a word is  polysemous , it may be associated with multiple  synsets . NLP NLP Introduction to NLP Regular Expressions (from  Jurafsky  and Martin) Writing correct expressions Exercise:  write a Perl regular expression to match the English article “the”: Writing correct expressions Exercise: write a Perl regular expression to match the English article “the”: /the/ /[tT]he/ /\b[tT]he\b/ /[^a-zA-Z][tT]he[^a-zA-Z]/ /(^|[^a-zA-Z])[tT]he[^a-zA-Z]/ A more complex example Exercise:  Write a regular expression that will match “any PC with more than 500MHz and 32 Gb of disk space for less than $1000”: A more complex example Exercise:  Write a regular expression that will match “any PC with more than 500MHz and 32 Gb of disk space for less than $1000”: /$[0-9]+/ /$[0-9]+\.[0-9][0-9]/ /\b$[0-9]+(\.[0-9][0-9])?\b/ /\b[0-9]+  *(MHz|[ Mm] egahertz|Ghz |[ Gg ] igahertz )\b/ /\b[0-9]+ *(Mb|[Mm] egabytes ?)\b/ /\b[0-9](\.[0-9]+) *(Gb|[ Gg ] igabytes ?)\b/ Perl regular expressions Perl regular expressions Perl regular expressions Examples Match the word “cat”. Match words that start with a capital letter but are not in all caps. Match numerical expressions (e.g., dollar amounts) Replace “New York City” with “NYC” Figure 2.1 Figure 2.2     Figure 2.3     Figure 2.4     Figure 2.5     Figure 2.6     Figure 2.7     Figure 2.8     Substitutions and memory Substitutions Memory ( \1 ,  \2 , etc. refer back to matches) s/colour/color/ s/([0-9]+)/<\1>/ Eliza example http://blog.oxforddictionaries.com/2012/06/turing-test/ http://www.d.umn.edu/~xuxxx401/eliza-perldoc.html     Eliza>  Hi, I'm Eliza. What's your name?   You>  I'm  Yanbo   Eliza>  Nice to meet you,  Yanbo . Is there anything bothering you?   Yanbo >  Yes   Eliza>  Why don't you tell me a little about this.   Yanbo >  quit   Eliza>  Goodbye. It was nice talking to you,  Yanbo . NLP NLP Language and Complexity Natural Language Processing Languages and Automata Language: Set of strings Automaton: Abstract model Defines a language (all the strings that it accepts) Input: string Output: yes/no The Chomsky Hierarchy recursively enumerable context-sensitive mildly context-sensitive context-free regular [slide from  Jurafsky  and Martin] Examples Languages and Automata Regular finite-state regular expressions named entity recognition, shallow parsing Context-free push-down automata most NL and PL phenomena Context-sensitive Linear bounded automaton captures typing in PL Are Natural Languages Regular In practice Almost In theory No Center-embedding requires context-free rules Example Proof that NL are not Regular Pumping Lemma for regular languages If L is an infinite language over A, then there are strings  x,y,z  ∈ A* such that y ≠ e and  xy n z  ∈ L for all n≥0.  Regular languages are closed under intersection [figure from  Jurafsky  and Martin] Proof (cont’d) Consider the language L= a n b n There are only three options for y: a + b + a + b + In each case, repeating y (or skipping y) will result in a string that is not in L Another Example Sentences The cat caught a fish The cat that the dog saw caught a fish The cat that the dog that the girl found saw caught a fish Language (the N) n  (verb) n-1  caught a fish A = {the cat, the dog, the rat, the girl …} B = {ate, saw, caught, chased, found …}  Intersect with English A* B* caught a fish L = “A n  B n-1  caught a fish” is not regular (see previous slide) Representing  a n b n  as a CFL [figure from  Jurafsky  and Martin] Are Natural Languages Context-Free In practice Almost In theory No Cross-serial dependencies cannot be captured using a CFG [figure from  Jurafsky  and Martin] Example a m b n c m d n  is not context-free Intersect some sentences in Dutch or Swiss German http://nacloweb.org/resources/problems/2016/N2016-P.pdf NLP NLP Text Similarity Morphological Similarity: Stemming Morphological Similarity Words with the same root: scan (base form) scans, scanned, scanning (inflected forms) scanner (derived forms, suffixes) rescan (derived forms, prefixes) rescanned (combinations) Stemming Definition To stem a word is to reduce it to a base form, called the  stem , after removing various suffixes and endings and, sometimes, performing some additional transformations Examples scanned  →  scan indication  →  indicate Note In practice, prefixes are sometimes preserved, so  rescan  will not be stemmed to  scan Porter’s Stemming Method History: Porter’s stemming method is a rule-based algorithm introduced by Martin Porter in 1980 The paper (“An algorithm for suffix stripping”) has been cited more than 7,000 times according to Google Scholar Input: The input is an individual word. The word is then transformed in a series of steps to its stem Accuracy: The method is not always accurate Porter’s Algorithm Example 1:  Input =  computational Output =  comput Example 2: Input =  computer Output =  comput The two input words end up stemmed the same way Porter’s Algorithm The  measure  of a word is an indication of the number of syllables in it Each sequence of consonants is denoted by C Each sequence of vowels is denoted as V The initial C and the final V are optional So, each word is represented as  [C]VCVC ... [V], or [C](VC){m}[V], where  m  is its  measure Examples of Measures m=0 : I, AAA, CNN, TO, GLEE m=1 : OR, EAST, BRICK, STREET, DOGMA m=2 : OPAL, EASTERN, DOGMAS m=3 : EASTERNMOST, DOGMATIC Porter’s Algorithm Transformation patterns The initial word is then checked against a sequence of transformation patterns, in order. Example (m>0) ATION   ->  ATE          medication    ->  medicate Note that this pattern matches  medication  and  dedication , but not  nation . Actions Whenever a pattern matches, the word is transformed and the algorithm restarts from the beginning of the list of patterns with the transformed word. If no pattern matches, the algorithm stops and outputs the most recently transformed version of the word. Example Rules Step 1a 	 SSES -> SS              presses  ->  press 	IES  -> I               lies     ->  li 	SS   -> SS              press    ->  press 	S    -> ø               lots     ->  lot Step 1b       (m>0) EED -> EE          refereed      ->  referee                (doesn’t apply to  bleed  since m(‘BL’)=0) Example Rules Step 2           (m>0) ATIONAL ->  ATE            inflational     ->  inflate     (m>0) TIONAL  ->  TION          notional       ->  notion     (m>0) IZER    ->  IZE           nebulizer      ->  nebulize     (m>0) ENTLI   ->  ENT            intelligentli   ->  intelligent     (m>0) OUSLI   ->  OUS            analogousli     ->  analogous     (m>0) IZATION ->  IZE           realization    ->  realize     (m>0) ATION   ->  ATE           predication    ->  predicate     (m>0) ATOR    ->  ATE           indicator      ->  indicate     (m>0) IVENESS ->  IVE           attentiveness  ->  attentive     (m>0) ALITI   ->  AL             realiti         ->  real     (m>0) BILITI  ->  BLE            abiliti         ->  able Example Rules Step 3  	 (m>0) ICATE -> IC             replicate ->  replic   	(m>0) ATIVE -> ø              informative -> inform  	(m>0) ALIZE -> AL             realize -> real  	(m>0) ICAL -> IC              electrical -> electric  	(m>0) FUL -> ø                blissful -> bliss  	(m>0) NESS ->                 tightness -> tight Step 4  	 (m>1) AL -> ø                 appraisal ->  apprais   	(m>1) ANCE -> ø               conductance -> conduct  	(m>1) ER -> ø                 container -> contain  	(m>1) IC -> ø                 electric ->  electr   	(m>1) ABLE -> ø               countable -> count  	(m>1) IBLE -> ø               irresistible ->  irresist   	(m>1) EMENT -> ø              displacement ->  displac   	(m>1) MENT -> ø               investment -> invest  	(m>1) ENT -> ø                respondent -> respond Examples Example 1:  Input =  computational Step 2: replace  ational   with  ate :  computate Step 4: replace  ate  with  ø:  comput Output =  comput Example 2: Input =  computer Step 4: replace  er  with  ø :  comput Output =  comput The two input words end up stemmed the same way External Pointers Online demo http://text-processing.com/demo/stem/   Martin Porter’s official site http://tartarus.org/martin/PorterStemmer/ Quiz How will the Porter stemmer stem these words? Check the Porter paper (or the code for the stemmer) in order to answer these questions. Is the output what you expected?  If not, explain why. construction 		? increasing 			? unexplained 		? differentiable 	? Answers to the Quiz construction	 construct increasing		 increas unexplained		 unexplain differentiable	 differenti construction 		? increasing 			? unexplained 		? differentiable 	? NACLO Problem Thorny Stems, NACLO 2008 problem by Eric  Breck http://www.nacloweb.org/resources/problems/2008/N2008-H.pdf     Solution to the NACLO problem Thorny Stems http://www.nacloweb.org/resources/problems/2008/N2008-HS.pdf    NLP NLP Discourse Analysis Discourse  Parsing Sample Rhetorical Relations Example 1) Title: Bouquets in a basket - with living flowers  2) There is a gardening revolution going on.  3) People are planting flower baskets with living plants,  4) mixing many types in one container for a full summer of floral beauty.  5) To create your own "Victorian" bouquet of flowers,  6) choose varying shapes, sizes and forms, besides a variety of complementary colors.  7) Plants that grow tall should be surrounded by smaller ones and filled with others that tumble over the side of a hanging basket.  8) Leaf textures and colors will also be important.  9) There is the silver-white foliage of dusty miller, the feathery threads of lotus vine floating down from above, the deep greens, or chartreuse, even the widely varied foliage colors of the coleus.   Christian Science Monitor, April, 1983  from Mann/ Matthiessen /Thompson Example (cont’d) http://www.sfu.ca/rst/   Discourse Parsing Four RST relations: contrast, cause-explanation-evidence, condition, elaboration + non-relation Up to 4M automatically labeled examples per relation Naïve Bayes Word co-occurrence features [Marcu and  Echihabi  2002] Cross-document structure (CST) S=Sentence, P=Paragraph, D= ddocument Cross-document structure (CST) Argumentative Zoning Aim research  goal of the paper Textual statements  about section structure Own description  of the authors’ work (methodology, results, discussion) Background generally  accepted scientific background Contrast comparison  with other work Basis statements  of agreement with other work Other description  of other researchers’  work [Teufel and  Moens  2002] NLP NLP Introduction to NLP Support Vector Machines Which Boundary Is Best? Which Boundary Is Best? Does a new data point change your answer? Which Boundary Is Best? What if the new data point was here instead? Support Vector Machines Find the decision boundary as a hyperplane that maximize the so-called “margins” (space around the hyperplane with no examples in it) Main advantages theoretically well founded (structural risk minimization) certain guarantee of generalization ability Main problems slow training (quadratic programming) need many examples kernel design  Feature Mapping In many cases, data are not linearly separable, so we map the original  feature space  to  some higher-dimensional feature space where the training set is separable Slide from Ray Mooney Feature Mapping Example (mapping to a higher-dimensional space) Kernels and SVMs Some Well-Known Kernels Polynomial kernel: Sigmoid kernel: RBF kernel: Many other kernels are useful for IR: e.g., string kernels, subsequence kernels, tree kernels, etc. NLP NLP Text Similarity Spelling Similarity: Edit Distance Spelling Similarity Typos: Brittany Spears -> Britney Spears Catherine Hepburn -> Katharine Hepburn Reciept  -> receipt Variants in spelling: Theater -> theatre Who is this? معمر القذافي Hints معمر القذافي M Hints معمر القذافي M F Hints معمر القذافي M F AL Hints معمر القذافي M F AL Muammar (al-)Gaddafi,  or   Moamar   Khadafi ,  or … Quiz How many different transliterations can there be? el al El Al  ø   Q G  Gh  K  Kh a e u  d dh  ddh   dhdh   th   zz    a  f  ff i  y m  u o  a  m mm  a e r A lot! m  u o  a  m mm  a e r el al El Al  ø   Q G  Gh  K  Kh a e u  d dh  ddh   dhdh   th   zz    a  f  ff i  y 8 5 360 14,400 x x = Edit Operations Insertion/deletion behaviour  - behavior Substitution string - spring Multiple edits sleep - slept  Levenshtein  Method Based on dynamic programming Insertions, deletions, and substitutions usually all have a cost of 1. Example Recurrence relation Recursive dependencies D(i,0)= i D(0,j)=j D( i,j )=min[ D(i-1,j)+1 D(i,j-1 )+1 D(i-1,j-1)+t( i,j ) ] Simple edit distance:  t( i,j )=0  iff  s 1 ( i )=s 2 (j) t( i,j )=1 ,  otherwise Definitions s 1 ( i )  –  i th  character in string s 1 s 2 (j)  –  j th  character in string s 2 D( i,j )  – edit distance between a prefix of s 1  of length  i  and a prefix of s 2  of length j t( i,j )  – cost of aligning the  i th  character in string s 1  with the  j th  character in string s 2 Example Example Example Example Example Edit Transcript Other Costs Damerau  modification Swaps of two adjacent characters also have a cost of 1 E.g., Lev(“ cats”,”cast ”) = 2, Dam(“ cats”,”cast ”) = 1 Quiz Some distance functions can be more specialized. Why do you think that the edit distances for these pairs are as follows? Dist  (“sit  clown”,“sit  down”) = 1 Dist  (“ qeather ”,”weather”) = 1,  but  Dist  (“ leather”,”weather ”) = 2 Quiz Answers Dist (“sit  down”,”sit  clown”) is lower in this example because we want to model the type of errors common with optical character recognition (OCR) Dist (“ qeather ”,”weather”) <  Dist (“ leather”,”weather ”) because we want to model spelling errors introduced by “fat fingers” (clicking on an adjacent key on the keyboard) Quiz: Guess the Language AACCTGCGGAAGGATCATTACCGAGTGCGGGTCCTTTGGGCCCAACCTCCCATCCGTGTCTATTGTACCC  TGTTGCTTCGGCGGGCCCGCCGCTTGTCGGCCGCCGGGGGGGCGCCTCTGCCCCCCGGGCCCGTGCCCGC  CGGAGACCCCAACACGAACACTGTCTGAAAGCGTGCAGTCTGAGTTGATTGAATGCAATCAGTTAAAACT  TTCAACAATGGATCTCTTGGTTCCGGC Quiz Answer This is a genetic sequence (nucleotides AGCT) >U03518  Aspergillus   awamori  internal transcribed spacer 1 (ITS1)  AACCTGCGGAAGGATCATTACCGAGTGCGGGTCCTTTGGGCCCAACCTCCCATCCGTGTCTATTGTACCC  TGTTGCTTCGGCGGGCCCGCCGCTTGTCGGCCGCCGGGGGGGCGCCTCTGCCCCCCGGGCCCGTGCCCGC  CGGAGACCCCAACACGAACACTGTCTGAAAGCGTGCAGTCTGAGTTGATTGAATGCAATCAGTTAAAACT  TTCAACAATGGATCTCTTGGTTCCGGC Other uses of Edit Distance In biology, similar methods are used for aligning non-textual sequences Nucleotide sequences, e.g., GTTCGTGATGGAGCG, where A=adenine, C=cytosine, G=guanine, T=thymine, U=uracil, “-”=gap of any length, N=any one of ACGTU, etc. Amino acid sequences, e.g., FMELSEDGIEMAGSTGVI, where A=alanine, C= cystine , D=aspartate, E=glutamate, F=phenylalanine, Q=glutamine, Z=either glutamate or glutamine, X=“any”, etc. The costs of alignment are determined empirically and reflect evolutionary divergence between protein sequences. For example, aligning V ( valine ) and I (isoleucine) is lower-cost than aligning V and H (histidine). External URLs Levenshtein  demo http://www.let.rug.nl/~kleiweg/lev/    Biological sequence alignment http://www.bioinformatics.org/sms2/pairwise_align_dna.html   http://www.sequence-alignment.com/sequence-alignment-software.html   http://www.ebi.ac.uk/Tools/msa/clustalw2/   http://www.animalgenome.org/bioinfo/resources/manuals/seqformats   NACLO Problem “ Nok-Nok ”, NACLO 2009 problem by Eugene Fink: http://www.nacloweb.org/resources/problems/2009/N2009-B.pdf  ‎  Solution to the NACLO Problem “ Nok-Nok ” http://www.nacloweb.org/resources/problems/2009/N2009-BS.pdf   NACLO Problem “The Lost Tram” - NACLO 2007 problem by Boris  Iomdin : http://www.nacloweb.org/resources/problems/2007/N2007-F.pdf   Solution to the NACLO problem “The Lost Tram” www.nacloweb.org/resources/problems/2007/N2007-FS.pdf   NLP NLP Introduction to NLP Learning in Hidden Markov Models HMM Learning Supervised Training sequences are labeled  Unsupervised Training sequences are unlabeled Known number of states Semi-supervised Some training sequences are labeled Supervised HMM Learning Estimate the static transition probabilities using MLE Estimate the observation probabilities using MLE Use smoothing Unsupervised HMM Training Given:  observation sequences Goal:  build the HMM Use EM (Expectation Maximization) methods  forward-backward (Baum-Welch) algorithm Baum-Welch finds an approximate solution for P(O|µ) Outline of Baum-Welch Algorithm Randomly set the parameters of the HMM Until the parameters converge repeat: E step – determine the probability of the various state sequences for generating the observations M step –  reestimate  the parameters based on these probabilities Notes the algorithm guarantees that at each iteration the likelihood of the data  P(O|µ)  increases it can be stopped at any point and give a partial solution it converges to a local maximum NLP NLP Introduction to NLP Summarization Evaluation Evaluation Criteria Appropriate length Fidelity Salience Grammaticality Non-redundancy Referential well- formedness Structure and coherence Ideal Evaluation R>C Types of evaluation methods Extrinsic techniques (task-based) Can you make the same decision using the summary as with the full text, but in less time Intrinsic techniques Comparing summaries against gold standards Precision and Recall Rouge (Lin and  Hovy  2003) Characteristics Based on  Papineni  et al.’s BLEU (used for Machine Translation) R stands for  Recall Correlated with manual evaluations when averaged over many examples Very convenient for  prototyping Variants ROUGE-n is a measure of n-gram overlap between a summary and a set of reference summaries ROUGE-L uses longest common subsequence instead of n-gram overlap Disadvantage It can be easily gamed Relative Utility (Radev et al. 2000) U (system, ideal)=  % of ideal utility  covered by system summary Relative Utility RU = Relative Utility 13 17 RU = = 0.765 Normalized System Performance Random Performance Example 0.833 - 0.732 0.841 - 0.732 = 0.927 D {14} =  (S-R) (J-R) =  J = 0.841 R = 0.732 S = 0.833 Normalized Evaluation of {14} Subsumption  and Equivalence Subsumption : If the information content of sentence a (denoted as I(a)) is contained within sentence b, then a becomes  informationally  redundant and the content of b is said to subsume that of a: 			 I(a)    I(b) Equivalence : If I(a)    I(b)    I(b)    I(a) Subsumption (1) John Doe was found guilty of the murder. (2) The court found John Doe guilty of the murder of Jane Doe last August and sentenced him to life. Subsumption Subsumption (Cont’d) SCORE (s) =  S i  ( w c C i  +  w p P i  +  w f F i )  -  w R R s R s  = cross-sentence word overlap R s  = 2 * (# overlapping words) / (# words in sentence 1 + # words in sentence 2) w R  =  Max s  (SCORE(s)) The Pyramid Method Nenkova   and Passonneau  2004 Based on Semantic Content Units (SCU) Used for multi-document summarization Different surface realizations with equivalent meanings The Pyramid Method Available Corpora DUC and TAC corpora http:// duc.nist.gov   http://www.nist.gov/tac/ SummBank  corpus http:// www.summarization.com/summbank   SUMMAC corpus NY Times corpus (from LDC) NLP NLP Introduction to NLP Introduction  to Deep  Learning Neural Networks Remember Logistic Regression? Input: vector of features Output: one or more nodes Trained using SGD One-layer LR ( maxent  model) is a simple Neural Network Neuron (perceptron): output is a non-linear combination of the inputs Sample code: http://ai.stanford.edu/~ajoulin/code/nn.zip Perceptron x 1 x 2 x 3 x 4 y w 1 w 2 w 3 w 4 Non- linearities  (e.g., Sigmoid,  Tanh ) Multi-Layer Perceptron (Multi-layer LR) x 1 x 2 x 3 x 4 y 1 v 1 w 2,1 w 3,1 w 4,1 y 2 w 1,2 w 2,2 w 3,2 w 4,2 z w 1,1 v 2 [hidden layer] [output layer] [input layer] Network with a Hidden Layer It is a universal function  approximator It can model any continuous function ( Hornik  1991) This is not true for a simpler network It cannot learn XOR, for example (no one-layer linear classifier can) The deeper network can represent XOR  as a disjunction of two lower-level features An even deeper network can also represent an arbitrary function but with fewer nodes Multi-Layer Perceptron Neural Networks Remember Logistic Regression? Input: vector of features Output: one or more nodes Trained using SGD One-layer LR ( maxent  model) is a simple Neural Network Neuron (perceptron): output is a  non-linear  combination of the inputs Non- linearities  (e.g., Sigmoid,  Tanh ) Deeper Networks Multiple layers Input layer Low-level feature layer Higher-level feature layer Output layer Intuition Learning features automatically + = Example (from Lee et al. 2009) Example (from Lee et al. 2009) What is Deep Learning Architecture  Neural Networks with multiple layers Non- linearities  (e.g., sigmoid,  tanh ) Use Single-layer NN used as simple classifier Multi-layer NN can express more complex functions and learn complex features Hot area of research Very popular these days, especially in speech and vision, but also in NLP Works best with high-performance computers using GPUs and very large data sets Why  Deep Learning ? Amazing results Speech: Dahl  et al. 2010 – 30% improvement in WER (word error rate)  Vision: ImageNet  ( Krizhevsky  et al. 2012) – scarily complex architecture  The big players (Google, Facebook, Baidu, Microsoft, IBM…) are investing billions in DL The hot new thing? Actually, many of the architectures that we’ll talk about were invented in the 1980s and 1990s What’s new is hardware (GPU) that can use these architectures at scale   ImageNet Architecture [ Krizhevsky  et al. 2012] [ Krizhevsky  et al. 2012] [clarifai.com demo] Autoencoder x 1 x 2 x 3 x 4 y 1 y 2 z 2 [hidden layer] [output layer] [input layer] z 1 z 3 z 4 z  = x Autoencoder The size of the hidden layer should be smaller than the size of the input layer. Otherwise the hidden layer will learn the identity mapping Minimize loss difference between  z i  and x i D enoising   autoencoder Add some domain-independent noise to the input Example: rotate or stretch the image Still try to have the output match the input The  autoencoder  is closely related to SVD Quick Math Review 3 x 2 y 5 z 7 This equation can be written as a dot product of two  vectors :  3 x 2 y 5 z Linear Algebra Likewise, these equations can be written as a dot product of a  matrix  and a vector:  7 3 x 2 y 5 z 12 -1 x 3 y 9 z 8 4 x y 0 z Supervised   Machine  Learning Sigmoid or other nonlinearity Supervised   Machine  Learning w σ ŷ y cost(     ,     ) Update parameters A Simplified Diagram w x y σ NLP NLP Introduction to NLP Alternative Formalisms for Parsing Parsing with Finite-State Automata Shallow parsing , chunking Useful for information extraction noun phrases, verb phrases, locations, etc. Example FASTUS (Appelt and Israel, 1997) Sample rules for noun groups: NG   Pronoun | Time-NP | Date-NP NG  (DETP) (Adjs) HdNns | DETP Ving HdNns DETP  DETP-CP | DETP-CP Sample FASTUS Output Incident: Date  					19 Apr 89  Incident: Location  				El Salvador: San Salvador (CITY)  Incident: Type  					Bombing  Perpetrator: Individual ID  		"urban guerrillas"  Perpetrator: Organization ID  		"FMLN"  Perpetrator: Organization  		Suspected or Accused by Authorities: "FMLN"  Confidence   Physical Target: Description  	"vehicle"  Physical Target: Effect  			Some Damage: "vehicle"  Human Target: Name  			"Roberto Garcia Alvarado"  Human Target: Description  		"attorney general": "Roberto Garcia Alvarado" "driver" "bodyguards"  Human Target: Effect  		Death: "Roberto Garcia Alvarado" No Injury: "driver" Injury: "bodyguards" NLP NLP Introduction to NLP Probabilities Probabilities in NLP Very important for language processing Example in speech recognition: “recognize speech” vs “wreck a nice beach” Example in machine translation: “l’avocat general”: “the attorney general” vs. “the general avocado” Example in information retrieval: If a document includes three occurrences of “stir” and one of “rice”, what is the probability that it is a recipe Probabilities make it possible to combine evidence from multiple sources in a systematic way Probabilities Probability theory predicting how likely it is that something will happen Experiment (trial) e.g., throwing a coin Possible outcomes heads or tails Sample spaces discrete (number of occurrences of “rice”) or continuous (e.g., temperature) Events  is the certain event   is the impossible event event space - all possible events Sample Space Random experiment: an experiment with uncertain outcome e.g., flipping a coin, picking a word from text Sample space: all possible outcomes, e.g.,  Tossing 2 fair coins,    ={HH, HT, TH, TT} Events Event: a subspace of the sample space E    , E happens  iff  outcome is in E, e.g.,  E={HH} (all heads)  E={HH,TT} (same face) Impossible event (  ) C ertain event (  )	 Probability of Event : 0    P(E)  ≤ 1,  s.t. P(  )=1 (outcome always in   ) P(A B)=P(A)+P(B), if (AB)=  (e.g., A=same face, B=different face) Example: Toss a Die Sample space:    = {1,2,3,4,5,6} Fair die: p(1) = p(2) = p(3) = p(4) = p(5) = p(6) = 1/6 Unfair die:  p(1) = 0.3, p(2) = 0.2, ... N-dimensional die:   = {1, 2, 3, 4, …, N} Example in modeling text: Toss a die to decide which word to write in the next position   = {cat, dog, tiger, …} Example: Flip a Coin   : {Head, Tail} Fair coin:  p(H) = 0.5, p(T) = 0.5 Unfair coin, e.g.: p(H) = 0.3, p(T) = 0.7 Flipping two fair coins: Sample space: {HH, HT, TH, TT} Example in modeling text: Flip a coin to decide whether or not to include a word in a document Sample space = {appear, absence} Probabilities Probabilities numbers between 0 and 1 Probability distribution distributes a probability mass of 1 throughout the sample space . Example:  A fair coin is tossed three times.  What is the probability of 3 heads? What is the probability of 2 heads? Meaning of probabilities Frequentist I threw the coin 10 times and it turned up heads 5 times Subjective I am willing to bet 50 cents on heads Probabilities Joint probability:  P( AB ), also written as P(A, B) Conditional Probability: P(B|A)=P( AB )/P(A) P( AB ) = P(A)P(B|A) = P(B)P(A|B) So, P(A|B) = P(B|A)P(A)/P(B) (Bayes’ Rule) For independent events, P( AB ) = P(A)P(B), so P(A|B)=P(A) Total probability: If A 1 , …, A n  form a partition of S, then P(B) = P(B S) =  P(B , A 1 ) + … + P(B, A n ) (why?) So, P( A i |B ) = P( B|A i )P(A i )/P(B) = P( B|A i )P(A i )/[P(B|A 1 )P(A 1 )+…+P( B|A n )P(A n )]  This allows us to compute P( A i |B ) based on P( B|A i )                        Conditional Probability Prior and posterior probability Conditional probability P(A|B) =  P(A    B) P(B)  Properties of Probabilities p() = 0  P(certain event)=1 p(X)  p(Y), if X  Y p(X  Y) = p(X) + p(Y), if X  Y= Conditional Probability Six-sided fair die P(D even)=? P(D>=4)=? P(D even|D>=4)=? P(D odd|D>=4)=? Multiple conditions P(D  odd|D >=4, D<=5)=? Answers Six-sided fair die P(D even)=3/6=1/2 P(D>=4)=3/6=1/2 P(D even|D>=4)=2/3 P(D odd|D>=4)=1/3 Multiple conditions P(D odd|D>=4, D<=5)=1/2 The Chain Rule P(w 1 ,w 2 ,w 3 … w n ) = ? Using the chain rule: P(w 1 ,w 2 ,w 3 … w n ) =P(w 1 ) P(w 2 |w 1 ) P(w 3 |w 1 ,w 2 )… P(w n |w 1 ,w 2 …w n-1 ) This rule is used in many ways in statistical NLP, more specifically in Markov Models Independence Two events are independent when  P(A B) = P(A)P(B) Unless  P(B)=0 this is equivalent to saying that P(A) = P(A|B) If two events are not independent, they are considered dependent Adding vs. Removing Constraints Adding constraints P(walk=yes|weather=nice) P(walk=yes|weather=nice,freetime=yes,crowded=yes) More accurate But more difficult to estimate Removing constraints ( Backoff ) P(walk=yes|weather=nice,freetime=yes,crowded=yes) P(walk=yes|weather=nice,freetime=yes) P(walk=yes|weather=nice) Note that it is  not  possible to do  backoff  on the left hand side of the conditional [Example modified from Jason Eisner] X:       R n Random Variables Simply a function: The numbers are generated by a  stochastic process  with a certain probability distribution Example the discrete random variable X that is the sum of the faces of two randomly thrown fair dice Probability mass function ( pmf ) which gives the probability that the random variable has different numeric values: P( x ) = P(X =  x )  = P(A x ) where A x  = {         : X(  ) =  x } Random Variables If a random variable X is distributed according to the  pmf  p( x ), then we write X  ~  p( x ) For a discrete random variable, we have S p ( x i ) = P(  ) = 1 Six-sided Fair Die p(1) = 1/6 p(2) = 1/6 etc. P ( D )=? P ( D ) = {1/6, 1/6, 1/6, 1/6, 1/6, 1/6} P ( D |odd) = {1/3, 0, 1/3, 0, 1/3, 0} NLP NLP Neural Question  Answering  (including  Qanta  Assignment) Deep Learning [ Iyyer  et al. 2014] Quiz Bowl Questions [ Iyyer  et al. 2014] [ Rajpurkar  et al. 2016] Stanford QA corpus [ Rajpurkar  et al. 2016] Links about  Theano  and  Qanta http://deeplearning.net/software/theano/tutorial /    Tutorial about  Theano https://cs.umd.edu/~miyyer/qblearn / A Neural Network for Factoid Question Answering over  Paragraphs Mohit   Iyyer , Jordan Boyd-Graber, Leonardo  Claudino , Richard  Socher , and Hal  Daumé   III EMNLP 2014 https://cs.umd.edu/~ miyyer/data/question_data.tar.gz   https://cs.umd.edu/~ miyyer/qblearn/qanta.tar.gz   FQA as a classification task A predefined set of answers, say 10 answers Given a question, pick the best one out of 10. QANTA Assignment Get dependency parsing of the question Build Recursive Neural Network over dependency tree Train this Recursive Neural Network over training data Use this Recursive Neural Network as a feature extractor Train a Logistic Regression Classifier over the features from the Recursive Neural Network as our predictor Dependency-Tree Recursive Neural Networks Unlike constituent parses, words reside at nodes other than leaves Need to combine word vector of node with “hidden” vectors of children [slides from Chris Hidey] Dependency-Tree Recursive Neural Networks Dependency-Tree Recursive Neural Networks Activation Functions Sigmoid Tanh  (hyperbolic tangent) Rectified linear unit ( ReLU ) Normalized  Tanh Cost Function NLP NLP Introduction to NLP The Penn Treebank Description Background From the early ‘90s Developed at the University of Pennsylvania (Marcus, Santorini, and  Marcinkiewicz  1993) Size 40,000 training sentences 2,400 test sentences Genre Mostly Wall Street Journal news stories and some spoken conversations Importance Helped launch modern automatic parsing methods External links Treebank-3 http://catalog.ldc.upenn.edu/LDC99T42   Original version http://catalog.ldc.upenn.edu/LDC95T7   Tokenization guidelines http://www.cis.upenn.edu/~treebank/tokenization.html The American National Corpus http://www.anc.org   Penn Treebank  Tagset  (1/2) Penn Treebank  Tagset  (2/2) Example Sentence WSJ/12/WSJ_1273.MRG, sentence 11  Because the CD had an effective yield of 13.4 % when it was issued in 1984 , and interest rates in general had declined sharply since then , part of the price Dr. Blumenfeld paid was a premium -- an additional amount on top of the CD 's base value plus accrued interest that represented the CD 's increased market value . (S   (SBAR-PRP     (IN Because)     (S       (S         (NP-SBJ (DT the) (NNP CD))         (VP           (VBD had)           (NP             (NP (DT an) (JJ effective) (NN yield))             (PP (IN of) (NP (CD 13.4) (NN %))))           (SBAR-TMP             (WHADVP-4 (WRB when))             (S               (NP-SBJ-1 (PRP it))               (VP                 (VBD was)                 (VP                   (VBN issued)                   (NP (-NONE- *-1))                   (PP-TMP (IN in) (NP (CD 1984)))                   (ADVP-TMP (-NONE- *T*-4))))))))                       ... Parsed Sentence   (VP     (VBD was)     (NP-PRD       (NP (DT a) (NN premium))       (: --)       (NP         (NP           (NP (DT an) (JJ additional) (NN amount))           (PP-LOC             (IN on)             (NP               (NP (NN top))               (PP                 (IN of)                 (NP                   (NP (DT the) (NNP CD) (POS 's))                   (NN base)                   (NN value))))))         (CC plus)         (NP (VBN accrued) (NN interest)))       (SBAR         (WHNP-2 (WDT that))         (S           (NP-SBJ (-NONE- *T*-2))           (VP             (VBD represented)             (NP               (NP (DT the) (NNP CD) (POS 's))               (VBN increased)               (NN market)               (NN value)))))))   (. .))   (, ,)   ( NP-SBJ     (NP (NN part))     (PP       (IN of)       (NP         (NP (DT the) (NN price))         (SBAR           (WHNP-3 (-NONE- 0))           (S             (NP-SBJ (NNP Dr.) (NNP Blumenfeld))             (VP (VBD paid) (NP (-NONE- *T*-3))))))))    Peculiarities Complementizers e.g., “that” Gaps *NONE* SBAR SBAR   COMP S E.g., “that *NONE* represented the CD’ market value” tgrep A < B      A  immediately dominates B A << B     A  dominates B A <- B     B   is the last child of A A  <<, B   B   is a leftmost descendant of A A  <<` B   B   is a rightmost descendant of A A  . B     A  immediately precedes B A .. B    A  precedes B A $ B     A  and B are sisters A $. B    A  and B are sisters and A immediately precedes B A $.. B   A  and B are sisters and A precedes B   The use of treebanks Disadvantages A lot more work to annotate 40K+ sentences than to write a grammar. Advantages Statistics about different constituents and phenomena Training systems Evaluating systems Multilingual extensions Introduction to NLP Issues with  Context-free Grammars Agreement Number Chen is/people are Person I am/Chen is Tense Chen was reading/Chen is reading/Chen will be reading Case not in English but in many other languages such as German, Russian, Greek Gender not in English but in many other languages such as German, French, Spanish Combinatorial explosion Many combinations of rules are needed to express agreement S    NP VP S    1sgNP 1sgVP S    2sgNP 2sgVP S    3sgNP 3sgVP … 1sgNP    1sgN ... Subcategorization  frames Direct object The dog ate a sausage Prepositional phrase Mary left the car in the garage Predicative adjective The receptionist looked worried Bare infinitive She helped me buy this place To-infinitive The girl wanted to be alone Participial phrase He stayed crying after the movie ended That-clause Ravi doesn’t believe that it will rain tomorrow Question-form clauses She wondered where to  go Empty (  ) She slept CFG independence assumptions Non-independence All NPs 11% NP PP, 9% DT NN, 6% PRP NPs under S 9% NP PP, 9% DT NN, 21% PRP NPs under VP 23% NP PP, 7% DT NN, 4% PRP (example from Dan Klein) Lexicalized grammars later Conclusions Syntax helps understand the meaning of a sentence. Bob gave Alice a flower Who gave a flower to Alice? What did Bob give to Alice? Context-free grammars are an appopriate representation for syntactic information Dynamic programming is needed for efficient parsing Cubic time to find one parse Still exponential time to find all parses Why? Answer Why does it still take an exponential time to find all parses? Very simple – because the number of parses can be exponential NLP NLP Machine Translation Decoding Decoding Find a translation that maximizes P(F|E)P(E) NP-complete for IBM model 1 Use a phrase translation table (e.g., Koehn’s Pharaoh system, 2004) Use A* search to find the subset of phrase translations that covers the source sentence Combine with beam search Machine Translation Syntax in Machine Translation (includes slides from Philipp Koehn) Notes Bilingual CKY parsing Notes Bilingual CKY parsing Notes Bilingual CKY parsing Notes Bilingual CKY parsing String to Tree Translation (Yamada and Knight 2001) String to Tree Translation (Yamada and Knight 2001) NACLO Problem #1 NACLO Solution #1 Clause restructuring (Collins et al.) Ich   werde   Ihnen  den Report  aushaendigen  …  damit   Sie  den  eventuell   uebernehment   koennen . I will  pass_on   to_you  the report,  so_that  you can adopt that perhaps  verb initial: that perhaps adopt can -> adopt that perhaps can verb second: so that you adopt…can -> so that you can adopt move subject: so that can you adopt -> so that you can adopt particles: we accept the presidency *Particle* -> we accept the presidency (in German, split-prefix phrasal verbs are very common,  e.g., “ anrufen ” -> “ rufen   sie   bitte   noch   einmal  an” – call right back please) Synchronous Grammars Generate parse trees in parallel in two languages using different rules E.g., NP -> ADJ N (in English) NP -> N ADJ (in Spanish) ITG (Inversion Transduction Grammar) [Wu 1995] Don’t allow all permutations in derivations Only <> and [ ] are allowed NACLO problem #2 [Problem by Jonathan May] NACLO problem #2 NACLO problem #2 NACLO solution #2 NACLO solution #2 NLP NLP Introduction to  Natural Language Processing Introduction to NLP Language and Communication Speaker Intention (goals, shared knowledge and beliefs) Generation (tactical) Synthesis (text or speech) Listener Perception Interpretation (syntactic, semantic, pragmatic) Incorporation (internalization, understanding) Both Context (grounding) Basic NLP Pipeline (U) nderstanding  and (G) eneration Computer Language Language Introduction to NLP Examples of Text What NLP is not about Romeo loves Juliet. ZZZZZ is a great stock to buy. What it is about (1/2) After the ball, in what is now called the "balcony scene", Romeo sneaks into the Capulet orchard and overhears Juliet at her window vowing her love to him in spite of her family's hatred of the Montagues. What it is about (2/2) ZZZZZ  Resources  ( NYSE:ZZZZZ)  in their third quarter financials present a picture of a company with a relatively high amount of debt versus shareholder equity, and versus revenues. The company had total liabilities in the third quarter of $4,416 million versus shareholders' equity of only $1,518 million. That is a very high 3 to 1 debt to equity ratio. The company had third quarter revenues of $306 million. On an annualized basis, revenues would come out to $1,224 million. The company's debt level is almost 3 times its annual revenues. And remember, third quarter revenue is from before oil prices dropped in half. It looks like  ZZZZZ  may have bitten off more than it can chew. XXXXX  Petroleum  ( NYSE:XXXXX)  is another company whose third quarter financials present a relatively high debt load. The company had total liabilities in the third quarter of $3,272 million versus shareholder equity of only $1,520 million. That represents a high 2 to 1 debt to equity ratio. The company had third quarter revenues of $350 million. On an annualized basis revenues would come out to $1,400 million. The company's debt is more than 2 times its annual revenue. While  XXXXX  is a very good operator, it looks like they have taken on the high debt strategy at the wrong time. YYYYY  Energy  ( NYSE:YYYYY)  has a relatively high debt load according to their third quarter financials. The company had total liabilities of $2,026 million versus shareholder equity of $1,079. That is almost a 2 to 1 debt to equity ratio. Their third quarter revenues were $207 million. When annualized, their third quarter revenues come out to $827 million. The company's debt is almost 2 1/2 times its annualized revenues, and that is before the collapse of oil prices in the fourth quarter.  YYYYY  has taken the Brigham model to heart and has been aggressively growing the company. Opposition lawmakers in Poland have been occupying parliament for three weeks in a protest against the government. Frantic efforts are now being made to end the crisis before the next plenary session on Wednesday . Almost all of Poland's important party leaders met on Monday at the invitation of the president of the senate to try to find a solution to the parliamentary crisis.  Jaroslaw  Kaczynski, the head of the governing national-conservative Law and Justice Party ( PiS ), also took part. The  PiS  is clearly trying to displace the conflict to a secondary theater - the senate, Poland's upper house of parliament. However,  Grzegorz   Schetyna , the head of the biggest opposition party, the PO, steered clear of what he referred to as an "absurd" meeting. He said that instead he expected the  PiS  speaker of the lower house to "return to the [lower house's] plenary assembly hall." After all, he said, the conflict had not begun in the senate. The leader of the other liberal opposition party, Modern, did take part in the meeting - which goes to show once again how divided the opposition is. Over  the past few weeks, Poland has been caught up in a fight between the national-conservative government and the liberal opposition. The crisis began on December 16 with the announcement by the governing  PiS  that they would be restricting media access to parliament. The fight escalated inside the parliament - among other things, a group of opposition representatives occupied the rostrum in the plenary chamber. The government's parliamentary party left the chamber and "passed" the 2017 budget in an adjacent room. The main bone of contention now is whether this improvised vote, by a show of hands, was legitimate or not. Ultimately, the issue at stake is whether a parliamentary party with a small absolute majority, like the  PiS , has to stick to the rules, or whether it can casually set them aside .  Understanding a News Story http://www.dw.com/en/is-there-a-resolution-to-polands-parliamentary-crisis/a-37070705 Why did I highlight some of the phrases above? Opposition lawmakers in Poland have been occupying parliament for three weeks in a protest against the government. Frantic efforts are now being made to end the crisis before the next plenary session  on Wednesday . Almost all of Poland's important party leaders met on Monday at the invitation of the president of the senate to try to find a solution to  the parliamentary crisis .  Jaroslaw  Kaczynski , the head of the governing national-conservative Law and Justice Party ( PiS ), also took part.  The  PiS  is clearly trying to displace the conflict to a secondary theater - the senate, Poland's upper house of parliament. However,  Grzegorz   Schetyna , the head of the biggest opposition party,  the PO , steered clear of what he referred to as an "absurd" meeting. He said that instead he expected the  PiS  speaker of the lower house to "return to the [lower house's] plenary assembly hall." After all,  he said , the conflict had not begun in the senate. The leader of the other liberal opposition party,  Modern , did take part in the meeting - which goes to show once again how divided the opposition is. Over  the past few weeks, Poland has been caught up in a fight between the national-conservative government and the liberal opposition. The crisis began on December 16 with the announcement by the governing  PiS  that they would be restricting media access to parliament. The fight escalated inside the parliament - among other things, a group of opposition representatives occupied the rostrum in the plenary chamber. The government's parliamentary party left the chamber and "passed" the 2017 budget in an adjacent room. The main bone of contention now is whether this improvised vote, by a show of hands, was legitimate or not. Ultimately, the issue at stake is whether a parliamentary party with a small absolute majority, like the  PiS , has to stick to the rules, or whether it can casually set them aside .  Understanding a News Story Highlighted Phrases on  Wednesday January 11, 2017 the parliamentary  crisis What caused the crisis? Jaroslaw   Kaczyński Former prime minister; brother of the former president the  PO	 Platforma   Obywatelska  (Civic Platform) he  said he =  Grzegorz   Schetyna Modern Nowoczesna   (liberal political party) Genres of Text Blogs, emails, press releases, chats, debates, etc. Each presents different challenges to NLP Plos  ONE DOI: 10.1371/journal.pone.0018780 Named entities +  variants ( human  parainfluenza  virus  type, HPIV-1 ) Speculation ( reported,  suggesting ) Species ( human ) Cell  types ( nasal epithelial cells  ) Facts References Recent advances in molecular genetics have permitted the development of novel virus-based vectors for the delivery of genes and expression of gene products [6,7,8]. These live vectors have the advantage of promoting robust immune responses due to their ability to replicate, and induce expression of genes at high efficiency.  Sendai virus  is a member of the  Paramyxoviridae  family, belongs in the genus  respirovirus  and shares 60–80% sequence homology to  human  parainfluenza  virus type  1 ( HPIV-1 ) [9,10]. The viral genome consists of a negative sense, non-segmented RNA. Although  Sendai viru s was originally isolated from humans during an outbreak of pneumonitis [11] subsequent human exposures to  Sendai virus  have not resulted in observed pathology [12]. The virus is commonly isolated from mouse colonies and Sendai virus infection in mice leads to bronchopneumonia, causing severe pathology and inflammation in the respiratory tract. The sequence homology and similarities in respiratory pathology have made Sendai virus a mouse model for HPIV-1. Immunization with Sendai virus promotes an immune response in non-human primates that is protective against  HPIV-1  [13,14] and clinical trials are underway to determine the efficacy of this virus for protection against HPIV-1 in humans [15]. Sendai virus naturally infects the respiratory tract of mice and  recombinant viruses  have been  reported  to efficiently transduce  luciferase ,  lac Z  and  green fluorescent protein  (GFP) genes in the airways of  mice  or  ferrets  as well as primary  human   nasal epithelial cells  [16] . These data support the hypothesis that intranasal ( i.n .) immunization with a recombinant Sendai virus will mediate heterologous gene expression in mucosal tissues and induce antibodies that are specific to a recombinant protein. A major advantage of a recombinant Sendai virus based vaccine is the observation that recurrence of  parainfluenza  virus  infections is common in humans [12,17]  suggesting  that anti-vector responses are limited, making repeated administration of such a vaccine possible. Medical Records http://www.upassoc.org/upa_publications/jus/2009february/smelcer5.html   Literary Texts Project Gutenberg ( http://www.gutenberg.org/browse/scores/top ) A team of horses passed from  Finglas  with toiling plodding tread, dragging through the funereal silence a creaking  waggon  on which lay a granite block. The  waggoner  marching at their head saluted. Ulysses -  http://www.gutenberg.org/files/4300/4300-h/4300-h.htm   There was no possibility of taking a walk that day.  We had been wandering, indeed, in the leafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no company, dined early) the cold winter wind had brought with it clouds so  sombre , and a rain so penetrating, that further out-door exercise was now out of the question.  Jane Eyre -  http://www.gutenberg.org/files/1260/1260-h/1260-h.htm   Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt  Em , who was the farmer's wife. Their house was small, for the lumber to build it had to be carried by wagon many miles. There were four walls, a floor and a roof, which made one room; and this room contained a rusty looking  cookstove , a cupboard for the dishes, a table, three or four chairs, and the beds. Uncle Henry and Aunt  Em  had a big bed in one corner, and Dorothy a little bed in another corner. There was no garret at all, and no cellar--except a small hole dug in the ground, called a cyclone cellar, where the family could go in case one of those great whirlwinds arose, mighty enough to crush any building in its path. It was reached by a trap door in the middle of the floor, from which a ladder led down into the small, dark hole. The Wizard of Oz -  http://www.gutenberg.org/files/55/55-h/55-h.htm   A Really Long Literary Sentence Try parsing this “Bloat is one of the co-tenants of the place, a  maisonette  erected last century, not far from the Chelsea Embankment, by Corydon  Throsp , an acquaintance of the  Rossettis ' who wore hair smocks and liked to cultivate pharmaceutical plants up on the roof (a tradition young  Osbie  Feel has lately revived), a few of them hardy enough to survive fogs  and frosts, but most returning, as fragments of peculiar alkaloids, to rooftop earth, along with manure from a trio of prize  Wessex  Saddleback sows quartered there by  Throsp's  successor, and dead leaves off many decorative trees transplanted to the roof by later tenants, and the odd  unstomachable  meal thrown or vomited there by this or that sensitive epicurean-all got  scumbled  together, eventually, by the knives of the seasons, to an impasto, feet thick, of unbelievable black topsoil in which anything could grow, not the least being bananas.“ Do you know the source? Quiz Answer “Gravity’s Rainbow” (by Thomas Pynchon), known for its use of very arcane words and complicated sentence (and plot) structure. Another such work is “ Finnegans  Wake” by James Joyce. Poetry is even more difficult. Introduction to NLP Funny Sentences Silly Sentences Children make delicious snacks Stolen painting found by tree I saw the Rockies flying to San Francisco Court to try shooting defendant Ban on nude dancing on Governor’s desk Red tape holds up new bridges Government head seeks arms Cameron wins on budget, more lies ahead Local high school dropouts cut in half Hospitals are sued by seven foot doctors Dead  e xpected to rise Miners refuse to work after death Patient at death's door - doctors pull him through In America a woman has a baby every 15 minutes.  How does she do that ? Ambiguous Recommendations Ambiguous Recommendations Ambiguous Recommendations From Beatrice Santorini’s collection NLP NLP Introduction to NLP Text Generation Basic NLP Pipeline (U) nderstanding  and (G) eneration Language One Definition of NLG Natural language generation is the process of deliberately constructing a natural language text in order to meet specified communicative goals. 												[McDonald 1992] What is NLG? Mapping meaning to text Stages: Content selection Discourse structure Sentence  structure Lexical choice Linguistic realization Examples Weather forecasts Directions Train schedules Descriptions of objects Content Selection What is important? What content should be presented to the user Depends on the domain, the audience, communicative goal Semantic representation Discourse Structure Rhetorical structure Order in which to present the information Lexical Choice What words to use Paraphrases Different syntactic structures Sentence Structure Linking words e.g., therefore, and, but Aggregation  You have two messages from Jason and one from Claire Linguistic Realization Grammaticality Agreement Morphology Adjective order Articles (a/an) Relative clauses Referring expressions Presentation constraints Media Length User background Genre constraints Extralinguistic  issues Layout  Choices NLG is about choices Content Coherence Style Media Syntax Aggregation Referring expressions Lexical  choice Syntax Evaluation NLP NLP Introduction to NLP Smoothing and Interpolation Smoothing If the vocabulary size is |V|=1M Too many parameters to estimate even a unigram model MLE assigns values of 0 to unseen (yet not impossible) data Let alone bigram or trigram models Smoothing (regularization) Reassigning some probability mass to unseen data Smoothing How to model novel words? Or novel bigrams? Distributing some of the probability mass to allow for novel events Add-one (Laplace) smoothing:  Bigrams: P(w i |w i-1 ) = (c(w i-1 ,w i )+1)/(c(w i-1 )+V) This method reassigns too much probability mass to unseen events Possible to do add- k  instead of add-one Both of these don’t work well in practice Advanced Smoothing Good-Turing Try to predict the probabilities of unseen events based on the probabilities of seen events Kneser-Ney Class-based n-grams Example Corpus: cat dog cat rabbit mouse fish  fish  mouse hamster  hamster  fish turtle tiger cat rabbit cat dog  dog  fox lion What is the probability the next item is “mouse”? P MLE  (mouse) = 2/20 What is the probability the next item is “elephant” or some other previously unseen animal? Trickier Is it 0/20? Note that P (that the next animal is unseen) > 0 Therefore we need to discount the probabilities of the animals that have already been seen P MLE  (mouse) < 2/20 Good Turing Idea Estimate the frequencies of unseen events based on the events seen once Actual counts c N r  = number of n-grams that occur exactly c times in the corpus N 0  = total number of n-grams in the corpus Revised counts c* c* = (c+1) N c+1 / N c Example Corpus: cat dog cat rabbit mouse fish  fish  mouse hamster  hamster  fish turtle tiger cat rabbit cat dog  dog  fox lion Counts C(cat) = 4 C(dog) = 3 C(fish) = 3 C(mouse) = 2 C(rabbit) = 2 C(hamster) = 2 C(fox) = 1 C(turtle) = 1 C(tiger) = 1 C(lion) = 1 N 1 =4, N 2 =3, N 3 =2, N 4 =1 Example (cont’d) N 1 =4, N 2 =3, N 3 =2, N 4 =1 Revised counts c* = (c+1) N c+1 / N c   (or best-fit Power Law estimate if counts are unreliable) C*(cat) = 4  C*(dog) = (3+1) x 1/2 = 2 C*(mouse) = (2+1) x 2/3 = 2 C*(rabbit) = (2+1) x 2/3 = 2 C*(hamster) = (2+1) x 2/3 = 2 C*(fox) = (1+1) x 3/4 = 6/4 C*(turtle) = (1+1) x 3/4 = 6/4 C*(tiger) = (1+1) x 3/4 = 6/4 C*(lion) = (1+1) x 3/4 = 6/4 C*(elephant) = N1/N = 4/20 Note that these counts don’t necessarily add to 1, so they still need to be normalized. P*(lion) = 6/4 / 20 = 6/80 More information http://beyondexpectations.quora.com/An-Intuitive-Explanation-of-Good-Turing-Smoothing   Dealing with Sparse Data Two main techniques used Backoff Interpolation Backoff Going back to the lower-order n-gram model if the higher-order model is sparse (e.g.,  frequency <=  1) Learning the parameters From a development data set Interpolation If P’(w i |w i-1 ,w i-2 ) is sparse: Use  λ 1 P’(w i |w i-1 ,w i-2 ) + λ 2 P’(w i |w i-1 )+ λ 3 P’( w i ) Ensure that  λ 1 + λ 2 + λ 3 =1,  λ 1 , λ 2 , λ 3 ≤1,  λ 1 , λ 2 , λ 3 ≥0 Better than  backoff Estimate the hyper-parameters  λ 1 , λ 2 , λ 3  from held-out data (or using EM), e.g., using 5-fold cross-validation See [Chen and Goodman 1998] for more details Software: http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html [slide from Michael Collins] NLP NLP Introduction to NLP Semantic Role Labeling Syntactic Variation Last week, Min broke the window with a hammer. The window was broken with a hammer by Min last week With a hammer, Min broke the window last week Last week, the window was broken by Min with a hammer Min broke the window The window broke The window was broken with a hammer Semantic Role Labeling Determining  who  did what to whom when where why h ow Uses Question answering Machine translation Text summarization Case Theory (Fillmore 1968) Agent Actor of an action The musician  performed a new piece Patient Entity affected by the action Samantha hurt  her hand Instrument Tool used in performing action Min broke the window  with a hammer Beneficiary Entity for whom action is performed The mother bought ice cream  for the children Source Origin of the affected entity I got the book  from my friend Destination Destination of the affected entity Using syntactic information Syntactic information “by X” for agent “with X” for instrument Exceptions “by car” “with pleasure” SRL task Input The teacher gave the test to the students in the morning. Output [The teacher] AGENT  gave [the test] OBJ  to [the students] RECIP  [in the morning] TMP . Illinois Demo http:// cogcomp.cs.illinois.edu/page/demo_view/SRL   Formatted Output [Carreras and Marquez 2005] FrameNet FrameNet Berkeley Chuck Fillmore https://framenet.icsi.berkeley.edu/   PropBank PropBank U. Colorado Martha Palmer http://verbs.colorado.edu/~mpalmer/projects/ace.html Arg0 usually agent Arg1 usually patient/theme 13 labels for Adjuncts (Time, Location, Manner)  PropBank Example Roleset id: break.01 ,  break, cause to not be  whole Roles :          Arg0 :  breaker  ( vnrole : 23.2-agent, 40.8.3-1-experiencer, 45.1-agent)           Arg1 :  thing broken  ( vnrole : 23.2-patient1, 40.8.3-1-patient, 45.1-patient)           Arg2 :  instrument  ( vnrole : 45.1-instrument)           Arg3 :  pieces  ( vnrole : 23.2-patient2)  Example: just transitive         Stock prices rallied as the Georgia-Pacific bid broke the  market's recent  gloom.          Arg0 : the Georgia-Pacific bid          Rel : broke          Arg1 : the market's recent gloom PropBank Example Example : with instrument         John broke the window with a rock.          Arg0 : John          Rel : broke          Arg1 : the window          Arg2 : with a rock Example: with pieces         John broke the window into a million pieces.          Arg0 : John          Rel : broke          Arg1 : the window          Arg3 : into a million pieces Example: inchoative         The window broke into a million pieces.          Arg1 : The window          Rel : broke          Arg3 : into a million pieces Papers Gildea   and  Jurafsky  2002 Xue  and Palmer  2004 Punakyanok  et al. 20 04 Pradhan et  al.  20 04 Yi and  Palmer 2005 Marquez et al.  2005 Haghighi  et al . 2005 Approaches Selectional restrictions instruments should be tools (e.g., *not* “with pleasure”) agents and beneficiaries should be animate (e.g., not “for a reason”) Use WordNet “the teacher” is a person is animate Parse node classification Features Used (1) Phrase type Governing category Parse tree path (e.g., N ↑ NP ↑ S ↓ VP ↓ V) Position (e.g., does the phrase precede or follow the predicate) Voice Head word Subcategorization Argument Set Argument  Order List from Palmer,  Gildea , and  Xue  2010 Features Used (2) Previous Role Head Word Part of Speech Named Entities in Constituents Verb Clustering Head Words of Objects of PPs First/Last Word/POS in Constituent Constituent Order Constituent Tree Distance Temporal Cue Words List from Palmer,  Gildea , and  Xue  2010 Results CONLL Shared task (since 2004) Best performance over 80% F1 measure NLP NLP Introduction to NLP Bayes’ Theorem Bayes’ Theorem Formula for joint probability p(A,B) = p(B|A)p(A) p(A,B) = p(A|B)p(B) Therefore p(B|A ) = p(A|B)p(B )/p(A) Bayes’ theorem is used to calculate P(A|B) given P(B|A) Example Diagnostic test Test accuracy p(positive | disease) = 0.05           – false positive p(negative | disease) = 0.05            – false negative So:  p(positive | disease) = 1-0.05 = 0.95 Same for p(negative |   disease) In general the rates of false positives and false negatives are different Example Diagnostic test with errors Example What is p(disease | positive)? P(disease|positive) = P(positive|disease)*P(disease)/P(positive) P(disease|positive) = P(positive| disease)*P(disease)/P(positive) P( disease|positive )/P( disease|positive ) = ? We don’t really care about p(positive) as long as it is not zero, we can divide both sides by this quantity Example P( disease|positive ) / P(disease|positive) =  	(P( positive|disease ) x P(disease))/(P(positive|disease) x P(disease)) Suppose P(disease) = 0.001   so P( disease) = 0.999 P( disease|positive ) / P(disease|positive) = (0.95 x 0.001)/(0.05 x 0.999) 	=0.019 P( disease|positive ) + P(disease|positive) = 1 P(disease|positive) ≈ 0.02 Notes P(disease) is called the prior probability P(disease|positive) is called the posterior probability In this example the posterior is 20 times larger than the prior Example p(well)=0.9, p(cold)=0.05, p(allergy)=0.05 p( sneeze|well )=0.1 p( sneeze|cold )=0.9 p( sneeze|allergy )=0.9 p( cough|well )=0.1 p( cough|cold )=0.8 p( cough|allergy )=0.7 p( fever|well )=0.01 p( fever|cold )=0.7 p( fever|allergy )=0.4 Example from Ray Mooney Example (cont’d) Features: sneeze, cough, no fever P( well|e )=(.9) * (.1)(.1)(.99) / p(e)=0.0089/p(e) P( cold|e )=(.05) * (.9)(.8)(.3) / p(e)=0.01/p(e) P( allergy|e )=(.05) * (.9)(.7)(.6) / p(e)=0.019/p(e) P(e) = 0.0089+0.01+0.019=0.379 P( well|e )=.23 P( cold|e )=.26 P( allergy|e )=.50 Bayes’ Theorem Hypothesis space: H={H 1  ,   …,   H n }		Evidence: E If we want to pick the most likely hypothesis H*,  we can drop P(E) In text classification: H: class space; E: data (features) [slide from  Qiaozhu  Mei] Getting to Statistics ... We are flipping an unfair coin, but P(Head)=?  (parameter estimation) If we see the results of a huge number of random experiments, then  But, what if we only see a small sample (e.g., 2)? Is this estimate still reliable? We flip twice and got two tails, does it mean P(Head) = 0? In general, statistics has to do with drawing conclusions on the whole population based on observations of a sample (data) [slide from  Qiaozhu  Mei] Parameter Estimation General setting: Given a (hypothesized & probabilistic) model that governs the random experiment The model gives a probability of any data p(D| ) that depends on the parameter  Now, given actual sample data X={x 1 ,…,x n },  what can we say about the value of ? Intuitively, take your best guess of  “best” means “best explaining/fitting the data” Generally, this is an optimization problem [slide from  Qiaozhu  Mei] Maximum Likelihood vs. Bayesian Maximum likelihood estimation “Best” means “data likelihood reaches maximum” Problem: small sample Bayesian estimation “Best” means being consistent with our “prior” knowledge and explaining data well Problem: how to define the prior? [slide from  Qiaozhu  Mei] Posterior:  p( |X)  p(X| ) p( )  [slide from  Qiaozhu  Mei] Bayesian Estimation Example: An Unfair Die It’s more likely to get a 6 and less likely to get a 1 p(6) > p(1) How likely? What if you toss the die 1000 times,  and observe “6” 501 times,  “1” 108 times? p (6) = 501/1000 = 0.501 p(1) = 108/1000 = 0.108 As simple as counting, but principled – maximum likelihood estimate [slide from  Qiaozhu  Mei] What if the Die has More Faces? Suitable to represent documents Every face corresponds to a word in vocabulary The author tosses a die  to write a word Apparently, an unfair die [slide from  Qiaozhu  Mei] NLP NLP Text Similarity The Vector Space Model Vectors, Matrices, and Tensors X = <x 1 , x 2 , …, x n >:  a vector of n dimensions.  x 1 , …, x n  can take either binary values {0, 1}, or real values Vectors  and  matrices provide  a natural way to represent the occurrence of words in a document/query.  In text analysis, n is usually the size of the vocabulary, so each dimension corresponds to a unique word X can be used to represent a document, or a query, or … So x i  indicates either “whether the  i-th  word in the vocabulary appears” (binary value), or “how many times does the  i-th  word appear” (real value).  The entire collection is thus represented as a matrix.  How? Example of Document Vectors Doc 1= “information retrieval” Doc 2 = “computer information retrieval” Doc 3 = “computer retrieval” Vocabulary: information, retrieval, computer Doc 1 = <1, 1, 0> Doc 2 = <1, 1, 1> Doc 3 = <0, 1, 1> Question: Doc  4 =  “retrieval information retrieval” ? D =  information, retrieval, computer Documents in a Vector Space Doc 1= “information retrieval” Doc 2 = “computer information retrieval” Doc 3 = “computer retrieval” Vocabulary: information, retrieval, computer Doc 1 = <1, 1, 0> Term  1: information Term  3: computer Term  2: retrieval Doc 1 Relevance as Vector Similarities Doc 1= “information retrieval” Doc 2 = “computer information retrieval” Doc 3 = “computer retrieval” Doc 1 Doc 2 Doc 3 Which document is closer to Doc 1? Doc 2 or Doc 3? What if we have a query “retrieval”? Term  3: computer Term  2: retrieval Term  1: information Document Similarity Used in information retrieval to determine which document ( d 1  or  d 2 ) is more similar to a given query  q . Documents and queries are represented in the same space. Angle (or cosine) is a proxy for similarity between two vectors Distance/Similarity Calculation The similarity/relevance of two vectors can be calculated based on distance/similarity measures S: X, Y   (0, 1) X: <x 1 , x 2 , …, x n > Y: <y 1 , y 2 , …, y n > S(X, Y) = ? The more dimensions in common, the larger the similarity What about real values? Normalization needed. Similarity Measures Similarity Measures X Y Z Euclidean Distance – distance of two points Similarity Measures (Cont.) X Y Which one do you think is suitable for retrieval? Jaccard? Euclidean? Cosine?  Cosine similarity: similarity of two vectors, normalized Example What is the cosine similarity between: D= “ cat,dog,dog ” = <1,2,0> Q= “ cat,dog,mouse,mouse ” = <1,1,2> Answer: In comparison: Quiz Given three documents D 1   = < 1,3> D 2   = < 10,30> D 3   = <3,1> Compute the cosine scores σ (D 1 ,D 2 )  σ (D 1 ,D 3 ) What do the numbers tell you? Answers to the Quiz σ (D 1 ,D 2 ) = 1 one of the two documents is a scaled version of the other σ (D 1 ,D 3 ) = 0.6 swapping the two dimensions results in a lower similarity Quiz What is the range of values that the cosine scores can take? Answer to the Quiz Mathematically, the cosine function has a  range of [-1,1] However, when the two vectors are both in the first quadrant (since all word counts are non-negative), the range is [0,1] For word embeddings, the range is [-1,1] (since the values don’t have to be non-negative) NLP NLP Introduction to NLP Collocations Collocations (phrases) Dictionary definitions Meaning of words in isolation “Know  a word by the company that it  keeps” Firth 1935 Examples dead end strong  tea Benazir Bhutto Fabry disease Collocations Properties Common use No general syntactic or semantic rules Important for non-native speakers Collocation acquisition Important for NLP Types of  Multiword Sequences Idioms Free-word combinations  Collocations Examples Idioms To kick the bucket Dead end To catch up Collocations To trade actively Table of contents Orthogonal projection Free-word combinations To take the bus The end of the road To buy a house Properties Arbitrariness: substitutions are usually not allowed: Make an effort vs. *make an exertion Running commentary vs. *running discussion Commit treason vs. *commit treachery Language- and dialect-specific R égler  la circulation = direct traffic Russian, German, Serbo-Croatian: direct translation  of regulate is  used American English:  set the table, make a decision British English:  lay the table, take a decision “ semer  le  désarroi ” - “to sow disarray” - “to wreak havoc” Common in technical language Recurrent in context Uses Disambiguation ( e.g , “bank”/“ loan”,“river ”) Translation Generation Types of Collocations Grammatical come to, put on; afraid that, fond of, by accident, witness to Semantic only certain synonyms Flexible find/discover/notice by chance Base- Collocator  Pairs Base – bears most of the meaning of the collocation. Writers think of the base first. Foreign language speakers search by base. For decoding purposes, it is more appropriate to store the collocation under the  collocator . Extracting collocations Most-common bigrams? Drop function words? Look at POS sequences? Extracting collocations Mutual information Larger means stronger What if I(x;y) = 0? no relation What if I( x;y ) < 0? complementary distribution (rare) Yule’s coefficient A - frequency of pairs involving both W and X B - frequency of pairs involving W only C - frequency of pairs involving X only D - frequency of pairs involving neither -1    Y    1 Example Example from the  Hansard  corpus (Brown, Lai, and Mercer) – “prime” Flexible and rigid collocations Example (from Smadja): “free” and “trade” Xtract (Smadja) The Dow Jones Industrial Average The NYSE’s composite index of all its listed common stocks fell *NUMBER* to *NUMBER* Translating Collocations Examples: Brush  up a lesson,  repasser   une   le ç on Bring about/ осуществлять Hansards examples  late spring fin du  printemps Atlantic Canada Opportunities Agency Agence  de promotion  économique  du Canada  atlantique Links Sample phrasal collocations http://en.wiktionary.org/wiki/Appendix:Collocations_of_do,_have,_make,_ and_take    List of English language idioms http:// en.wikipedia.org/wiki/List_of_English-language_idioms     Idiomsite http :// www.idiomsite.com       NLP NLP Introduction to NLP K-Nearest Neighbors K-Nearest Neighbor Classifier Keep all training examples Find k examples that are most similar to the new document (“neighbor” documents) Assign the category that is most common in these neighbor documents (neighbors vote for the category) Can be improved by considering the distance of a neighbor ( A closer neighbor has more influence) Example of K-NN Classifier ? Bias-Variance Tradeoff http://scott.fortmann-roe.com/docs/BiasVariance.html Example http://scott.fortmann-roe.com/docs/BiasVariance.html Example http://scott.fortmann-roe.com/docs/BiasVariance.html Example http://scott.fortmann-roe.com/docs/BiasVariance.html Example (n=1) http://scott.fortmann-roe.com/docs/BiasVariance.html Example (n=15) http://scott.fortmann-roe.com/docs/BiasVariance.html Example (n=65) http://scott.fortmann-roe.com/docs/BiasVariance.html k-NN Demo http://sleepyheads.jp/apps/knn/knn.html K-Nearest Neighbor Classifier Advantages No training needed Can be applied to any distance measure and document representation Empirically effective Disadvantages Finding nearest neighbors has high time complexity Imprecise when the number of examples is small, which is often true in high-dimensional spaces (neighbors cannot be trusted) NLP NLP Introduction to NLP Hidden Markov Models Markov Models Sequence of random variables that aren’t independent Examples  Weather reports Text Stock market numbers Properties Limited horizon: P(X t+1  = s k |X 1 ,…, X t ) = P(X t+1  =  s k |X t ) Time invariant (stationary) P(X t+1  =  s k |X t )  =  P(X 2 =s k |X 1 ) Definition: in terms of a transition matrix A and initial state probabilities  . Example start Visible MM P(X 1 ,…X T ) = P(X 1 ) P(X 2 |X 1 ) P(X 3 |X 1 ,X 2 ) … P(X T |X 1 ,…,X T-1 )                                    = P(X 1 ) P(X 2 |X 1 ) P(X 3 |X 2 ) … P(X T |X T-1 ) P(d, a, b) = P(X 1 =d) P(X 2 =a|X 1 =d) P(X 3 =b|X 2 =a)               = 1.0 x 0.7 x 0.8               = 0.56 Hidden MM Motivation Observing a sequence of symbols The sequence of states that led to the generation of the symbols is hidden The states correspond to hidden (latent) variables Definition Q = states O = observations, drawn from a vocabulary q 0 ,q f  = special (start, final) states A = state transition probabilities B = symbol emission probabilities   = initial state probabilities µ = (A,B,  ) = complete probabilistic model Hidden MM Uses Part of speech tagging Speech recognition Gene sequencing Hidden Markov Model (HMM) Can be used to model state sequences and observation sequences Example: P( s , w ) =    i  P(s i |s i-1 )P( w i |s i ) Generative Algorithm Pick start state from     For t = 1..T Move to another state based on A Emit an observation based on B State Transition Probabilities Emission Probabilities P( O t = k|X t =s i ,X t+1 = s j ) =  b ijk All Parameters of the Model Initial P( G|start ) = 1.0, P( H|start ) = 0.0 Transition P(G|G) = 0.8, P(G|H) = 0.6, P(H|G) = 0.2, P(H|H) = 0.4 Emission P( x|G ) = 0.7, P( y|G ) = 0.2, P( z|G ) = 0.1 P( x|H ) = 0.3, P( y|H ) = 0.5, P( z|H ) = 0.2 Observation sequence “ yz ” Starting in state G (or H), P( yz ) = ? Possible sequences of states: GG GH HG HH P( yz ) = P( yz|GG ) + P( yz|GH ) + P( yz|HG ) + P( yz|HH ) = 	= .8 x .2 x .8 x .1  	+ .8 x .2 x .2 x .2 	+ .2 x .5 x .4 x .2 	+ .2 x .5 x .6 x .1 	= .0128+.0064+.0080+.0060 =.0332 States and Transitions An HMM is essentially a weighted finite-state transducer The states encode the most recent history The transitions encode likely sequences of states e.g.,  Adj -Noun or Noun-Verb  or perhaps Art- Adj -Noun Use MLE to estimate the probabilities Another way to think of an HMM It’s a natural extension of Naïve Bayes to sequences Emissions Estimating the emission probabilities Harder than transition probabilities There may be novel uses of word/POS combinations Suggestions It is possible to use standard smoothing As well as heuristics (e.g., based on the spelling of the words) Sequence of Observations The observer can only see the emitted symbols Observation likelihood Given the observation sequence S and the model   =  (A,B, ), what is the probability P(S|) that the sequence was generated by that model. Being able to compute the probability of the observations sequence turns the HMM into a language model Tasks with HMM Given    = (A,B,  ), find P(O|  ) Uses the Forward Algorithm Given O,  , find (X 1 ,…X T+1 ) Uses the Viterbi Algorithm Given O and a space of all possible  1..m , find model  i  that best describes the observations Uses Expectation-Maximization Inference Find the most likely sequence of tags, given the sequence of words t* =  argmax t  P( t|w ) Given the model µ, it is possible to compute P ( t|w ) for all values of t In practice, there are way too many combinations Greedy Search Beam Search  One possible solution Uses partial hypotheses At each state, only keep the k best hypotheses so far May not work Viterbi Algorithm Find the best path up to observation  i  and state s Characteristics Uses dynamic programming Memoization Backpointers HMM Trellis HMM Trellis P( G,t =1) P( G,t =1) =  P(start) x P( G|start ) x P( y|G ) start end H H H H G G G G y z start start start end end end HMM Trellis . P( H,t =1) P( H,t =1) =  P(start) x P( H|start ) x P( y|H ) start end H H H H G G G G y z start start start end end end HMM Trellis . P( H,t =2) =  max (P( G,t =1) x P(H|G) x P( z|H ), 	 P( H,t =1) x P(H|H) x P( z|H)) P( H,t =2) start end H H H H G G G G y z start start start end end end HMM Trellis . P( H,t =2) start end H H H H G G G G y z start start start end end end HMM Trellis . start end H H H H G G G G y z start start start end end end HMM Trellis . P( end,t =3) start end H H H H G G G G y z start start start end end end HMM Trellis . P( end,t =3) P( end,t =3) =  max (P( G,t =2) x P( end|G ), 	 P( H,t =2) x P( end|H )) start end H H H H G G G G y z start start start end end end HMM Trellis . P( end,t =3) P( end,t =3) = best score for the sequence Use the  backpointers  to find the sequence of states. P( end,t =3) =  max (P( G,t =2) x P( end|G ), 	 P( H,t =2) x P( end|H )) Some Observations Advantages of HMMs Relatively high accuracy Easy to train Higher-Order HMM The previous example was about bigram HMMs How can you modify it to work with trigrams? How to compute P(O) Viterbi was used to find the most likely sequence of states that matches the observation What if we want to find all sequences that match the observation We can add their probabilities (because they are mutually exclusive) to form the probability of the observation This is done using the Forward Algorithm The Forward Algorithm Very similar to Viterbi Instead of  max  we use  sum Source: Wikipedia NLP NLP Parsing Prepositional Phrase Attachment  (2/3) Supervised Learning: Evaluation Prepare the experimental data Manually label a set of instances. Split  the labeled data into training and  testing sets. Use the training data to find patterns. Apply these patterns on the testing data set. For evaluation:  use  accuracy  (the percentage of correct labels that a given algorithm has assigned on the testing data). Compare with a simple baseline   method. What is the simplest baseline method? Answer The simplest supervised baseline method is to find the more common class (label) in the training data and assign it to  all  instances of the testing data set. Algorithm 1 Random baseline A  random  unsupervised baseline would have been to label each instance in the testing data set with a random label, 0 or 1.  Practically, random performance is the lower bound against which any non-random methods should be compared. Measuring the accuracy of the supervised baseline method (Algorithm 1) In the official training data set (RRR94), the rate  of occurrence of the 1 label  (low attachment) is  52.2% ( 10,865/20,801). Is the accuracy of this baseline method then equal to 52.2%? Answer No, this is not how accuracy is computed.  It has to be computed on the testing set, not the training set. Using the official split, the accuracy of this method on  the testing set is 59.0% ( 1,826/3,097).  One shouldn’t think of this number as a good result.  The difference (+6.8% going from training to testing) could have been in the opposite direction, resulting in a performance below random. Observations If the baseline method is simple and if the testing set is randomly drawn from the full data set and the data set is large enough, one could expect that the accuracy on the testing set is comparable to the one on the training set.  Note that the PTB data set is drawn from business news stories. If one were to train a method on this data and test it on a different set of sentences, e.g., from a novel, it is possible that the two sets will have very different characteristics. The more complicated the method is, however, the more likely it will be that it will “ overfit ” the training data, learning patterns that are too specific to the training data itself and which may not appear in the testing data or which may be associated with the opposite class in the testing data. Upper bounds on accuracy The 52% accuracy we’ve seen so far is our current lower bound.  Now, what is the upper bound? Usually, human performance is used for the upper bound. For PP attachment, using the four features mentioned earlier, human accuracy is around 88%. So, a hypothetical algorithm that achieves an accuracy of 87% is in fact very close to the upper bound (on a scale from 52% to 88%, it is 97% of the way to the upper bound). Using linguistic knowledge One way to beat the two baselines is to use linguistic information.  For example, the preposition “of” is much more likely to be associated with low attachment than high attachment. In the training data set, this number is an astounding 98.7% (5,534/5,607) Therefore the feature  prep_of  is very valuable.  What are the two main reasons? Answer Reason 1 I t is very informative (98.7% of the time it is linked with the low attachment class) Reason 2 I t is very frequent (27.0% of the  entire training set -  5607/20801). Reason 1 alone would not be sufficient! Sidebar 1/3 Evaluation pipeline The PTB (Penn Treebank) data set has  been used for competitive evaluation of pp attachment algorithms since 1994.  Each  new algorithm is allowed to look at the training and development sets and use any knowledge extracted from them.  The  test set data can never be looked at and can be used only once per algorithm  for  its evaluation.  Doing  the contrary (repeatedly tuning a new algorithm based on its performance on the designated test set) results in a performance level that is irreproducible on new data and such approaches are not allowed in NLP research.  Note  that the development set can be used in a well-specified way as part of the training process. Sidebar 2/3 Let’s  look at the training data then and see if we can learn some patterns that would help us improve over the silly “label everything as noun attachment” baseline and its 52% expected accuracy.  For  example, some prepositions tend to favor particular attachments.  Let’s  start with “against”.  It  appears 172 times in the training set, of which 82 (48%) are noun attached, the rest (52%) being high attached.  This  ratio (48:52) is very similar to the baseline (52:48), so clearly, knowing that the preposition is “against” gives us very little new information.  Sidebar 3/3 Cont’d Furthermore , the total occurrence of “against” in the training corpus is rather small (less than 1% of the prepositional phrases).  In two special cases, however, the identity of the preposition gives a lot of information.  At one extreme, “of” is associated with low attachment in 99% of the cases, whereas at the other end of the scale, “to” is associated with high attachment in 82% of its appearances.  It is also important to note that both of these prepositions are fairly common in the training set (“to” representing 27% of all prepositions and “of” accounting for another 11% of them).  With this knowledge, we can build a very simple decision list algorithm (Brill and  Resnik ) that looks like this (the rules are sorted by their expected accuracy on their majority class ). Are there any other such features? Yes,  prep_to :  2,182/2,699 = 80.8% in favor of high attachment. Which leads us to our next algorithm: Algorithm 2 Sidebar 1/2 Let’s compare the performance of this simple decision list algorithm with that of the baseline.  On  the training set, the first rule would fire in  5,577  cases, of which  5,527  will be correctly labeled as low attachment and another 50 will be incorrectly labeled as high attachment (the accuracy of this rule is expected to be 99% on the training set).  The  second rule (with an expected  accuracy of 82% on the training set) would result in  2,672  decisions, of which  2,172  will be correctly processed as high attachment and 500 will be mislabeled as low attachment.  Finally, the default rule will kick in, whenever the first two rules are not applicable. In this case, it will apply on all remaining phrases (20,801 – 5,577 – 2,672 = 12,552 cases).  Specifically, among these, it will result in 4,837 phrases correctly labeled as low and 7,714 incorrectly labeled as high.  Sidebar 2/2 Overall, we have 5,527 + 2,172 + 4,837 = 12,536 correct decisions, or an accuracy of 12,536/20,801 = 60%.  Clearly Algorithm 2 outperforms Algorithm 1 on the training data.  This is not surprising, since its expected accuracy should be no less than the worst expected accuracy of its rules (that of rule 3) and it is likely to be higher than that lowest number because of the priority given to the easier to classify cases (rules 1 and 2 ). More complicated algorithms can look at additional  features E.g.,  the nouns or the verb, or some combination thereof. For example, the verb “dropped” appears 75/81 times (93%) along with high attachment and only 7% with low attachment. NLP NLP Introduction to NLP Probabilities 2/2 The Chain Rule P(w 1 ,w 2 ,w 3 … w n ) = ? Using the chain rule: P(w 1 ,w 2 ,w 3 … w n ) =P(w 1 ) P(w 2 |w 1 ) P(w 3 |w 1 ,w 2 )… P(w n |w 1 ,w 2 …w n-1 ) This rule is used in many ways in statistical NLP, more specifically in Markov  Models Independence Two  events are independent when  P(A B) =  P(A)P(B) Unless  P(B)=0 this is equivalent to saying that P(A) = P(A|B) If two events are not independent, they are considered dependent Adding vs. Removing Constraints Adding constraints P(walk=yes|weather=nice) P(walk=yes|weather=nice,freetime=yes,crowded=yes) More accurate But more difficult to estimate Removing constraints ( Backoff ) P(walk=yes|weather=nice,freetime=yes,crowded=yes) P(walk=yes|weather=nice,freetime=yes) P(walk=yes|weather=nice) Note that it is  not  possible to do  backoff  on the left hand side of the conditional [Example modified from Jason Eisner] X:       R n Random Variables Simply a function : The numbers are generated by a  stochastic process  with a certain probability  distribution Example the  discrete random variable X that is the sum of the faces of two randomly thrown  fair dice Probability mass function ( pmf ) which gives the probability that the random variable has different numeric values: P( x ) = P(X =  x )  = P(A x ) where A x  = {         : X(  ) =  x } Random Variables If a random variable X is distributed according to the  pmf  p( x ),  then  we write X  ~   p( x ) For a discrete random variable, we  have S p ( x i ) =  P (  ) = 1 Six-sided Fair Die p(1) = 1/6 p(2) = 1/6 etc. P ( D )=? P ( D ) = {1/6, 1/6, 1/6, 1/6, 1/6, 1/6} P ( D |odd) = {1/3, 0, 1/3, 0, 1/3, 0} [slide from Brendan O’Connor] [slide from Brendan O’Connor] NLP NLP Introduction to NLP Finite-State Automata (2/2)  (slides from  Jurafsky  and Martin ) NFSA - Figure 2.17     Epsilon transitions - Figure 2.18     Three tenets of NFSA Backup Lookahead Parallelism Figure 2.19     LIFO - Figure 2.20     FIFO - Figure 2.21     Regular languages Closures Notes about regular languages NFSA and DFSA are equivalent. How? Converting  regexps  into automata … Base case - Figure 2.22     Figure 2.23     Figure 2.24     Figure 2.25     Assignment 2.8 - Figure 2.26     Write a regular language for this NFSA. The FSM toolkit and friends Developed at AT&T Research Riley, Pereira,  Mohri ,  Sproat 4 useful parts FSM,  Lextools , GRM, Dot (separate) Introduction to NLP Finite State Transducers FSA for adjectives Big, bigger, biggest Cool, cooler, coolest, coolly Red, redder, reddest Clear, clearer, clearest, clearly, unclear, unclearly Happy, happier, happiest, happily Unhappy, unhappier, unhappiest, unhappily What about: unbig, redly, and realest? Irregular plurals Notes Is a string a legitimate word or not? Two-level morphology: lexical level + surface level (Koskenniemi 83) Finite-state transducers (FST) – used for regular relations Inversion and composition of FST Transducers for transliteration Exercise FST for English plurals Weights? アメフト 		 	 amefuto 	 	 Ame ( rican ) Foot(ball) アイスクリーム 	 	 aisu   kurīmu 	ice cream アイドル 		 	 aidoru 	 	idol アパート 		 	 apāto 	 	apartment バイク 		 	 baiku 	 	bike バリアフリー 	 	 bariafurī 	 	barrier  free コンピューター 	 	 konpyūtā 	 	computer デスク 		 	 desuku 	 	desk  (at a news agency, e.g., the Sports desk) ラマ 		 	 dorama 	 	drama  (on TV) エレベーター 	 	 erebētā 	 	elevator エスカレーター 	 	 esukarētā 	 	escalator フライドポテト 	 	 furaidopoteto 	fried potato (French fries) グラス 		 	 gurasu 	 	glass  (for drinking) ハッピーエンド 	 	 happīendo 	happy end( ing ) ハンカチ 		 	 hankachi 	 	handkerchief ホットケーキ 	 	 hottokēki 	 	hotcake  (pancake) カシューナッツ 	 	 kashū   nattsu 	cashew nut コーヒー 		 	 kōhī 	 	coffee クラブ 		 	 kurabu 	 	club キーボード 	 	 kībōdo 	 	keyboard キャンペーン 	 	 kyanpēn 	 	campaign キャップ 		 	 kyappu 	 	cap パソコン 		 	 pāsokon 	 	 perso ( nal ) com( puter ) パーソナルコンピューター 	 pāsonaru   konpyūtā 	personal computer ペーパーテスト 	 	 pēpātesuto 	paper (written) test レジュメ 		 	 rejume 	 	resume レストラン 	 	 resutoran 	 	restaurant リモコン 		 	 rimokon 	 	 remo ( te ) con( trol ) サラダ 		 	 sarada 	 	salad タバコ 		 	 tabako 	 	tobacco テレビゲーム 	 	 terebigēmu 	television (video) game ゼミナール 	 	 zemināru 	 	seminar Transliteration Conjugation NLP NLP Machine Translation Basic Techniques Automatic Translation Systran Google Translate Default system: 100 languages, 200M users daily https:// research.googleblog.com/2006/04/statistical-machine-translation-live.html   Newer (neural) system https:// research.googleblog.com/2016/09/a-neural-network-for-machine.html   Amazon, Microsoft, etc. Translation as Decoding “One  naturally wonders if the problem of translation could conceivably be treated as a problem in cryptography. When I look at an article in Russian, I say: 'This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.'  “ Warren Weaver, “Translation (1955)” Question for the audience If you know (some) French Look at the next two slides They contain a recipe in English and its automatic translation to French by Google Can you identify the problems in the translation? Ingredients  Original recipe makes 4 - 6 servings 2 cups heavy whipping cream                                 1 tablespoon chopped fresh basil  1 tablespoon chopped fresh thyme  2 teaspoons salt  2 teaspoons ground black pepper  1 1/2 teaspoons crushed red pepper flakes  1 teaspoon ground white pepper  1 cup chopped green onions  1 cup chopped parsley  1/2 pound shrimp, peeled and deveined  1/2 pound scallops  1/2 cup shredded Swiss cheese  1/2 cup grated Parmesan cheese  1 pound dry fettuccine pasta  Directions Cook pasta in a large pot of boiling salted water until al dente. Meanwhile, pour cream into large skillet. Cook over medium heat, stirring constantly, until just about boiling. Reduce heat, and add herbs, salt, peppers, onions, and parsley. Simmer 7 to 8 minutes, or until thickened. Stir in seafood, cooking until shrimp is no longer transparent. Stir in cheeses, blending well. Drain pasta. Serve sauce over noodles. http://translate.google.com/   ingrédients   Recette originale fait 4 - 6 portions  2 tasses de crème épaisse à fouetter  1 cuillère à soupe de basilic frais haché  De thym frais 1 cuillère à soupe  2 cuillères à café de sel  2 cuillères à café de poivre noir moulu  1 1/2 cuillères à café écrasés de flocons de piment rouge  Sol 1 cuillère à café de poivre blanc  1 tasse oignons verts hachés  1 tasse de persil haché  1/2 livre de crevettes, décortiquées et  déveinées   1/2 escalopes  de livres   1/2 tasse de fromage râpé suisse  1/2 tasse parmesan râpé  £ 1 sec  pâtes  fettucine    instructions   Cuire les pâtes dans une grande casserole d'eau bouillante salée jusqu'à ce qu'elles soient al dente. Pendant ce temps, versez la crème dans une grande poêle. Cuire à feu moyen, en remuant constamment, jusqu'à ce que presque ébullition. Réduire le feu et ajoutez les herbes, sel, poivrons, oignons et persil. Laisser mijoter 7 à 8 minutes, ou jusqu'à ce que le mélange épaississe. Incorporer les fruits de mer, cuisson jusqu'à ce que les crevettes ne soit plus transparente. Incorporer le fromage en mélangeant bien. Égoutter les pâtes. Servir la sauce sur les nouilles. Answer Cuire  les pâtes dans une grande casserole d'eau bouillante salée jusqu'à ce qu'elles soient al dente. Pendant ce temps, versez la crème dans une grande poêle. Cuire à feu moyen, en remuant constamment,  jusqu'à ce que presque ébullition .  Réduire  le feu et  ajoutez  les herbes, sel, poivrons, oignons et persil. Laisser mijoter 7 à 8 minutes, ou jusqu'à ce que le mélange épaississe.  Incorporer  les fruits de mer,  cuisson  jusqu'à ce que les crevettes ne soit plus  transparente . Incorporer le fromage en mélangeant bien. Égoutter les pâtes. Servir la sauce sur les nouilles. Vauquois’s  Triangle (F) oreign (I) nterlingua (E) nglish F E I Basic Strategies of MT Direct Approach 50’s,60’s  naïve the flesh is weak, but the spirit is strong out of sight, out of mind Indirect: Transfer Indirect: Interlingua No looking back Language-neutral No influence on the target language F E I Basic Strategies of MT Example: This is a blue house Direct Approach translate each word separately doesn’t work well across word orders Syntactic Transfer Eng  ( adj  noun)  →  Fr (noun  adj ) Interlingua  h: House(h)  ∧  Blue(h) String-to-String Translation Phrase-Based Translation Tree-to-Tree Translation Tree-to-String Translation NLP NLP Introduction to NLP Machine Translation Multilingual Users Content languages  for websites            Percentage  of Internet users by language http:// en.wikipedia.org/wiki/Global_Internet_usage   [The Tower of Babel,  by Pieter  Bruegel  the  Elder, 1563] Genesis  11:1-9  http://www.ancientegypt.co.uk/writing/rosetta.html   Carved in 196 BC in Egypt Deciphered by Champollion in 1822 Mixture of Egyptian (hieroglyphs and Demotic) and Greek The Rosetta Stone English-Cebuano Bible Example In the beginning  God  created the  heaven  and the  earth . Sa  sinugdan   gibuhat   sa  Dios  ang   mga   langit   ug   ang   yuta .  And   God   called  the firmament  Heaven .  Ug   gihinganlan   sa  Dios  ang   hawan   nga   Langit .  And   God   called  the dry land  Earth   Ug   ang   mamala   nga   dapit   gihinganlan   sa  Dios  nga   Yuta   use: co-occurrence, word order, cognates corpora are needed  sentence alignment needs to be done first http://en.wikipedia.org/wiki/Bible_translations_by_language   NACLO Problem http:// nacloweb.org/resources/problems/2012/N2012-C.pdf http:// nacloweb.org/resources/problems/2012/N2012-CS.pdf   Problem  by  Simon  Zwarts , based on work by Kevin Knight www.nacloweb.org Arcturan  Problem – 1/4 Arcturan  Problem – 2/4 Arcturan  Problem – 3/4 Arcturan  Problem – 4/4 Arcturan  Solution – 1/3 Arcturan  Solution – 2/3 Arcturan  Solution – 3/3 Parallel Corpora The Rosetta Stone The  Hansards   Corpus The Bible Europarl Hansards  Example English <s id=960001> I would like the government and the Postmaster General to agree that we place the union and the Postmaster General under trusteeship so that we can look at his books and records, including those of his management people and all the memos he has received from them, some of which must have shocked him rigid. <s id=960002> If the minister would like to propose that, I for one would be prepared to support him. French <s id=960001> Je  voudrais   que  le  gouvernement  et le  ministre  des  Postes   conviennent  de placer le  syndicat  et le  ministre  des  Postes  sous  tutelle   afin   que  nous  puissions  examiner  ses  livres et  ses  dossiers, y  compris   ceux  de  ses   collaborateurs , et  tous  les  mémoires   qu'il  a  reçus   d'eux ,  dont   certains   l'ont   sidéré . <s id=960002> Si le  ministre   voulait  proposer  cela , je  serais  pour ma part  disposé  à  l'appuyer . Language Differences (1/3) [Example from  Jurafsky  and Martin] Language Differences  (2/3 ) Word  order in phrases  (Fr.) la  maison   bleue , the blue house Word order in sentences (Jap.) I like to drink coffee watashi   wa          kohii   o      nomu   no  ga              suki   desu        I- subj      coffee- obj    drink- dat - rheme      like vocabulary (Sp.) wall pared,  muro phrases (Fr.) play pi è ce de  th éâ tre   Language Differences  (3/3 ) Prepositions  (Jap.) to Mariko, Mariko- ni Inflection (Sp.) have:  tengo ,  tienes ,  tenemos ,  tienen ,  tener    Lexical distinctions (Sp.): the bottle floated out - la  botella   salió   flotando Brother (Jap.) otooto  (younger),  oniisan  (older) They (Fr.) elles  (feminine),  ils  (masculine) NLP IR Information Retrieval Query Modification and Relevance Feedback Query Modification Also known as query reformulation, query substitution … Problem:  initial query may not be the most appropriate to satisfy a given information  need Idea: modify the original query so that it gets closer to the right documents in the vector space Types of Query Modification Morphological: Spelling check Semantic: Query expansion Query substitution Query suggestion Original Query Query Expansion Query substitution Spelling error correction Query suggestion Spelling Error Correction Roughly  10-15% of  the queries sent to search engines contain errors . ( Cucerzan  and Brill 2004) Traditional techniques rely on dictionary match, combined with Common keyboard mistakes Phonetic  mistakes Context mistakes Cognitive mistakes Modern techniques rely on query log analysis + string similarity Query Reformulation – Spelling Correction Query Recommendation/Suggestion Recommend alternative queries to the user. Alternative queries could be totally different from the original query.  Usually done with query log analysis Query Expansion Refining the information need of search by adding new terms to query Sometimes also remove terms … Traditional methods:  Thesaurus-based expansion Ontology based expansion Hyponyms and  hypernyms Corpus-based methods Mining related terms from large scale corpus Feedback: most effective method in IR Query log based methods (later) Query Expansion Query Expansion Corpus-based: mine query logs NLP-based Vector-space relevance feedback Relevance Feedback Problem: initial query may not be the most appropriate to satisfy a given information need. Idea: modify the original query so that it gets closer to the right documents in the vector space Intuition in Feedback Query expansion: feedback can help discover related query terms Query = “information retrieval”  Relevant or pseudo-relevant docs may would likely share words related to “information retrieval”, e.g., “search engine”, “search”, “user”, “query”, etc.  These words generally have higher frequency in these relevant or pseudo-relevant documents than in the whole collection They can be used to expand the original query to increase recall and sometimes also precision A Machine Learning Interpretation Machine learning/pattern recognition Query = a  special  labeled example Relevant documents = labeled examples (supervised learning) Pseudo-relevant documents = unlabeled examples  (semi-supervised learning)  Overview of Feedback Techniques  Feedback as machine learning: many possibilities Standard ML: Given examples of relevant (and non-relevant) documents, learn how to classify a new document  as either “relevant” or “non-relevant”. “Modified” ML: Given a query and examples of relevant (and non-relevant) documents, learn how to rank new documents based on relevance Challenges:  Sparse data  Censored sample How to deal with query?  Modeling noise in pseudo feedback (as semi-supervised learning) Overview of Feedback Techniques (Cont.)  Feedback as query expansion: traditional IR  Step 1: Term selection Step 2: Query expansion Step 3: Query term re-weighting Traditional IR is still robust (Rocchio), but ML approaches can potentially be more accurate  Relevance Feedback in the Vector Space Model Basic setting: Learn from examples Positive examples: docs known to be relevant Negative examples: docs known to be non-relevant How do you learn from this to improve performance? General method: Query modification Adding new (weighted) terms Adjusting weights of old terms Doing both The most well-known and effective approach is Rocchio  [Rocchio 1971] + Rocchio Feedback: Illustration q + + + + + + + + + + + + + + + - - - - - - - - - - - - - - - - - - - - - - - - - - - + + + Rocchio Feedback:  Intuition Query is represented as a vector (VSM) Query can be updated by adding document vectors to query vectors If a document is labeled as relevant – add it to the query vector If a document is labeled as irrelevant – subtract it from the query vector Weighting needed.  Rocchio Feedback: Formula Original query  vector Relevant  docs Non-rel docs Parameters New  query vector Relevance Feedback Automatic Manual Method: identifying feedback terms Q’ = a 1 Q + a 2 R - a 3 N Often a 1  = 1 ,  a 2  = 1/|R| and a 3  = 1/|N| Example Q = “safety minivans” D 1  = “car safety minivans tests injury statistics” - relevant D 2  = “liability tests safety” - relevant D 3  = “car passengers injury reviews” - non-relevant R = ? N  = ? Q’ = ? Rocchio in Practice Negative (non-relevant) examples are not very important (why?) Often project the vector onto a lower dimension (i.e., consider only a small number of words that have high weights in the centroid vector) (efficiency concern) Avoid “training bias” (keep relatively high weight on the original query weights) (why?) Can be used for relevance feedback and pseudo feedback Usually robust and effective Pseudo Relevance Feedback Automatic query expansion Thesaurus-based expansion (e.g., using latent semantic indexing – later…) Distributional similarity Query log mining Examples Book : publication, product, fact, dramatic composition, record  Computer : machine, expert, calculator, reckoner, figurer  Fruit : reproductive structure, consequence,  product Politician : leader, schemer  Newspaper : press, publisher, product, paper, newsprint  Distributional clustering: Lexical semantics (Hypernymy): Book : autobiography, essay, biography, memoirs, novels Computer :   adobe, computing, computers, developed, hardware Fruit : leafy, canned, fruits, flowers, grapes Politician : activist, campaigner, politicians, intellectuals, journalist Newspaper :  daily, globe, newspapers,  newsday , paper Examples (query logs) Book:  booksellers bookmark  blue Computer:  sales notebook stores  shop Fruit: recipes cake salad basket company Games: online play  gameboy  free video Politician: careers federal office history Newspaper: online website college information Schools: elementary high ranked yearbook California:  berkeley  san  francisco  southern French: embassy dictionary learn [Otterbacher et al. HLT EMNLP 2005] IR NLP Introduction to NLP Search Engine Architecture Search Engine Architecture Decide what to index Collect it Index it (efficiently) Keep the index up to date Provide user-friendly query facilities Search Engine Architecture Document Representations Term-document matrix (m  x  n) Document-document matrix (n  x  n) Typical example in a medium-sized collection n=3,000,000 documents m=50,000 terms Typical example on the Web n=30,000,000,000 m=1,000,000 Boolean vs. integer-valued matrices Storage Issues Example Imagine a medium-sized collection with n=3,000,000 and m=50,000 How large a term-document matrix will be needed? Is there any way to do better?  Any heuristic? Inverted Index Instead of an incidence vector, use a posting table VERMONT:  D1, D2, D6 MASSACHUSETTS: D1, D5, D6, D7 Use linked lists to be able to insert new document postings in order and to remove existing postings. Can be used to compute document frequency Keep everything sorted! This gives you a logarithmic improvement in access. Basic operations on inverted indexes Conjunction (AND) iterative merge of the two postings: O( x+y ) Disjunction (OR) very similar Negation (NOT) can we still do it in O( x+y )?  Example: VERMONT AND NOT MASSACHUSETTS Example: MASSACHUSETTS OR NOT VERMONT Recursive operations Optimization start with the smallest sets The Vector Model Queries as Documents Advantages: Mathematically easier to manage Problems: Different lengths Syntactic differences Repetitions of words (or lack thereof) Vector queries Each document is represented as a vector Non-efficient representation Dimensional compatibility The matching process Document space Matching is done between a document and a query (or between two documents) Distance vs. similarity measures. Euclidean distance (define) Manhattan distance (define) Word overlap Jaccard  coefficient S imilarity Measures The Cosine measure (normalized dot product) The  Jaccard  coefficient Exercise Compute the cosine scores   (D 1 ,D 2 )  (D 1 ,D 3 )  for the documents D 1  = <1,3> D 2  = <100,300> D 3  = <3,1> Compute the corresponding Euclidean distances, Manhattan distances, and  Jaccard  coefficients. Phrase-based Queries Examples “ New   York   City ” “Ann Arbor” “Barack Obama” We don’t want to match York  is a  city  in  New  Hampshire Positional Indexing Keep track of all words and their positions in the documents To find a multi-word phrase, look for the matching words appearing next to each other Document Ranking Compute the similarity between the query and each of the documents Use cosine similarity Use TF*IDF weighting Return the top K matches to the user IDF: Inverse document frequency Motivation Example N : number of documents d k : number of documents containing term  k f ik : absolute frequency of term  k  in document  i  w ik : weight of term  k  in document  i  idf k  =  log 2 ( N / d k ) + 1 = log 2 N  - log 2 d k  + 1 NLP NLP Introduction to NLP Examples of Text Classification Who wrote which Federalist papers? 1787-8: anonymous essays try to convince New York to ratify U.S Constitution:    Jay, Madison, Hamilton.   Authorship of 12 of the letters in dispute 1963: solved by  Mosteller  and Wallace using Bayesian methods James Madison Alexander Hamilton Male or female author? By  1925 present-day Vietnam was divided into three parts under French colonial rule. The  southern  region embracing Saigon and the Mekong delta was the colony of Cochin-China; the  central  area with its imperial capital at Hue was the protectorate of  Annam… Clara  never failed to be astonished by the extraordinary felicity of her own name. She found it  hard  to trust herself to the mercy of fate, which had managed over the years to convert  her greatest  shame into one of her greatest  assets… S.  Argamon , M. Koppel, J. Fine, A. R.  Shimoni , 2003. “Gender, Genre, and Writing Style in Formal Written Texts,” Text, volume 23, number 3, pp. 321–346 Positive or negative movie review? unbelievably  disappointing  Full of  zany characters and richly applied satire, and some great plot  twists  this is the greatest screwball comedy ever  filmed  It was pathetic. The worst part about it was the boxing scenes. What is the subject of this article? Antogonists  and Inhibitors Blood Supply Chemistry Drug Therapy Embryology Epidemiology … MeSH  Subject Category Hierarchy ? MEDLINE Article Well-known Datasets 20 newsgroups http://qwone.com/~jason/20Newsgroups/   Reuters-21578 http://www.daviddlewis.com/resources/testcollections/reuters21578/   Cats: grain, acquisitions, corn, crude, wheat, trade… WebKB http://www-2.cs.cmu.edu/~webkb/   course, student, faculty, staff, project,  dept , other RCV1 http://www.daviddlewis.com/resources/testcollections/rcv1 / Larger Reuters corpus NLP NLP Python NLP External Links https :// www.coursera.org/learn/python   https :// www.coursera.org/specializations/python   http ://www.tutorialspoint.com/python /   http ://www.learnpython.org /   NLP NLP Introduction to NLP Sentiment Lexicons Sentiment Lexicons SentiWordNet http://sentiwordnet.isti.cnr.it/ General Inquirer 2,000 positive words and 2,000 negative words http://www.wjh.harvard.edu/~inquirer/   LIWC http://www.liwc.net/   MPQA subjectivity lexicon http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ General Inquirer Annotations Strong  Power Weak Submit Active Passive Pleasur Pain Feel  Arousal EMOT  Virtue Vice Ovrst Undrst Academ Doctrin Econ@ Exch  ECON Exprsv  Legal Milit Polit@ POLIT Relig Role COLL Work  Ritual SocRel  Race Kin@ MALE Female Nonadlt HU ANI PLACE  Social Region  Route Aquatic Land Sky Object Tool Food Vehicle  BldgPt ComnObj  NatObj BodyPt ComForm COM Say Need Goal Try  Means Persist  Complet Fail NatrPro Begin Vary Increas Decreas Finish  Stay Rise  Exert Fetch Travel Fall Think Know Causal Ought  Perceiv Compare  Eval@ EVAL Solve Abs@ ABS Quality Quan NUMB  ORD CARD FREQ DIST Time@ TIME Space POS DIM Rel COLOR Self Our You Name Yes No Negate Intrj IAV DAV SV  IPadj IndAdj PowGain PowLoss PowEnds PowAren PowCon PowCoop  PowAuPt PowPt  PowDoct PowAuth PowOth PowTot RcEthic RcRelig RcGain RcLoss  RcEnds RcTot  RspGain RspLoss RspOth RspTot AffGain AffLoss AffPt AffOth  AffTot WltPt  WltTran WltOth WltTot WlbGain WlbLoss WlbPhys WlbPsyc WlbPt  WlbTot EnlGain  EnlLoss EnlEnds EnlPt EnlOth EnlTot SklAsth SklPt SklOth  SklTot TrnGain  TrnLoss TranLw MeansLw EndsLw ArenaLw PtLw Nation Anomie  NegAff PosAff  SureLw If NotLw  TimeSpc http://www.webuse.umd.edu:9090/tags Positive: able, accolade, accuracy, adept, adequate… Negative: addiction, adversity, adultery, affliction, aggressive… Dictionary-based Methods Start from known seeds e.g., happy, angry Expand using WordNet synonyms hypernyms Random-walk based methods words with known polarity as absorbing boundary Automatic Extraction of Sentiment Words Semi-supervised methods Vasileios   Hatzivassiloglou  and Kathleen R.  McKeown . 1997. Predicting the Semantic Orientation of Adjectives. ACL, 174–181 Molistic NACLO problem (2007) PMI (Turney) PMI=pointwise mutual information Check how often a given unlabeled word appears with a known positive word (“excellent”) Same for a known negative word (“poor”) Datasets http://www.cs.cornell.edu/people/pabo/movie-review-data / http://www.cs.jhu.edu/~mdredze/datasets/sentiment/   http ://help.sentiment140.com/other-resources NLP NLP Introduction to NLP Evaluation of Classification Evaluation of text classification Microaveraging   average over classes Macroaveraging   uses pooled table Well-known Datasets 20 newsgroups http://qwone.com/~jason/20Newsgroups/   Reuters-21578 http://www.daviddlewis.com/resources/testcollections/reuters21578/   Cats: grain, acquisitions, corn, crude, wheat, trade… WebKB http://www-2.cs.cmu.edu/~webkb/   course, student, faculty, staff, project,  dept , other RCV1 http://www.daviddlewis.com/resources/testcollections/rcv1 / Larger Reuters corpus The 2-by-2 contingency table Precision and  Recall Precision : % of selected items that are  correct  Recall : % of correct items that are  selected Pick a card… A combined measure: F A combined measure that assesses the P/R tradeoff is F measure (weighted harmonic mean): The harmonic mean is a very conservative average; see  IIR  § 8.3 People usually use balanced F1 measure   i.e., with    = 1 (that is,   = ½):   		      F  = 2 PR /( P + R ) Common categories (#train, #test) Classic  Reuters-21578 Data Set   Earn (2877, 1087)   Acquisitions (1650, 179)  Money- fx  (538, 179)  Grain (433, 149)  Crude (389, 189)  Trade (369,119)  Interest (347, 131)  Ship (197, 89)  Wheat (212, 71)  Corn (182, 56) Most (over)used data  set, 21,578 docs (each 90 types, 200  toknens ) 9603 training, 3299 test articles ( ModApte /Lewis split) 118 categories An article can be in more than one category Learn 118 binary category distinctions Average document (with at least one category) has 1.24 classes Only  about 10 out of 118 categories are  large Confusion matrix c For each  pair of classes <c 1 ,c 2 > how many documents from  c 1   were incorrectly assigned to  c 2 ? c 3,2 : 90 wheat documents incorrectly assigned to poultry Micro- vs. Macro-Averaging If  we have more than one class, how do we combine multiple performance measures into one quantity? Macroaveraging : Compute performance for each class, then average. Microaveraging : Collect decisions for all classes, compute contingency table, evaluate. Which Classifier Works the Best? SVM gives the best performance Discriminative approaches tend to be more effective than generative approaches, but in general, the difference between different classifiers is not so significant as that between different feature extraction methods Need to consider other factors (e.g., efficiency, interpretability) NLP NLP Recent Research Natural Language Processing *** http:// googleresearch.blogspot.com/2016/05/announcing-syntaxnet-worlds-most.html   http://metamind.io/research/new-neural-network-building-block-allows-faster-and-more-accurate-text-understanding /   https:// research.googleblog.com/2016/11/zero-shot-translation-with-googles.html https :// research.googleblog.com/2016/08/meet-parseys-cousins-syntax-for-40.html NLP NLP Introduction to NLP Relation  Extraction Relation Extraction Person-person ParentOf ,  MarriedTo , Manages Person-organization WorksFor Organization-organization IsPartOf Organization-location IsHeadquarteredAt Relation Extraction Core NLP task Used for building knowledge bases, question answering Input Mazda North American Operations  is headquartered in   Irvine, Calif. , and oversees the sales, marketing, parts and customer service support of Mazda vehicles in the United States and Mexico through nearly 700 dealers.  Output IsHeadquarteredIn  (Mazda North American Operations, Irvine) Relation extraction Using patterns Regular expressions Gazetteers Supervised learning Semi-supervised learning Using seeds Relation Extraction The ACE  Evaluation 2002 newspaper data Entities:  Person, Organization, Facility, Location, Geopolitical Entity Relations:  Role, Part, Located, Near, Social The ACE Evaluation Semantic Relations Extracting IS-A Relations Hearst’s patterns X and other Y X or other Y Y such as X Y, including X Y, especially X Example Evolutionary relationships between the platypus and other mammals Hypernym Extraction (Hearst) Supervised Relation Extraction Look for sentences that have two entities that we know are part of the target relation Look at the other words in the sentence, especially the ones between the two entities Use a classifier to determine whether the relation exists Example English Beethoven   was born  in December  1770  in Bonn Born  in Bonn in  1770 ,  Beethoven  … After his  birth  on December 16,  1770 ,  Beethoven  grew up in a musical family Ludwig van Beethoven  ( 1770 -1827) While this evidence supports the case for 16 December  1770  as  Beethoven 's  date of birth Example  (Non-English ) German Ludwig van Beethoven  wurde  am 17.  Dezember   1770  in Bonn  getauft Ludwig van Beethoven  wurde  in Bonn, 15.  Dezember   1770 ,  eine   Familie   ursprünglich   aus  Brabant in  Belgien   geboren Der  Geburtstag  von  Ludwig van Beethoven  wurde   im  Winter  1770  in Bonn  nicht   genau   dokumentiert Spanish Ludwig van Beethoven  nació   en  Bonn el 17 de  diciembre  de  1770 Nacido   en  Bonn  1770 ,  Beethoven  … Ludwig van Beethoven ,  nace   en   diciembre  de  1770 Semi-supervised Relation Extraction Start with some seeds, e.g., Beethoven   was born  in December  1770  in Bonn Look for other sentences with the same words Look for expressions that appear nearby Look for other sentences with the same expressions Bootstrapping Bootstrapping Bootstrapping Evaluating Relation Extraction Precision P correctly extracted relations/all extracted relations Recall R correctly extracted relations/all existing relations F1 measure F1 = 2PR/(P+R) If there is no annotated data only measure precision  NLP NLP Word  Embeddings Deep Learning What Is the Feature Vector  x ? Typically a vector representation of a single character or word Often reflects the  context  in which that word is found Could just do counts, but that leads to sparse vectors Commonly used techniques:  word2vec  or  GloVe  word embeddings https://code.google.com/p/word2vec/   includes the models and pre-trained embeddings Pre-trained is good, because training takes a lot of data	 Gensim : Python library that works with word2vec https://radimrehurek.com/gensim / Embeddings  Are Magic, Part 1 Image courtesy of  Jurafsky  & Martin Embeddings  Are Magic, Part 2 GloVe  vectors for comparative and superlative adjectives http://nlp.stanford.edu/projects/glove/images/comparative_superlative.jpg More Examples Examples from Richard  Socher Skip-grams Predict each neighboring word  in a context window of 2 C  words  from the current word . E.g., for  C=2, we are given word  w t  and predicting these 4 words: Skip-grams learn 2  embeddings  for each w input  embedding  v,  in the input matrix  W Column  i  of the input matrix  W  is the 1× d  embedding  v i   for word  i   in the vocabulary.  output  embedding  v ′, in output matrix W’ Row  i   of the output matrix  W ′  is a  d  × 1 vector embedding  v ′ i   for word  i   in the  vocabulary. [ Jurafsky  & Martin] Setup Walking through corpus pointing  at  word  w ( t ), whose index in the vocabulary is  j , so we’ll call it  w j   ( 1 <  j  < | V |).  Let’s predict  w ( t +1),  whose index in the vocabulary is  k  (1 <  k  < | V  |). Hence our task is to compute  P ( w k | w j ).  Slide courtesy of  Jurafsky  & Martin One-hot vectors A vector of length |V|  Example: [0,0,0,0,1,0,0,0,0…….0] CBOW and  skipgram  ( Mikolov  2013)  w i-2 w i-1 w i+1 w i+2 w i Skip-gram Slide courtesy of  Jurafsky  & Martin Skip-gram h =  v j o =  W’h o =  W’h Slide courtesy of  Jurafsky  & Martin Notes Sparse vs. dense vectors 100,000 dimensions vs. 300 dimensions <10 non-zero dimensions vs. 300 non-zero dimensions Dense vectors Semantic similarity (cf. LSA) Similarity Computation Softmax Evaluating  Embeddings Nearest Neighbors Analogies (A:B): :(C:?) Information Retrieval Semantic Hashing Similarity Data Sets [Table from  Faruqui  et al. 2016] [ Mikolov  et al. 2013] Semantic Hashing [ Salakhutdinov  and Hinton 2007] WEVI (Xin  Rong ) https://ronxin.github.io/wevi /   eat|apple ,  eat|orange ,  eat|rice ,  drink|juice ,  drink|milk ,  drink|water ,  orange|juice ,  apple|juice ,  rice|milk ,  milk|drink ,  water|drink ,  juice|drink Embeddings for Word Senses [ Rothe  and  Schuetze  2015] Non-compositionality BLACK CAT = BLACK + CAT BLACK MARKET    BLACK + MARKET Notes Word  embeddings  perform matrix factorization of the co-occurrence matrix Word2vec is a simple feed-forward neural network Training is done using backpropagation using SGD Negative sampling for training NLP NLP Natural Language Processing Dialogue https:// www.youtube.com/watch?v=kTcRRaXV-fg https:// en.wikipedia.org/wiki/Who's_on_First%3F  – credit attributed to Michael J.  Musto  and/or Irving Gordon  “Who’s on first ” Abbott   You know, strange as it may seem, they give ball players nowadays very peculiar names...Now, on the Cooperstown team we have Who's on first, What's on second, I Don't Know is on third-  Costello   That's what I want to find out. I want you to tell me the names of the fellows on the Cooperstown team.  Abbott   I'm telling you. Who's on first, What's on second, I Don't Know is on third.  Costello   You know the fellows' names?  Abbott   Yes.  Costello   Well, then, who's  playin ' first?  Abbott   Yes.  Costello   I mean the fellow's name on first base.  Abbott   Who.  Costello   The fellow's name on first base for Cooperstown.  Abbott   Who.  Costello   The guy on first base.  Abbott   Who is on first base.  Costello   Well, what are you asking me for?  Abbott   I'm not asking you--I'm telling you. Who is on first.  [Example from Erdogan 2001] Natural Language Dialogue Example What makes dialogue different Turn-taking Default turn-taking rule Only take a turn at a relevant place (e.g., a pause, after a question) Barge-in Prosody i ntonation, emphasis Sidebar: Emphasis  Example Try saying this sentence seven times, each time with an emphasis on a different word: I never said she stole my money. Conversational  Implicature Example: How can I help you? I am looking for a Thai restaurant. Implicature Meaningful inferences that the listener can make Grice’s Maxims Maxim of quantity make your contribution informative but not more than needed Maxim of quality do not say what you believe is false do not say that for which you lack evidence Maxim of relevance Maxim of manner avoid ambiguity avoid obscurity be brief be orderly Speech Acts Assertives suggesting, putting forward, swearing, boasting, concluding Directives asking, ordering, requesting, inviting, advising, begging Commissives promising, planning, vowing, betting, opposing Expressives thanking, apologizing, welcoming, deploring Declarations I resign, you’re fired. Example from  Jurafsky  and Martin Notes Dialogue Act Recognition Partially Observable Markov Decision Processes (POMDP) Reinforcement Learning NACLO: Grice’s  Grifter  Gadgets http://www.naclo.cs.cmu.edu/problems2013/N2013-Q.pdf http:// www.naclo.cs.cmu.edu/problems2013/N2013-QS.pdf Author Jordan  Boyd-Graber Solution NLP NLP Parsing Prepositional Phrase  Attachment Penn Treebank representation ( (S      (NP-SBJ        (NP (NNP Pierre) (NNP  Vinken ) )       (, ,)        (ADJP          (NP (CD 61) (NNS years) )         (JJ old) )       (, ,) )     (VP (MD will)        (VP (VB  join )          (NP (DT the) (NN  board ) )         (PP-CLR (IN  as )            (NP (DT a) (JJ nonexecutive) (NN  director ) ))         (NP-TMP (NNP Nov.) (CD 29) )))     (. .)  )) Penn Treebank representation (  (S      (NP-SBJ (NNP Mr.) (NNP  Vinken ) )     (VP (VBZ  is )        (NP-PRD          (NP (NN  chairman ) )         (PP (IN  of )            (NP              (NP (NNP  Elsevier ) (NNP N.V.) )             (, ,)              (NP (DT the) (NNP Dutch) (VBG publishing) (NN group) )))))     (. .) )) Prepositional phrase attachment join board  as director  High (verbal): Low (nominal): is chairman  of Elsevier  Jane NNP VBD DT NN caught the butterfly NP VP NP S with the net DT NN NP IN PP Examples Examples: Lucy’s plane leaves Detroit  on Monday . - high Jenna met Mike  at the concert . - high This painting must cost millions  of dollars . -  low Examples High  or low attachment? Alicia ate spaghetti  from Italy . Alicia ate spaghetti  with meatballs . Alicia ate spaghetti  with a fork . Alicia ate spaghetti  with Justin . Alicia ate spaghetti  with delight . Alicia ate spaghetti  on Friday . Solution High or low attachment? Alicia ate spaghetti  from Italy . - low Alicia ate spaghetti  with meatballs . - low Alicia ate spaghetti  with a fork . - high Alicia ate spaghetti  with Justin . - high Alicia ate spaghetti  with delight . - high Alicia ate spaghetti  on Friday . -  high Actual Headline Police shoot man with box cutters.        (S (NP (N Police)) (VP (V shoot) (NP (N man) (PP (P with) (NP (N box) (N cutters)))))) (?)  (S (NP (N Police)) (VP (V shoot) (NP (N man)) (PP (P with) (NP (N box) (N cutters))))) Prepositional Phrase Attachment Input a prepositional phrase and the surrounding context Output a binary label: 0(high) or 1(low) In practice the context consists only of four words: the preposition, the verb before the preposition, the noun before the preposition, and the noun after the preposition Example:  join board as director Why? Answer Because almost all the information needed to classify a prepositional phrase’s attachment as high or low is contained in these four features. Furthermore, using only these  tuples  of four features allows for a consistent and  scaleable  approach. Sample Tuples Sidebar (1/2) The  linguistics (and psycholinguistics) literature offers competitive explanations for attachment.  One  theory (Kimball 1973) favors the so-called  right association  rule. It says that, given a new phrase and two choices for attachment, people tend to attach the new phrase with the more recent (“rightmost” within the sentence) of the candidate nodes, resulting in low attachment.  Alternatively , the  minimal attachment  principle (Frazier 1978) favors an attachment that results in the syntax tree having fewer additional syntactic nodes (in this case, favoring high attachment).  As  one can see from the statistics, none of these methods alone can explain the high prevalence of both types of attachment. Sidebar (2/2) Some observations  can be made  using statistical  analysis of  the  training set.  The  standard corpus used for this sort of analyses comes from (RRR 1994) and includes 27,937 prepositional phrases extracted from the Penn Treebank (Marcus et al. 1993), divided into three groups (20,801 training, 4039 development, and 3097  test).  This  data representation makes the assumption that additional context is only  marginally  more useful for classification purposes compared to the four features in the table (verb, noun1, preposition, and noun2).  For  comparison, the sentence matching the  data  point  “bring  attention to problem” is actually “Although preliminary findings were reported more than a year ago, the latest results appear in today's New England Journal of Medicine, a forum likely to bring new attention to the problem.” It is unlikely that the information in the first ¾ of the sentence will affect the classification of the prepositional phrase “to the problem”.  Supervised Learning: Evaluation Prepare the experimental data Manually label a set of instances. Split  the labeled data into training and  testing sets. Use the training data to find patterns. Apply these patterns on the testing data set. For evaluation:  use  accuracy  (the percentage of correct labels that a given algorithm has assigned on the testing data). Compare with a simple baseline   method. What is the simplest baseline method? Answer The simplest supervised baseline method is to find the more common class (label) in the training data and assign it to  all  instances of the testing data set. Algorithm 1 Random baseline A  random  unsupervised baseline would have been to label each instance in the testing data set with a random label, 0 or 1.  Practically, random performance is the lower bound against which any non-random methods should be compared. Measuring the accuracy of the supervised baseline method (Algorithm 1) In the official training data set (RRR94), the rate  of occurrence of the 1 label  (low attachment) is  52.2% ( 10,865/20,801). Is the accuracy of this baseline method then equal to 52.2%? Answer No, this is not how accuracy is computed.  It has to be computed on the testing set, not the training set. Using the official split, the accuracy of this method on  the testing set is 59.0% ( 1,826/3,097).  One shouldn’t think of this number as a good result.  The difference (+6.8% going from training to testing) could have been in the opposite direction, resulting in a performance below random. Observations If the baseline method is simple and if the testing set is randomly drawn from the full data set and the data set is large enough, one could expect that the accuracy on the testing set is comparable to the one on the training set.  Note that the PTB data set is drawn from business news stories. If one were to train a method on this data and test it on a different set of sentences, e.g., from a novel, it is possible that the two sets will have very different characteristics. The more complicated the method is, however, the more likely it will be that it will “ overfit ” the training data, learning patterns that are too specific to the training data itself and which may not appear in the testing data or which may be associated with the opposite class in the testing data. Upper bounds on accuracy The 52% accuracy we’ve seen so far is our current lower bound.  Now, what is the upper bound? Usually, human performance is used for the upper bound. For PP attachment, using the four features mentioned earlier, human accuracy is around 88%. So, a hypothetical algorithm that achieves an accuracy of 87% is in fact very close to the upper bound (on a scale from 52% to 88%, it is 97% of the way to the upper bound). Using linguistic knowledge One way to beat the two baselines is to use linguistic information.  For example, the preposition “of” is much more likely to be associated with low attachment than high attachment. In the training data set, this number is an astounding 98.7% (5,534/5,607) Therefore the feature  prep_of  is very valuable.  What are the two main reasons? Answer Reason 1 I t is very informative (98.7% of the time it is linked with the low attachment class) Reason 2 I t is very frequent (27.0% of the  entire training set -  5607/20801). Reason 1 alone would not be sufficient! Sidebar 1/3 Evaluation pipeline The PTB (Penn Treebank) data set has  been used for competitive evaluation of pp attachment algorithms since 1994.  Each  new algorithm is allowed to look at the training and development sets and use any knowledge extracted from them.  The  test set data can never be looked at and can be used only once per algorithm  for  its evaluation.  Doing  the contrary (repeatedly tuning a new algorithm based on its performance on the designated test set) results in a performance level that is irreproducible on new data and such approaches are not allowed in NLP research.  Note  that the development set can be used in a well-specified way as part of the training process. Sidebar 2/3 Let’s  look at the training data then and see if we can learn some patterns that would help us improve over the silly “label everything as noun attachment” baseline and its 52% expected accuracy.  For  example, some prepositions tend to favor particular attachments.  Let’s  start with “against”.  It  appears 172 times in the training set, of which 82 (48%) are noun attached, the rest (52%) being high attached.  This  ratio (48:52) is very similar to the baseline (52:48), so clearly, knowing that the preposition is “against” gives us very little new information.  Sidebar 3/3 Cont’d Furthermore , the total occurrence of “against” in the training corpus is rather small (less than 1% of the prepositional phrases).  In two special cases, however, the identity of the preposition gives a lot of information.  At one extreme, “of” is associated with low attachment in 99% of the cases, whereas at the other end of the scale, “to” is associated with high attachment in 82% of its appearances.  It is also important to note that both of these prepositions are fairly common in the training set (“to” representing 27% of all prepositions and “of” accounting for another 11% of them).  With this knowledge, we can build a very simple decision list algorithm (Brill and  Resnik ) that looks like this (the rules are sorted by their expected accuracy on their majority class ). Are there any other such features? Yes,  prep_to :  2,182/2,699 = 80.8% in favor of high attachment. Which leads us to our next algorithm: Algorithm 2 Sidebar 1/2 Let’s compare the performance of this simple decision list algorithm with that of the baseline.  On  the training set, the first rule would fire in  5,577  cases, of which  5,527  will be correctly labeled as low attachment and another 50 will be incorrectly labeled as high attachment (the accuracy of this rule is expected to be 99% on the training set).  The  second rule (with an expected  accuracy of 82% on the training set) would result in  2,672  decisions, of which  2,172  will be correctly processed as high attachment and 500 will be mislabeled as low attachment.  Finally, the default rule will kick in, whenever the first two rules are not applicable. In this case, it will apply on all remaining phrases (20,801 – 5,577 – 2,672 = 12,552 cases).  Specifically, among these, it will result in 4,837 phrases correctly labeled as low and 7,714 incorrectly labeled as high.  Sidebar 2/2 Overall, we have 5,527 + 2,172 + 4,837 = 12,536 correct decisions, or an accuracy of 12,536/20,801 = 60%.  Clearly Algorithm 2 outperforms Algorithm 1 on the training data.  This is not surprising, since its expected accuracy should be no less than the worst expected accuracy of its rules (that of rule 3) and it is likely to be higher than that lowest number because of the priority given to the easier to classify cases (rules 1 and 2 ). More complicated algorithms can look at additional  features E.g.,  the nouns or the verb, or some combination thereof. For example, the verb “dropped” appears 75/81 times (93%) along with high attachment and only 7% with low attachment. Algorithm 2a Some Observations (1/2) First, even though the expected performance of rule 3 was 52%, its actual performance on the training set dropped to 39% after rules 1 and 2 were applied.  In other words, these rules used up some of the information hidden in the data ahead of rule 3 and left it less useful information to rely upon.  Even more, one can see that a better decision would have been to replace rule 3 with its exact opposite, label everything left at this stage as “high”, which would have boosted the combined performance.  Algorithm 2a would achieve 5,527 + 2,172 + 7,714 = 15,413 correct decisions for an overall accuracy of 74% on the training set. Second, one cannot help but notice that Algorithms 2 and 2a each have only three rules.  We  can imagine a classifier with 20,801 rules, one per training example, each rule of the form “if the preposition is “of” and the nouns are such and such and the verbs are such and such, then classify the data point as the actual class observed in the training set”.  Some Observations (2/2) Third, we so far reported performance on the training set.  Can we project the performance on the training set to the test set?  Let’s start with Algorithms 1 and 3.  Algorithm 1 labels everything as low attachment. It achieved 52% on the training set. We expect its performance on the test set to be similar. In fact it is 59% (1,826/3,097 ). This clearly demonstrates the variability of text across subsets of the data.  In this case, this variability favors Algorithm 1 since its performance actually goes up when moving to the test set. In other cases (e.g., if we had swapped the training and test sets), its performance would have gone down.  On average though, its performance on the test data is expected to vary around the performance on the training data.  Algorithm 3 Some Observations 1/5 Now, let’s consider Algorithm 3.  It  achieved a very high performance on the training data (way above the “upper bound” achieved by humans).  However, we will now see the meaning of the word  overfitting  in action.  Algorithm 3 was so specific to the training data that most of the rules it learned don’t apply at all in the test set.  Only 117 combinations (out of 3032) of words in the test set match a combination previously seen in the training set.  In other words, Algorithm 3 learned a lot of good rules, but it failed to learn many more. In fact, its accuracy on the test data is only around 4%.  Some Observations 2/5 An  alternative to Algorithm 3 would be to combine it with a default rule (just like rule 3 in Algorithm 2) that labels everything that Algorithm 3 missed as noun attachment.  Unfortunately, even this algorithm (let’s call it Algorithm 3a) would only achieve a performance slightly above the baseline (Algorithm 1) of 59% on the test data.  The lesson to learn here is that, on unseen data, a simple algorithm (Algorithm 1) is much better than a really complicated one that  overfits  (Algorithm 3). Also, the combination of the two ( overfitting  + baseline) just barely outperforms the baseline itself and is nowhere close to competitive.  Some Observations 3/5 Clearly this algorithm (Algorithm 3) would achieve close to 100% accuracy on the training set.  Why “close to 100%” and not “100%”?  It turns out that the training set there are mutually inconsistent labels for the same data point.  For example, “won verdict in case” appears once as high and once as low attachment.  There are a total of 56 such “discrepancies” in the training set.  Some of them are caused by inconsistent annotators whereas others would require more context (e.g., the entire paragraph or document) to be correctly disambiguated . Some Observations 4/5 Next, let’s see how algorithms 2 and 2a will fare on the test set.  First, let’s look at Algorithm 2.  There are 3097 items to classify in the test set.  Rule 1 correctly classifies 918 out of 926 instances of “of” (99% accuracy) while rule 2 gets 70% accuracy (234/332 correctly classified).  Rule 3 achieves 810/1,839 = 44%.  Overall the accuracy of Algorithm 2 on the test set is 63% (1,962/3,097).  Again, on the test data, Algorithm 2a outperforms Algorithm 2. Its Rule 3 gets 1,029/1,839 = 56% accuracy and the overall accuracy of Algorithm 2a on the test set is 70% (2,181/3,097).  Some Observations 5/5 Let’s  now summarize  the performance  of the five algorithms that we have looked at so far. What’s Next? So far, so good. We have been able to go from 59% test set accuracy to 70% with two simple rules.  What  additional sources of information can we use to improve the algorithm?  Here  are some  ideas: use  a few more  good  word features (e.g., more prepositions, perhaps some verb and  nouns) use  clever ways to deal with missing  information use  lexical semantic information (e.g., synonyms ) use  additional context beyond the four feature types used so far. What’s Next? Let’s first consider a combination of the first two ideas above: looking for ways to use all possible information that can be extracted from the training data.  This is the approach that was used by Collins and Brooks (1995).  Their method was based on a principle called  backoff  which is somewhat of a combination of all the algorithms used so far (e.g., Algorithms 1, 2, and 3).  Backoff  allows us to use the most specific evidence from the training data, when available but then make reasonable approximations for the missing evidence . Backoff  Method Collins and Brooks algorithm: If a 4-tuple is available, use it.  If not, combine the evidence from the triples that form the 4-tuple (looking only at the triples that include the preposition).  If that is not available, look at the pairs, then the singletons, and finally use a default class.  A 4-tuple is just a set of 4 features in a particular order, e.g., (verb, noun1, preposition, noun2).  The matching term for 3 features is a triple; for 2 features it is a pair; and for 1 feature, the word singleton is used . What’s Next? The idea behind Algorithm 3 was quite reasonable – assume that if the same object appear again (as defined by the same set of four features), it will likely have the same tag.  The problem with this approach is that there is not enough data in the training set to learn the likely classes of all possible combinations of features.  Let’s do the math. To cover all the data points in the test set, we’d need information in the training set for a total of 102,998,280,840 combinations (more than 100 Billion combinations)!  How did we arrive at this number?  It  is simply the product of the numbers 1123, 1295, 52, and 1362, which are, respectively, the numbers of distinct verbs, noun1s, prepositions, and noun2s in the test set.  It is impossible to label so much data and even if it could be done, there would be billions more combinations needed to cover a new test set.  Other Methods Zhao and Lin  2004 nearest  neighbors Find most similar examples – 86.5% best accuracy Similar to  Zavrel ,  Daelemans , and  Veenstra  1997 – memory-based learning Abney  et al.  1999 Boosting Stetina   and Nagao  1997 Semantics Toutanova   et al.  2004 Graph-based  method Comparative Results NLP NLP Introduction to NLP Transition-based Dependency Parsing Transition-Based Parsing Similar to shift-reduce Produces a single (projective) tree Data structures Stack of partially processed (unattached) words Input buffer Set of dependency arcs Attach the word on the top of the stack to the word at the current position in the buffer (or in the other direction) Transition-Based Parsing Initial configuration Stack (including the root token w0) Buffer (sentence) Arcs (empty) Goal configuration Stack (empty) Buffer (empty) Arcs (complete tree) MaltParser (Nivre 2008) The reduce operations combine an element from the stack and one from the buffer Arc-standard parser The actions are shift, left-arc, right-arc Arc-eager parser The actions are shift, reduce, left-arc, right-arc (Arc-Eager)  MaltParser  Actions [Example from Nivre and Kuebler] [Example from  Kuebler , McDonald,  Nivre ] Example Example: “People want to be free”             [ROOT] 			[People, want, to, be, free] 	 Ø Shift     [ROOT, People] 	[want, to, be, free]  LA nsubj    [ROOT] 			[want, to, be, free]  			A 1  = {nsubj(want, people)} RA root    [ROOT, want]	[to, be, free]  					A 2  = A 1  ∪ {root(ROOT, want)} Characteristics The next action is chosen locally using a classifier (e.g. SVM) There is no search The final list of arcs is returned as the dependency tree Trained on a dependency treebank Very fast method Parsing Oracle-based Assuming an oracle, parsing is deterministic In practice Approximate the oracle with a classifier o(c) =  argmax t   w . f ( c,t ) [Example from McDonald and  Nivre ] Greedy Transition-Based Parsing Beam search with q=1 Score is computed using a linear model Because of the greedy property, errors can propagate Parse (sent = w 1 ... w n ) c =  cs  (sent) While c is not in C t 	t* =  argmax t  score ( c,t ) 	c = t*(c) Return  Gc Feature Model [Example from  Kuebler , McDonald,  Nivre ] Feature Vectors [Example from  Kuebler , McDonald,  Nivre ] Complexity Arc-eager is O(n 3 ) – like Eisner Arc-standard is O(n 5 ) NLP NLP Introduction to NLP NLP Tasks Part of Speech Tagging The swimmer is getting ready to run in the final race. Part of Speech Tagging Run  –   verb or  noun?  Final – noun or adjective? Race – verb or noun? The swimmer is getting ready to  run  in the final race. Part of Speech Tagging The candidate is preparing for his  run  for the presidency. The swimmer is getting ready to  run  in the final race. Parsing Myriam slept. Myriam wrote a novel. Myriam gave Sally flowers. Myriam ate pizza with olives. Myriam ate pizza with Sally. Myriam ate pizza with a fork. Myriam ate pizza with remorse. Phrase-Structure Grammar S     NP  VP NP    DET N NP    NP  PP VP   VBD   VP   VBD NP   VP    VBD NP NP VP   VP  PP   PP   PRP NP   DET    the  DET    that  DET    a  N      child   N      window    N      car    VBD    found    VBD    ate   VBD    saw   PRP    in   PRP    of    PRP   through Parse Trees S NP VP NP VBD saw the car PP PRP NP through the window The child DET N DET N DET N Stanford Parser Parser Output (ROOT   (S     (S       (NP         (NP (NN Housing) (NNS starts))         (, ,)         (NP           (NP (DT the) (NN number))           (PP (IN of)             (NP               (NP (JJ new) (NNS homes))               (VP (VBG being)                 (VP (VBN built))))))         (, ,))  (VP (VBD rose)         (NP (CD 7.2) (NN %))         (PP (IN in)           (NP (NNP March)))         (PP (TO to)           (NP             (NP (DT an) (JJ annual) (NN rate))             (PP (IN of)               (NP (CD 549,000) (NNS units)))))         (, ,)         (ADVP (RB up)           (PP (IN from)             (NP               (NP (DT a) (VBN revised) (CD 512,000))               (PP (IN in)                 (NP (NNP February))))))))     (, ,)     (NP (DT the) (NNP Commerce) (NNP Department))     (VP (VBD said))     (. .))) This Problem is Pretty // Easy Commercial for a phone company Garden path sentences Don’t bother coming Don’t bother coming early Take the turkey out of the oven at five Take the turkey out of the over at five to four I got canned I got canned peaches for dinner All Americans need to buy a house All Americans need to buy a house is a lot of money Can you think of more such examples? Solution This problem is pretty // easy http :// www.nacloweb.org/resources/problems/2007/N2007-HS.pdf    Criteria The  part before // should be a complete  sentence The  full sentence has a different meaning than the part before  //  The  part before // should not already be  ambiguous Dependency Parsing likes Mary apples yellow Dependency Parsing IL-2 and IL-15 induced the production of IL-17 and IFN- γ  by PBMCs in a dose dependent manner. Parser Output nn (starts-2, Housing-1) nsubj (rose-12, starts-2) det (number-5, the-4) appos (starts-2, number-5) prep(number-5, of-6) amod (homes-8, new-7) pobj (of-6, homes-8) auxpass (built-10, being-9) partmod (homes-8, built-10) ccomp (said-36, rose-12) num (%-14, 7.2-13) dobj (rose-12, %-14) prep(rose-12, in-15) pobj (in-15, March-16) prep(rose-12, to-17) det (rate-20, an-18) amod (rate-20, annual-19) pobj (to-17, rate-20) prep(rate-20, of-21) num (units-23, 549,000-22) pobj (of-21, units-23) advmod (rose-12, up-25) dep (up-25, from-26) det (512,000-29, a-27) amod (512,000-29, revised-28) pobj (from-26, 512,000-29) prep(512,000-29, in-30) pobj (in-30, February-31) det (Department-35, the-33) nn (Department-35, Commerce-34) nsubj (said-36, Department-35) Information Extraction RESEARCH ALERT- Wells Fargo   cuts   PPD  Inc  to  market perform  China Southern Air  Upgraded  To  Overweight  From  Neutral - HSBC CITIGROUP   RAISES   INGERSOLL RAND  <IR.N>  TO  HOLD  FROM  SELL   TCF Financial Corp  Raised  To  Overweight  From  Neutral  By  JPMorgan   BAIRD   CUTS   KIOR INC  <KIOR.O>  TO  UNDERPERFORM  RATING  BRIEF-RESEARCH ALERT- Global Equities Research  cuts   LinkedIn  to  equal weight   Information Extraction False Positives Examples of false positives BARCLAYS  CUTS FLAGSTONE REINSURANCE <FSR.N>  PRICE TARGET  TO $9 FROM $11  Rimage   To Buy  Qumu   For $52M;; Raises Dividend;;  Lowers EPS  View S&P rates Ameren Illinois commercial paper  'A-3' BRIEF-Moody's  changes  otlk   for  Stirling  Water  Seafield  Finance  to positive BRIEF-RESEARCH ALERT-HSBC  cuts  price targets on  European  telcos Stifel   cuts  Philip Morris  price target Media General  shares plummet  on  Moody's  downgrade Explain why these are false positives. Answers to the Quiz BARCLAYS CUTS FLAGSTONE REINSURANCE <FSR.N>  PRICE TARGET  TO $9 FROM $11  Didn’t cut the ratings but the price target Rimage   To Buy  Qumu   For $52M;; Raises Dividend;;  Lowers EPS  View Lowers eps view S&P rates Ameren Illinois commercial paper  'A-3‘ Debt rating BRIEF-Moody's  changes  otlk   for  Stirling  Water  Seafield  Finance  to positive Changes outlook BRIEF-RESEARCH ALERT-HSBC  cuts  price targets on  European  telcos Not a company but a group of companies Stifel   cuts  Philip Morris  price  target Price target, not rating Media General  shares plummet  on  Moody's  downgrade Event in the past Semantics First order logic Inference/deduction Semantic analysis  x,y :  Mother ( x,y )     Parent ( x,y ) NACLO Problem “Bertrand and Russell”, 2014 problem by Ben King http:// www.nacloweb.org/resources//problems/2014/N2014-H.pdf to be covered in a later lecture   NACLO Solution Bertrand and Russell http://www.nacloweb.org/resources/problems/2014/N2014-HS.pdf    Reading Comprehension Pranav  Anand , Eric  Breck , Brianne Brown, Marc Light, Gideon Mann, Ellen  Riloff , Mats  Rooth , Michael  Thelen . 2000. Fun with Reading Comprehension Text Understanding There are four bungalows in our cul-de-sac. They are made from these materials: straw, wood, brick and glass.  Mrs. Scott's bungalow is somewhere to the left of the wooden one and the third one along is brick. Mrs. Umbrella owns a straw bungalow and Mr. Tinsley does not live at either end, but lives somewhere to the right of the glass bungalow. Mr.  Wilshaw  lives in the fourth bungalow, whilst the first bungalow is not made from straw.  Who lives where, and what is their bungalow made from? http://www.brainbashers.com/showpuzzles.asp?puzzle=ZSOP   Word Sense Disambiguation “The thieves took off with 100 gold  bars ”. Did they steal 100 drinking establishments? Or 100 measures of a song? Word Sense Disambiguation Bar=Noun     S: (n) barroom, bar, saloon,  ginmill , taproom (a room or establishment where alcoholic drinks are served over a counter) "he drowned his sorrows in whiskey at the bar"     S: (n) bar (a counter where you can obtain food or drink) "he bought a hot dog and a coke at the bar"     S: (n) bar (a rigid piece of metal or wood; usually used as a fastening or obstruction or weapon) "there were bars in the windows to prevent escape"     S: (n) measure, bar (musical notation for a repeating pattern of musical beats) "the orchestra omitted the last twelve bars of the song"     S: (n) bar (an obstruction (usually metal) placed at the top of a goal) "it was an excellent kick but the ball hit the bar"     S: (n) prevention, bar (the act of preventing) "there was no bar against leaving"; "money was allocated to study the cause and prevention of influenza"     S: (n) bar ((meteorology) a unit of pressure equal to a million dynes per square centimeter) "unfortunately some writers have used bar for one dyne per square centimeter"     S: (n) bar (a submerged (or partly submerged) ridge in a river or along a shore) "the boat ran aground on a submerged bar in the river"     S: (n) legal profession, bar, legal community (the body of individuals qualified to practice law in a particular jurisdiction) "he was admitted to the bar in New Jersey"     S: (n) stripe, streak, bar (a narrow marking of a different color or texture from the background) "a green toad with small black stripes or bars"; "may the Stars and Stripes forever wave"     S: (n) cake, bar (a block of solid substance (such as soap or wax)) "a bar of chocolate"     S: (n) Browning automatic rifle, BAR (a portable .30 caliber automatic rifle operated by gas pressure and fed by cartridges from a magazine; used by United States troops in World War I and in World War II and in the Korean War)     S: (n) bar (a horizontal rod that serves as a support for gymnasts as they perform exercises)     S: (n) bar (a heating element in an electric fire) "an electric fire with three bars"     S: (n) bar ((law) a railing that encloses the part of the courtroom where the judges and lawyers sit and the case is tried) "spectators were not allowed past the bar" Bar=Verb     S: (v) bar, debar, exclude (prevent from entering; keep out) "He was barred from membership in the club"     S: (v) barricade, block, blockade, stop, block off, block up, bar (render unsuitable for passage) "block the way"; "barricade the streets"; "stop the busy road"     S: (v) banish, relegate, bar (expel, as if by official decree) "he was banished from his own country"     S: (v) bar (secure with, or as if with, bars) "He barred the door" WSD is Important for Translation Paul plays soccer Paul  joue   au  football Paul plays the guitar Paul  joue   de la  guitare “wall” in German die  Chinesische   Mauer  (The Great Wall of China) (otherwise Wand) “wall” in Spanish pared,  muro ,  muralla Named Entity Recognition http://cogcomp.cs.illinois.edu/page/demo_view/NER http://nlp.stanford.edu:8080/ner/           Wolff B-PER              , O      currently O              a O     journalist O             in O      Argentina B-LOC              , O         played O           with O            Del B-PER         Bosque I-PER             in O            the O          final O          years O             of O            the O      seventies O             in O           Real B-ORG         Madrid I-ORG              .  O Wolff, currently a journalist in Argentina, played with Del Bosque in the final years of the seventies in Real Madrid.  Named Entity Recognition http://pages.cs.wisc.edu/~bsettles/ abner   Semantic Role Labeling [ A0  He ] [ AM-MOD  would ] [ AM-NEG   n't  ] [ V   accept  ] [ A1  anything of value ] from [ A2  those he was writing about ] .  V:  verb A0 :  acceptor   A1:  thing accepted  A2:  accepted-from  A3:  attribute  AM-MOD:  modal  AM-NEG:  negation  http://cogcomp.cs.illinois.edu/page/demo_view/SRL Coreference Resolution Barack Obama  visited China.  The US president  met with his Chinese counterpart. Cynthia  went to see  her aunt  at the hospital.  She  was scheduled for surgery on Monday. Because  he  was sick,  Michael  stayed home on Friday. Question Answering "The antagonist of Stevenson's Treasure Island." (Who is Long John Silver?)  http://blog.reddit.com/2011/02/ibm-watson-research-team-answers-your.html “Watson is powered by 10 racks of IBM Power 750 servers running Linux, and uses 15 terabytes of RAM, 2,880 processor cores and is capable of operating at 80 teraflops. Watson was written in mostly Java but also significant chunks of code are written C++ and Prolog, all components are deployed and integrated using UIMA.” Jeopardy Questions From the competition between the IBM Watson system and two human champions (Ken Jennings and Brad Rutter) Sample questions: On December 8, 2008 this national newspaper raised its newsstand price by 25 cents to $1 :  USA Today In 2010 this former first lady published the memoir "Spoken From the Heart" :  Laura Bush* This person is appointed by a testator to carry out the directions & requests in his will :  Executor* Familiarity is said to breed this, from the Latin for "Despise" :  Contempt* As of 2010, Croatia & Macedonia are candidates but this is the only former Yugoslav republic in the EU :  Slovenia The ancient "Lion of Nimrud" went missing from this city's national museum in 2003 (along with a lot of other stuff) :  Baghdad It's just a bloody nose! You don't have this hereditary disorder once endemic to European royalty :  Haemophilia It's Michelangelo's fresco on the wall of the Sistine Chapel, Depicting the saved and the damned :  The Last  Judgement She "Died in the church and was buried along with her name. Nobody came" :  Eleanor Rigby It's a 4-letter term for a summit; the first 3 letters mean a type of simian :  Apex A camel is a horse designed by this :  Committee Watson’s answers: 66 correct and 9 incorrect   (e.g., the one in the category “US Cities” about a city with two airports named after a World War II hero and a World War II battle) Watson's two day winning streak was $77,147. Ken Jennings ended with $24,000 and Brad Rutter with $21,600.   Watson donated $500,000 to both World Vision and World Community Grid charities from the $1,000,000 prize. http://www.quora.com/What-questions-were-asked-in-the-Jeopardy-episode-involving-Watson Sentiment Analysis “ I like  the camera  because I can edit images so easily, exactly as I do my  iPad . I have found that its difficult to frame a picture when there isn't a zoom function as with the  iPad . With this camera I can adjust my images by cropping as I did with my  iPad  but  better yet , this camera has a built in zoom. A stretch or pinch of the fingers bring in the subject closer or back out again. With this iPhone I can also, as I dido with my  iPad , enhance, crop, rotate, red eye reduce, and set a range of tints.  I am also quite impressed with  the quality of the images . Pretty darn good especially  better than I expected  for low light situations where I can use the built-in flash! Quite frankly  I was quite surprised  with these built in features . I also hope too experiment with and learn what HDR photography is. It's built into this iPhone and can be activated by a the touch of an icon. ” http://www.epinions.com/review/apple_iphone_5c_latest_model_16gb_graphite_unlocked_smartphone/content_640679317124   Machine Translation あけましておめでとうございます。  Happy New Year! Machine Translation Moses www.statmt.org 	Elephants  are social animals. They live with their families, give hugs and call each other by using their trunks as trumpets. They also might know how to help each other. 	In  a recent elephant study by researchers from the United States and Thailand, pairs of giant animals learned to work together to get some ears of corn. Other animals, especially some primates, are already known to work together to complete tasks, but now elephants have joined the club. Perhaps the finding is not too surprising: Scientists suspect that elephants, with their big brains and survival savvy, may be among the smartest animals on the planet. 	Joshua  Plotnik , who worked on the study, told Science News that the animals didn’t just learn a trick. Instead, the ways the elephants behaved show that they understand how working together brings benefits to everyone involved.  Plotnik  is a comparative psychologist now at the University of Cambridge in England. Psychology is the study of behaviors and mental processes, and comparative psychologists study how animals other than humans behave. 	Les  éléphants sont des animaux sociaux . Ils vivent avec leur famille, faire des câlins et appeler les uns les autres en utilisant leurs troncs trompettes. Ils pourraient également savoir comment aider les uns les autres. 	 Dans  une étude récente d'éléphants par des chercheurs des États-Unis et la Thaïlande, des paires d'animaux géants ont appris à travailler ensemble pour obtenir des épis de maïs . D'autres animaux, en particulier des primates, sont déjà connus pour travailler ensemble pour accomplir des tâches, mais maintenant, les éléphants ont rejoint le club. Peut-être le résultat n'est pas trop surprenant: Les scientifiques soupçonnent que les éléphants, avec leurs gros cerveaux et de bon sens de survie, peut-être parmi les plus intelligents des animaux sur la planète. 	Joshua  Plotnick , qui a travaillé sur l'étude, dit Nouvelles de la Science que les animaux n'ont pas seulement appris un truc. Au lieu de cela, les moyens les éléphants se comportent montrent qu'ils comprennent comment travailler ensemble apporte des avantages à toutes les personnes impliquées.  Plotnik  est un psychologue comparative maintenant à l'Université de Cambridge en Angleterre.  La psychologie est l'étude des comportements et des processus mentaux , et étude comparative des psychologues comment les animaux autres que les humains se comportent. https://student.societyforscience.org/article/theres-no-i-elephant 	Elephants  are social animals. They live with their families, give hugs and call each other by using their trunks as trumpets. They also might know how to help each other. 	In  a recent elephant study by researchers from the United States and Thailand, pairs of giant animals learned to work together to get some ears of corn. Other animals, especially some primates, are already known to work together to complete tasks, but now elephants have joined the club. Perhaps the finding is not too surprising: Scientists suspect that elephants, with their big brains and survival savvy, may be among the smartest animals on the planet. 	Joshua  Plotnik , who worked on the study, told Science News that the animals didn’t just learn a trick. Instead, the ways the elephants behaved show that they understand how working together brings benefits to everyone involved.  Plotnik  is a comparative psychologist now at the University of Cambridge in England. Psychology is the study of behaviors and mental processes, and comparative psychologists study how animals other than humans behave. 	 Les  éléphants sont des animaux sociaux. Ils  vivent  avec leur famille,  faire  des câlins et  appeler  les uns les autres en utilisant leurs troncs trompettes. Ils pourraient également savoir comment aider les uns les autres. 	Dans  une étude récente d'éléphants par des chercheurs des États-Unis et la Thaïlande, des paires d'animaux géants ont appris à travailler ensemble pour obtenir des épis de maïs. D'autres animaux, en particulier des primates, sont déjà connus pour travailler ensemble pour accomplir des tâches, mais maintenant, les éléphants ont rejoint le club. Peut-être le résultat n'est pas trop surprenant: Les scientifiques soupçonnent que  les éléphants , avec leurs gros cerveaux et de bon sens de survie,  peut-être  parmi les plus intelligents des animaux sur la planète. 	Joshua  Plotnick , qui a travaillé sur l'étude, dit  Nouvelles de la Science  que les animaux n'ont pas seulement appris un truc. Au lieu de cela, les moyens les éléphants se comportent montrent qu'ils comprennent comment travailler ensemble apporte des avantages à toutes  les personnes  impliquées.  Plotnik  est un psychologue  comparative  maintenant à l'Université de Cambridge en Angleterre. La psychologie est l'étude des comportements et des processus mentaux, et  étude comparative des psychologues  comment les animaux autres que les humains se comportent. https://student.societyforscience.org/article/theres-no-i-elephant Machine Translation Noisy channel model (“Chinese Whispers”) e f e’ E   F F   E encoder decoder e’ = argmax P(e|f) = argmax P(f|e) P(e) e e translation model language model Machine Translation IBM Method IS THIS YOUR FAVORITE           PLAY ?    IS THIS YOUR FAVORITE PLAY  PLAY   PLAY  ? ** ** ** THIS IS YOUR PLAY  PLAY   PLAY  FAVORITE ? EST-CE QUE C’ EST VOTRE PIECE DE THEATRE PREFEREE ? Language model Translation model Text Summarization Health Benefits Eating a diet rich in vegetables and fruits as part of an overall healthy diet may reduce risk for heart disease, including heart attack and stroke. Eating a diet rich in some vegetables and fruits as part of an overall healthy diet may protect against certain types of cancers. Diets rich in foods containing fiber, such as some vegetables and fruits, may reduce the risk of heart disease, obesity, and type 2 diabetes. Eating vegetables and fruits rich in potassium as part of an overall healthy diet may lower blood pressure, and may also reduce the risk of developing kidney stones and help to decrease bone loss. Eating foods such as vegetables that are lower in calories per cup instead of some other higher-calorie food may be useful in helping to lower calorie intake. Nutrients Most vegetables are naturally low in fat and calories. None have cholesterol. (Sauces or seasonings may add fat, calories, or cholesterol.) Vegetables are important sources of many nutrients, including potassium, dietary fiber, folate (folic acid), vitamin A, and vitamin C. Diets rich in potassium may help to maintain healthy blood pressure. Vegetable sources of potassium include sweet potatoes, white potatoes, white beans, tomato products (paste, sauce, and juice), beet greens, soybeans, lima beans, spinach, lentils, and kidney beans. Dietary fiber from vegetables, as part of an overall healthy diet, helps reduce blood cholesterol levels and may lower risk of heart disease. Fiber is important for proper bowel function. It helps reduce constipation and diverticulosis. Fiber-containing foods such as vegetables help provide a feeling of fullness with fewer calories. Folate (folic acid) helps the body form red blood cells. Women of childbearing age who may become pregnant should consume adequate folate from foods, and in addition 400 mcg of synthetic folic acid from fortified foods or supplements. This reduces the risk of neural tube defects,  spina  bifida, and anencephaly during fetal development. Vitamin A keeps eyes and skin healthy and helps to protect against infections. Vitamin C helps heal cuts and wounds and keeps teeth and gums healthy. Vitamin C aids in iron absorption. Summary Eating vegetables is healthy. http://www2.research.att.com/~ttsweb/tts/demo.php Text to Speech Text to Speech www.ivona.com Entailment and Paraphrasing Ido  Dagan, Oren Glickman and Bernardo  Magnini . The PASCAL  Recognising  Textual Entailment Challenge Discourse Analysis Anaphoric relations: 1. Mary helped Peter get out of the car. He thanked her. 2. Mary helped the other passenger out of the car.     The man had asked her for help because of his foot injury. Tom appeared on the sidewalk with a bucket of whitewash and a long-handled brush. He surveyed the fence, and all gladness left him and a deep melancholy settled down upon his spirit. (Tom Sawyer) Dialogue Systems I would like to make a reservation at Sorrento. For when? 8 pm Friday night. We only have availability for 7 pm and 10 pm. Sorry, these don't work for me. NLP NLP Text Similarity Entailment and Paraphrasing MSR Paraphrase Corpus Entailment Entailment and Presupposition Entailment One fact follows from another “All cats have whiskers” and “Martin is a cat” entail the statement “Martin has whiskers” “Martin has whiskers and a tail” entails “Martin has whiskers” Presupposition “The Queen of Utopia is dead” presupposes that Utopia has a queen Entailment and Presupposition NACLO problem from 2010 Author:  Aleka  Blackwell http:// www.nacloweb.org/resources/problems/2010/M.pdf   http:// www.nacloweb.org/resources/problems/2010/MS.pdf   NACLO Problem Entailment and Presupposition Entailment and Presupposition Answers Entailment and Presupposition Entailment and Presupposition Using Multiple Translations The opening sentence of Madame Bovary by Gustave Flaubert (1857) Original  Nous  étions  à  l’Étude ,  quand  le  Proviseur   entra ,  suivi  d’un nouveau  habillé  en bourgeois et d’un  garçon  de  classe  qui  portait  un grand  pupitre .  Ceux  qui  dormaient  se  réveillèrent , et  chacun  s ’  éleva   comme   surpris   dans  son travail.   Translation 1 (Eleanor Marx- Aveling ) We were in class when the head-master came in, followed by a "new fellow," not wearing the school uniform, and a school servant carrying a large desk. Those who had been asleep woke up, and every one rose as if just surprised at his work. Translation 2 We were in the study-hall when the headmaster entered, followed by a new boy not yet in school uniform and by the handy man carrying a large desk.  Their arrival disturbed the slumbers of some of us, but we all stood up in our places as though rising from our work. Translation 3 We were in the prep.-room when the Head came in, followed by a new boy  in 'mufti ' and a beadle carrying a big desk. The sleepers aroused themselves,  and we  all stood up, putting on a startled look, as if we had been buried in  our work . [ Barzilay ] Dependency Representation [ Androutsopoulos   and  Malakasiotis  2010] Use in information extraction [ Bunescu  and Mooney 2005] Use in information extraction [Erkan et al. 2007] [ figure from  Androutsopoulos  and  Malakasiotis   2010, based on  Barzilay   and Lee  2003] Using lattices for paraphrasing NLP NLP Libraries for Deep Learning Deep Learning Matrix Multiplication in Python http://stackoverflow.com/questions/10508021/matrix-multiplication-in-python Matrix Multiplication in  Numpy Libraries for Deep Learning Torch ( Lua ):  http://torch.ch/ PyTorch  (Python) http://pytorch.org/ TensorFlow  (Python and C++): https://www.tensorflow.org/ Theano  (Python) http://deeplearning.net/software/theano/   No longer maintained Keras ,  PaddlePaddle , CNTK Libraries for Deep Learning:  Tensorflow (slides by Jason Chu) Deep Learning What is  TensorFlow ? Open source software library for numerical computation using data flow graphs Developed by Google Brain Team for machine learning and deep learning and made open-source TensorFlow  provides an extensive suite of functions and classes that allow users to build various models from scratch These slides are adapted from the following Stanford lectures: https://web.stanford.edu/class/cs20si/2017/lectures/slides_01.pdf https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf What’s a tensor? Formally, tensors are multilinear maps from vector spaces to the real numbers Think of them as n-dimensional array, with 0-d tensors being scalars, 1-d tensor vectors, 2-d tensor matrices,  etc Some Basic Terminology Dataflow Graphs : entire computation Data Nodes:  individual data or operations Edges:  implicit dependencies between nodes Operations:  any computation  Constants:  single values (tensors) “ TensorFlow  programs are usually structured into a construction phase, that assembles a graph, and an execution phase that uses a session to execute ops in the graph.” -  TensorFlow  docs   All nodes return  tensors , or higher-dimensional matrices You are metaprogramming . No computation occurs yet! Data Flow Graphs import  tensorflow  as  tf a =  tf.add (2, 3) TF automatically names nodes if you do not x = 2 y = 3 print a >> Tensor("Add:0", shape=(),  dtype =int32) Note: a is NOT 5 a TensorFlow  Session Session object encapsulates the environment in which Operation objects are executed and Tensor objects, like a in the previous slide, are evaluated import  tensorflow  as  tf a =  tf.add (2, 3) with  tf.Session () as  sess : print  sess.run (a) TensorFlow  Sessions There are 3 arguments for a Session, all of which are optional. target — The execution engine to connect to. graph — The Graph to be launched. config  — A  ConfigProto  protocol buffer with configuration options for the session TensorFlow  Variables “When you train a model you use variables to hold and update parameters. Variables are in-memory buffers containing tensors” -  TensorFlow  Docs. TensorFlow  variables must be initialized before they have values Placeholders and Feed Dictionaries You can input data from  Numpy  using  tf.convert_to  _tensor, but not scalable Use  tf.placeholder  variables (dummy nodes that provide entry points for data to computational graph) A  feed_dict  is a python dictionary mapping from  tf . placeholder  vars  (or their names) to data ( numpy  arrays, lists, etc.) input1 =  tf.placeholder (tf.float32) input2 =  tf.placeholder (tf.float32) output =  tf.mul (input1, input2) with  tf.Session () as  sess : 	print( sess.run ([output],  feed_dict ={input1:[7.], input2:[2.]}))) Variable Scope tf.variable_scope () provides simple name-spacing to avoid clashes of variables with  tf.variable_scope ("foo"): 	with  tf.variable_scope ("bar"): 		v =  tf.get_variable ("v", [1]) assert v.name == "foo/bar/v:0“ tf.get_variable () creates/accesses variables from within a variable scope. tf.get_variable_scope (). reuse_variables () Linear Regression Example Linear Regression Example Linear Regression Example Linear Regression Example Computation Graphs in  Tensorflow Homework 4 Some useful functions: tf.expand_dims (input, axis= None,name = None,dim =None) Inserts a dimension of 1 to a tensor’s shape t is tensor of shape [2],  tf.shape ( tf.expand_dims (t, 0)) -> t becomes [1 , 2] tf.gather ( params,indices,validate_indices = None,name = None,axis =0) Gathers the elements at the passed-in indices of the given axis of  params x  =  [ 1,2,3,4,3,2,1 ]  tf . gather (x, 3) . eval () More functions tf.reduce_sum ( input_tensor , axis=None,  keepdims =None, name=None,  reduction_indices =None,  keep_dims =None) Computes the sum of elements across dimensions of a tensor axis : The dimensions to reduce. If None (the default), reduces all dimensions. Must be in the range [-rank( input_tensor ), rank( input_tensor )) Ex:  x = tf.constant([[1, 1, 1], [1, 1, 1]]) tf.reduce_sum(x)  # 6 tf.reduce_sum(x, axis=0)  # [2, 2, 2] tf.reduce_sum(x, axis=1)  # [3, 3] x  =  tf.constant ([[1, 1, 1], [1, 1, 1]]) tf.reduce_sum ( x )  -> 6 tf.matmul (a,  b,transpose_a = False,transpose_b = False,adjoint_a =False, adjoint_b =False,  a_is_sparse =False,  b_is_sparse =False, name=None) Multiplies matrix a by b tf.nn  Module Provides functions for neural network support tf.nn.l2_loss(t, name=None): Computes half the L2 norm of a tensor without the  sqrt tf.nn.relu (features, name=None): computes rectified linear unit ( ReLU ) activation function;  f ( x )= max (0,x) tf.nn.sparse_softmax_cross_entropy_with_logits (_sentinel=None, labels=None, logits=None, name=None) Computes sparse  softmax  cross entropy between logits and labels; Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class) https://www.tensorflow.org/api_docs/python/tf/nn tf.train  Module Module for training support; choose an optimizer to perform optimization; many different types of optimizer Class  AdamOptimizer Optimizer that implements the Adam algorithm Adam  alg  can be found here:  https://arxiv.org/pdf/1412.6980v8.pdf Class  GradientDescentOptimizer  Implements the gradient descent algorithm Calling  optimizer.minimize () will return an Operation (computation) object Adam  alg  allows it to use a larger step size than  GDOptimizer , so it will converge to that step size without a lot of tuning, but it requires more computation and more state/storage tf.argmax () tf.argmax (     input,     axis=None,     name=None,     dimension=None,      output_type =tf.int64 ) Returns the index with the largest value across axes of a tensor Libraries for Deep Learning:  PyTorch (slides by Rui Zhang) Deep Learning PyTorch  Tensor import torch mat1= torch.randn (2,3) mat2= torch.randn (3,3) print mat1 print mat2 Matrix Multiplication in  PyTorch import torch mat1= torch.randn (2,3) mat2= torch.randn (3,3) res= torch.mm (mat1,mat2) print  res.size () Output: (2L, 3L) Batch Matrix Multiplication in  PyTorch import torch batch1= torch.randn (10,3,4) batch2= torch.randn (10,4,5) res= torch.bmm (batch1,batch2) print  res.size () Output: (10L, 3L, 5L) Many Tensor operations in  PyTorch …… torch.mm Matrix multiplication torch.bmm Batch matrix multiplication torch.cat Tensor Concatenation torch.sqeueeze / torch.unsqueeze Change Tensor dimensions ….. ….. Check documentation at http:// pytorch.org /docs/master/ torch.html#tensors PyTorch  Variables import torch from  torch.autograd  import Variable # PyTorch  Tensor x =  torch.ones (2,2) y =  torch.ones (2,1) w =  torch.randn (2,1) b =  torch.randn (1) # PyTorch  Variable x = Variable(x,  requires_grad =False) y = Variable(y,  requires_grad =False) w = Variable(w,  requires_grad =True) b = Variable(b,  requires_grad =True) A  PyTorch  Variable is a wrapper around a  PyTorch  Tensor, and represents a node in a computational graph Computational Graphs   # Computational Graph p_1 =  torch.sigmoid ( torch.mm (x, w) + b) # prediction xent  = -y *  torch.log (p_1) - (1-y) *  torch.log (1-p_1) # cross-entropy loss cost =  xent.mean () # the cost to minimize Automatic Gradient Computation   # Computational Graph p_1 =  torch.sigmoid ( torch.mm (x, w) + b) # prediction xent  = -y *  torch.log (p_1) - (1-y) *  torch.log (1-p_1) # cross-entropy loss cost =  xent.mean () # the cost to minimize cost.backward () print  w.grad print  b.grad Build Neural Networks using  PyTorch Neural networks can be constructed using the  torch.nn  package. Forward An  nn.Module  contains layers, and a method forward(input) that returns the output You can use any of the Tensor operations in the forward function Backward nn  depends on  autograd  to define models and differentiate them You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using  autograd Define a Network Class You don’t need to define a backward function! CNN for MNIST: A Full Example Example from http :// pytorch.org /tutorials/beginner/blitz/ neural_networks_tutorial.html Define a CNN Network Class Compute Loss input is a random image target is a dummy label Backpropagation Use  torch.optim  package to do  backpropagation Links About Deep Learning AAN: our search engine for resources and papers http://tangra.cs.yale.edu/newaan/   Richard  Socher’s  Stanford class http://cs224d.stanford.edu/ Libraries for Deep Learning:  Theano (Slides by  Rui  Zhang) 	(for reference only) Deep Learning Matrix Multiplication in  Theano import  theano import  theano.tensor  as T Import  numpy  as  np # “symbolic” variables x  =  T.matrix('x ') y  =  T.matrix(‘y ’) dot =  T.dot(x ,  y ) Matrix Multiplication in  Theano import  theano import  theano.tensor  as T Import  numpy  as  np # “symbolic” variables x  =  T.matrix('x ') y =  T.matrix (‘y’) dot =  T.dot(x ,  y ) #this is the slow part f =  theano.function ([ x,y ],[dot]) #now we can use this function a = np.random.random((2,3)) b  = np.random.random((3,4)) c  =  f(a ,  b ) #now a 2  x  4 array Sigmoid in  Theano in =  T.vector(‘in ’) sigmoid = 1 / (1 +  T.exp (-in)) #same as  T.nnet.sigmoid sigmoid =  T.nnet.sigmoid (x) Shared Variables  vs  Symbolic Variables # This is symbolic x =  T.matrix ('x') #shared means that it is not symbolic w  =  theano.shared(np.random.randn(n )) b  = theano.shared(0.) Computational Graph # This is symbolic x =  T.matrix ('x') #shared means that it is not symbolic w =  theano.shared ( np.random.randn (n)) b =  theano.shared (0.) # Computational Graph p_1 = sigmoid(T.dot(x, w) + b) xent  = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy cost =  xent.mean () # The cost to minimize Automatic Gradient Computation p_1 =  sigmoid(T.dot(x ,  w ) +  b ) xent  = - y  * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy cost =  xent.mean () # The cost to minimize gw ,  gb  =  T.grad(cost , [ w ,  b ])  Compile a Function train =  theano.function (           inputs=[ x,y ],           outputs=[prediction,  xent ],           updates=(( w ,  w  - 0.1 *  gw ), ( b ,  b  - 0.1 *  gb ))) Computation Graphs in  Theano LSTM Sentiment Analysis Demo If you’re new to deep learning and want to work with  Theano , do yourself a favor and work through  http://deeplearning.net/tutorial/ A LSTM demo is described here:  http://deeplearning.net/tutorial/lstm.html Sentiment analysis model trained on IMDB movie reviews LSTMs: One Time Step ~ [Slides from Catherine  Finegan-Dollak ] LSTMs: Building a Sequence The cat sat on … Theano  Implementation of an LSTM Step (lstm.py, L. 174)  def  _step(m_, x_, h_, c_):          preact  = tensor.dot(h_,  tparams [_p(prefix, 'U')])          preact  += x_          i  =  tensor.nnet.sigmoid (_slice( preact , 0, options[' dim_proj ']))         f =  tensor.nnet.sigmoid (_slice( preact , 1, options[' dim_proj ']))         o =  tensor.nnet.sigmoid (_slice( preact , 2, options[' dim_proj ']))         c =  tensor.tanh (_slice( preact , 3, options[' dim_proj ']))         c = f * c_ +  i  * c         c = m_[:, None] * c + (1. - m_)[:, None] * c_         h = o *  tensor.tanh (c)         h = m_[:, None] * h + (1. - m_)[:, None] * h_         return h, c theano.scan  iterates through a series of steps rval , updates =  theano.scan (_step,    sequences=[mask,  state_below ],     outputs_info =[ tensor.alloc ( numpy_floatX (0.),                    n_samples ,  dim_proj ),                   tensor.alloc ( numpy_floatX (0.),                    n_samples ,  dim_proj )],    name=_p(prefix, '_layers'),     n_steps = nsteps ) (lstm.py, L. 195)  NLP NLP Introduction to NLP Statistical POS Tagging Part of Speech Tagging Methods Rule-based Stochastic HMM (generative) Maximum Entropy MM (discriminative) Transformation-based HMM-based POS Tagging Find tag sequence that maximizes the probability formula P( word|tag ) * P( tag|previous  n tags)  A bigram-based HMM tagger chooses the tag  t i  for word  w i  that is most probable given the previous tag t i-1  and the current word  w i : t i  =  argmax j  P(t j |t i-1 ,w i ) t i  =  argmax j  P(t j |t i-1 )P( w i |t j ) : HMM equation for a single tag HMM Tagging T =  argmax  P(T|W) where T=t 1 ,t 2 ,…, t n By Bayes’ theorem P(T|W) = P(T)P(W|T)/P(W) Thus we are attempting to choose the sequence of tags that maximizes the right hand side of the equation P(W) can be ignored P(T) is called the prior, P(W|T) is called the likelihood. HMM Tagging Complete formula P(T)P(W|T) =  Π P(w i |w 1 t 1 …w i-1 t i-1 t i )P(t i |t 1 …t i-2 t i-1 ) Simplification 1:  P(W|T) =  Π P( w i |t i ) Simplification 2:  P(T)=  Π P(t i |t i-1 ) Bigram approximation T =  argmax  P(T|W) =  argmax   Π P( w i |t i ) P(t i |t i-1 ) Maximum Likelihood Estimates Transitions P(NN|JJ) = C(JJ,NN)/C(JJ)=22301/89401 = .249 Emissions P( this|DT ) = C( DT,this )/C(DT)=7037/103687 = .068 Example The/DT rich/JJ like/VBP to/TO travel/VB ./. Example Evaluating Taggers Data set Training set Development set Test set Tagging accuracy how many tags right Results Baseline is very high – 90% for English Trigram HMM about 95% total accuracy, 55% on unknown words Highest accuracy around 97% on PTB trained on 800,000 words (50-85% on unknown words; 50% for trigrams) Upper bound 97-98%  noise (e.g., errors and inconsistencies in the data, e.g., NN vs JJ) Notes on POS New domains Lower performance New languages Morphology matters! Also availability of training data Distributional clustering Combine statistics about semantically related words Example: names of companies Example: days of the week Example: animals Notes on POS British National Corpus http://www.natcorp.ox.ac.uk/ Tagset  sizes PTB 45, Brown 85, Universal 12, Twitter 25 Dealing with unknown words Look at features like  twoDigitNum ,  allCaps ,  initCaps ,  containsDigitAndSlash  ( Bikel   et al. 1999) Brown Clustering Words with similar vector representations are clustered together, in an agglomerative (recursive) way For example, “Monday”, “Tuesday”, etc. may form a new vector “Day of the week” Published by Brown et al.  [1992] Example Friday Monday Thursday Wednesday Tuesday Saturday Sunday weekends Sundays people guys folks fellows CEOs chaps doubters commies unfortunates blokes down backwards ashore sideways southward northward overboard aloft downwards adrift water gas coal liquid acid sand carbon steam shale iron great big vast sudden mere sheer gigantic lifelong scant colossal American Indian European Japanese German African Catholic Israeli Italian Arab mother wife father son husband brother daughter sister boss uncle machine device controller processor CPU printer spindle subsystem compiler plotter John George James Bob Robert Paul William Jim David Mike feet miles pounds degrees inches barrels tons acres meters bytes had hadn't hath would've could've should've must've might've that  tha   theat head body hands eyes voice arm seat eye hair mouth Example Input: this is one document . it has two sentences but the program only cares about spaces . here is another document . it also has two sentences . and here is a third document with one sentence . this document is short . the dog ran in the park . the cat was chased by the dog . the dog chased the cat . [code by Michael  Heilman : https://github.com/mheilman/tan-clustering] [code by Michael  Heilman ] .       	1011    	9 the     	011    	7 is      	110     	4 document	1110    	4 dog     	000     	3 it      	101001  	2 one     	11111   	2 sentences	1010111 	2 chased  	00111   	2 two     	1010100 	2 has     	1010110 	2 here    	111101  	2 this    	1000    	2 cat     	0010    	2 and     	11110010	1 sentence	11110011	1 ran     	01011		1 in      	0100		1 spaces  	10101011011	1 another 	1010001	1 cares   	101010111	1 also    	1010000	1 only    	10101011010	1 program 	10101011001	1 was     	001100		1 park    	01010		1 but     	10101011000	1 short   	1001		1 with    	111100001	1 by      	001101		1 a       	111100000	1 about   	10101010	1 third   	11110001	1 External Link Jason Eisner’s awesome interactive spreadsheet about learning HMMs http://cs.jhu.edu/~jason/papers/#eisner-2002-tnlp   http://cs.jhu.edu/~jason/papers/eisner.hmm.xls   Introduction to NLP Transformation-Based Learning Transformation Based Learning Idea: change some labels given specific input patterns [Brill 1995] Example P( NN|sleep ) = .9 P( VB|sleep ) = .1 Change NN to VB when the previous tag is TO Types of rules: The preceding (following) word is tagged z The word two before (after) is tagged z One of the two preceding (following) words is tagged z One of the three preceding (following) words is tagged z The preceding word is tagged z and the following word is tagged w Transformation Based Tagger Transformation Based Tagger Unknown Words NLP NLP Introduction to NLP Question Answering Siri Ask Jeeves (ask.com) WolframAlpha IBM’s Watson http://www.geekwire.com/2013/ibm-takes-watson-cloud/   Watson on Jeopardy Sample questions On  December 8, 2008 this national newspaper raised its newsstand pice by 25 cents to $ 1 USA  Today In 2010 this former first lady published the memoir "Spoken From the  Heart“ Laura Bush This person is appointed by a testator to carry out the directions & requests in his  will Executor Familiarity is said to breed this, from the Latin for " Despise“ Contempt As of 2010, Croatia & Macedonia are candidates but this is the only former Yugoslav republic in the  EU Slovenia The ancient "Lion of Nimrud" went missing from this city's national museum in 2003 (along with a lot of other  stuff) Baghdad It's just a bloody nose! You don't have this hereditary disorder once endemic to European  royalty Haemophilia It's Michelangelo's fresco on the wall of the Sistine Chapel,  depicting  the saved and the  damned The  Last Judgement She "Died in the church and was buried along with her name. Nobody  came“ Eleanor  Rigby A  camel is a horse designed by  this Committee Watson Videos IBM's Watson Supercomputer Destroys Humans in Jeopardy | Engadget  https://www.youtube.com/watch?v=WFR3lOm_xhE "What is IBM Watson?" 7 Videos from the Jeopardy! Era http://mentalfloss.com/article/51543/what-ibm-watson-7-videos-jeopardy-era   People Ask Questions Online Excite corpus 2,477,283 queries (one day’s worth) 8.4% of them are questions 43.9% factual (what is the country code for Belgium) 56.1% procedural (how do I get out of debt) or other The Excite Corpus In what year did baseball become an offical sport? how do i get out of debt? Where can I found out how to pass a drug test? When is the Super Bowl? who is California's District State Senator? where can I buy extra nibs for a foutain pen? what time is it in west samoa? Where can I buy a little kitty cat? what are the symptoms of attention deficit disorder? Where can I get some information on Michael Jordan? How does the character Seyavash in Ferdowsi's Shahnameh exhibit characteristics of a hero? When did the Neanderthal man live? Which Frenchman declined the Nobel Prize for Literature for ideological reasons? Murax Questions What  U.S. city is at the junction of the Allegheny and Monongahela rivers? Who wrote “Across the River and into the Trees” ? Who married actress Nancy Davis? What’s  the capital of the Netherlands? Who was the last of the Apache warrior chiefs? What chief justice headed the commission that declared: “Lee Harvey Oswald . . . acted alone.”? What famed falls are split in two by Goat Island? What is November’s birthstone? Who’s  won the most Oscars for costume design? What is the state flower of Alaska? Julian Kupiec 1993. MURAX: A Robust Linguistic Approach For Question Answering AOL Corpus Questions what does cerebral cortical atrophy mean  what fraction is closest to pi what is the highest calories consumed by a person in a 24 hor period what happens to limewater in the presence of carbon dioxide what language is numa numa in who discovered saturn who discovered the gene for huntington's disease who is the daugther of indian sitar maestro ravi Shankar who invented marshmallow sandwich cookies who invented lithograph who was the first michigan football player to put on the cover of a video game who invented smallpox vaccine when was the berlin wall torn down when was yahoo launched when was the wedding march by mendelssohn composed when was the declaration of independence signed when did france become a republic when was alexander the great born which fish contains low mercury when is the youngest a minor can move out of thier parents house in texas when should take a child to see an psychologist what hormone imbalance could mimic pregnancy symptoms Question Type Taxonomy Question Types Yes/no  vs.  wh - Factual vs. procedural Single answer vs. multiple answers Objective vs. subjective Context-specific (personalized) vs .  generic Known answer in the collection,  y/n State of the Art Question types Mostly factual, short-answer questions System architecture IR component Statistical approaches using lots of data Relatively little knowledge Older Systems (pre-TREC) BASEBALL (Green et al.,  1961) Baseball ELIZA (Weizenbaum 1966) Dialogue, Therapist SHRDLU (Winograd 1972) Blocks world LUNAR (Woods , 1973 ) Lunar rocks Murax (Kupiec 1993) Open-domain, encyclopedia-based START (Katz 1997) Open-domain, web-based Deep Red (Hirschman & al. 1999) Reading comprehension Jupiter (Zue et al., 2000) Spoken questions Eliza (Weizenbaum 1966) Eliza plays a therapist and doesn’t really answer questions   Simple pattern matching Sample dialogue E: I am the psychotherapist.  Please, describe your problems. P: I am always tired. E: Why do you say you are always tired? P: Because I cannot sleep. E: Is it because you cannot sleep that you came to me? P: Yes, help me please. E: Earlier you said you cannot sleep? Lunar (Woods et al., 1972) Lunar Sciences Natural Language Information System Apollo 11 moon rocks Questions in English Components a general-purpose ATN (augmented transition network) grammar for English a procedural-semantics framework for semantic interpretation mapping from the syntactic grammar to a logic representation 3,500 word dictionary 13,000 entries in the knowledge base Lunar answered 78% of sentences asked by geologists at 1971 Lunar Rocks conference How many breccias contain olivine? What are they? Lunar Grammar Fragment SHRDLU (Winograd 1972) Simple questions about the blocks world Person: Pick up a big red block.  Computer: OK.  Person: Grasp the pyramid.  Computer: I don't understand which pyramid you mean.  Person (changing their mind): Find a block which is taller than the one you are holding  and put it into the box.  Computer: By "it", I assume you mean the block which is taller than the one I am holding.  Computer: OK.  Person: What does the box contain?  Computer: The blue pyramid and the blue block.  Person: What is the pyramid supported by?  Computer: The box.  Person: How many blocks are not in the box?  Computer: Four of them.  Person: Is at least one of them narrower than the one which I told you to pick up?  Computer: Yes, the red cube. http://hci.stanford.edu/~winograd/shrdlu/ Start (Katz 1997) http://start.csail.mit.edu Open-domain Uses the Web Online since 1993 NLP NLP Introduction to NLP Parsing Evaluation Parsing Model GEN/EVAL framework GEN maps the input to a set of candidate parses EVAL ranks the candidate parses y *  =  argmax  EVAL (X,Y)                y    GEN(X) Evaluation Methodology (1/2) Classification tasks Document retrieval Part of speech tagging Parsing Data split Training Dev-test Test Evaluation Methodology (2/2) Baselines Dumb baseline Intelligent baseline Human performance (ceiling) New method Evaluation methods Accuracy Precision and Recall Multiple references Interjudge agreement Kappa Agreement vs. expected agreement P(A) is the level of agreement of the judges P(E) is the expected probability of agreement by chance When  k  > .7 – agreement is considered high Question Judge agreement on a binary classification task is 60%, is this high? Answer Data P(A) = .6 P(E) = .5 Kappa k = .1/.5 = .2 not high Parsing Evaluation Parseval : precision and recall get the proper constituents Labeled precision and recall also get the correct non-terminal labels F1 harmonic mean of precision and recall Crossing brackets (A (B C)) vs ((A B) C) PTB corpus training 02-21, development 22, test 23 Evaluation Example GOLD = (S (NP (DT The) (JJ Japanese) (JJ industrial) (NNS companies))            (VP (MD should) (VP (VB know) (ADVP (JJR better)))) (. .) Bracketing Recall         =  80.00 Bracketing Precision      =  66.67 Bracketing FMeasure       =  72.73 Complete match            =   0.00 No crossing               = 100.00 Tagging accuracy          =  87.50 CHAR = (S (NP (DT The) (JJ Japanese) (JJ industrial) (NNS companies))             (VP (MD should) (VP (VB know )) (( ADVP ( RBR  better)))) (. .)) NLP NLP Introduction to NLP Unlexicalized  Parsing Klein and Manning 2003 One extreme PTB categories Another extreme individual words Try something in the middle e.g., collapse table/N and house/N but don’t collapse of/PP and in/PP Markovization Markovization horizontal vertical Tag Splitting Split  some part-of-speech tags into tags marked with semantic function  labels E.g., IN-SBAR State Splitting Example: Distinguish between “and” and “but” from the other CCs State Splitting Example: Distinguish between “and” and “but” from the other CCs Done manually So hard to port Petrov  et al. (2006) Replace each nonterminal A by a set of categories A[X 1 ]…A[ X n ] X 1 ..X n  are latent variables EM-based Iterative splits and merges Split as long as accuracy improves Merge if accuracy doesn’t improve NLP NLP Introduction to NLP Part of Speech Tagging The POS task Example Bahrainis vote in second  round  of parliamentary election Jabberwocky (by Lewis Carroll, 1872) `Twas   brillig , and the  slithy   toves  Did gyre and  gimble  in the  wabe : All  mimsy  were the  borogoves , And the  mome   raths   outgrabe . Parts of speech Open class: nouns, non-modal verbs, adjectives, adverbs Closed class:  prepositions, modal verbs, conjunctions, particles, determiners, pronouns Penn Treebank  tagset  (1/2) Penn Treebank  tagset  (2/2) Universal POS http://universaldependencies.org/u/pos/ Universal Features http://universaldependencies.org/u/feat/ Some Observations Ambiguity count (noun) vs. count (verb) 11% of all types but 40% of all tokens in the Brown corpus are ambiguous. Examples like  can be tagged as ADP VERB ADJ ADV NOUN  present  can be tagged as ADJ NOUN VERB ADV Example from J&M POS Ambiguity Some Observations More examples:  transport, object, discount, address content French pronunciation:  est, pr é sident, fils Three main techniques:  rule-based machine learning (e.g., conditional random fields, maximum entropy Markov  models, neural networks) transformation-based Useful for parsing, translation, text to speech, word sense disambiguation, etc. Example Bethlehem/NNP Steel/NNP Corp./NNP ,/, hammered/VBN by/IN higher/JJR  costs/NNS Bethlehem/NNP Steel/NNP Corp./NNP ,/, hammered/VBN by/IN higher/JJR  costs/VBZ Classifier-based POS Tagging A baseline method would be to use a classifier to map each individual word into a likely POS tag Why is this method unlikely to work well? Sources of Information Bethlehem/NNP Steel/NNP Corp./NNP ,/, hammered/VBN by/IN higher/JJR  costs/NNS Bethlehem/NNP Steel/NNP Corp./NNP ,/, hammered/VBN by/IN higher/JJR  costs/VBZ Knowledge about individual words lexical information spelling (-or) capitalization (IBM) Knowledge about neighboring words Evaluation Baseline tag each word with its most likely tag tag each OOV word as a noun. around 90% Current accuracy around 97% for English compared to 98% human performance Rule-based POS tagging Use dictionary or finite-state transducers to find all possible parts of speech Use disambiguation rules  e.g., ART+V Hundreds of constraints need to be designed manually Example in French <S>         ^                     beginning of sentence  La           rf  b  nms  u            article teneur        nfs   nms                noun feminine singular moyenne       jfs   nfs  v1s v2s v3s   adjective feminine singular en           p a b                 preposition uranium      nms                    noun masculine singular         des         p r                   preposition        rivières      nfp                    noun feminine plural      ,           x                     punctuation         bien_que      cs                     subordinating conjunction        d é licate      jfs                    adjective feminine singular à            p                     preposition calculer     v                     verb Sample Rules BS3 BI1 A BS3 (3rd person subject personal pronoun) cannot be followed by a BI1 (1st person indirect personal pronoun).  In the example: “ il  nous  faut ” (= “we need”) – “ il ” has the tag BS3MS and “nous” has the tags [BD1P BI1P BJ1P BR1P BS1P].  The negative constraint “BS3 BI1” rules out “BI1P'', and thus leaves only 4 alternatives for the word “nous”. N K The tag N (noun) cannot be followed by a tag K (interrogative pronoun); an example in the test corpus would be: “...  fleuve  qui ...” (...river that...).  Since “qui” can be tagged both as an “E” (relative pronoun) and a “K” (interrogative pronoun), the “E” will be chosen by the tagger since an interrogative pronoun cannot follow a noun (“N”). R V A word tagged with R (article) cannot be followed by a word tagged with V (verb): for example “l'  appelle ” (calls him/her).  The word “ appelle ” can only be a verb, but “l'” can be either an article or a personal pronoun.   Thus, the rule will eliminate the article tag, giving preference to the pronoun. NLP NLP Introduction to NLP Preprocessing Text Preprocessing Removing non-text ads,  javascript Dealing with text encoding e.g., Unicode Sentence segmentation later slide Normalization  labeled/labelled,  extra-terrestrial/extraterrestrial, extra terrestrial Stemming computer/computation Morphological analysis car/cars Capitalization Now/NOW, led/LED Named  entity  extraction USA/ usa Text Preprocessing Types vs. Tokens To be or not to be Tokenization : ALS  vs.  A.L.S.  Paul’s, Willow Dr., Dr. Willow, New York, ad hoc, can’t “The New York-Los Angeles flight” vs.  “Minneapolis- St.Paul ” Numbers , e.g., (888) 555-1313, 1-888-555-1313 Dates , e.g., Jan-13-2012, 20120113, 13 January 2012, 01/13/12 URLs Text preprocessing Kanji ,  Katakana ,  Hiragana ,  Rōmaji , (numbers) Nyūyōku   wa ,  Amerikagasshūkoku   nyūyōku-shū   ni   aru   toshi ニューヨーク   (New York)   は 、 アメリカ 合衆国 ニューヨーク 州 にある 都 市 Word segmentation 金属製品製造の日立金属は１９日、世界最大手の鉄鋳物メーカー「ワウパカ　ファウンドリー　ホールディングス」（米国・デラウェア州）を米投資ファンドから買収し、完全子会社にすると発表した。買収額は１３億ドル（約１３３０億円）で、１０月中にも手続きを終える。 Word segmentation Arabic: Japanese:                         ( kono   hon  ha  omoi ) German :  	 Finanzdienstleistung  = financial services Chinese: 	 电视  (television)  	 电  ( diàn  = electric)  视  ( shì  = to look at) كتاب この本は重い。   Sentence Boundary Recognition Decision trees Features punctuation formatting fonts spacing capitalization case use of abbreviations, e.g., Dr., a.m. Example If there is no space after a period, don’t assume that there is a sentence boundary NLP NLP Text Similarity Text Kernels Unigram Kernels Same as a bag of words For example, we can split the sentence “Google bought  Kaggle ” into the three unigrams: “Google” “bought” “ Kaggle ” Similarly, we can split “ Kaggle  bought Google” into: “ Kaggle ” “bought” “Google” However, the vector representations of the two sentences will be identical (<1,1,1>) and therefore their cosine similarity will be computed as 1. The Basic Idea behind Kernels x x x x o o o The Basic Idea behind Kernels x x x x o o o The Basic Idea behind Kernels φ x x x x o o o φ (x) φ (x) φ (o) φ (x) φ (o) φ (o) φ (x) The Basic Idea behind Kernels φ x x x x o o o φ (x) φ (x) φ (o) φ (x) φ (o) φ (o) φ (x) Bigram Kernels Using a bigram kernel, we can split the sentence  “Google bought  Kaggle ”  into the two bigrams: “Google bought” “bought  Kaggle ” Similarly, we can split  “ Kaggle  bought Google”  into: “ Kaggle  bought” “bought Google” Using this representation, the vectors corresponding to the two sentences are <1,1,0,0> and <0,0,1,1>, respectively, and their cosine similarity is 0 . BLEU Example One  hundred artists from 16 countries will exhibit 270 pieces of  artistic work made on porcelain  in the Palace of Arts in Cairo in a an exhibition inaugurated earlier this week and that will last for two weeks.  One hundred artists from 61 states are exhibiting 270 pieces of  porcelain artwork  at the Arts Palace in Cairo, as part of a two-week exhibition that opened at the beginning of this week . 100  artists from 16 countries are exhibiting 270 pieces of work  on porcelain  in Cairo's Arts Palace as part of a 2-week exhibition, which opened at the beginning of the week.   One  hundred artists from 16 countries will display 270 pieces of art works  undertaken on porcelain  in the Palace of Arts in Cairo in an exhibition opened at the beginning of this week and which will continue for two weeks.  A  hundred artists from 16 countries will exhibit 270 pieces of artistic works  that are made on Porcelain  in the arts palace in Cairo in a an exhibition inaugurated earlier this week, which will last for two weeks.   One  hundred artists from 61 countries exhibited 270 types of art pieces  done on porcelain  in the Cultural Palace of Cairo, in an exhibition that was inaugurated at the beginning of the current week. The exhibition would last two weeks.  One  hundred artists from 16 countries will exhibit  270 works  of art  made with porcelain  in the Arts Castle in Cairo in  an exhibition  that opened earlier this week and will last for two weeks.  A  hundred artists from 16 countries are displaying 270  porcelain  art pieces in an exhibition that opened early in the week, and will continue over a span of two weeks.  A  hundred artists from 16 countries display 270 artistic  porcelain  pieces at the Palace of Arts in Cairo, in an exhibition that was opened earlier this week and continuing for two weeks.  Letter and Substring Kernels Letter n-grams can be used for various applications such as spelling correction, language recognition, and named entity recognition. For example, the word  stop  can be represented as the  set  of all of its substrings: s, t, o, p,  st , to, op,  sto , top, and stop. In this representation,  sim ( stop,stops ) >  sim ( stop,plot ), even though all three words are different. Subsequences Unlike a substring, a subsequence doesn’t need to consist of contiguous words (or letters). comp ,  cotr ,  opter ,  cpute  – all of these are  letter-based  subsequences of  computer Subsequence kernels (of words, not letters) are most useful for measuring similarity between sentences. Dependencies likes John apples green Dependencies green : modifier, child apple : head, parent green apple Dependency Structure Unionized workers are usually better paid than their non-union counterparts.        1               2        3       4          5       6     7      8         9                10         ROOT The Dependency  Tree Kernel likes John apples likes apples green likes John likes apples apples green likes John apples green likes John apples green Dependency Tree Kernel Example [ Özateş ,  Özgür , and Radev, LREC 2016] The Syntactic Tree Kernel Note that siblings are not split up (Collins and Duffy, 2002) Quiz On the next slide, you will see a set of published headlines relating the same scientific study published in 2009. The study suggests that Wolfgang Amadeus Mozart may have died of complications caused by strep throat. Take the time to read all these headlines. What interesting (class-related) observations can you make based on your reading? What kernels would be most appropriate for clustering all these headlines together? In other words, these kernels should assign high pairwise similarity scores for the headlines in the group. Did Mozart die of strep throat? what killed  mozart ? study suggests strep infection what  killed  mozart ? strep, study suggests what killed  mozart ? study suggest it might have been a strep infection wstc / wnlk  local news what killed  mozart ? a new theory emerges strep throat may have led to  mozart's  death mozart  done in by strep throat? study says  mozart  died of strep throat dutch  researchers suggest 'super-bug' as cause of  mozart's  death new theory on what killed  mozart what killed  mozart ? study suggests strep study suggests strep infection killed  mozart 'strep throat may have killed  mozart ' what killed  mozart ? study suggests strep what killed  mozart ? study hints at complications from strep infection what killed  mozart ? study suggests just strep throat study suggests  mozart  died of strep infection strep throat may have  killed  mozart did strep throat kill  mozart ? mozart   may have died from strep throat: study strep  throat theory in  mozart's  death medical study suggests  mozart  died of strep mozart  died of strep throat? infection killed  mozart ? did strep infection kill  mozart ? did a strep infection cause  mozart's  death? mozart  may have died from strep study reviews  mozart's  death infection killed  mozart  - report mozart's  killer revealed: it was not  salieri infection killed  mozart , says study did poison or strep infection kill  mozart ? mozart  may have died from strep throat, says study cause of  mozart's  death revealed mozart  died from strep what really killed  mozart ? possibly strep mozart  may have been killed by strep throat Answer to Quiz Observations All sentences have Mozart The syntax varies a lot (e.g.,  passive/active voice) They all have some word related to dying (e.g., “kill/die/disease”) Suggestions N-gram  kernel will probably not work very well here Some semantic information should be encoded in the kernel Possibly, use  word2vec NLP NLP Introduction to NLP Text Clustering Clustering Exclusive/overlapping clusters Hierarchical/flat clusters The cluster hypothesis Documents in the same cluster are relevant to the same query How do we use it in practice? Example k-means Iteratively determine which cluster a point belongs to, then adjust the cluster centroid, then repeat Needed: small number k of desired clusters hard decisions k-means 1 initialize cluster centroids to arbitrary vectors 2  while  further improvement is possible  do 3      for  each document  d   do 4        find the cluster  c  whose centroid is  closest  to  d 5        assign  d  to cluster  c 6    end for 7    for  each cluster  c   do 8        recompute  the centroid of cluster  c  based on its documents 9    end for 10  end while Example Cluster the following vectors into two groups: A = <1,6> B = <2,2> C = <4,0> D = <3,3> E = <2,5> F = <2,1> Demos http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/AppletKM.html http://cgm.cs.mcgill.ca/~godfried/student_projects/bonnef_k-means   http://www.cs.washington.edu/research/imagedatabase/demo/kmcluster   http://www.cc.gatech.edu/~dellaert/FrankDellaert/Software.html http://www-2.cs.cmu.edu/~awm/tutorials/kmeans11.pdf   http://web.archive.org/web/20110223234358/http://www.ece.neu.edu/groups/rpl/projects/kmeans/ Probability and likelihood Example: What is  J  in this case? Bayesian formulation Posterior  ∞  likelihood x prior EM Algorithms Class of iterative algorithms for maximum likelihood estimation in problems with incomplete data. Given a model of data generation and data with some missing values, EM alternately uses the current model to estimate the missing values, and then uses the missing value estimates to improve the model. Using all the available data, EM will locally maximize the likelihood of the generative parameters giving estimates for the missing values. [ Dempster  et al. 77] [McCallum & Nigam 98] EM Algorithm Initialize  probability model Repeat E-step : use the best available current classifier to classify some  datapoints M-step : modify the classifier based on the classes produced by the E-step. Until  convergence Soft clustering method EM Example  Figure from Chris Bishop Demos http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/mixture.html http ://lcn.epfl.ch/tutorial/english/gaussian/html/ http ://www.cs.cmu.edu/~alad/em/ http ://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html   http ://people.csail.mit.edu/mcollins/papers/wpeII.4.ps   Evaluation of Clustering Purity  considering the majority class in each cluster RAND index See next slide Purity Three clusters XXXOO OOOX% %%%%XX Purity:  (3+3+4)/16=62.5% Rand Index Accuracy when preserving object-object relationships RI=(TP+TN)/(TP+FP+FN+TN) In the example:  Rand Index RI =  (TP+TN)/(TP+TN+FP+FN)=(13+64)/(13+64+22+21)=0.64 Hierarchical clustering methods Single-linkage One common pair is sufficient disadvantages: long chains Complete-linkage All pairs have to match Disadvantages: too conservative Average-linkage Non-hierarchical methods Also known as flat clustering Centroid method (online) K-means Expectation maximization Hierarchical clustering Hierarchical agglomerative clustering Dendrograms http://odur.let.rug.nl/~ kleiweg/clustering/clustering.html E.g., language similarity: Clustering using dendrograms REPEAT Compute pairwise similarities Identify closest pair Merge pair into single node UNTIL only one node left Example: cluster the following sentences: 	A B C B A 	A D C  C  A D E 	C D E F C D A 	E F G F D A 	A C D A B A Q: what is the equivalent Venn diagram representation? NLP NLP Introduction to NLP Summarization Techniques 3/3 Conroy and O’Leary (2001) Using Hidden Markov Models Takes into account the local dependencies between sentences Features Position, number of terms, similarity to document terms Properties HMM  alternates between summary and non-summary  states Tuned to extract lead sentences and additional supporting sentences Osborne (2002) Don’t assume feature independence Use  maxent  (log-linear) models Better than Naïve Bayes Features Sentence length Sentence position Inside introduction Inside conclusion Lexrank  (Erkan and Radev 2004) Single and multi-document summarization Lexical Centrality Represent text as graph Graph centrality Graph clustering Random walks Lexrank 1 (d1s1) Iraqi Vice President  Taha  Yassin Ramadan announced today, Sunday, that Iraq refuses to back down from its decision to stop cooperating with disarmament inspectors before its demands are met. 2 (d2s1) Iraqi Vice president  Taha  Yassin Ramadan announced today, Thursday, that Iraq rejects cooperating with the United Nations except on the issue of lifting the blockade imposed upon it since the year 1990. 3 (d2s2) Ramadan told reporters in Baghdad that "Iraq cannot deal positively with whoever represents the Security Council unless there was a clear stance on the issue of lifting the blockade off of it. 4 (d2s3) Baghdad had decided late last October to completely cease cooperating with the inspectors of the United Nations Special Commission (UNSCOM), in charge of disarming Iraq's weapons, and whose work became very limited since the fifth of August, and announced it will not resume its cooperation with the Commission even if it were subjected to a military operation. 5 (d3s1) The Russian Foreign Minister, Igor Ivanov, warned today, Wednesday against using force against Iraq, which will destroy, according to him, seven years of difficult diplomatic work and will complicate the regional situation in the area. 6 (d3s2) Ivanov contended that carrying out air strikes against Iraq, who refuses to cooperate with the United Nations inspectors, ``will end the tremendous work achieved by the international group during the past seven years and will complicate the situation in the region.'' 7 (d3s3) Nevertheless, Ivanov stressed that Baghdad must resume working with the Special Commission in charge of disarming the Iraqi weapons of mass destruction (UNSCOM). 8 (d4s1) The Special Representative of the United Nations Secretary-General in Baghdad, Prakash Shah, announced today, Wednesday, after meeting with the Iraqi Deputy Prime Minister Tariq Aziz, that Iraq refuses to back down from its decision to cut off cooperation with the disarmament inspectors. 9 (d5s1) British Prime Minister Tony Blair said today, Sunday, that the crisis between the international community and Iraq ``did not end'' and that Britain is still ``ready, prepared, and able to strike Iraq.'' 10 (d5s2) In a gathering with the press held at the Prime Minister's office, Blair contended that the crisis with Iraq ``will not end until Iraq has absolutely and unconditionally respected its commitments'' towards the United Nations. 11 (d5s3) A spokesman for Tony Blair had indicated that the British Prime Minister gave permission to British Air Force Tornado planes stationed in Kuwait to join the aerial bombardment against Iraq. Lexrank Lexrank Cosine centrality (t=0.3) Cosine centrality (t=0.2) Cosine centrality (t=0.1) Sentences vote for the most central sentence! Cosine centrality (t=0.1) Sentences vote for the most central sentence! Lexrank  (Advanced Material) Square connectivity matrix  Directed vs. undirected An eigenvalue for a square matrix  A  is a scalar    such that there exists a vector  x  0 such that  Ax  =   x The normalized eigenvector associated with the largest   is called the principal eigenvector of  A A matrix is called a stochastic matrix when the sum of entries in each row sum to 1 and none is negative. All stochastic matrices have a principal eigenvector Lexrank  (Advanced Material) The connectivity matrix used in PageRank [Page & al. 1998] is irreducible [ Langville  & Meyer 2003] An iterative method (power method) can be used to compute the principal eigenvector That eigenvector corresponds to the stationary value of the Markov stochastic process described by the connectivity matrix This is also equivalent to performing a random walk on the matrix Lexrank  (Advanced Material) The stationary value of the Markov stochastic matrix can be computed using an iterative power method: PageRank  adds an extra twist to deal with dead-end pages. With a probability 1-  , a random starting point is chosen. This has a natural interpretation in the case of Web page  ranking Eigenvector centrality: the paths in the random walk are weighted by the centrality of the nodes that the path connects su  = successor nodes pr  = predecessor nodes Gong and Liu (2001) Using Latent Semantic Analysis (LSA) Single and multi-document Not using WordNet Each document is represented as a word by sentence matrix   (row=word, column=sentence) TF*IDF weights in the matrix SVD: A = USV T The rows of V T  are independent topics Select sentences that cover these independent topics Submodular Lin et al. 2009 Haghighi  and  Vanderwende Topic modeling McDonald 2007 Globally optimal search Gillick  & Favre 2008 Integer Linear Programming Gous  – information geometry Survey Generation NLP NLP Introduction to NLP Sentence Simplification Sentence Simplification Removing some parts of sentences Quotes Appositions Adjectives and adverbs Embedded clauses Attribution clauses Applications Subtitling Headline generation Mobile devices A pplications for the visually impaired Knight and  Marcu  2002 Use structured (syntactic) information Two approaches Noisy channel Decision based Knight and  Marcu  2002 The documentation is typical of Epson quality; excellent. Documentation is excellent. All of our design goals were achieved and the delivered performance matches the speed of the underlying device.   All design goals were achieved. Although the modules themselves may be physically and/or electronically incompatible, the cable-specific jacks on them provide industry-standard connections.   Cable-specific jacks provide industry-standard connections. Beyond the basic level, the operations of the three products vary widely. The operations of the three products vary widely. Arborscan  is reliable and worked accurately in testing, but it produces very large  dxf  files.  Arborscan  produces very large  dxf  files.   Many debugging features, including user-defined break points and variable-watching and message-watching windows, have been added. Many debugging features have been added.  English Wikipedia Anthony Charles Lynton Blair  (born 6 May 1953) [1]  is a former  British   Labour  Party  politician who served as the  Prime Minister  of the  United Kingdom  from 2 May 1997 to 27 June 2007. He was the  Member of Parliament  (MP) for  Sedgefield  from 1983 to 2007 and  Leader of the  Labour  Party  from 1994 to 2007. He resigned from all of these positions in June 2007. Tony Blair was elected Leader of the  Labour  Party  in the  leadership election of July 1994 , following the sudden death of his predecessor,  John Smith . Under his leadership, the party adopted the term " New  Labour " [2]  and moved away from its traditional  left wing  position towards the  centre  ground . [3][4]  Blair subsequently led  Labour  to a landslide victory in the  1997 general election . At 43 years old, he became the youngest Prime Minister since  Lord Liverpool  in 1812. In the first years of the New  Labour  government, Blair's government implemented a number of  1997 manifesto  pledges, introducing the  minimum wage ,  Human Rights Act  and  Freedom of Information Act , and carrying out regional devolution, establishing the  Scottish Parliament , the  National Assembly for Wales , and the  Northern Ireland Assembly . Blair's role as Prime Minister was particularly visible in foreign and security policy, including in  Northern Ireland , where he was involved in the 1998  Good Friday Agreement . From the start of the  War on Terror  in 2001, Blair strongly supported the foreign policy of US President  George W. Bush , notably by participating in the  2001 invasion of Afghanistan  and  2003 invasion of Iraq . Blair is the  Labour  Party's longest-serving Prime Minister, the only person to have led the  Labour  Party to three consecutive general election victories, and the only  Labour  Prime Minister to serve consecutive terms more than one of which was at least four years long. He was succeeded as Leader of the  Labour  Party on 24 June 2007 and as Prime Minister on 27 June 2007 by  Gordon Brown . [5]  On the day he resigned as Prime Minister, he was appointed the official Envoy of the  Quartet on the Middle East . In May 2008, Blair launched his  Tony Blair Faith Foundation . [6]  This was followed in July 2009 by the launching of the  Faith and  Globalisation  Initiative  with Yale University in the USA,  Durham University  in the UK and the  National University of Singapore  in Asia to deliver a postgraduate  programme  in partnership with the Foundation. [7][8] Simple English Wikipedia Anthony Charles  Lyton  Blair , usually called  Tony Blair , is a former  Prime Minister  of the  United Kingdom . He was born in  Edinburgh . He was Prime Minister from May 1997 until June 2007 and was succeeded by the former  Chancellor of the Exchequer ,  Gordon Brown . As leader of the  Labour  party, he won three general  elections  in the UK, in  1997 ,  2001  and  2005 . He is married to  Cherie Booth . They met on the top deck of a  double-decker bus  in 1975. Cherie claimed "It was a double-decker and we went upstairs. It was completely empty and by the time we got off we knew each other better than when we'd got on. And even better the next morning. He was a very good-looking young man, tall and slim, yet broad in the shoulders. A really strong body." [1] . Cherie Blair is a  lawyer , who graduated from the  London School of Economics  with a first class  honours  degree. Blair himself left  Oxford University  with a second class degree. They have four children,  Euan , Nicky, Kathryn, and Leo. There was a controversy over Blair sending his eldest son  Euan  to a  grant-maintained  school. As a result of this,  Alastair Campbell  discovered Blair "standing stark naked reading the  Daily Mail " [2] He attributes his success in politics to a pair of  lucky brogues  which he wore for every single  Prime Ministers Questions  of his leadership. He claimed that "cheap shoes are a false economy". [3] NLP NLP Introduction to NLP Evaluation of Dependency Parsing Evaluation of Dependency Parsing Attachment Score (Buchholz &  Marsi  2006)  # correct  deps /#  deps                (attached to the right head) Unlabeled dependency accuracy (UAS) Labeled dependency accuracy (LAS) 1       Unionized        Unionized        VBN      VBN      -       2       NMOD    -       - 2       workers          workers          NNS      NNS      -       3       SBJ     -       - 3       are              are              VBP      VBP      -       0       ROOT    -       - 4       usually          usually          RB       RB       -       3       TMP     -       - 5       better           better           RBR      RBR      -       4       ADV     -       - 6       paid             paid             VBN      VBN      -       5       AMOD    -       - 7       than             than             IN      IN      -       5       AMOD    -       - 8       their            their            PRP$    PRP$    -       10      NMOD    -       - 9       non-union        non-union        JJ       JJ       -       10      NMOD    -       - 10      counterparts     counterparts     NNS      NNS      -       7       PMOD    -       - Complexity Projective (CKY)  O ( n 5 ) Projective (Eisner)  O ( n 3 ) Non-projective (MST - Chu-Liu-Edmonds)  O ( n 2 ) Projective (Malt)  O(n) Use in Information Extraction [Erkan et al. 2007] Dependency Kernels [Bunescu and Mooney 2005] External Links http://ilk.uvt.nl/conll/ CONLL-X Shared task http://ufal.mff.cuni.cz/pdt2.0/ Prague Dependency Treebank http://nextens.uvt.nl/depparse-wiki/SharedTaskWebsite http://nextens.uvt.nl/depparse-wiki/DataOverview    http://maltparser.org/ Joakim Nivre’s Maltparser http://www.cs.ualberta.ca/~lindek/minipar.htm   Dekang Lin’s Minipar http://www.link.cs.cmu.edu/link/   Daniel Sleator and Davy Temperley’s Link parser  Notes The original versions of  MSTParser  and  MaltParser  from 2007 achieve about 81% accuracy Highest in Japanese (91-92%) Lowest in Arabic and Turkish (63-67%) Non-projective parsing is harder than projective parsing NLP NLP Introduction to NLP Knowledge Representation Knowledge Representation Ontologies Categories and objects Events Times Beliefs Knowledge Representation Object Martin the cat Categories Cat Ontology Mammal includes Cat, Dog, Whale Cat includes PersianCat, ManxCat ISA relation ISA (Martin,Cat) AKO relation AKO (PersianCat,Cat) HASA relation HASA (Cat, Tail) FOL sentences can be assigned a value of  true  or  false. ISA(Milo,Cat) = true Milo is younger than Martin <(AgeOf(Milo),AgeOf(Martin)) = true =(AgeOf(Milo),AgeOf(Martin)) = false Semantics of FOL Examples with Quantifiers All cats eat fish x:ISA(x,Cat)EatFish(x) Representing Events Martin ate Martin ate in the morning Martin ate fish Martin ate fish in the morning One Possible Representation FOL representations Eating1(Martin) Eating2(Martin,Morning) Eating3(Martin,Fish) Eating4(Martin,Fish,Morning) Meaning postulates Eating4(x,y,z) -> Eating3(x,y) Eating4(x,y,z) -> Eating2(x,z) Eating4(x,y,z) -> Eating1(x) Example from Jurafsky and Martin Second Possible Representation Eating4( x,y,z ) With some arguments unspecified Problems Too many commitments Hard to combine Eating4( Martin,Fish,z ) with Eating4( Martin,y,Morning ) Example from Jurafsky and Martin Third Possible Representation Reification ∃ e: ISA(e,Eating) ∧ Eater(e,Martin) ∧ Eaten(e,Fish)  Example from Jurafsky and Martin Representing Time Example Martin went from the kitchen to the yard ISA(e,Going) ^ Goer(e,Martin) ^ Origin (e,kitchen) ^ Target (e,yard) Issue no tense information: past? present? future? Fluents A predicate that is true at a given time: T(f,t) Representing Time Example from Russell and  Norvig Representing Time Example from Jurafsky and Martin Representing time ∃   i,e,w,t : Isa( w,Arriving )  ∧  Arriver( w,Speaker )  ∧  Destination( w,NewYork )  ∧   IntervalOf ( w,i )  ∧   EndPoint ( i,e )  ∧  Precedes ( e,Now ) ∃   i,e,w,t : Isa( w,Arriving )  ∧  Arriver( w,Speaker )  ∧  Destination( w,NewYork )  ∧   IntervalOf ( w,i )  ∧   MemberOf ( i,Now ) ∃   i,e,w,t : Isa( w,Arriving )  ∧  Arriver( w,Speaker )  ∧  Destination( w,NewYork )  ∧   IntervalOf ( w,i )  ∧   StartPoint ( i,s )  ∧  Precedes ( Now,s ) Example from Jurafsky and Martin Representing time We fly from San Francisco to Boston at 10. Flight 1390 will be at the gate an hour now. Use of tenses Flight 1902 arrived late. Flight 1902 had arrived late. “similar” tenses When Mary’s flight departed, I ate lunch When Mary’s flight departed, I had eaten lunch reference point Example from Jurafsky and Martin Aspect Stative I  know my departure gate Activity John  is flying (no  particular end  point) Accomplishment  Sally  booked her flight (natural  end point and result in a particular  state) Achievement  She  found her gate Figuring out  statives : I  am needing the cheapest  fare. I  am wanting to go  today. Need  the cheapest fare! Example from Jurafsky and Martin Representing Beliefs Example Milo believes that Martin ate fish One possible representation ∃ e,b: ISA(e,Eating) ∧ Eater(e,Martin) ∧ Eaten(e,Fish) ∧ ISA(b,Believing) ∧ Believer(b,Milo) ∧ Believed(b,e) However this implies (by dropping some of the terms) that “Martin ate fish” (without the Belief event) Modal logic Possibility, Temporal Logic, Belief Logic Representing Beliefs Want, believe, imagine, know - all introduce hypothetical worlds I believe that Mary ate British food. Reified example: ∃   u,v : Isa( u,Believing )  ∧  Isa( v,Eating )  ∧  Believer ( u,Speaker )  ∧   BelievedProp ( u,v )  ∧  Eater( v,Mary )  ∧  Eaten( v,BritishFood ) However this implies also : ∃   u,v :  Isa( v,Eating )  ∧  Eater( v,Mary )  ∧  Eaten( v,BritishFood ) Modal operators: Believing( Speaker,Eating ( Mary,BritishFood ))    -  not FOPC! – predicates in FOPC hold between objects, not between relations. Believes(Speaker,  ∃  v:  ISA( v,Eating )  ∧  Eater( v,Mary )  ∧  Eaten( v,BritishFood )) Modal operators Beliefs Knowledge Assertions Issues:  If you are interested in baseball, the Red Sox are playing tonight. NLP NLP Introduction to NLP Morphology and the Lexicon Mental Lexicon What is the meaning of cat? Its pronunciation?  Part of speech? What is the meaning of  wug ? What is the meaning of  cluvious ? Compare  traftful  and  traftless . Morphology of these words Intuition and productivity  “Runs” Two interpretations Allomorphs cats/oxen, played/swung Affixes Derivational Morphology Example “ er ” (multiple interpretations) What do these morphemes mean? prefix, stem, suffix, ending ness, able,  ing , re, un,  er  ( adj ) JJ   V + “-able” Recursion:  unconcernednesses Ambiguity uncloggable  vs. unbelievable Answer to the Quiz Uncloggable unable to be clogged able to be unclogged Unbelievable unable to be believed ? able to be  unbelieved Morphological Examples Reduplication amigo = friend,  amimígo  = friends (in  Pangasinan ) [ Rubino  2001] savali  = he travels,  savavali  = they travel (in Samoan) Templatic  morphology (e.g., Semitic languages):  lmd  (learn),  lamad  (he studied), limed (he taught),  lumad  (he was taught) Circumfixes spielen  –  gespielt  (in German) Pig Latin appyhay Verlan “ céfran ”, “ ripou ” (from “ l’envers ”, “ Français ”, “ pourri ”) Massa- freakin ’ - chusetts where can you insert “ freakin ’” in “education”? Answer to the Quiz The “ freakin ’” infix is inserted … to the left of the syllable that bears the main stress edu- freakin ’ - cation   *  educa- freakin ’ - tion   * e- freakin ’ - ducation though there can be exceptions More Examples Clitics l’enfant , cat’s cradle Portmanteau words motel, brunch, spork Synthetic vs. isolating languages Isolating languages (typically with fixed word order): English, Chinese, Bulgarian, Thai Synthetic languages (high morpheme-per-word ratio): Inuktitut, Ainu, Basque, Lakota Fusional vs. agglutinative languages Agglutinative: Turkish, Hungarian, Swahili Fusional: Lithuanian, Hebrew, Latin Inflectional Morphology Many forms Tense, number, person, mood, aspect Five verb forms in English 40+ forms in French Six cases in Russian: http://www.departments.bucknell.edu/russian/language/case.html 	 Up to 40,000 forms in Turkish E.g., you cause X to cause Y to … do Z) Morphological Analysis sleeps = sleep + V + 3P  + SG  done = do + V + PP Turkish Vowel Harmony Back vowels in the room     oda da at the door     kapı da Front vowels at home     ev de at the lake     göl de on the bridge    köprü de NACLO Problem Turkish www.nacloweb.org/resources/problems/2010/F.pdf   by  Bozhidar   Bozhanov     NACLO Solution Turkish www.nacloweb.org/resources/problems/2010/FS.pdf Slide from Kemal  Oflazer Agglutinative Languages アメフト 			 amefuto 			 Ame ( rican ) Foot(ball) アイスクリーム 		 aisu   kurīmu 		ice cream アイドル 			 aidoru 			idol アパート 		 apāto 			apartment バイク 			 baiku 			bike バリアフリー 		 bariafurī 		barrier free コンピューター 		 konpyūtā 		computer デスク 			 desuku 			desk (at a news agency) ラマ 			 dorama 			drama (on TV) エレベーター 		 erebētā 			elevator エスカレーター 		 esukarētā 		escalator フライドポテト 		 furaidopoteto 		fried potato (French fries) グラス 			 gurasu 			glass (for drinking) ハッピーエンド 		 happīendo 		happy end( ing ) ホットケーキ 		 hottokēki 		hotcake (pancake) カシューナッツ 		 kashū   nattsu 		cashew nut コーヒー 		 kōhī 			coffee クラブ 			 kurabu 			club キーボード 		 kībōdo 			keyboard キャンペーン 		 kyanpēn 			campaign キャップ 			 kyappu 			cap パソコン 			 pāsokon 			 perso ( nal ) com( puter ) パーソナルコンピューター pāsonaru   konpyūtā 	personal computer レジュメ 			 rejume 			resume レストラン 		 resutoran 		restaurant リモコン 			 rimokon 			 remo ( te ) con( trol ) サラダ 			 sarada 			salad タバコ 			 tabako 			tobacco テレビゲーム 		 terebigēmu 		television game ゼミナール 		 zemināru 		seminar Introduction to NLP Other Levels of Linguistic Analysis Semantics Semantics Lexical semantics and compositional semantics Lexical Semantics Hypernyms , hyponyms, antonyms,  meronyms  and  holonyms  (part-whole relationship, tire is a  meronym  of car), synonyms, homonyms Senses of words,  polysemous  words Collocations white hair ,  white wine Idioms to kick the bucket Compositional Semantics How to understand the meaning of a sentence based on the meaning of its components. Pragmatics The study of how knowledge about the world and language conventions interact with literal meaning. Speech acts Resolution of anaphoric relations Modeling of speech acts in dialogue Other Areas of Linguistics Sociolinguistics interactions of social organization and language. Historical linguistics change over time. Linguistic typology Language acquisition L1 and L2 Psycholinguistics real-time production and perception of language NLP 