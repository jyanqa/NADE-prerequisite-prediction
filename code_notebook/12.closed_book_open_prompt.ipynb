{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDtLQaf-CxFL",
        "outputId": "dd3b5138-3241-4e09-f894-19d8e0506e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openprompt in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.35.2)\n",
            "Requirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.10/dist-packages (from openprompt) (0.1.96)\n",
            "Requirement already satisfied: tqdm>=4.62.2 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.66.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (from openprompt) (2.6.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from openprompt) (3.8.1)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.10/dist-packages (from openprompt) (0.1.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from openprompt) (0.3.8)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from openprompt) (2.17.0)\n",
            "Requirement already satisfied: rouge==1.0.0 in /usr/local/lib/python3.10/dist-packages (from openprompt) (1.0.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openprompt) (15.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from openprompt) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.4.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.9.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (1.3.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->openprompt) (3.20.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.10.0->openprompt) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2023.4)\n",
            "Tue Feb 13 13:32:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install datasets --quiet\n",
        "!pip install torch --quiet\n",
        "#\n",
        "!pip install openprompt\n",
        "!nvidia-smi\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QnK2l0wGCy4b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from openprompt.data_utils import InputExample\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "from openprompt.prompts import MixedTemplate\n",
        "from openprompt import PromptDataLoader\n",
        "from openprompt import PromptForClassification\n",
        "from openprompt.prompts import ManualVerbalizer\n",
        "import torch\n",
        "from pprint import pprint\n",
        "from openprompt.plms import load_plm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ2Q8XzwSr8_",
        "outputId": "2d2a4c53-0040-4a7f-c1da-bbcdd49073f0"
      },
      "outputs": [],
      "source": [
        "#check the avaialbility of cuda\n",
        "import torch\n",
        "torch.cuda.is_available()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK8QrSfkPma2"
      },
      "source": [
        "# 1. Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "2KqBSPLJC2aO",
        "outputId": "fe096226-b756-47f4-e143-a85c67bc1426"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !ls '/content/drive/MyDrive/thesis/'\n",
        "\n",
        "import json, csv\n",
        "from datasets import Dataset, DatasetDict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrI2QHG6MLGW"
      },
      "source": [
        "### Pairs to dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vZM6Uy2spJfm"
      },
      "outputs": [],
      "source": [
        "# df to dict\n",
        "def df_to_dict(df):\n",
        "    data_dicts = []\n",
        "    for idx, row in df.iterrows():  # Use iterrows for DataFrame iteration\n",
        "        concept1, concept2, label = row['concept1'], row['concept2'], row['label'] # Access columns correctly\n",
        "        data_dicts.append({\n",
        "            'concept1': concept1,\n",
        "            'concept2': concept2,\n",
        "            'idx': idx,\n",
        "            'label': label\n",
        "        })\n",
        "    return data_dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the data from the \"data-base\" folder\n",
        "dev = pd.read_csv('./data/data-base/data_dev.csv')\n",
        "test = pd.read_csv('./data/data-base/data_test.csv')\n",
        "train = pd.read_csv('./data/data-base/data_train.csv')\n",
        "\n",
        "dev_dict = df_to_dict(dev)\n",
        "test_dict = df_to_dict(test)\n",
        "train_dict = df_to_dict(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiCSKe-wIFbW"
      },
      "source": [
        "# 2. Training **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WpuhWmY3EkZa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from openprompt.data_utils import InputExample\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "from openprompt.prompts import MixedTemplate\n",
        "from openprompt import PromptDataLoader\n",
        "from openprompt import PromptForClassification\n",
        "from openprompt.prompts import ManualVerbalizer\n",
        "import torch\n",
        "from pprint import pprint\n",
        "from openprompt.plms import load_plm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d13ZQSjaDDva"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2118\n",
            "202\n",
            "402\n"
          ]
        }
      ],
      "source": [
        "data_dicts_split = {\n",
        "    'train': train_dict,\n",
        "    'validation': dev_dict,\n",
        "    'test': test_dict\n",
        "}\n",
        "\n",
        "print(len(data_dicts_split['train']))\n",
        "print(len(data_dicts_split['validation']))\n",
        "print(len(data_dicts_split['test']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqCj2bdlP35e",
        "outputId": "0be50c2d-4652-441b-eb29-d4c4569862ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"guid\": 0,\n",
            "  \"label\": 0,\n",
            "  \"meta\": {},\n",
            "  \"text_a\": \"Concurrency (computer science)\",\n",
            "  \"text_b\": \"Integral\",\n",
            "  \"tgt_text\": null\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# data_dicts_split to dataset (read-to-use one)\n",
        "dataset = {}\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    dataset[split] = []\n",
        "    for data in data_dicts_split[split]:\n",
        "        input_example = InputExample(text_a = data['concept1'], text_b = data['concept2'], label=int(data['label']), guid=int(data['idx']))\n",
        "        dataset[split].append(input_example)\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niiXnrfwSSbY"
      },
      "source": [
        "## Defining promtp model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3PXZFylHQ5l4"
      },
      "outputs": [],
      "source": [
        "# #construct a prompt model\n",
        "# from openprompt import PromptForClassification\n",
        "# promptModel = PromptForClassification(\n",
        "#     template = promptTemplate,\n",
        "#     plm = plm,\n",
        "#     verbalizer = promptVerbalizer,\n",
        "# )\n",
        "\n",
        "# #load dataset to PromptDataloader\n",
        "# from openprompt import PromptDataLoader\n",
        "# data_loader = PromptDataLoader(\n",
        "#     dataset = dataset,\n",
        "#     tokenizer = tokenizer,\n",
        "#     template = promptTemplate,\n",
        "#     tokenizer_wrapper_class=WrapperClass,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "X4v4ZbLLL-SQ",
        "outputId": "574f395d-bc89-4617-c051-8927cce21761"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6268b2b5157b4208af34079069639d09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b486137cdf946d1b52ab23d0c9b48b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae3a131c348045819e7e3532bfe1518e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3bbad739ce84c439bf73368936f4f63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:217: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'loss_ids': 0, 'shortenable_ids': 0, 'soft_token_ids': 1, 'text': '▁I'},\n",
            "  {'loss_ids': 0, 'shortenable_ids': 0, 'soft_token_ids': 2, 'text': 's'},\n",
            "  {'loss_ids': 0,\n",
            "   'shortenable_ids': 1,\n",
            "   'soft_token_ids': 0,\n",
            "   'text': ' Concurrency (computer science)'},\n",
            "  {'loss_ids': 0, 'shortenable_ids': 0, 'soft_token_ids': 3, 'text': '▁'},\n",
            "  {'loss_ids': 0, 'shortenable_ids': 0, 'soft_token_ids': 4, 'text': 'a'},\n",
            "  {'loss_ids': 0,\n",
            "   'shortenable_ids': 0,\n",
            "   'soft_token_ids': 5,\n",
            "   'text': '▁prerequisite'},\n",
            "  {'loss_ids': 0, 'shortenable_ids': 0, 'soft_token_ids': 6, 'text': '▁of'},\n",
            "  {'loss_ids': 0,\n",
            "   'shortenable_ids': 1,\n",
            "   'soft_token_ids': 0,\n",
            "   'text': 'Integral'},\n",
            "  {'loss_ids': 0, 'shortenable_ids': 0, 'soft_token_ids': 0, 'text': '?'},\n",
            "  {'loss_ids': 1, 'shortenable_ids': 0, 'soft_token_ids': 0, 'text': '<mask>'},\n",
            "  {'loss_ids': 0, 'shortenable_ids': 0, 'soft_token_ids': 0, 'text': '.'}],\n",
            " {'guid': 0, 'label': 0}]\n",
            "{\n",
            "  \"guid\": 0,\n",
            "  \"label\": 0,\n",
            "  \"meta\": {},\n",
            "  \"text_a\": \"Concurrency (computer science)\",\n",
            "  \"text_b\": \"Integral\",\n",
            "  \"tgt_text\": null\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load plm and tokenizer, and initialize template\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")\n",
        "\n",
        "mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer, text='{\"soft\":\"Is\"} {\"placeholder\":\"text_a\"} {\"soft\":\"a prerequisite of\"}{\"placeholder\":\"text_b\"}?{\"mask\"}.')\n",
        "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
        "pprint(wrapped_example)\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zLH5jDJ8MnOM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizing: 2118it [00:09, 228.95it/s]\n",
            "tokenizing: 202it [00:00, 253.85it/s]\n"
          ]
        }
      ],
      "source": [
        "#todo: read params documentation\n",
        "wrapped_t5tokenizer=WrapperClass(max_seq_length=128,decoder_max_length=3,tokenizer=tokenizer,truncate_method='head')\n",
        "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
        "    batch_size=8,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
        "    batch_size=8,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "F76sv32DMqic"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[ 4273],\n",
            "         [ 2024],\n",
            "         [ 1176],\n",
            "         [  269],\n",
            "         [ 2024],\n",
            "         [ 1176],\n",
            "         [  269]],\n",
            "\n",
            "        [[  150],\n",
            "         [12153],\n",
            "         [ 6136],\n",
            "         [ 1786],\n",
            "         [12153],\n",
            "         [ 6136],\n",
            "         [ 1786]]])\n",
            "Vocab length: 32100\n",
            "For each kind of possibility:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-3.3017, -2.6786],\n",
              "        [-3.0837, -2.7418]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#define verbalizer : mapping from logits on the vocabulary to the final label probability\n",
        "classes = [\n",
        "     1,\n",
        "     0,\n",
        "]\n",
        "myverbalizer = ManualVerbalizer(\n",
        "    classes=classes,\n",
        "    label_words={\n",
        "        1: [\"yes\", \"correct\", \"true\", \"right\", \"correct\", \"true\", \"right\"],\n",
        "        0: [\"no\", \"incorrect\", \"false\", \"wrong\", \"incorrect\", \"false\", \"wrong\"],\n",
        "    },\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "print(myverbalizer.label_words_ids)\n",
        "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm\n",
        "print('Vocab length:',len(tokenizer))\n",
        "print('For each kind of possibility:')\n",
        "myverbalizer.process_logits(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9GARHllRRgi"
      },
      "outputs": [],
      "source": [
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
        "if use_cuda:\n",
        "    prompt_model = prompt_model.cuda()\n",
        "#---\n",
        "\n",
        "loss_func=torch.nn.CrossEntropyLoss()\n",
        "no_decay=['bias', 'LayerNorm.weight']\n",
        "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "optimizer_grouped_parameters1 = [\n",
        "    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "# Using different optimizer for prompt parameters and model parameters\n",
        "optimizer_grouped_parameters2 = [\n",
        "    {'params': [p for n,p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n",
        "]\n",
        "\n",
        "#---\n",
        "for n,p in prompt_model.named_parameters():\n",
        "    print(n)\n",
        "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-4)\n",
        "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1spx_NFeXRAB"
      },
      "outputs": [],
      "source": [
        "#----\n",
        "from tqdm import tqdm\n",
        "loss_list=[]\n",
        "for epoch in range(1):\n",
        "    tot_loss = 0\n",
        "    for step, inputs in enumerate(tqdm(train_dataloader)):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.backward()\n",
        "        tot_loss += loss.item()\n",
        "        optimizer1.step()\n",
        "        optimizer1.zero_grad()\n",
        "        optimizer2.step()\n",
        "        optimizer2.zero_grad()\n",
        "        loss_list.append(loss.item())\n",
        "    tot_loss=tot_loss/len(train_dataloader)\n",
        "    print('Training Loss: {:.6f}'.format(tot_loss))\n",
        "#---\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(dpi=200)\n",
        "plt.rcParams['font.family']='serif'\n",
        "plt.plot(range(len(loss_list)),loss_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3JHULsPRoc7"
      },
      "outputs": [],
      "source": [
        "allpreds = []\n",
        "alllabels = []\n",
        "for step, inputs in enumerate(tqdm(validation_dataloader)):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkLfj9zpRqw2"
      },
      "outputs": [],
      "source": [
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "print(acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
